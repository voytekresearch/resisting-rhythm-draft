
@article{2012,
  title = {Executive Functions: What They Are, How They Work, and Why They Evolved},
  shorttitle = {Executive Functions},
  year = {2012},
  month = dec,
  volume = {50},
  pages = {50-2366-50-2366},
  issn = {0009-4978, 1523-8253},
  doi = {10.5860/CHOICE.50-2366},
  file = {/Users/qualia/Documents/Papers/2012 - Executive functions what they are, how they work,.pdf},
  journal = {Choice Reviews Online},
  language = {en},
  number = {04}
}

@book{Aaron2013,
  title = {Combinatorial {{Game Theory}}},
  author = {Aaron, Siegel},
  year = {2013},
  publisher = {{American Mathematical Society}},
  series = {Graduate {{Studies}} in {{Mathematics}} ({{Book}} 146)}
}

@inproceedings{Abadi2016,
  title = {Deep {{Learning}} with {{Differential Privacy}}},
  booktitle = {Proceedings of the 2016 {{ACM SIGSAC Conference}} on {{Computer}} and {{Communications Security}} - {{CCS}}'16},
  author = {Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H. Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  year = {2016},
  pages = {308--318},
  publisher = {{ACM Press}},
  address = {{Vienna, Austria}},
  doi = {10.1145/2976749.2978318},
  abstract = {Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.},
  file = {/Users/qualia/Documents/Papers/Abadi et al. - 2016 - Deep Learning with Differential Privacy.pdf},
  isbn = {978-1-4503-4139-4},
  language = {en}
}

@article{Abbott1993,
  title = {Analysis of {{Neuron Models}} with {{Dynamically Regulated Conductances}}},
  author = {Abbott, L. F. and LeMasson, Gwendal},
  year = {1993},
  month = nov,
  volume = {5},
  pages = {823--842},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1993.5.6.823},
  file = {/Users/qualia/Documents/Papers/1993 - Abbott, LeMasson - Analysis of Neuron Models with Dynamically Regulated Conductances.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@incollection{Abbott2005,
  title = {Drivers and Modulators from Push-Pull and Balanced Synaptic Input},
  booktitle = {Progress in {{Brain Research}}},
  author = {Abbott, L.F. and Chance, Frances S.},
  year = {2005},
  volume = {149},
  pages = {147--155},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(05)49011-1},
  abstract = {In 1998, Sherman and Guillery proposed that there are two types of inputs to cortical neurons; drivers and modulators. These two forms of input are required to explain how, for example, sensory driven responses are controlled and modified by attention and other internally generated gating signals. One might imagine that driver signals are carried by fast ionotropic receptors, whereas modulators correspond to slower metabotropic receptors. Instead, we have proposed a novel mechanism by which both driver and modulator inputs could be carried by transmission through the same types of ionotropic receptors. In this scheme, the distinction between driver and modulator inputs is functional and changeable rather than anatomical and fixed. Driver inputs are carried by excitation and inhibition acting in a push-pull manner. This means that increases in excitation are accompanied by decreases in inhibition and vice versa. Modulators correspond to excitation and inhibition that covary so that they increase or decrease together. Theoretical and experimental work has shown that such an arrangement modulates the gain of a neuron, rather than driving it to respond. Constructing drivers and modulators in this manner allows individual excitatory synaptic inputs to play either role, and indeed to switch between roles, depending on how they are linked with inhibition.},
  file = {/Users/qualia/Documents/Papers/2005 - Abbott, Chance - Drivers and modulators from push-pull and balanced synaptic input.pdf},
  isbn = {978-0-444-51679-4},
  language = {en}
}

@article{Abbott2016,
  title = {Building Functional Networks of Spiking Model Neurons},
  author = {Abbott, L F and DePasquale, Brian and Memmesheimer, Raoul-Martin},
  year = {2016},
  month = mar,
  volume = {19},
  pages = {350--355},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4241},
  file = {/Users/qualia/Documents/Papers/Abbott et al. - 2016 - Building functional networks of spiking model neur.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Abdi2009,
  title = {How to Compute Reliability Estimates and Display Confidence and Tolerance Intervals for Pattern Classifiers Using the {{Bootstrap}} and 3-Way Multidimensional Scaling ({{DISTATIS}})},
  author = {Abdi, Herv{\'e} and Dunlop, Joseph P. and Williams, Lynne J.},
  year = {2009},
  month = mar,
  volume = {45},
  pages = {89--95},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.11.008},
  abstract = {When used to analyze brain imaging data, pattern classifiers typically produce results that can be interpreted as a measure of discriminability or as a distance between some experimental categories. These results can be analyzed with techniques such as multidimensional scaling (MDS), which represent the experimental categories as points on a map. While such a map reveals the configuration of the categories, it does not provide a reliability estimate of the position of the experimental categories, and therefore cannot be used for inferential purposes. In this paper, we present a procedure that provides reliability estimates for pattern classifiers. This procedure combines bootstrap estimation (to estimate the variability of the experimental conditions) and a new 3-way extension of MDS, called DISTATIS, that can be used to integrate the distance matrices generated by the bootstrap procedure and to represent the results as MDS-like maps. Reliability estimates are expressed as (1) tolerance intervals which reflect the accuracy of the assignment of scans to experimental categories and as (2) confidence intervals which generalize standard hypothesis testing. When more than two categories are involved in the application of a pattern classifier, the use of confidence intervals for null hypothesis testing inflates Type I error. We address this problem with a Bonferonni-like correction. Our methodology is illustrated with the results of a pattern classifier described by O'Toole et al. (O'Toole, A., Jiang, F., Abdi, H., Haxby, J., 2005. Partially distributed representations of objects and faces in ventral temporal cortex. J. Cogn. Neurosci. 17, 580\textendash{}590) who re-analyzed data originally collected by Haxby et al. (Haxby, J., Gobbini, M., Furey, M., Ishai, A., Schouten, J., Pietrini, P., 2001. Distributed and overlapping representation of faces and objects in ventral temporal cortex. Science 293, 2425\textendash{}2430).},
  file = {/Users/qualia/Documents/Papers/2009 - Abdi, Dunlop, Williams - How to compute reliability estimates and display confidence and tolerance intervals for pattern classifi.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Abrams2004,
  title = {Chimera {{States}} for {{Coupled Oscillators}}},
  author = {Abrams, Daniel M. and Strogatz, Steven H.},
  year = {2004},
  month = oct,
  volume = {93},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.93.174102},
  file = {/Users/qualia/Documents/Papers/2004 - Abrams, Strogatz - Chimera states for coupled oscillators.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {17}
}

@article{Adams2012,
  title = {Neuroethology of Decision-Making},
  author = {Adams, Geoffrey K and Watson, Karli K and Pearson, John and Platt, Michael L},
  year = {2012},
  month = dec,
  volume = {22},
  pages = {982--989},
  issn = {09594388},
  doi = {10.1016/j.conb.2012.07.009},
  file = {/Users/qualia/Documents/Papers/Adams et al. - 2012 - Neuroethology of decision-making.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {6}
}

@article{Advani,
  title = {Statistical Mechanics of High-Dimensional Inference},
  author = {Advani, Madhu and Ganguli, Surya},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/Advani and Ganguli - Statistical mechanics of high-dimensional inferenc.pdf},
  language = {en}
}

@article{Aflalo,
  title = {Decoding Motor Imagery from the Posterior Parietal Cortex of a Tetraplegic Human},
  author = {Aflalo, Tyson and Kellis, Spencer and Klaes, Christian and Lee, Brian and Shi, Ying and Pejsa, Kelsie and Shanfield, Kathleen and {Hayes-Jackson}, Stephanie and Aisen, Mindy and Heck, Christi and Liu, Charles and Andersen, Richard A},
  pages = {6},
  file = {/Users/qualia/Documents/Papers/Aflalo et al. - Decoding motor imagery from the posterior parietal.pdf},
  language = {en}
}

@article{Afshar2011,
  title = {Single-{{Trial Neural Correlates}} of {{Arm Movement Preparation}}},
  author = {Afshar, Afsheen and Santhanam, Gopal and Yu, Byron M. and Ryu, Stephen I. and Sahani, Maneesh and Shenoy, Krishna V.},
  year = {2011},
  month = aug,
  volume = {71},
  pages = {555--564},
  issn = {08966273},
  doi = {10.1016/j.neuron.2011.05.047},
  abstract = {The process by which neural circuitry in the brain plans and executes movements is not well understood. Until recently, most available data were limited either to single-neuron electrophysiological recordings or to measures of aggregate field or metabolism. Neither approach reveals how individual neurons' activities are coordinated within the population, and thus inferences about how the neural circuit forms a motor plan for an upcoming movement have been indirect. Here we build on recent advances in the measurement and description of population activity to frame and test an ``initial condition hypothesis'' of arm movement preparation and initiation. This hypothesis leads to a model in which the timing of movements may be predicted on each trial using neurons' moment-by-moment firing rates and rates of change of those rates. Using simultaneous microelectrode array recordings from premotor cortex of monkeys performing delayed-reach movements, we compare such single-trial predictions to those of other theories. We show that our model can explain approximately 4-fold more arm-movement reaction-time variance than the best alternative method. Thus, the initial condition hypothesis elucidates a view of the relationship between single-trial preparatory neural population dynamics and single-trial behavior.},
  file = {/Users/qualia/Documents/Papers/2011 - Afshar et al. - Single-trial neural correlates of arm movement preparation.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Afsharpour1985,
  title = {Light Microscopic Analysis of Golgi-Impregnated Rat Subthalamic Neurons},
  author = {Afsharpour, Salman},
  year = {1985},
  month = jun,
  volume = {236},
  pages = {1--13},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.902360102},
  file = {/Users/qualia/Documents/Papers/1985 - Afsharpour - Light microscopic analysis of golgi‐impregnated rat subthalamic neurons.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Aguilera2013,
  title = {The Situated {{HKB}} Model: How Sensorimotor Spatial Coupling Can Alter Oscillatory Brain Dynamics},
  shorttitle = {The Situated {{HKB}} Model},
  author = {Aguilera, Miguel and Bedia, Manuel G. and Santos, Bruno A. and Barandiaran, Xabier E.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00117},
  abstract = {Despite the increase of both dynamic and embodied/situated approaches in cognitive science, there is still little research on how coordination dynamics under a closed sensorimotor loop might induce qualitatively different patterns of neural oscillations compared to those found in isolated systems. We take as a departure point the Haken-Kelso-Bunz (HKB) model, a generic model for dynamic coordination between two oscillatory components, which has proven useful for a vast range of applications in cognitive science and whose dynamical properties are well understood. In order to explore the properties of this model under closed sensorimotor conditions we present what we call the situated HKB model: a robotic model that performs a gradient climbing task and whose ``brain'' is modeled by the HKB equation. We solve the differential equations that define the agent-environment coupling for increasing values of the agent's sensitivity (sensor gain), finding different behavioral strategies. These results are compared with two different models: a decoupled HKB with no sensory input and a passively-coupled HKB that is also decoupled but receives a structured input generated by a situated agent. We can precisely quantify and qualitatively describe how the properties of the system, when studied in coupled conditions, radically change in a manner that cannot be deduced from the decoupled HKB models alone. We also present the notion of neurodynamic signature as the dynamic pattern that correlates with a specific behavior and we show how only a situated agent can display this signature compared to an agent that simply receives the exact same sensory input. To our knowledge, this is the first analytical solution of the HKB equation in a sensorimotor loop and qualitative and quantitative analytic comparison of spatially coupled vs. decoupled oscillatory controllers. Finally, we discuss the limitations and possible generalization of our model to contemporary neuroscience and philosophy of mind.},
  file = {/Users/qualia/Documents/Papers/Aguilera et al. - 2013 - The situated HKB model how sensorimotor spatial c.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Ahilan2019,
  title = {Learning to Use Past Evidence in a Sophisticated World Model},
  author = {Ahilan, Sanjeevan and Solomon, Rebecca B. and Breton, Yannick-Andr{\'e} and Conover, Kent and Niyogi, Ritwik K. and Shizgal, Peter and Dayan, Peter},
  editor = {Haefner, Ralf},
  year = {2019},
  month = jun,
  volume = {15},
  pages = {e1007093},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007093},
  abstract = {Humans and other animals are able to discover underlying statistical structure in their environments and exploit it to achieve efficient and effective performance. However, such structure is often difficult to learn and use because it is obscure, involving long-range temporal dependencies. Here, we analysed behavioural data from an extended experiment with rats, showing that the subjects learned the underlying statistical structure, albeit suffering at times from immediate inferential imperfections as to their current state within it. We accounted for their behaviour using a Hidden Markov Model, in which recent observations are integrated with evidence from the past. We found that over the course of training, subjects came to track their progress through the task more accurately, a change that our model largely attributed to improved integration of past evidence. This learning reflected the structure of the task, decreasing reliance on recent observations, which were potentially misleading.},
  file = {/Users/qualia/Documents/Papers/Ahilan et al. - 2019 - Learning to use past evidence in a sophisticated w.pdf},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {6}
}

@article{Ahn2008,
  title = {Comparison of {{Decision Learning Models Using}} the {{Generalization Criterion Method}}},
  author = {Ahn, Woo-Young and Busemeyer, Jerome and Wagenmakers, Eric-Jan and Stout, Julie},
  year = {2008},
  month = dec,
  volume = {32},
  pages = {1376--1402},
  issn = {0364-0213},
  doi = {10.1080/03640210802352992},
  abstract = {It is a hallmark of a good model to make accurate a priori predictions to new conditions (Busemeyer \& Wang, 2000). This study compared 8 decision learning models with respect to their generalizability. Participants performed 2 tasks (the Iowa Gambling Task and the Soochow Gambling Task), and each model made a priori predictions by estimating the parameters for each participant from 1 task and using those same parameters to predict on the other task. Three methods were used to evaluate the models at the individual level of analysis. The first method used a post hoc fit criterion, the second method used a generalization criterion for short-term predictions, and the third method again used a generalization criterion for long-term predictions. The results suggest that the models with the prospect utility function can make generalizable predictions to new conditions, and different learning models are needed for making short- versus long-term predictions on simple gambling tasks.},
  file = {/Users/qualia/Documents/Papers/2008 - Ahn et al. - Comparison of decision learning models using the generalization criterion method.pdf},
  journal = {Cognitive Science: A Multidisciplinary Journal},
  language = {en},
  number = {8}
}

@article{Ahrens1974,
  title = {Computer Methods for Sampling from Gamma, Beta, Poisson and Bionomial Distributions},
  author = {Ahrens, J. H. and Dieter, U.},
  year = {1974},
  month = sep,
  volume = {12},
  pages = {223--246},
  issn = {0010-485X, 1436-5057},
  doi = {10.1007/BF02293108},
  abstract = {Zusammenfassung Computer Methods for Sampling from Gamma, Beta, Poisson and Binomial Distributions. Accurate computer methods are evaluated which transform uniformly distributed random numbers into quantities that follow gamma, beta, Poisson, binomial and negative-binomial distributions. All algorithms are designed for variable parameters. The known convenient methods are slow when the parameters are large. Therefore new procedures are introduced which can cope efficiently with parameters of all sizes. Some algorithms require sampling from the normal distribution as an intermediate step. In the reported computer experiments the normal deviates were obtained from a recent method which is also described.},
  file = {/Users/qualia/Documents/Papers/1974 - Ahrens, Dieter - Computer methods for sampling from gamma, beta, poisson and bionomial distributions.pdf},
  journal = {Computing},
  language = {en},
  number = {3}
}

@article{Aitchison2016,
  title = {The {{Hamiltonian Brain}}: {{Efficient Probabilistic Inference}} with {{Excitatory}}-{{Inhibitory Neural Circuit Dynamics}}},
  shorttitle = {The {{Hamiltonian Brain}}},
  author = {Aitchison, Laurence and Lengyel, M{\'a}t{\'e}},
  editor = {Kording, Konrad P.},
  year = {2016},
  month = dec,
  volume = {12},
  pages = {e1005186},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005186},
  file = {/Users/qualia/Documents/Papers/Aitchison and Lengyel - 2016 - The Hamiltonian Brain Efficient Probabilistic Inf.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{Akam2014,
  title = {Oscillatory Multiplexing of Population Codes for Selective Communication in the Mammalian Brain},
  author = {Akam, Thomas and Kullmann, Dimitri M.},
  year = {2014},
  month = feb,
  volume = {15},
  pages = {111--122},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3668},
  abstract = {Mammalian brains exhibit population oscillations, the structures of which vary in time and space according to behavioural state. A proposed function of these oscillations is to control the flow of signals among anatomically connected networks. However, the nature of neural coding that may support selective communication that depends on oscillations has received relatively little attention. Here, we consider the role of multiplexing, whereby multiple information streams share a common neural substrate. We suggest that multiplexing implemented through periodic modulation of firing-rate population codes enables flexible reconfiguration of effective connectivity among brain areas.},
  file = {/Users/qualia/Documents/Papers/Akam and Kullmann - 2014 - Oscillatory multiplexing of population codes for s.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {2}
}

@article{Albers2018,
  title = {Decoupling of {{BOLD}} Amplitude and Pattern Classification of Orientation-Selective Activity in Human Visual Cortex},
  author = {Albers, Anke Marit and Meindertsma, Thomas and Toni, Ivan and {de Lange}, Floris P.},
  year = {2018},
  month = oct,
  volume = {180},
  pages = {31--40},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2017.09.046},
  file = {/Users/qualia/Documents/Papers/Albers et al. - 2018 - Decoupling of BOLD amplitude and pattern classific.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Albin1989,
  title = {The Functional Anatomy of Basal Ganglia Disorders},
  author = {Albin, Roger L. and Young, Anne B. and Penney, John B.},
  year = {1989},
  month = jan,
  volume = {12},
  pages = {366--375},
  issn = {01662236},
  doi = {10.1016/0166-2236(89)90074-X},
  file = {/Users/qualia/Documents/Papers/1983 - Albin, Penney, Young - The Functional Anatomy of Basal Ganglia Disorders.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {10}
}

@article{Alekseichuk2016,
  title = {Spatial {{Working Memory}} in {{Humans Depends}} on {{Theta}} and {{High Gamma Synchronization}} in the {{Prefrontal Cortex}}},
  author = {Alekseichuk, Ivan and Turi, Zsolt and {Amador de Lara}, Gabriel and Antal, Andrea and Paulus, Walter},
  year = {2016},
  month = jun,
  volume = {26},
  pages = {1513--1521},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.04.035},
  abstract = {Previous, albeit correlative, findings have shown that the neural mechanisms underlying working memory critically require cross-structural and cross-frequency coupling mechanisms between theta and gamma neural oscillations. However, the direct causality between cross-frequency coupling and working memory performance remains to be demonstrated. Here we externally modulated the interaction of theta and gamma rhythms in the prefrontal cortex using novel cross-frequency protocols of transcranial alternating current stimulation to affect spatial working memory performance in humans. Enhancement of working memory performance and increase of global neocortical connectivity were observed when bursts of high gamma oscillations (80\textendash{}100 Hz) coincided with the peaks of the theta waves, whereas superimposition on the trough of the theta wave and low gamma frequency protocols were ineffective. Thus, our results demonstrate the sensitivity of working memory performance and global neocortical connectivity to the phase and rhythm of the externally driven thetagamma cross-frequency synchronization.},
  file = {/Users/qualia/Documents/Papers/Alekseichuk et al. - 2016 - Spatial Working Memory in Humans Depends on Theta .pdf},
  journal = {Current Biology},
  language = {en},
  number = {12}
}

@article{Aljadeff2015,
  title = {Eigenvalues of Block Structured Asymmetric Random Matrices},
  author = {Aljadeff, Johnatan and Renfrew, David and Stern, Merav},
  year = {2015},
  month = oct,
  volume = {56},
  pages = {103502},
  issn = {0022-2488, 1089-7658},
  doi = {10.1063/1.4931476},
  abstract = {We study the spectrum of an asymmetric random matrix with block structured variances. The rows and columns of the random square matrix are divided into \$D\$ partitions with arbitrary size (linear in \$N\$). The parameters of the model are the variances of elements in each block, summarized in \$g\textbackslash{}in\textbackslash{}mathbb\{R\}\^\{D\textbackslash{}times D\}\_+\$. Using the Hermitization approach and by studying the matrix-valued Stieltjes transform we show that these matrices have a circularly symmetric spectrum, we give an explicit formula for their spectral radius and a set of implicit equations for the full density function. We discuss applications of this model to neural networks.},
  archivePrefix = {arXiv},
  eprint = {1411.2688},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Aljadeff, Renfrew, Stern - Eigenvalues of block structured asymmetric random matrices.pdf;/Users/qualia/Documents/Papers/Aljadeff et al. - 2015 - Eigenvalues of block structured asymmetric random .pdf},
  journal = {Journal of Mathematical Physics},
  keywords = {Mathematics - Probability},
  language = {en},
  number = {10}
}

@article{Aljadeff2015a,
  title = {Transition to {{Chaos}} in {{Random Networks}} with {{Cell}}-{{Type}}-{{Specific Connectivity}}},
  author = {Aljadeff, Johnatan and Stern, Merav and Sharpee, Tatyana},
  year = {2015},
  month = feb,
  volume = {114},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.114.088101},
  file = {/Users/qualia/Documents/Papers/2015 - Aljadeff, Stern, Sharpee - Transition to chaos in random networks with cell-type-specific connectivity.pdf;/Users/qualia/Documents/Papers/Aljadeff et al. - 2015 - Transition to Chaos in Random Networks with Cell-T.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {8}
}

@article{Aljadeff2016,
  title = {On the Low Dimensional Dynamics of Structured Random Networks},
  author = {Aljadeff, Johnatan and Renfrew, David and Vegu{\'e}, Marina and Sharpee, Tatyana O.},
  year = {2016},
  month = feb,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.022302},
  abstract = {Using a generalized random recurrent neural network model, and by extending our recently developed mean-field approach [J. Aljadeff, M. Stern, T. Sharpee, Phys. Rev. Lett. 114, 088101 (2015)], we study the relationship between the network connectivity structure and its low dimensional dynamics. Each connection in the network is a random number with mean 0 and variance that depends on pre- and post-synaptic neurons through a sufficiently smooth function \$g\$ of their identities. We find that these networks undergo a phase transition from a silent to a chaotic state at a critical point we derive as a function of \$g\$. Above the critical point, although unit activation levels are chaotic, their autocorrelation functions are restricted to a low dimensional subspace. This provides a direct link between the network's structure and some of its functional characteristics. We discuss example applications of the general results to neuroscience where we derive the support of the spectrum of connectivity matrices with heterogeneous and possibly correlated degree distributions, and to ecology where we study the stability of the cascade model for food web structure.},
  archivePrefix = {arXiv},
  eprint = {1509.02546},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Aljadeff et al. - On the low dimensional dynamics of structured random networks.pdf;/Users/qualia/Documents/Papers/Aljadeff et al. - 2016 - On the low dimensional dynamics of structured rand.pdf},
  journal = {Physical Review E},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {2}
}

@article{Alkemade2015,
  title = {Topographic Organization of the Human and Non-Human Primate Subthalamic Nucleus},
  author = {Alkemade, Anneke and Schnitzler, Alfons and Forstmann, Birte U.},
  year = {2015},
  month = nov,
  volume = {220},
  pages = {3075--3086},
  issn = {1863-2653, 1863-2661},
  doi = {10.1007/s00429-015-1047-2},
  file = {/Users/qualia/Documents/Papers/2015 - Alkemade, Schnitzler, Forstmann - Topographic organization of the human and non-human primate subthalamic nucleus.pdf;/Users/qualia/Documents/Papers/Alkemade et al. - 2015 - Topographic organization of the human and non-huma.pdf},
  journal = {Brain Structure and Function},
  language = {en},
  number = {6}
}

@article{Almeida2015,
  title = {Neural Circuit Basis of Visuo-Spatial Working Memory Precision: A Computational and Behavioral Study},
  shorttitle = {Neural Circuit Basis of Visuo-Spatial Working Memory Precision},
  author = {Almeida, Rita and Barbosa, Jo{\~a}o and Compte, Albert},
  year = {2015},
  month = sep,
  volume = {114},
  pages = {1806--1818},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00362.2015},
  abstract = {The amount of information that can be retained in working-memory (WM) is limited. Limitations of WM have been the subject of intense research, especially trying to specify algorithmic models for WM. Comparatively, neural circuit perspectives have barely been used to test WM limitations in behavioral experiments. Here, we used a neuronal microcircuit model for visuo-spatial WM (vsWM) to investigate memory of several items. The model assumes that there is a topographic organization of the circuit responsible for spatial memory retention. This assumption leads to specific predictions, which we tested in psychophysics experiments. According to the model, nearby locations should be recalled with a bias, as if the two memory traces merged during the delay period. Another prediction is that the previously reported loss of memory precision for increasing number of memory items (memory load) should vanish when the distances between items are controlled for. Both predictions were confirmed experimentally. Taken together, our findings provide support for a topographic neural-circuit organization of vsWM, they suggest that interference between similar memories underlies some WM limitations, and they put forward a circuit-based explanation that reconciles previous conflicting results on the dependence of WM precision with load.},
  file = {/Users/qualia/Documents/Papers/2015 - Almeida, Barbosa, Compte - Neural circuit basis of visuo-spatial working memory precision a computational and behavioral study.pdf;/Users/qualia/Documents/Papers/Almeida et al. - 2015 - Neural circuit basis of visuo-spatial working memo.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Alonso-Frech2006,
  title = {Slow Oscillatory Activity and Levodopa-Induced Dyskinesias in {{Parkinson}}'s Disease},
  author = {{Alonso-Frech}, F.},
  year = {2006},
  month = jul,
  volume = {129},
  pages = {1748--1757},
  issn = {0006-8950, 1460-2156},
  doi = {10.1093/brain/awl103},
  file = {/Users/qualia/Documents/Papers/2006 - Alonso-Frech et al. - Slow oscillatory activity and levodopa-induced dyskinesias in Parkinson's disease.pdf},
  journal = {Brain},
  language = {en},
  number = {7}
}

@techreport{Alonso2019,
  title = {Temperature Compensation in a Small Rhythmic Circuit},
  author = {Alonso, Leandro M. and Marder, Eve},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/716761},
  abstract = {Temperature affects the conductances and kinetics of the ionic channels that underlie neuronal activity. Each membrane conductance has a different characteristic temperature sensitivity, which raises the question of how neurons and neuronal circuits can operate robustly over wide temperature ranges. To address this, we employed computational models of the pyloric network of crabs and lobsters. We employed a landscape optimization scheme introduced previously (Alonso and Marder, 2019) to produce multiple different models that exhibit triphasic pyloric rhythms over a range of temperatures. We use the currentscapes introduced in (Alonso and Marder, 2019) to explore the dynamics of model currents and how they change with temperature. We found that temperature changes the relative contributions of the currents to neuronal activity so that rhythmic activity smoothly slides through changes in mechanisms. Moreover, the responses of the models to extreme perturbations\textemdash{}such as gradually decreasing a current type\textemdash{}are often qualitatively different at different temperatures.},
  file = {/Users/qualia/Documents/Papers/Alonso and Marder - 2019 - Temperature compensation in a small rhythmic circu.pdf},
  language = {en},
  type = {Preprint}
}

@article{Alvarez2004,
  title = {Simulating Cortical Network Activity States Constrained by Intracellular Recordings},
  author = {Alvarez, Fabi{\'a}n P. and Destexhe, Alain},
  year = {2004},
  month = jun,
  volume = {58-60},
  pages = {285--290},
  issn = {09252312},
  doi = {10.1016/j.neucom.2004.01.057},
  abstract = {We present a method for studying states of network activity while incorporating constraints provided by intracellular measurements. Taking into account measurements of the average membrane potential, input resistance changes and membrane potential uctuations, narrows down the possible region of parameter space (connectivity, quantal conductances) where this activity can appear in networks. Searching in those speci{\"y}c regions greatly enhances the e ciency of the network-level modeling because irrelevant parameter combinations are automatically eliminated. We illustrate this approach by modeling self-sustained stochastic states in networks of excitatory and inhibitory neurons, based on intracellular recordings in vivo.},
  file = {/Users/qualia/Documents/Papers/2004 - Alvarez, Destexhe - Simulating cortical network activity states constrained by intracellular recordings.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{Amilhon2015,
  title = {Parvalbumin {{Interneurons}} of {{Hippocampus Tune Population Activity}} at {{Theta Frequency}}},
  author = {Amilhon, B{\'e}n{\'e}dicte and Huh, Carey Y.L. and Manseau, Fr{\'e}d{\'e}ric and Ducharme, Guillaume and Nichol, Heather and Adamantidis, Antoine and Williams, Sylvain},
  year = {2015},
  month = jun,
  volume = {86},
  pages = {1277--1289},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.05.027},
  abstract = {Hippocampal theta rhythm arises from a combination of recently described intrinsic theta oscillators and inputs from multiple brain areas. Interneurons expressing the markers parvalbumin (PV) and somatostatin (SOM) are leading candidates to participate in intrinsic rhythm generation and principal cell (PC) coordination in distal CA1 and subiculum. We tested their involvement by optogenetically activating and silencing PV or SOM interneurons in an intact hippocampus preparation that preserves intrinsic connections and oscillates spontaneously at theta frequencies. Despite evidence suggesting that SOM interneurons are crucial for theta, optogenetic manipulation of these interneurons modestly influenced theta rhythm. However, SOM interneurons were able to strongly modulate temporoammonic inputs. In contrast, activation of PV interneurons powerfully controlled PC network and rhythm generation optimally at 8 Hz, while continuously silencing them disrupted theta. Our results thus demonstrate a pivotal role of PV but not SOM interneurons for PC synchronization and the emergence of intrinsic hippocampal theta.},
  file = {/Users/qualia/Documents/Papers/Amilhon et al. - 2015 - Parvalbumin Interneurons of Hippocampus Tune Popul.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Amir2019,
  title = {The Developmental Origins of Risk and Time Preferences across Diverse Societies.},
  author = {Amir, Dorsa and Jordan, Matthew R. and McAuliffe, Katherine and Valeggia, Claudia R. and Sugiyama, Lawrence S. and Bribiescas, Richard G. and Snodgrass, J. Josh and Dunham, Yarrow},
  year = {2019},
  month = sep,
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0000675},
  abstract = {Risk and time preferences have often been viewed as reflecting inherent traits such as impatience and self-control. Here, we offer an alternative perspective, arguing that they are flexible and environmentally-informed. In Study 1, we investigated risk and time preferences among children in the United States, India, and Argentina, as well as forager-horticulturalist Shuar children in Amazonian Ecuador. We find striking cross-cultural differences in behavior: children in India, the U.S., and Argentina are more risk-seeking and future-oriented, while Shuar children are more risk-averse and exhibit more heterogeneous time preferences, on average preferring more today choices. To explore one of the socioecological forces that may be shaping these preferences, in Study 2, we compared the behavior of more and less- marketintegrated Shuar children, finding that those in market-integrated regions are more futureoriented and risk-seeking. These findings indicate that cross-cultural differences in risk and time preferences can be traced into childhood and may be influenced by the local environment. More broadly, our results contribute to a growing understanding of plasticity and variation in the development of behavior.},
  file = {/Users/qualia/Documents/Papers/Amir et al. - 2019 - The developmental origins of risk and time prefere.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en}
}

@article{Amirnovin2004,
  title = {Visually {{Guided Movements Suppress Subthalamic Oscillations}} in {{Parkinson}}'s {{Disease Patients}}},
  author = {Amirnovin, R.},
  year = {2004},
  month = dec,
  volume = {24},
  pages = {11302--11306},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3242-04.2004},
  file = {/Users/qualia/Documents/Papers/2004 - Amirnovin - Visually Guided Movements Suppress Subthalamic Oscillations in Parkinson's Disease Patients.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {50}
}

@article{Anastassiou2015,
  title = {Ephaptic Coupling to Endogenous Electric Field Activity: Why Bother?},
  shorttitle = {Ephaptic Coupling to Endogenous Electric Field Activity},
  author = {Anastassiou, Costas A and Koch, Christof},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {95--103},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.09.002},
  file = {/Users/qualia/Documents/Papers/Anastassiou and Koch - 2015 - Ephaptic coupling to endogenous electric field act.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Anderson2000,
  title = {The {{Contribution}} of {{Noise}} to {{Contrast Invariance}} of {{Orientation Tuning}} in {{Cat Visual Cortex}}},
  author = {Anderson, J. S.},
  year = {2000},
  month = dec,
  volume = {290},
  pages = {1968--1972},
  issn = {00368075, 10959203},
  doi = {10.1126/science.290.5498.1968},
  file = {/Users/qualia/Documents/Papers/2000 - Anderson et al. - The contribution of noise to contrast invariance of orientation tuning in cat visual cortex.pdf},
  journal = {Science},
  language = {en},
  number = {5498}
}

@article{Andrychowicz,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  pages = {15},
  abstract = {Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at https://goo.gl/SMrQnI.},
  file = {/Users/qualia/Documents/Papers/Andrychowicz et al. - Hindsight Experience Replay.pdf},
  language = {en}
}

@article{Apesteguia2007,
  title = {Imitation\textemdash{}Theory and Experimental Evidence},
  author = {Apesteguia, Jose and Huck, Steffen and Oechssler, J{\"o}rg},
  year = {2007},
  month = sep,
  volume = {136},
  pages = {217--235},
  issn = {00220531},
  doi = {10.1016/j.jet.2006.07.006},
  abstract = {We introduce a generalized theoretical approach to study imitation models and subject the models to rigorous experimental testing. In our theoretical analysis we {\TH}nd that the different predictions of previous imitation models are due to different informational assumptions, not to different behavioral rules. It is more important whom one imitates rather than how. In a laboratory experiment we test the different theories by systematically varying information conditions. We {\TH}nd that the generalized imitation model predicts the differences between treatments well. The data also provide support for imitation on the individual level, both in terms of choice and in terms of perception. But imitation is not unconditional. Rather individuals' propensity to imitate more successful actions is increasing in payoff differences.},
  file = {/Users/qualia/Documents/Papers/2007 - Steffen Huck - Imitation - theory and experimental evidence.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {1}
}

@article{Apps2015,
  title = {Vicarious {{Reinforcement Learning Signals When Instructing Others}}},
  author = {Apps, M. A. J. and Lesage, E. and Ramnani, N.},
  year = {2015},
  month = feb,
  volume = {35},
  pages = {2904--2913},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3669-14.2015},
  file = {/Users/qualia/Documents/Papers/Apps et al. - 2015 - Vicarious Reinforcement Learning Signals When Inst.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Arakaki2017,
  title = {Capturing the Diversity of Biological Tuning Curves Using Generative Adversarial Networks},
  author = {Arakaki, Takafumi and Barello, Gregory and Ahmadian, Yashar},
  year = {2017},
  month = jul,
  doi = {10.1101/167916},
  abstract = {Tuning curves characterizing the response selectivities of biological neurons often exhibit large degrees of irregularity and diversity across neurons. Theoretical network models that feature heterogeneous cell populations or random connectivity also give rise to diverse tuning curves. However, a general framework for fitting such models to experimentally measured tuning curves is lacking. We address this problem by proposing to view mechanistic network models as generative models whose parameters can be optimized to fit the distribution of experimentally measured tuning curves. A major obstacle for fitting such models is that their likelihood function is not explicitly available or is highly intractable to compute. Recent advances in machine learning provide ways for fitting generative models without the need to evaluate the likelihood and its gradient. Generative Adversarial Networks (GAN) provide one such framework which has been successful in traditional machine learning tasks. We apply this approach in two separate experiments, showing how GANs can be used to fit commonly used mechanistic models in theoretical neuroscience to datasets of measured tuning curves. This fitting procedure avoids the computationally expensive step of inferring latent variables, e.g., the biophysical parameters of individual cells or the particular realization of the full synaptic connectivity matrix, and directly learns model parameters which characterize the statistics of connectivity or of single-cell properties. Another strength of this approach is that it fits the entire, joint distribution of experimental tuning curves, instead of matching a few summary statistics picked a priori by the user. More generally, this framework opens the door to fitting theoretically motivated dynamical network models directly to simultaneously or non-simultaneously recorded neural responses.},
  file = {/Users/qualia/Documents/Papers/Arakaki et al. - 2017 - Capturing the diversity of biological tuning curve.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Arandia-Romero2016,
  title = {Multiplicative and {{Additive Modulation}} of {{Neuronal Tuning}} with {{Population Activity Affects Encoded Information}}},
  author = {{Arandia-Romero}, I{\~n}igo and Tanabe, Seiji and Drugowitsch, Jan and Kohn, Adam and {Moreno-Bote}, Rub{\'e}n},
  year = {2016},
  month = mar,
  volume = {89},
  pages = {1305--1316},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.01.044},
  abstract = {Numerous studies have shown that neuronal responses are modulated by stimulus properties and also by the state of the local network. However, little is known about how activity fluctuations of neuronal populations modulate the sensory tuning of cells and affect their encoded information. We found that fluctuations in ongoing and stimulus-evoked population activity in primate visual cortex modulate the tuning of neurons in a multiplicative and additive manner. While distributed on a continuum, neurons with stronger multiplicative effects tended to have less additive modulation and vice versa. The information encoded by multiplicatively modulated neurons increased with greater population activity, while that of additively modulated neurons decreased. These effects offset each other so that population activity had little effect on total information. Our results thus suggest that intrinsic activity fluctuations may act as a ``traffic light'' that determines which subset of neurons is most informative.},
  file = {/Users/qualia/Documents/Papers/Arandia-Romero et al. - 2016 - Multiplicative and Additive Modulation of Neuronal.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Araque1999,
  title = {Tripartite Synapses: Glia, the Unacknowledged Partner},
  shorttitle = {Tripartite Synapses},
  author = {Araque, Alfonso and Parpura, Vladimir and Sanzgiri, Rita P. and Haydon, Philip G.},
  year = {1999},
  month = may,
  volume = {22},
  pages = {208--215},
  issn = {01662236},
  doi = {10.1016/S0166-2236(98)01349-6},
  file = {/Users/qualia/Documents/Papers/Araque et al. - 1999 - Tripartite synapses glia, the unacknowledged part.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {5}
}

@article{Arbesman2009,
  title = {Superlinear Scaling for Innovation in Cities},
  author = {Arbesman, Samuel and Kleinberg, Jon M. and Strogatz, Steven H.},
  year = {2009},
  month = jan,
  volume = {79},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.79.016115},
  file = {/Users/qualia/Documents/Papers/2009 - Arbesman, Kleinberg, Strogatz - Superlinear scaling for innovation in cities.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {1}
}

@article{Archer,
  title = {Low-Dimensional Models of Neural Population Activity in Sensory Cortical Circuits},
  author = {Archer, Evan W and Koster, Urs and Pillow, Jonathan W and Macke, Jakob H},
  pages = {9},
  abstract = {Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures temporal dynamics and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a multiple idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations, for which inference scales linearly in both population size and recording duration. We test this model to multi-channel recordings from primary visual cortex and show that it accounts for neural tuning properties as well as cross-neural correlations.},
  file = {/Users/qualia/Documents/Papers/Archer et al. - Low-dimensional models of neural population activi.pdf},
  language = {en}
}

@article{Arnoldt2015,
  title = {Toward the {{Darwinian}} Transition: {{Switching}} between Distributed and Speciated States in a Simple Model of Early Life},
  shorttitle = {Toward the {{Darwinian}} Transition},
  author = {Arnoldt, Hinrich and Strogatz, Steven H. and Timme, Marc},
  year = {2015},
  month = nov,
  volume = {92},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.92.052909},
  file = {/Users/qualia/Documents/Papers/2015 - Arnoldt, Strogatz, Timme - Toward the Darwinian transition Switching between distributed and speciated states in a simple model o.pdf;/Users/qualia/Documents/Papers/Arnoldt et al. - 2015 - Toward the Darwinian transition Switching between.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Arpit,
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrzebski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and {Lacoste-Julien}, Simon},
  pages = {10},
  abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
  file = {/Users/qualia/Documents/Papers/Arpit et al. - A Closer Look at Memorization in Deep Networks.pdf},
  language = {en}
}

@article{Asmuth2009,
  title = {A {{Bayesian Sampling Approach}} to {{Exploration}} in {{Reinforcement Learning}}},
  author = {Asmuth, John and Li, Lihong and Littman, Michael L and Nouri, Ali and Wingate, David},
  year = {2009},
  pages = {8},
  abstract = {We present a modular approach to reinforcement learning that uses a Bayesian representation of the uncertainty over models. The approach, BOSS (Best of Sampled Set), drives exploration by sampling multiple models from the posterior and selecting actions optimistically. It extends previous work by providing a rule for deciding when to resample and how to combine the models. We show that our algorithm achieves nearoptimal reward with high probability with a sample complexity that is low relative to the speed at which the posterior distribution converges during learning. We demonstrate that BOSS performs quite favorably compared to state-of-the-art reinforcement-learning approaches and illustrate its flexibility by pairing it with a non-parametric model that generalizes across states.},
  file = {/Users/qualia/Documents/Papers/Asmuth et al. - 2009 - A Bayesian Sampling Approach to Exploration in Rei.pdf},
  language = {en}
}

@article{Assisi2011,
  title = {Using the {{Structure}} of {{Inhibitory Networks}} to {{Unravel Mechanisms}} of {{Spatiotemporal Patterning}}},
  author = {Assisi, Collins and Stopfer, Mark and Bazhenov, Maxim},
  year = {2011},
  month = jan,
  volume = {69},
  pages = {373--386},
  issn = {08966273},
  doi = {10.1016/j.neuron.2010.12.019},
  abstract = {Neuronal networks exhibit a rich dynamical repertoire, a consequence of both the intrinsic properties of neurons and the structure of the network. It has been hypothesized that inhibitory interneurons corral principal neurons into transiently synchronous ensembles that encode sensory information and subserve behavior. How does the structure of the inhibitory network facilitate such spatiotemporal patterning? We established a relationship between an important structural property of a network, its colorings, and the dynamics it constrains. Using a model of the insect antennal lobe, we show that our description allows the explicit identification of the groups of inhibitory interneurons that switch, during odor stimulation, between activity and quiescence in a coordinated manner determined by features of the network structure. This description optimally matches the perspective of the downstream neurons looking for synchrony in ensembles of presynaptic cells and allows a low-dimensional description of seemingly complex high-dimensional network activity.},
  file = {/Users/qualia/Documents/Papers/2011 - Assisi, Stopfer, Bazhenov - Using the structure of inhibitory networks to unravel mechanisms of spatiotemporal patterning.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Atallah2009,
  title = {Instantaneous {{Modulation}} of {{Gamma Oscillation Frequency}} by {{Balancing Excitation}} with {{Inhibition}}},
  author = {Atallah, Bassam V. and Scanziani, Massimo},
  year = {2009},
  month = may,
  volume = {62},
  pages = {566--577},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.04.027},
  abstract = {Neurons recruited for local computations exhibit rhythmic activity at gamma frequencies. The amplitude and frequency of these oscillations are continuously modulated depending on stimulus and behavioral state. This modulation is believed to crucially control information flow across cortical areas. Here we report that in the rat hippocampus gamma oscillation amplitude and frequency vary rapidly, from one cycle to the next. Strikingly, the amplitude of one oscillation predicts the interval to the next. Using in vivo and in vitro whole-cell recordings, we identify the underlying mechanism. We show that cycle-bycycle fluctuations in amplitude reflect changes in synaptic excitation spanning over an order of magnitude. Despite these rapid variations, synaptic excitation is immediately and proportionally counterbalanced by inhibition. These rapid adjustments in inhibition instantaneously modulate oscillation frequency. So, by rapidly balancing excitation with inhibition, the hippocampal network is able to swiftly modulate gamma oscillations over a wide band of frequencies.},
  file = {/Users/qualia/Documents/Papers/2009 - Atallah, Scanziani - Instantaneous Modulation of Gamma Oscillation Frequency by Balancing Excitation with Inhibition.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Attwell2002,
  title = {The Neural Basis of Functional Brain Imaging Signals},
  author = {Attwell, David and Iadecola, Costantino},
  year = {2002},
  month = dec,
  volume = {25},
  pages = {621--625},
  issn = {01662236},
  doi = {10.1016/S0166-2236(02)02264-6},
  file = {/Users/qualia/Documents/Papers/2002 - Attwell, Iadecola - The neural basis of functional brain imaging signals.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {12}
}

@article{Aussel2018,
  title = {A Detailed Anatomical and Mathematical Model of the Hippocampal Formation for the Generation of Sharp-Wave Ripples and Theta-Nested Gamma Oscillations},
  author = {Aussel, Am{\'e}lie and Buhry, Laure and Tyvaert, Louise and Ranta, Radu},
  year = {2018},
  month = dec,
  volume = {45},
  pages = {207--221},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-018-0704-x},
  abstract = {The mechanisms underlying the broad variety of oscillatory rhythms measured in the hippocampus during the sleep-wake cycle are not yet fully understood. In this article, we propose a computational model of the hippocampal formation based on a realistic topology and synaptic connectivity, and we analyze the effect of different changes on the network, namely the variation of synaptic conductances, the variations of the CAN channel conductance and the variation of inputs. By using a detailed simulation of intracerebral recordings, we show that this model is able to reproduce both the theta-nested gamma oscillations that are seen in awake brains and the sharp-wave ripple complexes measured during slow-wave sleep. The results of our simulations support the idea that the functional connectivity of the hippocampus, modulated by the sleep-wake variations in Acetylcholine concentration, is a key factor in controlling its rhythms.},
  file = {/Users/qualia/Documents/Papers/Aussel et al. - 2018 - A detailed anatomical and mathematical model of th.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Ay2015,
  title = {Information {{Geometry}} on {{Complexity}} and {{Stochastic Interaction}}},
  author = {Ay, Nihat},
  year = {2015},
  month = apr,
  volume = {17},
  pages = {2432--2458},
  issn = {1099-4300},
  doi = {10.3390/e17042432},
  abstract = {Interdependencies of stochastically interacting units are usually quantified by the Kullback-Leibler divergence of a stationary joint probability distribution on the set of all configurations from the corresponding factorized distribution. This is a spatial approach which does not describe the intrinsically temporal aspects of interaction. In the present paper, the setting is extended to a dynamical version where temporal interdependencies are also captured by using information geometry of Markov chain manifolds.},
  file = {/Users/qualia/Documents/Papers/2015 - Ay - Information geometry on complexity and stochastic interaction.pdf;/Users/qualia/Documents/Papers/Ay - 2015 - Information Geometry on Complexity and Stochastic .pdf},
  journal = {Entropy},
  language = {en},
  number = {4}
}

@article{Ayaz2009,
  title = {Gain {{Modulation}} of {{Neuronal Responses}} by {{Subtractive}} and {{Divisive Mechanisms}} of {{Inhibition}}},
  author = {Ayaz, Asli and Chance, Frances S.},
  year = {2009},
  month = feb,
  volume = {101},
  pages = {958--968},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.90547.2008},
  file = {/Users/qualia/Documents/Papers/2012 - Ayaz, Chance - Gain Modulation of Neuronal Responses by Subtractive and Divisive Mechanisms of Inhibition Gain Modulation of Neur.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Azodi-Avval2015,
  title = {Phase-Dependent Modulation as a Novel Approach for Therapeutic Brain Stimulation},
  author = {{Azodi-Avval}, Ramin and Gharabaghi, Alireza},
  year = {2015},
  month = feb,
  volume = {9},
  issn = {1662-5188},
  doi = {10.3389/fncom.2015.00026},
  file = {/Users/qualia/Documents/Papers/2015 - Azodi-Avval, Gharabaghi - Phase-dependent modulation as a novel approach for therapeutic brain stimulation.pdf;/Users/qualia/Documents/Papers/Azodi-Avval and Gharabaghi - 2015 - Phase-dependent modulation as a novel approach for 2.pdf;/Users/qualia/Documents/Papers/Azodi-Avval and Gharabaghi - 2015 - Phase-dependent modulation as a novel approach for.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Azulay2018,
  title = {Why Do Deep Convolutional Networks Generalize so Poorly to Small Image Transformations?},
  author = {Azulay, Aharon and Weiss, Yair},
  year = {2018},
  month = may,
  abstract = {Deep convolutional network architectures are often assumed to guarantee generalization for small image translations and deformations. In this paper we show that modern CNNs (VGG16, ResNet50, and InceptionResNetV2) can drastically change their output when an image is translated in the image plane by a few pixels, and that this failure of generalization also happens with other realistic small image transformations. Furthermore, the deeper the network the more we see these failures to generalize. We show that these failures are related to the fact that the architecture of modern CNNs ignores the classical sampling theorem so that generalization is not guaranteed. We also show that biases in the statistics of commonly used image datasets makes it unlikely that CNNs will learn to be invariant to these transformations. Taken together our results suggest that the performance of CNNs in object recognition falls far short of the generalization capabilities of humans.},
  archivePrefix = {arXiv},
  eprint = {1805.12177},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Azulay and Weiss - 2018 - Why do deep convolutional networks generalize so p.pdf},
  journal = {arXiv:1805.12177 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{Bachman2019,
  title = {Learning {{Representations}} by {{Maximizing Mutual Information Across Views}}},
  author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
  year = {2019},
  month = jun,
  abstract = {We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatiotemporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views \textendash{} e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1\% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12\% and concurrent results by 7\%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.},
  archivePrefix = {arXiv},
  eprint = {1906.00910},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Bachman et al. - 2019 - Learning Representations by Maximizing Mutual Info.pdf},
  journal = {arXiv:1906.00910 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Badre2012,
  title = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Cortico}}-{{Striatal Circuits}} 2: {{Evidence}} from {{fMRI}}},
  shorttitle = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Cortico}}-{{Striatal Circuits}} 2},
  author = {Badre, D. and Frank, M. J.},
  year = {2012},
  month = mar,
  volume = {22},
  pages = {527--536},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhr117},
  abstract = {The frontal lobes may be organized hierarchically such that more rostral frontal regions modulate cognitive control operations in caudal regions. In our companion paper (Frank MJ, Badre D. 2011. Mechanisms of hierarchical reinforcement learning in corticostriatal circuits I: computational analysis. 22:509--526), we provide novel neural circuit and algorithmic models of hierarchical cognitive control in cortico--striatal circuits. Here, we test key model predictions using functional magnetic resonance imaging (fMRI). Our neural circuit model proposes that contextual representations in rostral frontal cortex influence the striatal gating of contextual representations in caudal frontal cortex. Reinforcement learning operates at each level, such that the system adaptively learns to gate higher order contextual information into rostral regions. Our algorithmic Bayesian ``mixture of experts'' model captures the key computations of this neural model and provides trial-by-trial estimates of the learner's latent hypothesis states. In the present paper, we used these quantitative estimates to reanalyze fMRI data from a hierarchical reinforcement learning task reported in Badre D, Kayser AS, D'Esposito M. 2010. Frontal cortex and the discovery of abstract action rules. Neuron. 66:315\textendash{}326. Results validate key predictions of the models and provide evidence for an individual cortico--striatal circuit for reinforcement learning of hierarchical structure at a specific level of policy abstraction. These findings are initially consistent with the proposal that hierarchical control in frontal cortex may emerge from interactions among nested cortico--striatal circuits at different levels of abstraction.},
  file = {/Users/qualia/Documents/Papers/2012 - Badre, Frank - Mechanisms of hierarchical reinforcement learning in cortico-striatal circuits 2 Evidence from fMRI.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Baez2016,
  title = {Relative {{Entropy}} in {{Biological Systems}}},
  author = {Baez, John C. and Pollard, Blake S.},
  year = {2016},
  month = feb,
  volume = {18},
  pages = {46},
  issn = {1099-4300},
  doi = {10.3390/e18020046},
  abstract = {In this paper we review various information-theoretic characterizations of the approach to equilibrium in biological systems. The replicator equation, evolutionary game theory, Markov processes and chemical reaction networks all describe the dynamics of a population or probability distribution. Under suitable assumptions, the distribution will approach an equilibrium with the passage of time. Relative entropy\textemdash{}that is, the Kullback\textendash{}Leibler divergence, or various generalizations of this\textemdash{}provides a quantitative measure of how far from equilibrium the system is. We explain various theorems that give conditions under which relative entropy is nonincreasing. In biochemical applications these results can be seen as versions of the Second Law of Thermodynamics, stating that free energy can never increase with the passage of time. In ecological applications, they make precise the notion that a population gains information from its environment as it approaches equilibrium.},
  archivePrefix = {arXiv},
  eprint = {1512.02742},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Baez and Pollard - 2016 - Relative Entropy in Biological Systems.pdf},
  journal = {Entropy},
  keywords = {Computer Science - Information Theory,Mathematics - Probability,Quantitative Biology - Quantitative Methods},
  language = {en},
  number = {2}
}

@article{Bak1988,
  title = {Self-Organized Criticality},
  author = {Bak, Per and Tang, Chao and Wiesenfeld, Kurt},
  year = {1988},
  month = jul,
  volume = {38},
  pages = {364--374},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.38.364},
  file = {/Users/qualia/Documents/Papers/1988 - Bak, Tang, Wiensenfeld - Self-organized criticality.pdf},
  journal = {Physical Review A},
  language = {en},
  number = {1}
}

@article{Bakhtin2012,
  title = {A Neural Computation Model for Decision-Making Times},
  author = {Bakhtin, Yuri and Correll, Joshua},
  year = {2012},
  month = oct,
  volume = {56},
  pages = {333--340},
  issn = {00222496},
  doi = {10.1016/j.jmp.2012.05.005},
  abstract = {We introduce two new models for decision-making times for a two-choice decision task with no a priori bias. One of the models is the mean-field Curie\textendash{}Weiss model of neural computation, and the other is based on dynamics near an unstable equilibrium under a small noise perturbation. As in the existing literature, we interpret exit times as reaction times and show that our models lead to a specific shape of the exit time distributions in the vanishing noise limit. We test the distribution shape against experimental data and show that for almost 90\% of the participants, reaction times are described well by the model. Among the features of our model are: the dependence of the exit distribution only on two parameters, the elegance of rigorous mathematical analysis, and the microscopic nature of the noise.},
  file = {/Users/qualia/Documents/Papers/2012 - Bakhtin, Correll - A neural computation model for decision-making times.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {5}
}

@article{Baladron2012,
  title = {Mean-Field Description and Propagation of Chaos in Networks of {{Hodgkin}}-{{Huxley}} and {{FitzHugh}}-{{Nagumo}} Neurons},
  author = {Baladron, Javier and Fasoli, Diego and Faugeras, Olivier and Touboul, Jonathan},
  year = {2012},
  volume = {2},
  pages = {10},
  issn = {2190-8567},
  doi = {10.1186/2190-8567-2-10},
  file = {/Users/qualia/Documents/Papers/2012 - Baladron et al. - Mean-field description and propagation of chaos in networks of Hodgkin-Huxley and FitzHugh-Nagumo neurons.pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Balduzzi2018,
  title = {The {{Mechanics}} of N-{{Player Differentiable Games}}},
  author = {Balduzzi, David and Racaniere, Sebastien and Martens, James and Foerster, Jakob and Tuyls, Karl and Graepel, Thore},
  year = {2018},
  month = feb,
  abstract = {The cornerstone underpinning deep learning is the guarantee that gradient descent on an objective converges to local minima. Unfortunately, this guarantee fails in settings, such as generative adversarial nets, where there are multiple interacting losses. The behavior of gradient-based methods in games is not well understood \textendash{} and is becoming increasingly important as adversarial and multiobjective architectures proliferate. In this paper, we develop new techniques to understand and control the dynamics in general games. The key result is to decompose the second-order dynamics into two components. The first is related to potential games, which reduce to gradient descent on an implicit function; the second relates to Hamiltonian games, a new class of games that obey a conservation law, akin to conservation laws in classical mechanical systems. The decomposition motivates Symplectic Gradient Adjustment (SGA), a new algorithm for finding stable fixed points in general games. Basic experiments show SGA is competitive with recently proposed algorithms for finding stable fixed points in GANs \textendash{} whilst at the same time being applicable to \textendash{} and having guarantees in \textendash{} much more general games.},
  archivePrefix = {arXiv},
  eprint = {1802.05642},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Balduzzi et al. - 2018 - The Mechanics of n-Player Differentiable Games.pdf},
  journal = {arXiv:1802.05642 [cs]},
  keywords = {Computer Science - Computer Science and Game Theory,Computer Science - Machine Learning,Computer Science - Multiagent Systems,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Balkenius,
  title = {Cognitive {{Modeling}} with {{Context Sensitive Reinforcement Learning}}},
  author = {Balkenius, Christian and Winberg, Stefan},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/2004 - Balkenius, Winberg - Cognitive modeling with context sensitive reinforcement learning.pdf},
  language = {en}
}

@article{Bandt2002,
  title = {Permutation {{Entropy}}: {{A Natural Complexity Measure}} for {{Time Series}}},
  shorttitle = {Permutation {{Entropy}}},
  author = {Bandt, Christoph and Pompe, Bernd},
  year = {2002},
  month = apr,
  volume = {88},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.88.174102},
  file = {/Users/qualia/Documents/Papers/2002 - Bandt, Pompe - Permutation entropy a natural complexity measure for time series.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {17}
}

@article{Bannister2005,
  title = {Inter- and Intra-Laminar Connections of Pyramidal Cells in the Neocortex},
  author = {Bannister, A. Peter},
  year = {2005},
  month = oct,
  volume = {53},
  pages = {95--103},
  issn = {01680102},
  doi = {10.1016/j.neures.2005.06.019},
  abstract = {The flow of excitation through cortical columns has long since been predicted by studying the axonal projection patterns of excitatory neurones situated within different laminae. In grossly simplified terms and assuming random connectivity, such studies predict that input from the thalamus terminates primarily in layer 4, is relayed `forward' to layer 3, then to layers 5 and 6 from where the modified signal may exit the cortex. Projection patterns also indicate `back' projections from layer 5 to 3 and layer 6 to 4. More recently it has become clear that the interconnections between these layers are not random; forward projections primarily contact specific pyramidal subclasses and intracortical back projections innervate interneurones. This indicates that presynaptic axons or postsynaptic dendrites are capable of selecting their synaptic partners and that this selectivity is layer dependent.},
  file = {/Users/qualia/Documents/Papers/2005 - Bannister - Inter- and intra-laminar connections of pyramidal cells in the neocortex.pdf},
  journal = {Neuroscience Research},
  language = {en},
  number = {2}
}

@article{Bansal2018,
  title = {{{EMERGENT COMPLEXITY VIA MULTI}}-{{AGENT COMPETITION}}},
  author = {Bansal, Trapit and Pachocki, Jakub and Sidor, Szymon and Sutskever, Ilya and Mordatch, Igor},
  year = {2018},
  pages = {12},
  abstract = {Reinforcement learning algorithms can train agents that solve problems in complex, interesting environments. Normally, the complexity of the trained agent is closely related to the complexity of the environment. This suggests that a highly capable agent requires a complex environment for training. In this paper, we point out that a competitive multi-agent environment trained with self-play can produce behaviors that are far more complex than the environment itself. We also point out that such environments come with a natural curriculum, because for any skill level, an environment full of agents of this level will have the right level of difficulty.},
  file = {/Users/qualia/Documents/Papers/Bansal et al. - 2018 - EMERGENT COMPLEXITY VIA MULTI-AGENT COMPETITION.pdf},
  language = {en}
}

@article{Barbieri2014,
  title = {Stimulus {{Dependence}} of {{Local Field Potential Spectra}}: {{Experiment}} versus {{Theory}}},
  shorttitle = {Stimulus {{Dependence}} of {{Local Field Potential Spectra}}},
  author = {Barbieri, F. and Mazzoni, A. and Logothetis, N. K. and Panzeri, S. and Brunel, N.},
  year = {2014},
  month = oct,
  volume = {34},
  pages = {14589--14605},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5365-13.2014},
  file = {/Users/qualia/Documents/Papers/Barbieri et al. - 2014 - Stimulus Dependence of Local Field Potential Spect.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {44}
}

@article{Barbour2001,
  title = {An {{Evaluation}} of {{Synapse Independence}}},
  author = {Barbour, Boris},
  year = {2001},
  month = oct,
  volume = {21},
  pages = {7969--7984},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.21-20-07969.2001},
  file = {/Users/qualia/Documents/Papers/2001 - Barbour - An Evaluation of Synapse Independence.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {20}
}

@article{Bargmann2013,
  title = {From the Connectome to Brain Function},
  author = {Bargmann, Cornelia I and Marder, Eve},
  year = {2013},
  month = jun,
  volume = {10},
  pages = {483--490},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.2451},
  file = {/Users/qualia/Documents/Papers/2013 - Bargmann, Marder - From the connectome to brain function.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {6}
}

@article{Barraclough2004,
  title = {Prefrontal Cortex and Decision Making in a Mixed-Strategy Game},
  author = {Barraclough, Dominic J and Conroy, Michelle L and Lee, Daeyeol},
  year = {2004},
  month = apr,
  volume = {7},
  pages = {404--410},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1209},
  file = {/Users/qualia/Documents/Papers/2004 - Barraclough, Conroy, Lee - Prefrontal cortex and decision making in a mixed-strategy game.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Barreiro2010,
  title = {Time Scales of Spike-Train Correlation for Neural Oscillators with Common Drive},
  author = {Barreiro, Andrea K. and {Shea-Brown}, Eric and Thilo, Evan L.},
  year = {2010},
  month = jan,
  volume = {81},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.81.011916},
  file = {/Users/qualia/Documents/Papers/2010 - Barreiro, Shea-Brown, Thilo - Time scales of spike-train correlation for neural oscillators with common drive.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {1}
}

@article{Barreto2018,
  title = {Successor {{Features}} for {{Transfer}} in {{Reinforcement Learning}}},
  author = {Barreto, Andre and Dabney, Will and Munos, Remi and Hunt, Jonathan J and Schaul, Tom},
  year = {2018},
  volume = {1606.05312v2},
  pages = {1--11},
  abstract = {Transfer in reinforcement learning refers to the notion that generalization should occur not only within a task but also across tasks. We propose a transfer framework for the scenario where the reward function changes between tasks but the environment's dynamics remain the same. Our approach rests on two key ideas: successor features, a value function representation that decouples the dynamics of the environment from the rewards, and generalized policy improvement, a generalization of dynamic programming's policy improvement operation that considers a set of policies rather than a single one. Put together, the two ideas lead to an approach that integrates seamlessly within the reinforcement learning framework and allows the free exchange of information across tasks. The proposed method also provides performance guarantees for the transferred policy even before any learning has taken place. We derive two theorems that set our approach in firm theoretical ground and present experiments that show that it successfully promotes transfer in practice, significantly outperforming alternative methods in a sequence of navigation tasks and in the control of a simulated robotic arm.},
  file = {/Users/qualia/Documents/Papers/Barreto et al. - Successor Features for Transfer in Reinforcement L.pdf},
  journal = {Arxiv},
  language = {en}
}

@article{Barrett2008,
  title = {Optimal {{Learning Rules}} for {{Discrete Synapses}}},
  author = {Barrett, Adam B. and {van Rossum}, M. C. W.},
  editor = {Graham, Lyle J.},
  year = {2008},
  month = nov,
  volume = {4},
  pages = {e1000230},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000230},
  abstract = {There is evidence that biological synapses have a limited number of discrete weight states. Memory storage with such synapses behaves quite differently from synapses with unbounded, continuous weights, as old memories are automatically overwritten by new memories. Consequently, there has been substantial discussion about how this affects learning and storage capacity. In this paper, we calculate the storage capacity of discrete, bounded synapses in terms of Shannon information. We use this to optimize the learning rules and investigate how the maximum information capacity depends on the number of synapses, the number of synaptic states, and the coding sparseness. Below a certain critical number of synapses per neuron (comparable to numbers found in biology), we find that storage is similar to unbounded, continuous synapses. Hence, discrete synapses do not necessarily have lower storage capacity.},
  file = {/Users/qualia/Documents/Papers/2008 - Barrett, Van Rossum - Optimal learning rules for discrete synapses.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {11}
}

@article{Barth2012,
  title = {Experimental Evidence for Sparse Firing in the Neocortex},
  author = {Barth, Alison L. and Poulet, James F.A.},
  year = {2012},
  month = jun,
  volume = {35},
  pages = {345--355},
  issn = {01662236},
  doi = {10.1016/j.tins.2012.03.008},
  file = {/Users/qualia/Documents/Papers/2012 - Barth, Poulet - Experimental evidence for sparse firing in the neocortex.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {6}
}

@article{Bartos2007,
  title = {Synaptic Mechanisms of Synchronized Gamma Oscillations in Inhibitory Interneuron Networks},
  author = {Bartos, Marlene and Vida, Imre and Jonas, Peter},
  year = {2007},
  month = jan,
  volume = {8},
  pages = {45--56},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2044},
  abstract = {Gamma frequency oscillations are thought to provide a temporal structure for information processing in the brain. They contribute to cognitive functions, such as memory formation and sensory processing, and are disturbed in some psychiatric disorders. Fast-spiking, parvalbumin-expressing, soma-inhibiting interneurons have a key role in the generation of these oscillations. Experimental analysis in the hippocampus and the neocortex reveals that synapses among these interneurons are highly specialized. Computational analysis further suggests that synaptic specialization turns interneuron networks into robust gamma frequency oscillators.},
  file = {/Users/qualia/Documents/Papers/2007 - Bartos, Vida, Jonas - Synaptic mechanisms of synchronized gamma oscillations in inhibitory interneuron networks(2).pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {1}
}

@article{Bartunov,
  title = {Assessing the {{Scalability}} of {{Biologically}}-{{Motivated}}  {{Deep Learning Algorithms}} and {{Architectures}}},
  author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake A and Hinton, Geoffrey E and Lillicrap, Timothy P},
  pages = {14},
  abstract = {The backpropagation of error algorithm (BP) is often said to be impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might implement or approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present the first results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.},
  file = {/Users/qualia/Documents/Papers/Bartunov et al. - Assessing the Scalability of Biologically-Motivate.pdf},
  language = {en}
}

@article{Bastide2015,
  title = {Pathophysiology of {{L}}-Dopa-Induced Motor and Non-Motor Complications in {{Parkinson}}'s Disease},
  author = {Bastide, Matthieu F. and Meissner, Wassilios G. and Picconi, Barbara and Fasano, Stefania and Fernagut, Pierre-Olivier and Feyder, Michael and Francardo, Veronica and Alcacer, Cristina and Ding, Yunmin and Brambilla, Riccardo and Fisone, Gilberto and Jon Stoessl, A. and Bourdenx, Mathieu and Engeln, Michel and Navailles, Sylvia and De Deurwaerd{\`e}re, Philippe and Ko, Wai Kin D. and Simola, Nicola and Morelli, Micaela and Groc, Laurent and Rodriguez, Maria-Cruz and Gurevich, Eugenia V. and Quik, Maryka and Morari, Michele and Mellone, Manuela and Gardoni, Fabrizio and Tronci, Elisabetta and Guehl, Dominique and Tison, Fran{\c c}ois and Crossman, Alan R. and Kang, Un Jung and {Steece-Collier}, Kathy and Fox, Susan and Carta, Manolo and Angela Cenci, M. and B{\'e}zard, Erwan},
  year = {2015},
  month = sep,
  volume = {132},
  pages = {96--168},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2015.07.002},
  abstract = {Involuntary movements, or dyskinesia, represent a debilitating complication of levodopa (L-dopa) therapy for Parkinson's disease (PD). L-dopa-induced dyskinesia (LID) are ultimately experienced by the vast majority of patients. In addition, psychiatric conditions often manifested as compulsive behaviours, are emerging as a serious problem in the management of L-dopa therapy. The present review attempts to provide an overview of our current understanding of dyskinesia and other L-dopainduced dysfunctions, a field that dramatically evolved in the past twenty years. In view of the extensive literature on LID, there appeared a critical need to re-frame the concepts, to highlight the most suitable models, to review the central nervous system (CNS) circuitry that may be involved, and to propose a pathophysiological framework was timely and necessary. An updated review to clarify our understanding of LID and other L-dopa-related side effects was therefore timely and necessary.},
  file = {/Users/qualia/Documents/Papers/2015 - Bastide et al. - Pathophysiology of L-dopa-induced motor and non-motor complications in Parkinson's disease.pdf;/Users/qualia/Documents/Papers/Bastide et al. - 2015 - Pathophysiology of L-dopa-induced motor and non-mo.pdf},
  journal = {Progress in Neurobiology},
  language = {en}
}

@article{Bastin2014,
  title = {Inhibitory Control and Error Monitoring by Human Subthalamic Neurons},
  author = {Bastin, J and Polosan, M and Benis, D and Goetz, L and Bhattacharjee, M and Piallat, B and Krainik, A and Bougerol, T and Chabard{\`e}s, S and David, O},
  year = {2014},
  month = sep,
  volume = {4},
  pages = {e439-e439},
  issn = {2158-3188},
  doi = {10.1038/tp.2014.73},
  file = {/Users/qualia/Documents/Papers/2014 - Bastin et al. - Inhibitory control and error monitoring by human subthalamic neurons.pdf;/Users/qualia/Documents/Papers/Bastin et al. - 2014 - Inhibitory control and error monitoring by human s.pdf},
  journal = {Translational Psychiatry},
  language = {en},
  number = {9}
}

@article{Bastos2015,
  title = {Communication through Coherence with Inter-Areal Delays},
  author = {Bastos, Andre M and Vezoli, Julien and Fries, Pascal},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {173--180},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.11.001},
  file = {/Users/qualia/Documents/Papers/Bastos et al. - 2015 - Communication through coherence with inter-areal d.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Batty,
  title = {{{BehaveNet}}: Nonlinear Embedding and {{Bayesian}} Neural Decoding of Behavioral Videos},
  author = {Batty, Eleanor and Whiteway, Matthew R and Saxena, Shreya and Biderman, Dan and Abe, Taiga and Musall, Simon and Gillis, Winthrop and Markowitz, Jeffrey E and Churchland, Anne and Cunningham, John and Datta, Sandeep Robert and Linderman, Scott W and Paninski, Liam},
  pages = {12},
  abstract = {A fundamental goal of systems neuroscience is to understand the relationship between neural activity and behavior. Behavior has traditionally been characterized by low-dimensional, task-related variables such as movement speed or response times. More recently, there has been a growing interest in automated analysis of high-dimensional video data collected during experiments. Here we introduce a probabilistic framework for the analysis of behavioral video and neural activity. This framework provides tools for compression, segmentation, generation, and decoding of behavioral videos. Compression is performed using a convolutional autoencoder (CAE), which yields a low-dimensional continuous representation of behavior. We then use an autoregressive hidden Markov model (ARHMM) to segment the CAE representation into discrete ``behavioral syllables.'' The resulting generative model can be used to simulate behavioral video data. Finally, based on this generative model, we develop a novel Bayesian decoding approach that takes in neural activity and outputs probabilistic estimates of the full-resolution behavioral video. We demonstrate this framework on two different experimental paradigms using distinct behavioral and neural recording technologies.},
  file = {/Users/qualia/Documents/Papers/Batty et al. - BehaveNet nonlinear embedding and Bayesian neural.pdf},
  language = {en}
}

@article{Baumgartner1999,
  title = {Assessment of Cluster Homogeneity in {{fMRI}} Data Using {{Kendall}}'s Coefficient of Concordance},
  author = {Baumgartner, R. and Somorjai, R. and Summers, R. and Richter, W.},
  year = {1999},
  month = dec,
  volume = {17},
  pages = {1525--1532},
  issn = {0730725X},
  doi = {10.1016/S0730-725X(99)00101-0},
  file = {/Users/qualia/Documents/Papers/1999 - Concordance - q Technical Note ASSESSMENT OF CLUSTER HOMOGENEITY IN fMRI DATA USING KENDALL ’ S COEFFICIENT OF CONCORDANCE.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {10}
}

@article{Baumgartner2001,
  title = {Graphical Display of {{fMRI}} Data: Visualizing Multidimensional Space},
  shorttitle = {Graphical Display of {{fMRI}} Data},
  author = {Baumgartner, R and Somorjai, R},
  year = {2001},
  month = feb,
  volume = {19},
  pages = {283--286},
  issn = {0730725X},
  doi = {10.1016/S0730-725X(01)00296-X},
  abstract = {Visualization of multidimensional data is an integral part of computational statistics and exploratory data analysis (EDA). We show how visualization of fMRI time-courses may be used to reveal the fMRI data structure. We consider fMRI time-courses (TCs) as points in multidimensional space. In simulated and in vivo data, we show that minimum spanning tree (MST)-based sequencing of multivariate time-courses, in combination with a homogeneity map visualization, allows for effective and useful graphical display of the groups of coactivated time-courses obtained by temporal clustering. This display may serve as a tool for investigation of brain connectivity. We also suggest a simple overall display of the entire fMRI data set. \textcopyright{} 2001 Elsevier Science Inc. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2001 - Baumgartner, Somorjai - Graphical display of fMRI data visualizing multidimensional space.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {2}
}

@article{Bays2014,
  title = {Noise in {{Neural Populations Accounts}} for {{Errors}} in {{Working Memory}}},
  author = {Bays, P. M.},
  year = {2014},
  month = mar,
  volume = {34},
  pages = {3632--3645},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3204-13.2014},
  file = {/Users/qualia/Documents/Papers/Bays - 2014 - Noise in Neural Populations Accounts for Errors in.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {10}
}

@article{Bazargani2016a,
  title = {Astrocyte Calcium Signaling: The Third Wave},
  shorttitle = {Astrocyte Calcium Signaling},
  author = {Bazargani, Narges and Attwell, David},
  year = {2016},
  month = feb,
  volume = {19},
  pages = {182--189},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4201},
  file = {/Users/qualia/Documents/Papers/Bazargani and Attwell - 2016 - Astrocyte calcium signaling the third wave 2.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {2}
}

@techreport{Beam2019,
  title = {A Computational Knowledge Engine for Human Neuroscience},
  author = {Beam, Elizabeth and Potts, Christopher and Poldrack, Russell A. and Etkin, Amit},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/701540},
  abstract = {Functional neuroimaging has been a mainstay of human neuroscience for the past 25 years. The goal for this research has largely been to understand how activity across brain structures relates to mental constructs and computations. However, interpretation of fMRI data has often occurred within knowledge frameworks crafted by experts, which have the potential to reify historical trends and amplify the subjective biases that limit the replicability of findings.
            1
            In other words, we lack a comprehensive data-driven ontology for structure-function mapping in the human brain, through which we can also test the explanatory value of current dominant conceptual frameworks. Ontologies in other fields are popular tools for automated data synthesis,
            2, 3
            yet relatively few attempts have been made to engineer ontologies in a data-driven manner.
            4
            Here, we employ a computational approach to derive a data-driven ontology for neurobiological domains that synthesizes the texts and data of nearly 20,000 human neuroimaging articles. The data-driven ontology includes 6 domains, each defined by a circuit of brain structures and its associated mental functions. Several of these domains are omitted from the leading framework in neuroscience, while others uncover novel combinations of mental functions related to common brain circuitry. Crucially, the structure-function links in each domain better replicate across articles in held-out data than those mapped from the dominant frameworks in neuroscience and psychiatry. We further show that the data-driven ontology partitions the literature into modular subfields, for which the domains serve as generalizable archetypes of the structure-function patterns observed in single articles. The approach to computational ontology we present here is the most comprehensive functional characterization of human brain circuits quantifiable with fMRI. Moreover, our methods can be extended to synthesize other scientific literatures, yielding ontologies that are built up from the data of the field.},
  file = {/Users/qualia/Documents/Papers/Beam et al. - 2019 - A computational knowledge engine for human neurosc.pdf},
  language = {en},
  type = {Preprint}
}

@article{Beattie2016,
  title = {{{DeepMind Lab}}},
  author = {Beattie, Charles and Leibo, Joel Z. and Teplyashin, Denis and Ward, Tom and Wainwright, Marcus and K{\"u}ttler, Heinrich and Lefrancq, Andrew and Green, Simon and Vald{\'e}s, V{\'i}ctor and Sadik, Amir and Schrittwieser, Julian and Anderson, Keith and York, Sarah and Cant, Max and Cain, Adam and Bolton, Adrian and Gaffney, Stephen and King, Helen and Hassabis, Demis and Legg, Shane and Petersen, Stig},
  year = {2016},
  month = dec,
  abstract = {DeepMind Lab is a first-person 3D game platform designed for research and development of general artificial intelligence and machine learning systems. DeepMind Lab can be used to study how autonomous artificial agents may learn complex tasks in large, partially observed, and visually diverse worlds. DeepMind Lab has a simple and flexible API enabling creative task-designs and novel AI-designs to be explored and quickly iterated upon. It is powered by a fast and widely recognised game engine, and tailored for effective use by the research community.},
  archivePrefix = {arXiv},
  eprint = {1612.03801},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Beattie et al. - 2016 - DeepMind Lab.pdf},
  journal = {arXiv:1612.03801 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{Beggs2015,
  title = {Editorial: {{Can There Be}} a {{Physics}} of the {{Brain}}?},
  shorttitle = {Editorial},
  author = {Beggs, John},
  year = {2015},
  month = jun,
  volume = {114},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.114.220001},
  file = {/Users/qualia/Documents/Papers/Beggs - 2015 - Editorial Can There Be a Physics of the Brain.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {22}
}

@article{Behabadi2014,
  title = {Mechanisms Underlying Subunit Independence in Pyramidal Neuron Dendrites},
  author = {Behabadi, B. F. and Mel, B. W.},
  year = {2014},
  month = jan,
  volume = {111},
  pages = {498--503},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1217645111},
  file = {/Users/qualia/Documents/Papers/2013 - Behabadi, Mel - Mechanisms underlying subunit independence in pyramidal neuron dendrites.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {1}
}

@article{Beierlein2003,
  title = {Two {{Dynamically Distinct Inhibitory Networks}} in {{Layer}} 4 of the {{Neocortex}}},
  author = {Beierlein, Michael and Gibson, Jay R. and Connors, Barry W.},
  year = {2003},
  month = nov,
  volume = {90},
  pages = {2987--3000},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00283.2003},
  file = {/Users/qualia/Documents/Papers/2003 - Beierlein - Two Dynamically Distinct Inhibitory Networks in Layer 4 of the Neocortex.pdf;/Users/qualia/Documents/Papers/2003 - Beierlein - Two Dynamically Distinct Inhibitory Networks in Layer 4 of the Neocortex(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Belghazi,
  title = {Mutual {{Information Neural Estimation}}},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
  pages = {18},
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  file = {/Users/qualia/Documents/Papers/Belghazi et al. - Mutual Information Neural Estimation.pdf},
  language = {en}
}

@article{Bellemare2013,
  title = {The {{Arcade Learning Environment}}: {{An Evaluation Platform}} for {{General Agents}}},
  shorttitle = {The {{Arcade Learning Environment}}},
  author = {Bellemare, Marc G. and Naddaf, Yavar and Veness, Joel and Bowling, Michael},
  year = {2013},
  month = jun,
  volume = {47},
  pages = {253--279},
  issn = {1076-9757},
  doi = {10.1613/jair.3912},
  abstract = {In this article we introduce the Arcade Learning Environment (ALE): both a challenge problem and a platform and methodology for evaluating the development of general, domain-independent AI technology. ALE provides an interface to hundreds of Atari 2600 game environments, each one different, interesting, and designed to be a challenge for human players. ALE presents significant research challenges for reinforcement learning, model learning, model-based planning, imitation learning, transfer learning, and intrinsic motivation. Most importantly, it provides a rigorous testbed for evaluating and comparing approaches to these problems. We illustrate the promise of ALE by developing and benchmarking domain-independent agents designed using well-established AI techniques for both reinforcement learning and planning. In doing so, we also propose an evaluation methodology made possible by ALE, reporting empirical results on over 55 different games. All of the software, including the benchmark agents, is publicly available.},
  archivePrefix = {arXiv},
  eprint = {1207.4708},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2013 - Bellemare, Veness - The Arcade Learning Environment An Evaluation Platform for General Agents.pdf},
  journal = {Journal of Artificial Intelligence Research},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en}
}

@article{Bellemare2016,
  title = {Unifying {{Count}}-{{Based Exploration}} and {{Intrinsic Motivation}}},
  author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  year = {2016},
  month = jun,
  abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.},
  archivePrefix = {arXiv},
  eprint = {1606.01868},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Bellemare et al. - 2016 - Unifying Count-Based Exploration and Intrinsic Mot.pdf},
  journal = {arXiv:1606.01868 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Bellmann1954,
  title = {The Theory of Dynamic Programming},
  author = {Bellmann, Richard},
  year = {1954},
  volume = {60},
  pages = {503--515},
  journal = {Bull. Amer. Math. Soc},
  number = {6}
}

@article{Bellot-Saez2017,
  title = {Astrocytic Modulation of Neuronal Excitability through {{K}} + Spatial Buffering},
  author = {{Bellot-Saez}, Alba and K{\'e}kesi, Orsolya and Morley, John W. and Buskila, Yossi},
  year = {2017},
  month = jun,
  volume = {77},
  pages = {87--97},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2017.03.002},
  abstract = {The human brain contains two major cell populations, neurons and glia. While neurons are electrically excitable and capable of discharging short voltage pulses known as action potentials, glial cells are not. However, astrocytes, the prevailing subtype of glia in the cortex, are highly connected and can modulate the excitability of neurons by changing the concentration of potassium ions in the extracellular environment, a process called K+ clearance.},
  file = {/Users/qualia/Documents/Papers/Bellot-Saez et al. - 2017 - Astrocytic modulation of neuronal excitability thr.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en}
}

@article{Ben-Yakov2012,
  title = {Loss of Reliable Temporal Structure in Event-Related Averaging of Naturalistic Stimuli},
  author = {{Ben-Yakov}, Aya and Honey, Christopher J. and Lerner, Yulia and Hasson, Uri},
  year = {2012},
  month = oct,
  volume = {63},
  pages = {501--506},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.07.008},
  abstract = {To separate neural signals from noise, brain responses measured in neuroimaging are routinely averaged across space and time. However, such procedures may obscure some properties of neural activity. Recently, multi-voxel pattern analysis methods have demonstrated that patterns of activity across voxels contain valuable information that is concealed by spatial averaging. Here we show that temporal patterns of neural activity contain information that can discriminate different stimuli, even within brain regions that show no net activation to that stimulus class. Furthermore, we find that in many brain regions, responses to natural stimuli are highly context dependent. In such cases, prototypical event-related responses do not even exist for individual stimuli, so that averaging responses to the same stimulus within different contexts may worsen the effective signal-to-noise. As a result, analysis of the temporal structures of single events can reveal aspects of neural dynamics which cannot be detected using standard event-related averaging methods.},
  file = {/Users/qualia/Documents/Papers/2012 - Ben-Yakov et al. - Loss of reliable temporal structure in event-related averaging of naturalistic stimuli.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Benaim2009,
  title = {Learning in Games with Unstable Equilibria},
  author = {Bena{\"i}m, Michel and Hofbauer, Josef and Hopkins, Ed},
  year = {2009},
  month = jul,
  volume = {144},
  pages = {1694--1709},
  issn = {00220531},
  doi = {10.1016/j.jet.2008.09.003},
  abstract = {We propose a new concept for the analysis of games, the TASP, which gives a precise prediction about non-equilibrium play in games whose Nash equilibria are mixed and are unstable under fictitious play-like learning processes. We show that, when players learn using weighted stochastic fictitious play and so place greater weight on more recent experience, the time average of play often converges in these ``unstable'' games, even while mixed strategies and beliefs continue to cycle. This time average, the TASP, is related to the best response cycle first identified by Shapley (1964). Though conceptually distinct from Nash equilibrium, for many games the TASP is close enough to Nash to create the appearance of convergence to equilibrium. We discuss how these theoretical results may help to explain data from recent experimental studies of price dispersion.},
  file = {/Users/qualia/Documents/Papers/2009 - Benaïm, Hofbauer, Hopkins - Learning in games with unstable equilibria.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {4}
}

@article{Bendor2001,
  title = {{{ASPIRATION}}-{{BASED REINFORCEMENT LEARNING IN REPEATED INTERACTION GAMES}}: {{AN OVERVIEW}}},
  shorttitle = {{{ASPIRATION}}-{{BASED REINFORCEMENT LEARNING IN REPEATED INTERACTION GAMES}}},
  author = {Bendor, Jonathan and Mookherjee, Dilip and Ray, Debraj},
  year = {2001},
  month = jun,
  volume = {03},
  pages = {159--174},
  issn = {0219-1989, 1793-6675},
  doi = {10.1142/S0219198901000348},
  file = {/Users/qualia/Documents/Papers/2001 - BENDOR, MOOKHERJEE, RAY - Aspiration-Based Reinforcement Learning in Repeated Interaction Games an Overview.pdf},
  journal = {International Game Theory Review},
  language = {en},
  number = {02n03}
}

@article{Bengio2009,
  title = {Learning {{Deep Architectures}} for {{AI}}},
  author = {Bengio, Y.},
  year = {2009},
  volume = {2},
  pages = {1--127},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000006},
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent highlevel abstractions (e.g. in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  file = {/Users/qualia/Documents/Papers/2009 - Bengio - Learning Deep Architectures for AI.pdf},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  language = {en},
  number = {1}
}

@article{Bengio2015,
  title = {{{STDP}} as Presynaptic Activity Times Rate of Change of Postsynaptic Activity},
  author = {Bengio, Yoshua and Mesnard, Thomas and Fischer, Asja and Zhang, Saizheng and Wu, Yuhuai},
  year = {2015},
  month = sep,
  abstract = {We introduce a weight update formula that is expressed only in terms of firing rates and their derivatives and that results in changes consistent with those associated with spike-timing dependent plasticity (STDP) rules and biological observations, even though the explicit timing of spikes is not needed. The new rule changes a synaptic weight in proportion to the product of the presynaptic firing rate and the temporal rate of change of activity on the postsynaptic side. These quantities are interesting for studying theoretical explanation for synaptic changes from a machine learning perspective. In particular, if neural dynamics moved neural activity towards reducing some objective function, then this STDP rule would correspond to stochastic gradient descent on that objective function.},
  archivePrefix = {arXiv},
  eprint = {1509.05936},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Bengio et al. - STDP as presynaptic activity times rate of change of postsynaptic activity.pdf;/Users/qualia/Documents/Papers/Bengio et al. - 2015 - STDP as presynaptic activity times rate of change .pdf},
  journal = {arXiv:1509.05936 [cs, q-bio]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{Bengio2015a,
  title = {Towards {{Biologically Plausible Deep Learning}}},
  author = {Bengio, Yoshua and Lee, Dong-Hyun and Bornschein, Jorg and Mesnard, Thomas and Lin, Zhouhan},
  year = {2015},
  month = feb,
  abstract = {Neuroscientists have long criticised deep learning algorithms as incompatible with current knowledge of neurobiology. We explore more biologically plausible versions of deep representation learning, focusing here mostly on unsupervised learning but developing a learning mechanism that could account for supervised, unsupervised and reinforcement learning. The starting point is that the basic learning rule believed to govern synaptic weight updates (Spike-TimingDependent Plasticity) can be interpreted as gradient descent on some objective function so long as the neuronal dynamics push firing rates towards better values of the objective function (be it supervised, unsupervised, or reward-driven). The second main idea is that this corresponds to a form of the variational EM algorithm, i.e., with approximate rather than exact posteriors, implemented by neural dynamics. Another contribution of this paper is that the gradients required for updating the hidden states in the above variational interpretation can be estimated using an approximation that only requires propagating activations forward and backward, with pairs of layers learning to form a denoising auto-encoder. Finally, we extend the theory about the probabilistic interpretation of auto-encoders to justify improved sampling schemes based on the generative interpretation of denoising auto-encoders, and we validate all these ideas on generative learning tasks.},
  archivePrefix = {arXiv},
  eprint = {1502.04156},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Bengio et al. - Towards Biologically Plausible Deep Learning.pdf;/Users/qualia/Documents/Papers/Bengio et al. - 2015 - Towards Biologically Plausible Deep Learning.pdf},
  journal = {arXiv:1502.04156 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Bereby-Meyer2006,
  title = {The {{Speed}} of {{Learning}} in {{Noisy Games}}: {{Partial Reinforcement}} and the {{Sustainability}} of {{Cooperation}}},
  shorttitle = {The {{Speed}} of {{Learning}} in {{Noisy Games}}},
  author = {{Bereby-Meyer}, Yoella and Roth, Alvin E},
  year = {2006},
  month = aug,
  volume = {96},
  pages = {1029--1042},
  issn = {0002-8282},
  doi = {10.1257/aer.96.4.1029},
  abstract = {In an experiment, players' ability to learn to cooperate in the repeated prisoner's dilemma was substantially diminished when the payoffs were noisy, even though players could monitor one another's past actions perfectly. In contrast, in one-time play against a succession of opponents, noisy payoffs increased cooperation, by slowing the rate at which cooperation decays. These observations are consistent with the robust observation from the psychology literature that partial reinforcement (adding randomness to the link between an action and its consequences while holding expected payoffs constant) slows learning. This effect is magnified in the repeated game: When others are slow to learn to cooperate, the benefits of cooperation are reduced, which further hampers cooperation. These results show that a small change in the payoff environment, which changes the speed of individual learning, can have a large effect on collective behavior. And they show that there may be interesting comparative dynamics that can be derived from careful attention to the fact that at least some economic behavior is learned from experience.},
  file = {/Users/qualia/Documents/Papers/2006 - Bereby-Meyer, Roth - The speed of learning in noisy games Partial reinforcement and the sustainability of cooperation.pdf},
  journal = {American Economic Review},
  language = {en},
  number = {4}
}

@article{Berger-Tal2014,
  title = {The {{Exploration}}-{{Exploitation Dilemma}}: {{A Multidisciplinary Framework}}},
  shorttitle = {The {{Exploration}}-{{Exploitation Dilemma}}},
  author = {{Berger-Tal}, Oded and Nathan, Jonathan and Meron, Ehud and Saltz, David},
  editor = {Daunizeau, Jean},
  year = {2014},
  month = apr,
  volume = {9},
  pages = {e95693},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0095693},
  abstract = {The trade-off between the need to obtain new knowledge and the need to use that knowledge to improve performance is one of the most basic trade-offs in nature, and optimal performance usually requires some balance between exploratory and exploitative behaviors. Researchers in many disciplines have been searching for the optimal solution to this dilemma. Here we present a novel model in which the exploration strategy itself is dynamic and varies with time in order to optimize a definite goal, such as the acquisition of energy, money, or prestige. Our model produced four very distinct phases: Knowledge establishment, Knowledge accumulation, Knowledge maintenance, and Knowledge exploitation, giving rise to a multidisciplinary framework that applies equally to humans, animals, and organizations. The framework can be used to explain a multitude of phenomena in various disciplines, such as the movement of animals in novel landscapes, the most efficient resource allocation for a start-up company, or the effects of old age on knowledge acquisition in humans.},
  file = {/Users/qualia/Documents/Papers/Berger-Tal et al. - 2014 - The Exploration-Exploitation Dilemma A Multidisci.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Berger-Tal2014a,
  title = {The {{Exploration}}-{{Exploitation Dilemma}}: {{A Multidisciplinary Framework}}},
  shorttitle = {The {{Exploration}}-{{Exploitation Dilemma}}},
  author = {{Berger-Tal}, Oded and Nathan, Jonathan and Meron, Ehud and Saltz, David},
  editor = {Daunizeau, Jean},
  year = {2014},
  month = apr,
  volume = {9},
  pages = {e95693},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0095693},
  abstract = {The trade-off between the need to obtain new knowledge and the need to use that knowledge to improve performance is one of the most basic trade-offs in nature, and optimal performance usually requires some balance between exploratory and exploitative behaviors. Researchers in many disciplines have been searching for the optimal solution to this dilemma. Here we present a novel model in which the exploration strategy itself is dynamic and varies with time in order to optimize a definite goal, such as the acquisition of energy, money, or prestige. Our model produced four very distinct phases: Knowledge establishment, Knowledge accumulation, Knowledge maintenance, and Knowledge exploitation, giving rise to a multidisciplinary framework that applies equally to humans, animals, and organizations. The framework can be used to explain a multitude of phenomena in various disciplines, such as the movement of animals in novel landscapes, the most efficient resource allocation for a start-up company, or the effects of old age on knowledge acquisition in humans.},
  file = {/Users/qualia/Documents/Papers/Berger-Tal et al. - 2014 - The Exploration-Exploitation Dilemma A Multidisci 2.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Bergstra,
  title = {Random {{Search}} for {{Hyper}}-{{Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  pages = {25},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent ``High Throughput'' methods achieve surprising success\textemdash{}they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  file = {/Users/qualia/Documents/Papers/Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf},
  language = {en}
}

@article{Berlyne1950,
  title = {Novelty and Curiosity as Determinants of Exploratory Behaviour},
  author = {Berlyne, DE},
  year = {1950},
  volume = {41},
  pages = {68--80},
  file = {/Users/qualia/Documents/Papers/Novelty_and_curiosity_as_deter.pdf},
  journal = {British Journal of Psychology},
  number = {1}
}

@article{Berseth2020,
  title = {{{SMiRL}}: {{Surprise Minimizing RL}} in {{Dynamic Environments}}},
  shorttitle = {{{SMiRL}}},
  author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Rhinehart, Nicholas and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  year = {2020},
  month = feb,
  abstract = {All living organisms struggle against the forces of nature to carve out niches where they can maintain homeostasis. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called surprise minimizing RL (SMiRL). SMiRL trains an agent with the objective of maximizing the probability of observed states under a model trained on previously seen states. The resulting agents can acquire proactive behaviors that seek out and maintain stable conditions, such as balancing and damage avoidance, that are closely tied to an environment's prevailing sources of entropy, such as wind, earthquakes, and other agents. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls and navigate to escape enemy agents, without any task-specific reward supervision. We further show that SMiRL can be used together with a standard task reward to accelerate reward-driven learning.},
  archivePrefix = {arXiv},
  eprint = {1912.05510},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Berseth et al. - 2020 - SMiRL Surprise Minimizing RL in Dynamic Environme.pdf},
  journal = {arXiv:1912.05510 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,G.3,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Berseth2020a,
  title = {{{SMiRL}}: {{Surprise Minimizing RL}} in {{Dynamic Environments}}},
  shorttitle = {{{SMiRL}}},
  author = {Berseth, Glen and Geng, Daniel and Devin, Coline and Rhinehart, Nicholas and Finn, Chelsea and Jayaraman, Dinesh and Levine, Sergey},
  year = {2020},
  month = feb,
  abstract = {All living organisms struggle against the forces of nature to carve out a maintainable niche. We propose that such a search for order amidst chaos might offer a unifying principle for the emergence of useful behaviors in artificial agents. We formalize this idea into an unsupervised reinforcement learning method called Surprise Minimizing RL (SMiRL). SMiRL alternates between learning a density model to evaluate the surprise of a stimulus, and improving the policy to seek more predictable stimuli. This process maximizes a lower-bound on the negative entropy of the states, which can be seen as maximizing the agent's ability to maintain order in the environment. The policy seeks out stable and repeatable situations that counteract the environment's prevailing sources of entropy. This might include avoiding other hostile agents, or finding a stable, balanced pose for a bipedal robot in the face of disturbance forces. We demonstrate that our surprise minimizing agents can successfully play Tetris, Doom, control a humanoid to avoid falls, and navigate to escape enemies in a maze without any task-specific reward supervision. We further show that SMiRL can be used together with a standard task rewards to accelerate reward-driven learning.},
  archivePrefix = {arXiv},
  eprint = {1912.05510},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Berseth et al. - 2020 - SMiRL Surprise Minimizing RL in Dynamic Environme 2.pdf},
  journal = {arXiv:1912.05510 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,G.3,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Bertschinger2004,
  title = {Real-{{Time Computation}} at the {{Edge}} of {{Chaos}} in {{Recurrent Neural Networks}}},
  author = {Bertschinger, Nils and Natschl{\"a}ger, Thomas},
  year = {2004},
  month = jul,
  volume = {16},
  pages = {1413--1436},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976604323057443},
  file = {/Users/qualia/Documents/Papers/2004 - Bertschinger, Natschläger - Real-time computation at the edge of chaos in recurrent neural networks.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Beurrier1999,
  title = {Subthalamic {{Nucleus Neurons Switch}} from {{Single}}-{{Spike Activity}} to {{Burst}}-{{Firing Mode}}},
  author = {Beurrier, Corinne and Congar, Patrice and Bioulac, Bernard and Hammond, Constance},
  year = {1999},
  month = jan,
  volume = {19},
  pages = {599--609},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.19-02-00599.1999},
  file = {/Users/qualia/Documents/Papers/1999 - Beurrier et al. - Subthalamic nucleus neurons switch from single-spike activity to burst-firing mode.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{Beyeler2017,
  title = {Sparse Coding and Dimensionality Reduction in Cortex},
  author = {Beyeler, Michael and Rounds, Emily and Carlson, Kristofor and Dutt, Nikil and Krichmar, Jeffrey L.},
  year = {2017},
  month = jun,
  doi = {10.1101/149880},
  abstract = {Supported by recent computational studies, sparse coding and dimensionality reduction are emerging as a ubiquitous coding strategy across brain regions and modalities, allowing neurons to achieve nonnegative sparse coding (NSC) by efficiently encoding highdimensional stimulus spaces using a sparse and parts-based population code. Reducing the dimensionality of complex, multimodal sensory streams is critically important for metabolically constrained brain areas to represent the world. In this article, we provide an overview of NSC, summarize evidence for its role in neural computation in disparate regions of the brain, ranging from visual processing to spatial navigation, and speculate that specific forms of synaptic plasticity and homeostatic modulation may underlie its implementation. We suggest that NSC may be an organizing principle in the nervous system.},
  file = {/Users/qualia/Documents/Papers/Beyeler et al. - 2017 - Sparse coding and dimensionality reduction in cort.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Bhattacharya2013,
  title = {Implementing the Cellular Mechanisms of Synaptic Transmission in a Neural Mass Model of the Thalamo-Cortical Circuitry},
  author = {Bhattacharya, Basabdatta S.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00081},
  abstract = {A novel direction to existing neural mass modeling technique is proposed where the commonly used ``alpha function'' for representing synaptic transmission is replaced by a kinetic framework of neurotransmitter and receptor dynamics. The aim is to underpin neuro-transmission dynamics associated with abnormal brain rhythms commonly observed in neurological and psychiatric disorders. An existing thalamocortical neural mass model is modified by using the kinetic framework for modeling synaptic transmission mediated by glutamatergic and GABA (gamma-aminobutyric-acid)-ergic receptors. The model output is compared qualitatively with existing literature on in vitro experimental studies of ferret thalamic slices, as well as on single-neuron-level model based studies of neuro-receptor and transmitter dynamics in the thalamocortical tissue. The results are consistent with these studies: the activation of ligand-gated GABA receptors is essential for generation of spindle waves in the model, while blocking this pathway leads to low-frequency synchronized oscillations such as observed in slow-wave sleep; the frequency of spindle oscillations increase with increased levels of post-synaptic membrane conductance for AMPA (alpha-amino-3-hydroxy-5-methyl-4-isoxazolepropionic-acid) receptors, and blocking this pathway effects a quiescent model output. In terms of computational efficiency, the simulation time is improved by a factor of 10 compared to a similar neural mass model based on alpha functions. This implies a dramatic improvement in computational resources for large-scale network simulation using this model. Thus, the model provides a platform for correlating high-level brain oscillatory activity with low-level synaptic attributes, and makes a significant contribution toward advancements in current neural mass modeling paradigm as a potential computational tool to better the understanding of brain oscillations in sickness and in health.},
  file = {/Users/qualia/Documents/Papers/2013 - Bhattacharya - Implementing the cellular mechanisms of synaptic transmission in a neural mass model of the thalamo-cortical circu.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Binzegger2004,
  title = {A {{Quantitative Map}} of the {{Circuit}} of {{Cat Primary Visual Cortex}}},
  author = {Binzegger, T.},
  year = {2004},
  month = sep,
  volume = {24},
  pages = {8441--8453},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1400-04.2004},
  file = {/Users/qualia/Documents/Papers/2004 - Binzegger - A Quantitative Map of the Circuit of Cat Primary Visual Cortex.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {39}
}

@article{Binzegger2009,
  title = {Topology and Dynamics of the Canonical Circuit of Cat {{V1}}},
  author = {Binzegger, T. and Douglas, R.J. and Martin, K.A.C.},
  year = {2009},
  month = oct,
  volume = {22},
  pages = {1071--1078},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.07.011},
  abstract = {The neocortex is a major component of the most sophisticated and economically significant computer in existence, nevertheless the organisation and operation of its computational circuit is not yet understood. Here we make some steps toward relating anatomical structure to computational function. We use methods of quantitative neuroanatomy to estimate the cortical circuit by defining the projection matrix between the various cells types of the neocortex of the cat, and then we consider the implications of this connectivity for cortical signal processing. Our analyses show that for a reasonable choice of the ratio between excitatory and inhibitory efficacy, the overall cortical circuit lies near the border of dynamical stability. We discuss a model of co-operative competitive processing that is consistent with the observed connectivity in the superficial layers of the cortex, and consider also how the topology of the overall cortical circuit could be configured dynamically through average inhibition.},
  file = {/Users/qualia/Documents/Papers/2009 - Binzegger, Douglas, Martin - Topology and dynamics of the canonical circuit of cat V1.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {8}
}

@article{Bird2016,
  title = {Optimal {{Current Transfer}} in {{Dendrites}}},
  author = {Bird, Alex D. and Cuntz, Hermann},
  editor = {{van Rossum}, Mark C. W.},
  year = {2016},
  month = may,
  volume = {12},
  pages = {e1004897},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004897},
  file = {/Users/qualia/Documents/Papers/Bird and Cuntz - 2016 - Optimal Current Transfer in Dendrites.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {5}
}

@article{Bisio2014,
  title = {Emergence of {{Bursting Activity}} in {{Connected Neuronal Sub}}-{{Populations}}},
  author = {Bisio, Marta and Bosca, Alessandro and Pasquale, Valentina and Berdondini, Luca and Chiappalone, Michela},
  editor = {Vasilaki, Eleni},
  year = {2014},
  month = sep,
  volume = {9},
  pages = {e107400},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0107400},
  abstract = {Uniform and modular primary hippocampal cultures from embryonic rats were grown on commercially available microelectrode arrays to investigate network activity with respect to development and integration of different neuronal populations. Modular networks consisting of two confined active and inter-connected sub-populations of neurons were realized by means of bi-compartmental polydimethylsiloxane structures. Spontaneous activity in both uniform and modular cultures was periodically monitored, from three up to eight weeks after plating. Compared to uniform cultures and despite lower cellular density, modular networks interestingly showed higher firing rates at earlier developmental stages, and network-wide firing and bursting statistics were less variable over time. Although globally less correlated than uniform cultures, modular networks exhibited also higher intra-cluster than inter-cluster correlations, thus demonstrating that segregation and integration of activity coexisted in this simple yet powerful in vitro model. Finally, the peculiar synchronized bursting activity shown by confined modular networks preferentially propagated within one of the two compartments (`dominant'), even in cases of perfect balance of firing rate between the two sub-populations. This dominance was generally maintained during the entire monitored developmental frame, thus suggesting that the implementation of this hierarchy arose from early network development.},
  file = {/Users/qualia/Documents/Papers/Bisio et al. - 2014 - Emergence of Bursting Activity in Connected Neuron.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {9}
}

@article{Bjursten1976,
  title = {Behavioural Repertory of Cats without Cerebral Cortex from Infancy},
  author = {Bjursten, L.-M. and Norrsell, K. and Norrsell, U.},
  year = {1976},
  month = may,
  volume = {25},
  issn = {0014-4819, 1432-1106},
  doi = {10.1007/BF00234897},
  abstract = {Bilateral removal of the cerebral cortex was made in cats neonatally. Spontaneous and imposed behaviour was studied while they were growing up and after they had become adult. Special emphasis was put on the utilization of visual cues and on learning. The cats ate, drank and groomed themselves adequately. Adequate maternal and female sexual behaviour was observed. They utilized the visual and haptic senses with respect to external space. Two cats were trained to perform visual discrimination in a T-maze. The adequacy of the behaviour of these cats is compared to that of animals with similar lesions made at maturity.},
  file = {/Users/qualia/Documents/Papers/Bjursten et al. - 1976 - Behavioural repertory of cats without cerebral cor.pdf},
  journal = {Experimental Brain Research},
  language = {en},
  number = {2}
}

@article{Blevins2019,
  title = {On the Reorderability of Node-Filtered Order Complexes},
  author = {Blevins, Ann Sizemore and Bassett, Danielle S.},
  year = {2019},
  month = may,
  abstract = {Growing graphs describe a multitude of developing processes from maturing brains to expanding vocabularies to burgeoning public transit systems. Each of these growing processes likely adheres to proliferation rules that establish an effective order of node and connection emergence. When followed, such proliferation rules allow the system to properly develop along a predetermined trajectory. But rules are rarely followed. Here we ask what topological changes in the growing graph trajectories might occur after the specific but basic perturbation of permuting the node emergence order. Specifically we harness applied topological methods to determine which of six growing graph models exhibit topology that is robust to randomizing node order, termed global reorderability, and robust to temporally-local node swaps, termed local reorderability. We find that the six graph models fall upon a spectrum of both local and global reorderability, and furthermore we provide theoretical connections between robustness to node pair ordering and robustness to arbitrary node orderings. Finally we discuss real-world applications of reorderability analyses and suggest possibilities for designing reorderable networks.},
  archivePrefix = {arXiv},
  eprint = {1905.02330},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Blevins and Bassett - 2019 - On the reorderability of node-filtered order compl.pdf},
  journal = {arXiv:1905.02330 [math, q-bio]},
  keywords = {55U10,Mathematics - Algebraic Topology,Quantitative Biology - Quantitative Methods},
  language = {en},
  primaryClass = {math, q-bio}
}

@article{Bogacz2007,
  title = {The {{Basal Ganglia}} and {{Cortex Implement Optimal Decision Making Between Alternative Actions}}},
  author = {Bogacz, Rafal and Gurney, Kevin},
  year = {2007},
  month = feb,
  volume = {19},
  pages = {442--477},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.19.2.442},
  file = {/Users/qualia/Documents/Papers/2007 - Bogacz, Gurney - The Basal Ganglia and Cortex Implement Optimal Decision Making Between Alternative Actions.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Bolcskei2019,
  title = {Optimal {{Approximation}} with {{Sparsely Connected Deep Neural Networks}}},
  author = {B{\"o}lcskei, Helmut and Grohs, Philipp and Kutyniok, Gitta and Petersen, Philipp},
  year = {2019},
  month = jan,
  volume = {1},
  pages = {8--45},
  issn = {2577-0187},
  doi = {10.1137/18M118709X},
  abstract = {We derive fundamental lower bounds on the connectivity and the memory requirements of deep neural networks guaranteeing uniform approximation rates for arbitrary function classes in L2(Rd). In other words, we establish a connection between the complexity of a function class and the complexity of deep neural networks approximating functions from this class to within a prescribed accuracy. Additionally, we prove that our lower bounds are achievable for a broad family of function classes. Specifically, all function classes that are optimally approximated by a general class of representation systems\textemdash{}so-called affine systems\textemdash{}can be approximated by deep neural networks with minimal connectivity and memory requirements. Affine systems encompass a wealth of representation systems from applied harmonic analysis such as wavelets, ridgelets, curvelets, shearlets, {$\alpha$}-shearlets, and, more generally, {$\alpha$}-molecules. Our central result elucidates a remarkable universality property of neural networks and shows that they achieve the optimum approximation properties of all affine systems combined. As a specific example, we consider the class of {$\alpha{}-$}1-cartoon-like functions, which is approximated optimally by {$\alpha$}-shearlets. We also explain how our results can be extended to the approximation of functions on low-dimensional immersed manifolds. Finally, we present numerical experiments demonstrating that the standard stochastic gradient descent algorithm yields deep neural networks with close-to-optimal approximation rates. Moreover, these results indicate that stochastic gradient descent can learn approximations that are sparse in the representation systems optimally sparsifying the function class the network is trained on.},
  file = {/Users/qualia/Documents/Papers/Bölcskei et al. - 2019 - Optimal Approximation with Sparsely Connected Deep.pdf},
  journal = {SIAM Journal on Mathematics of Data Science},
  language = {en},
  number = {1}
}

@article{Bollimunta2008,
  title = {Neuronal {{Mechanisms}} of {{Cortical Alpha Oscillations}} in {{Awake}}-{{Behaving Macaques}}},
  author = {Bollimunta, A. and Chen, Y. and Schroeder, C. E. and Ding, M.},
  year = {2008},
  month = oct,
  volume = {28},
  pages = {9976--9988},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2699-08.2008},
  file = {/Users/qualia/Documents/Papers/2008 - Bollimunta et al. - Neuronal mechanisms of cortical alpha oscillations in awake-behaving macaques.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {40}
}

@article{Bonnefond2012,
  title = {Alpha {{Oscillations Serve}} to {{Protect Working Memory Maintenance}} against {{Anticipated Distracters}}},
  author = {Bonnefond, Mathilde and Jensen, Ole},
  year = {2012},
  month = oct,
  volume = {22},
  pages = {1969--1974},
  issn = {09609822},
  doi = {10.1016/j.cub.2012.08.029},
  abstract = {When operating in a complex world, it is essential to have mechanisms that can suppress distracting information [1, 2]. Such mechanisms might be related to neuronal oscillations, which are known to be involved in gating of incoming information [3]. We here apply a working memory (WM) task to investigate how neuronal oscillations are involved in the suppression of distracting information that can be predicted in time. We used a modified Sternberg WM task in which distracters were presented in the retention interval, while we recorded the ongoing brain activity using magnetoencephalography. The data revealed a robust adjustment of the phase of alpha oscillations in anticipation of the distracter. In trials with strong phase adjustment, response times to the memory probe were reduced. Further, the power of alpha oscillations increased prior to the distracter and predicted performance. Our findings demonstrate that the doors of perception close when a distracter is expected. The phase adjustment of the alpha rhythm adds to the computational versatility of brain oscillations, because such a mechanism allows for modulating neuronal processing on a fine temporal scale.},
  file = {/Users/qualia/Documents/Papers/2012 - Bonnefond, Jensen - Alpha oscillations serve to protect working memory maintenance against anticipated distracters.pdf},
  journal = {Current Biology},
  language = {en},
  number = {20}
}

@article{Bontrager2019,
  title = {Superstition in the {{Network}}: {{Deep Reinforcement Learning Plays Deceptive Games}}},
  shorttitle = {Superstition in the {{Network}}},
  author = {Bontrager, Philip and Khalifa, Ahmed and Anderson, Damien and Stephenson, Matthew and Salge, Christoph and Togelius, Julian},
  year = {2019},
  month = aug,
  abstract = {Deep reinforcement learning has learned to play many games well, but failed on others. To better characterize the modes and reasons of failure of deep reinforcement learners, we test the widely used Asynchronous Actor-Critic (A2C) algorithm on four deceptive games, which are specially designed to provide challenges to game-playing agents. These games are implemented in the General Video Game AI framework, which allows us to compare the behavior of reinforcement learningbased agents with planning agents based on tree search. We find that several of these games reliably deceive deep reinforcement learners, and that the resulting behavior highlights the shortcomings of the learning algorithm. The particular ways in which agents fail differ from how planning-based agents fail, further illuminating the character of these algorithms. We propose an initial typology of deceptions which could help us better understand pitfalls and failure modes of (deep) reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1908.04436},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Bontrager et al. - 2019 - Superstition in the Network Deep Reinforcement Le.pdf},
  journal = {arXiv:1908.04436 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Bopp2005,
  title = {Aging and {{Verbal Memory Span}}: {{A Meta}}-{{Analysis}}},
  shorttitle = {Aging and {{Verbal Memory Span}}},
  author = {Bopp, K. L. and Verhaeghen, P.},
  year = {2005},
  month = sep,
  volume = {60},
  pages = {P223-P233},
  issn = {1079-5014, 1758-5368},
  doi = {10.1093/geronb/60.5.P223},
  file = {/Users/qualia/Documents/Papers/2005 - Bopp, Verhaeghen - Aging and verbal memory span A meta-analysis.pdf},
  journal = {The Journals of Gerontology Series B: Psychological Sciences and Social Sciences},
  language = {en},
  number = {5}
}

@article{Borella2008,
  title = {Working Memory and Inhibition across the Adult Life-Span},
  author = {Borella, Erika and Carretti, Barbara and De Beni, Rossana},
  year = {2008},
  month = may,
  volume = {128},
  pages = {33--44},
  issn = {00016918},
  doi = {10.1016/j.actpsy.2007.09.008},
  abstract = {Research has shown that age-related changes in cognitive performance are due mostly to the decline of general factors such as working memory and inhibition. The present study is aimed at investigating age-related changes in these mechanisms across the adult life-span from 20 to 86 years of age. Results indicate a linear relationship between each working memory measure and age, independently of the nature of the task, and a quadratic relationship between the single inhibitory measures and age. Moreover, hierarchical regression analyses show that inhibition accounts for a significant, but modest, part of the age-related variance in working memory. Taken together, these results suggest that inhibition is not as crucial a contributor of age-related changes in the functional capacity of working memory across the adult life-span as previously thought.},
  file = {/Users/qualia/Documents/Papers/2008 - Borella, Carretti, De Beni - Working memory and inhibition across the adult life-span.pdf},
  journal = {Acta Psychologica},
  language = {en},
  number = {1}
}

@article{Borgers2005,
  title = {Background Gamma Rhythmicity and Attention in Cortical Local Circuits: {{A}} Computational Study},
  shorttitle = {Background Gamma Rhythmicity and Attention in Cortical Local Circuits},
  author = {Borgers, C. and Epstein, S. and Kopell, N. J.},
  year = {2005},
  month = may,
  volume = {102},
  pages = {7002--7007},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0502366102},
  file = {/Users/qualia/Documents/Papers/Borgers et al. - 2005 - Background gamma rhythmicity and attention in cort.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {19}
}

@article{Borgers2008,
  title = {Gamma Oscillations Mediate Stimulus Competition and Attentional Selection in a Cortical Network Model},
  author = {Borgers, C. and Epstein, S. and Kopell, N. J.},
  year = {2008},
  month = nov,
  volume = {105},
  pages = {18023--18028},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0809511105},
  file = {/Users/qualia/Documents/Papers/2008 - Börgers, Epstein, Kopell - Gamma oscillations mediate stimulus competition and attentional selection in a cortical network model.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {46}
}

@article{Borgers2014,
  title = {Approximate, Not {{Perfect Synchrony Maximizes}} the {{Downstream Effectiveness}} of {{Excitatory Neuronal Ensembles}}},
  author = {B{\"o}rgers, Christoph and Li, Jie and Kopell, Nancy},
  year = {2014},
  volume = {4},
  pages = {10},
  issn = {2190-8567},
  doi = {10.1186/2190-8567-4-10},
  abstract = {The most basic functional role commonly ascribed to synchrony in the brain is that of amplifying excitatory neuronal signals. The reasoning is straightforward: When positive charge is injected into a leaky target neuron over a time window of positive duration, some of it will have time to leak back out before an action potential is triggered in the target, and it will in that sense be wasted. If the goal is to elicit a firing response in the target using as little charge as possible, it seems best to deliver the charge all at once, i.e., in perfect synchrony. In this article, we show that this reasoning is correct only if one assumes that the input ceases when the target crosses the firing threshold, but before it actually fires. If the input ceases later\textemdash{}for instance, in response to a feedback signal triggered by the firing of the target\textemdash{}the ``most economical'' way of delivering input (the way that requires the least total amount of input) is no longer precisely synchronous, but merely approximately so. If the target is a heterogeneous network, as it always is in the brain, then ceasing the input ``when the target crosses the firing threshold'' is not an option, because there is no single moment when the firing threshold is crossed. In this sense, precise synchrony is never optimal in the brain.},
  file = {/Users/qualia/Documents/Papers/Börgers et al. - 2014 - Approximate, not Perfect Synchrony Maximizes the D.pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Borst1999,
  title = {Information Theory and Neural Coding},
  author = {Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {1999},
  month = nov,
  volume = {2},
  pages = {947--957},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/14731},
  file = {/Users/qualia/Documents/Papers/1999 - Borst, Theunissen - Information theory and neural coding.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Borst2015,
  title = {Common Circuit Design in Fly and Mammalian Motion Vision},
  author = {Borst, Alexander and Helmstaedter, Moritz},
  year = {2015},
  month = aug,
  volume = {18},
  pages = {1067--1076},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4050},
  file = {/Users/qualia/Documents/Papers/2015 - Borst, Helmstaedter - Common circuit design in fly and mammalian motion vision.pdf;/Users/qualia/Documents/Papers/Borst and Helmstaedter - 2015 - Common circuit design in fly and mammalian motion .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {8}
}

@article{Bosco2015,
  title = {Correlational Effect Size Benchmarks.},
  author = {Bosco, Frank A. and Aguinis, Herman and Singh, Kulraj and Field, James G. and Pierce, Charles A.},
  year = {2015},
  month = mar,
  volume = {100},
  pages = {431--449},
  issn = {1939-1854, 0021-9010},
  doi = {10.1037/a0038047},
  abstract = {Effect size information is essential for the scientific enterprise and plays an increasingly central role in the scientific process. We extracted 147,328 correlations and developed a hierarchical taxonomy of variables reported in Journal of Applied Psychology and Personnel Psychology from 1980 to 2010 to produce empirical effect size benchmarks at the omnibus level, for 20 common research domains, and for an even finer grained level of generality. Results indicate that the usual interpretation and classification of effect sizes as small, medium, and large bear almost no resemblance to findings in the field, because distributions of effect sizes exhibit tertile partitions at values approximately one-half to one-third those intuited by Cohen (1988). Our results offer information that can be used for research planning and design purposes, such as producing better informed non-nil hypotheses and estimating statistical power and planning sample size accordingly. We also offer information useful for understanding the relative importance of the effect sizes found in a particular study in relationship to others and which research domains have advanced more or less, given that larger effect sizes indicate a better understanding of a phenomenon. Also, our study offers information about research domains for which the investigation of moderating effects may be more fruitful and provide information that is likely to facilitate the implementation of Bayesian analysis. Finally, our study offers information that practitioners can use to evaluate the relative effectiveness of various types of interventions.},
  file = {/Users/qualia/Documents/Papers/Bosco et al. - 2015 - Correlational effect size benchmarks..pdf},
  journal = {Journal of Applied Psychology},
  language = {en},
  number = {2}
}

@article{Bostwick2020,
  title = {Antagonistic {{Inhibitory Circuits Integrate Visual}} and {{Gravitactic Behaviors}}},
  author = {Bostwick, Michaela and Smith, Eleanor L. and Borba, Cezar and {Newman-Smith}, Erin and Guleria, Iraa and Kourakis, Matthew J. and Smith, William C.},
  year = {2020},
  month = feb,
  volume = {30},
  pages = {600-609.e2},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.12.017},
  file = {/Users/qualia/Documents/Papers/Bostwick et al. - 2020 - Antagonistic Inhibitory Circuits Integrate Visual .pdf},
  journal = {Current Biology},
  language = {en},
  number = {4}
}

@article{Boughman2002,
  title = {How Sensory Drive Can Promote Speciation},
  author = {Boughman, Janette Wenrick},
  year = {2002},
  month = dec,
  volume = {17},
  pages = {571--577},
  issn = {01695347},
  doi = {10.1016/S0169-5347(02)02595-8},
  file = {/Users/qualia/Documents/Papers/Boughman - 2002 - How sensory drive can promote speciation.pdf},
  journal = {Trends in Ecology \& Evolution},
  language = {en},
  number = {12}
}

@article{Bourdoukan,
  title = {Enforcing Balance Allows Local Supervised Learning in Spiking Recurrent Networks},
  author = {Bourdoukan, Ralph and Den{\`e}ve, Sophie},
  pages = {9},
  abstract = {To predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback. However, it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible. Furthermore, reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.},
  file = {/Users/qualia/Documents/Papers/2015 - Bourdoukan, Deneve - Enforcing balance allows local supervised learning in spiking recurrent networks.pdf;/Users/qualia/Documents/Papers/Bourdoukan and Denève - Enforcing balance allows local supervised learning.pdf},
  language = {en}
}

@article{Bowling,
  title = {Rational and {{Convergent Learning}} in {{Stochastic Games}}},
  author = {Bowling, Michael and Veloso, Manuela},
  pages = {6},
  abstract = {This paper investigates the problem of policy learning in multiagent environments using the stochastic game framework, which we briefly overview. We introduce two properties as desirable for a learning agent when in the presence of other learning agents, namely rationality and convergence. We examine existing reinforcement learning algorithms according to these two properties and notice that they fail to simultaneously meet both criteria. We then contribute a new learning algorithm, WoLF policy hillclimbing, that is based on a simple principle: ``learn quickly while losing, slowly while winning.'' The algorithm is proven to be rational and we present empirical results for a number of stochastic games showing the algorithm converges.},
  file = {/Users/qualia/Documents/Papers/2001 - Bowling, Veloso - Rational and convergent learning in stochastic games.pdf},
  language = {en}
}

@article{Bowling2002,
  title = {Multiagent Learning Using a Variable Learning Rate},
  author = {Bowling, Michael and Veloso, Manuela},
  year = {2002},
  month = apr,
  volume = {136},
  pages = {215--250},
  issn = {00043702},
  doi = {10.1016/S0004-3702(02)00121-2},
  abstract = {Learning to act in a multiagent environment is a difficult problem since the normal definition of an optimal policy no longer applies. The optimal policy at any moment depends on the policies of the other agents. This creates a situation of learning a moving target. Previous learning algorithms have one of two shortcomings depending on their approach. They either converge to a policy that may not be optimal against the specific opponents' policies, or they may not converge at all. In this article we examine this learning problem in the framework of stochastic games. We look at a number of previous learning algorithms showing how they fail at one of the above criteria. We then contribute a new reinforcement learning technique using a variable learning rate to overcome these shortcomings. Specifically, we introduce the WoLF principle, ``Win or Learn Fast,'' for varying the learning rate. We examine this technique theoretically, proving convergence in self-play on a restricted class of iterated matrix games. We also present empirical results on a variety of more general stochastic games, in situations of self-play and otherwise, demonstrating the wide applicability of this method.},
  file = {/Users/qualia/Documents/Papers/2002 - Bowling, Veloso - Multiagent learning using a variable learning rate.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {2}
}

@article{Brafman,
  title = {R-Max \textendash{} {{A General Polynomial Time Algorithm}} for {{Near}}-{{Optimal Reinforcement Learning}}},
  author = {Brafman, Ronen I and Tennenholtz, Moshe},
  pages = {19},
  abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
  file = {/Users/qualia/Documents/Papers/Brafman and Tennenholtz - R-max – A General Polynomial Time Algorithm for Ne 2.pdf},
  language = {en}
}

@article{Brafman2002,
  title = {R-Max \textendash{} {{A General Polynomial Time Algorithm}} for {{Near}}-{{Optimal Reinforcement Learning}}},
  author = {Brafman, Ronen I and Tennenholtz, Moshe},
  year = {2002},
  volume = {2},
  pages = {19},
  abstract = {R-max is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-max, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-max improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
  file = {/Users/qualia/Documents/Papers/Brafman and Tennenholtz - R-max – A General Polynomial Time Algorithm for Ne.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@book{Braun1983,
  title = {Differential {{Equation Models}}},
  author = {Braun, Martin and Coleman, Courtney S and Drew, Donald A},
  year = {1983},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  abstract = {The purpose of this four volume series is to make available for college teachers and students samples of important and realistic applications of mathematics which can be covered in undergraduate programs. The goal is to provide illustrations of how modem mathematics is actually employed to solve relevant contemporary problems. Although these independent chapters were prepared primarily for teachers in the general mathematical sciences, they should prove valuable to students, teachers, and research scientists in many of the fields of application as well. Prerequisites for each chapter and suggestions for the teacher are provided. Several of these chapters have been tested in a variety of classroom settings, and all have undergone extensive peer review and revision. Illustrations and exercises are included in most chapters. Some units can be covered in one class, whereas others provide sufficient material for a few weeks of class time. Volume 1 contains 23 chapters and deals with differential equations and, in the last four chapters, problems leading to partial differential equations. Applications are taken from medicine, biology, traffic systems and several other fields. The 14 chapters in Volume 2 are devoted mostly to problems arising in political science, but they also address questions appearing in sociology and ecology. Topics covered include voting systems, weighted voting, proportional representation, coalitional values, and committees. The 14 chapters in Volume 3 emphasize discrete mathematical methods such as those which arise in graph theory, combinatorics, and networks.},
  file = {/Users/qualia/Documents/Papers/1983 - Lucas - Differential Equation Models.pdf;/Users/qualia/Documents/Papers/Braun et al. - 1983 - Differential Equation Models.pdf},
  isbn = {978-1-4612-5427-0 978-1-4612-5429-4},
  language = {en},
  note = {OCLC: 840280008}
}

@article{Breakspear2003,
  title = {Modulation of Excitatory Synaptic Coupling Facilitates Synchronization and Complex Dynamics in a Nonlinear Model of Neuronal Dynamics},
  author = {Breakspear, Michael and R. Terry, John and J. Friston, Karl},
  year = {2003},
  month = jun,
  volume = {52-54},
  pages = {151--158},
  issn = {09252312},
  doi = {10.1016/S0925-2312(02)00740-3},
  abstract = {We study dynamical synchronization in a model of a neural system constituted by local networks of densely interconnected excitatory and inhibitory neurons. Neural dynamics are determined by voltage- and ligand-gated ion channels. Coupling between the local networks is introduced via sparse excitatory connectivity. With modulation of this long-range synaptic coupling the system undergoes a transition from independent oscillations to chaotic synchronization. Between these states exists a 'weakly' stable state with epochs of synchronization and complex intermittent desynchronization. This may facilitate adaptive brain function by engendering a diverse repertoire of dynamics and contribute to the genesis of complexity in the EEG.},
  file = {/Users/qualia/Documents/Papers/2003 - Breakspear, Terry, Friston - Modulation of excitatory synaptic coupling facilitates synchronization and complex dynamics in a bio.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{Bressler2003,
  title = {Cortical {{Coordination Dynamics}} and the {{Disorganization Syndrome}} in {{Schizophrenia}}},
  author = {Bressler, Steven L},
  year = {2003},
  month = jul,
  volume = {28},
  pages = {S35-S39},
  issn = {0893-133X, 1740-634X},
  doi = {10.1038/sj.npp.1300145},
  file = {/Users/qualia/Documents/Papers/2003 - Bressler - Cortical Coordination Dynamics and the Disorganization Syndrome in Schizophrenia.pdf},
  journal = {Neuropsychopharmacology},
  language = {en},
  number = {S1}
}

@article{Brette2007,
  title = {Simulation of Networks of Spiking Neurons: {{A}} Review of Tools and Strategies},
  shorttitle = {Simulation of Networks of Spiking Neurons},
  author = {Brette, Romain and Rudolph, Michelle and Carnevale, Ted and Hines, Michael and Beeman, David and Bower, James M. and Diesmann, Markus and Morrison, Abigail and Goodman, Philip H. and Harris, Frederick C. and Zirpe, Milind and Natschl{\"a}ger, Thomas and Pecevski, Dejan and Ermentrout, Bard and Djurfeldt, Mikael and Lansner, Anders and Rochel, Olivier and Vieville, Thierry and Muller, Eilif and Davison, Andrew P. and El Boustani, Sami and Destexhe, Alain},
  year = {2007},
  month = dec,
  volume = {23},
  pages = {349--398},
  issn = {1573-6873},
  doi = {10.1007/s10827-007-0038-6},
  file = {/Users/qualia/Documents/Papers/2007 - Brette et al. - Simulation of networks of spiking neurons a review of tools and strategies.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Brette2013,
  title = {An Ecological Approach to Neural Computation},
  author = {Brette, Romain},
  year = {2013},
  month = jul,
  volume = {14},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-14-S1-P40},
  file = {/Users/qualia/Documents/Papers/2013 - Brette - An ecological approach to neural computation.pdf},
  journal = {BMC Neuroscience},
  language = {en},
  number = {S1}
}

@article{Brette2015,
  title = {What {{Is}} the {{Most Realistic Single}}-{{Compartment Model}} of {{Spike Initiation}}?},
  author = {Brette, Romain},
  editor = {Pillow, Jonathan W.},
  year = {2015},
  month = apr,
  volume = {11},
  pages = {e1004114},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004114},
  file = {/Users/qualia/Documents/Papers/Brette - 2015 - What Is the Most Realistic Single-Compartment Mode.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {4}
}

@techreport{Brette2017,
  title = {Is Coding a Relevant Metaphor for the Brain?},
  author = {Brette, Romain},
  year = {2017},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/168237},
  abstract = {"Neural coding" is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the neural code depends on experimental details that are not carried by the coding variable. Thus, the representational power of neural codes is much more limited than generally implied. Second, neural codes carry information only by reference to things with known meaning. In contrast, perceptual systems must build information from relations between sensory signals and actions, forming a structured internal model. Neural codes are inadequate for this purpose because they are unstructured. Third, coding variables are observables tied to the temporality of experiments, while spikes are timed actions that mediate coupling in a distributed dynamical system. The coding metaphor tries to fit the dynamic, circular and distributed causal structure of the brain into a linear chain of transformations between observables, but the two causal structures are incongruent. I conclude that the neural coding metaphor cannot provide a basis for theories of brain function, because it is incompatible with both the causal structure of the brain and the informational requirements of cognition.},
  file = {/Users/qualia/Documents/Papers/Brette - 2017 - Is coding a relevant metaphor for the brain.pdf},
  language = {en},
  type = {Preprint}
}

@article{Briggs2010,
  title = {Organizing Principles of Cortical Layer 6},
  author = {{Briggs}},
  year = {2010},
  issn = {16625110},
  doi = {10.3389/neuro.04.003.2010},
  file = {/Users/qualia/Documents/Papers/2010 - Briggs - Organizing principles of cortical layer 6.pdf},
  journal = {Frontiers in Neural Circuits},
  language = {en}
}

@article{Brittain2014,
  title = {The Highs and Lows of Beta Activity in Cortico-Basal Ganglia Loops},
  author = {Brittain, John-Stuart and Sharott, Andrew and Brown, Peter},
  year = {2014},
  month = jun,
  volume = {39},
  pages = {1951--1959},
  issn = {0953816X},
  doi = {10.1111/ejn.12574},
  abstract = {Oscillatory activity in the beta (13\textendash{}30 Hz) frequency band is widespread in cortico-basal ganglia circuits, and becomes prominent in Parkinson's disease (PD). Here we develop the hypothesis that the degree of synchronization in this frequency band is a critical factor in gating computation across a population of neurons, with increases in beta band synchrony entailing a loss of information-coding space and hence computational capacity. Task and context drive this dynamic gating, so that for each state there will be an optimal level of network synchrony, and levels lower or higher than this will impair behavioural performance. Thus, both the pathological exaggeration of synchrony, as observed in PD, and the ability of interventions like deep brain stimulation (DBS) to excessively suppress synchrony can potentially lead to impairments in behavioural performance. Indeed, under physiological conditions, the manipulation of computational capacity by beta activity may itself present a mechanism of action selection and maintenance.},
  file = {/Users/qualia/Documents/Papers/2014 - Brittain, Sharott, Brown - The highs and lows of beta activity in cortico-basal ganglia loops.pdf;/Users/qualia/Documents/Papers/2014 - Brittain, Sharott, Brown - The highs and lows of beta activity in cortico-basal ganglia loops(2).pdf;/Users/qualia/Documents/Papers/Brittain et al. - 2014 - The highs and lows of beta activity in cortico-bas 2.pdf;/Users/qualia/Documents/Papers/Brittain et al. - 2014 - The highs and lows of beta activity in cortico-bas.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {11}
}

@article{Brockwell2004,
  title = {Recursive {{Bayesian Decoding}} of {{Motor Cortical Signals}} by {{Particle Filtering}}},
  author = {Brockwell, A. E. and Rojas, A. L. and Kass, R. E.},
  year = {2004},
  month = apr,
  volume = {91},
  pages = {1899--1907},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00438.2003},
  file = {/Users/qualia/Documents/Papers/2004 - Brockwell, Rojas, Kass - Recursive Bayesian Decoding of Motor Cortical Signals by Particle Filtering.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Brogden1939,
  title = {Higher {{Order Conditioning}}},
  author = {Brogden, W. J.},
  year = {1939},
  month = oct,
  volume = {52},
  pages = {579},
  issn = {00029556},
  doi = {10.2307/1416470},
  file = {/Users/qualia/Documents/Papers/2012 - Press - Higher Order Conditioning Author ( s ) W . J . Brogden Reviewed work ( s ) Source The American Journal of Psychology , V.pdf},
  journal = {The American Journal of Psychology},
  language = {en},
  number = {4}
}

@article{Brown2005,
  title = {A {{Ballistic Model}} of {{Choice Response Time}}.},
  author = {Brown, Scott and Heathcote, Andrew},
  year = {2005},
  volume = {112},
  pages = {117--128},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.112.1.117},
  file = {/Users/qualia/Documents/Papers/2005 - Brown, Heathcote - A ballistic model of choice response time.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {1}
}

@article{Brozovic2008,
  title = {Mechanism of Gain Modulation at Single Neuron and Network Levels},
  author = {Brozovi{\'c}, M. and Abbott, L. F. and Andersen, R. A.},
  year = {2008},
  month = aug,
  volume = {25},
  pages = {158--168},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-007-0070-6},
  abstract = {Gain modulation, in which the sensitivity of a neural response to one input is modified by a second input, is studied at single-neuron and network levels. At the single neuron level, gain modulation can arise if the two inputs are subject to a direct multiplicative interaction. Alternatively, these inputs can be summed in a linear manner by the neuron and gain modulation can arise, instead, from a nonlinear input\textendash{}output relationship. We derive a mathematical constraint that can distinguish these two mechanisms even though they can look very similar, provided sufficient data of the appropriate type are available. Previously, it has been shown in coordinate transformation studies that artificial neurons with sigmoid transfer functions can acquire a nonlinear additive form of gain modulation through learning-driven adjustment of synaptic weights. We use the constraint derived for single-neuron studies to compare responses in this network with those of another network model based on a biologically inspired transfer function that can support approximately multiplicative interactions.},
  file = {/Users/qualia/Documents/Papers/2008 - Brozovi, Abbott, Andersen - Mechanism of gain modulation at single neuron and network levels.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Brunel2003,
  title = {What {{Determines}} the {{Frequency}} of {{Fast Network Oscillations With Irregular Neural Discharges}}? {{I}}. {{Synaptic Dynamics}} and {{Excitation}}-{{Inhibition Balance}}},
  shorttitle = {What {{Determines}} the {{Frequency}} of {{Fast Network Oscillations With Irregular Neural Discharges}}?},
  author = {Brunel, Nicolas and Wang, Xiao-Jing},
  year = {2003},
  month = jul,
  volume = {90},
  pages = {415--430},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.01095.2002},
  file = {/Users/qualia/Documents/Papers/2003 - Brunel, Wang - What determines the frequency of fast network oscillations with irregular neural discharges I. Synaptic dynamics a.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Brunet2014,
  title = {Gamma or No Gamma, That Is the Question},
  author = {Brunet, Nicolas and Vinck, Martin and Bosman, Conrado A. and Singer, Wolf and Fries, Pascal},
  year = {2014},
  month = oct,
  volume = {18},
  pages = {507--509},
  issn = {13646613},
  doi = {10.1016/j.tics.2014.08.006},
  file = {/Users/qualia/Documents/Papers/Brunet et al. - 2014 - Gamma or no gamma, that is the question.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {10}
}

@article{Brunner2018,
  title = {Using {{State Predictions}} for {{Value Regularization}} in {{Curiosity Driven Deep Reinforcement Learning}}},
  author = {Brunner, Gino and Fritsche, Manuel and Richter, Oliver and Wattenhofer, Roger},
  year = {2018},
  month = sep,
  abstract = {Learning in sparse reward settings remains a challenge in Reinforcement Learning, which is often addressed by using intrinsic rewards. One promising strategy is inspired by human curiosity, requiring the agent to learn to predict the future. In this paper a curiosity-driven agent is extended to use these predictions directly for training. To achieve this, the agent predicts the value function of the next state at any point in time. Subsequently, the consistency of this prediction with the current value function is measured, which is then used as a regularization term in the loss function of the algorithm. Experiments were made on grid-world environments as well as on a 3D navigation task, both with sparse rewards. In the first case the extended agent is able to learn significantly faster than the baselines.},
  archivePrefix = {arXiv},
  eprint = {1810.00361},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Brunner et al. - 2018 - Using State Predictions for Value Regularization i.pdf},
  journal = {arXiv:1810.00361 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Bruno2006,
  title = {Cortex {{Is Driven}} by {{Weak}} but {{Synchronously Active Thalamocortical Synapses}}},
  author = {Bruno, R. M.},
  year = {2006},
  month = jun,
  volume = {312},
  pages = {1622--1627},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1124593},
  file = {/Users/qualia/Documents/Papers/2006 - Bruno, Sakmann - Cortex is driven by weak but synchronously active thalamocortical synapses.pdf},
  journal = {Science},
  language = {en},
  number = {5780}
}

@article{Buesing2010,
  title = {A {{Spiking Neuron}} as {{Information Bottleneck}}},
  author = {Buesing, Lars and Maass, Wolfgang},
  year = {2010},
  month = aug,
  volume = {22},
  pages = {1961--1992},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2010.08-09-1084},
  file = {/Users/qualia/Documents/Papers/2010 - Buesing, Maass - A Spiking Neuron as Information Bottleneck.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Bunzeck2006,
  title = {Absolute {{Coding}} of {{Stimulus Novelty}} in the {{Human Substantia Nigra}}/{{VTA}}},
  author = {Bunzeck, Nico and D{\"u}zel, Emrah},
  year = {2006},
  month = aug,
  volume = {51},
  pages = {369--379},
  issn = {08966273},
  doi = {10.1016/j.neuron.2006.06.021},
  abstract = {Novelty exploration can enhance hippocampal plasticity in animals through dopaminergic neuromodulation arising in the substantia nigra/ventral tegmental area (SN/VTA). This enhancement can outlast the exploration phase by several minutes. Currently, little is known about dopaminergic novelty processing and its relationship to hippocampal function in humans. In two functional magnetic resonance imaging (fMRI) studies, SN/VTA activations in humans were indeed driven by stimulus novelty rather than other forms of stimulus salience such as rareness, negative emotional valence, or targetness of familiar stimuli, whereas hippocampal responses were less selective. SN/VTA novelty responses were scaled according to absolute rather than relative novelty in a given context, unlike adaptive SN/VTA responses recently reported for reward outcome in animal studies. Finally, novelty enhanced learning and perirhinal/parahippocampal processing of familiar items presented in the same context. Thus, the human SN/VTA can code absolute stimulus novelty and might contribute to enhancing learning in the context of novelty.},
  file = {/Users/qualia/Documents/Papers/2006 - Bunzeck, Düzel - Absolute Coding of Stimulus Novelty in the Human Substantia NigraVTA.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Buonomano1995,
  title = {Temporal Information Transformed into a Spatial Code by a Neural Network with Realistic Properties},
  author = {Buonomano, D. and Merzenich, M.},
  year = {1995},
  month = feb,
  volume = {267},
  pages = {1028--1030},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.7863330},
  file = {/Users/qualia/Documents/Papers/1995 - Buonomano, Mauk - Temporal information transformed into a spatial code by a neural network with realistic properties.pdf},
  journal = {Science},
  language = {en},
  number = {5200}
}

@article{Buonomano1998,
  title = {Net {{Interaction Between Different Forms}} of {{Short}}-{{Term Synaptic Plasticity}} and {{Slow}}-{{IPSPs}} in the {{Hippocampus}} and {{Auditory Cortex}}},
  author = {Buonomano, Dean V. and Merzenich, Michael M.},
  year = {1998},
  month = oct,
  volume = {80},
  pages = {1765--1774},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.80.4.1765},
  file = {/Users/qualia/Documents/Papers/1998 - Buonomano, Merzenich - Net interaction between different forms of short-term synaptic plasticity and slow-IPSPs in the hippocampu.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Buonomano1999,
  title = {A {{Neural Network Model}} of {{Temporal Code Generation}} and {{Position}}-{{Invariant Pattern Recognition}}},
  author = {Buonomano, Dean V. and Merzenich, Michael},
  year = {1999},
  month = jan,
  volume = {11},
  pages = {103--116},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300016836},
  file = {/Users/qualia/Documents/Papers/1999 - Buonomano, Merzenich - A neural network model of temporal code generation and position- invariant pattern recognition.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {1}
}

@article{Buonomano2009,
  title = {Harnessing {{Chaos}} in {{Recurrent Neural Networks}}},
  author = {Buonomano, Dean V.},
  year = {2009},
  month = aug,
  volume = {63},
  pages = {423--425},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.08.003},
  file = {/Users/qualia/Documents/Papers/2009 - Buonomano - Harnessing Chaos in Recurrent Neural Networks.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Burch1959,
  title = {Automatic Analysis of the Electroencephalogram: A Review and Classification of Systems},
  shorttitle = {Automatic Analysis of the Electroencephalogram},
  author = {Burch, Neil R},
  year = {1959},
  month = nov,
  volume = {11},
  pages = {827--834},
  issn = {00134694},
  doi = {10.1016/0013-4694(59)90133-6},
  file = {/Users/qualia/Documents/Papers/1958 - Burch - ANALYSIS OF THE ELECTROENCEPHALOGRAM.pdf;/Users/qualia/Documents/Papers/Burch - 1959 - Automatic analysis of the electroencephalogram a .pdf},
  journal = {Electroencephalography and Clinical Neurophysiology},
  language = {en},
  number = {4}
}

@article{Burda2018,
  title = {Large-{{Scale Study}} of {{Curiosity}}-{{Driven Learning}}},
  author = {Burda, Yuri and Edwards, Harri and Pathak, Deepak and Storkey, Amos and Darrell, Trevor and Efros, Alexei A.},
  year = {2018},
  month = aug,
  abstract = {Reinforcement learning algorithms rely on carefully engineering environment rewards that are extrinsic to the agent. However, annotating each environment with hand-designed, dense rewards is not scalable, motivating the need for developing reward functions that are intrinsic to the agent. Curiosity is a type of intrinsic reward function which uses prediction error as reward signal. In this paper: (a) We perform the first large-scale study of purely curiosity-driven learning, i.e. without any extrinsic rewards, across 54 standard benchmark environments, including the Atari game suite. Our results show surprisingly good performance, and a high degree of alignment between the intrinsic curiosity objective and the handdesigned extrinsic rewards of many game environments. (b) We investigate the effect of using different feature spaces for computing prediction error and show that random features are sufficient for many popular RL game benchmarks, but learned features appear to generalize better (e.g. to novel game levels in Super Mario Bros.). (c) We demonstrate limitations of the prediction-based rewards in stochastic setups. Game-play videos and code are at https://pathak22.github. io/large-scale-curiosity/.},
  archivePrefix = {arXiv},
  eprint = {1808.04355},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Burda et al. - 2018 - Large-Scale Study of Curiosity-Driven Learning.pdf},
  journal = {arXiv:1808.04355 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Burda2018a,
  title = {Exploration by {{Random Network Distillation}}},
  author = {Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},
  year = {2018},
  month = oct,
  abstract = {We introduce an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed. The bonus is the error of a neural network predicting features of the observations given by a fixed randomly initialized neural network. We also introduce a method to flexibly combine intrinsic and extrinsic rewards. We find that the random network distillation (RND) bonus combined with this increased flexibility enables significant progress on several hard exploration Atari games. In particular we establish state of the art performance on Montezuma's Revenge, a game famously difficult for deep reinforcement learning methods. To the best of our knowledge, this is the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state of the game, and occasionally completes the first level.},
  archivePrefix = {arXiv},
  eprint = {1810.12894},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Burda et al. - 2018 - Exploration by Random Network Distillation.pdf},
  journal = {arXiv:1810.12894 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Burgelman2002,
  title = {Strategy as {{Vector}} and the {{Inertia}} of {{Coevolutionary Lock}}-In},
  author = {Burgelman, Robert A.},
  year = {2002},
  month = jun,
  volume = {47},
  pages = {325},
  issn = {00018392},
  doi = {10.2307/3094808},
  file = {/Users/qualia/Documents/Papers/Burgelman - 2002 - Strategy as Vector and the Inertia of Coevolutiona.pdf},
  journal = {Administrative Science Quarterly},
  language = {en},
  number = {2}
}

@article{Burke1956,
  title = {The Electrical Properties of the Slow Muscle Fibre Membrane},
  author = {Burke, W. and Ginsborg, B. L.},
  year = {1956},
  month = jun,
  volume = {132},
  pages = {586--598},
  issn = {00223751},
  doi = {10.1113/jphysiol.1956.sp005551},
  file = {/Users/qualia/Documents/Papers/1956 - Burke, Ginsborg - THlE ELECTRICAL PROPERTIES OF THE SLOW MUSCLE FIBRE MEMBRANE From the Biophysics Department , University Colleg.pdf;/Users/qualia/Documents/Papers/Burke and Ginsborg - 1956 - The electrical properties of the slow muscle fibre.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {3}
}

@article{Burke2015,
  title = {Human Intracranial High-Frequency Activity during Memory Processing: Neural Oscillations or Stochastic Volatility?},
  shorttitle = {Human Intracranial High-Frequency Activity during Memory Processing},
  author = {Burke, John F and Ramayya, Ashwin G and Kahana, Michael J},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {104--110},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.09.003},
  file = {/Users/qualia/Documents/Papers/Burke et al. - 2015 - Human intracranial high-frequency activity during .pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Burnham2004,
  title = {Multimodel {{Inference}}: {{Understanding AIC}} and {{BIC}} in {{Model Selection}}},
  shorttitle = {Multimodel {{Inference}}},
  author = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  month = nov,
  volume = {33},
  pages = {261--304},
  issn = {0049-1241, 1552-8294},
  doi = {10.1177/0049124104268644},
  file = {/Users/qualia/Documents/Papers/2004 - Burnham - Multimodel Inference Understanding AIC and BIC in Model Selection.pdf},
  journal = {Sociological Methods \& Research},
  language = {en},
  number = {2}
}

@article{Burns2010,
  title = {Comparisons of the {{Dynamics}} of {{Local Field Potential}} and {{Multiunit Activity Signals}} in {{Macaque Visual Cortex}}},
  author = {Burns, S. P. and Xing, D. and Shapley, R. M.},
  year = {2010},
  month = oct,
  volume = {30},
  pages = {13739--13749},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0743-10.2010},
  abstract = {The local field potential (LFP) and multi-unit activity (MUA) are extracellularly recorded signals that describe local neuronal network dynamics. In our experiments, the LFP and MUA, recorded from the same electrode in macaque V1 in response to drifting grating visual stimuli, were evaluated on coarse time-scales (\textasciitilde{}1-5s) and fine time-scales ({$<$} 0.1s) . On coarse time-scales, MUA and the LFP both produced sustained visual responses to optimal and nonoptimal oriented visual stimuli. The sustainedness of the two signals across the population of recording sites was correlated (correlation coefficient \textasciitilde{}0.4). At most recording sites the MUA was at least as sustained as the LFP and significantly more sustained for optimal orientations. In previous literature the BOLD (blood oxygen level dependent) signal of fMRI (functional magnetic resonance imaging) studies was found to be more strongly correlated with the LFP than with the MUA due to the lack of sustained response in the MUA signal. Since we found that MUA was as sustained as the LFP, MUA may also be correlated with BOLD. On fine time-scales, we computed the coherence between the LFP and MUA over the frequency range 10-150Hz. The LFP and MUA were weakly but significantly coherent (\textasciitilde{} 0.14) in the gamma-band (20-90Hz). The amount of gamma-band coherence was correlated with the power in the gamma-band of the LFP. The data were consistent with the proposal that the LFP and MUA are generated in a noisy, resonant cortical network.},
  file = {/Users/qualia/Documents/Papers/2012 - Manuscript - NIH Public Access.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Burns2011,
  title = {Is {{Gamma}}-{{Band Activity}} in the {{Local Field Potential}} of {{V1 Cortex}} a "{{Clock}}" or {{Filtered Noise}}?},
  author = {Burns, S. P. and Xing, D. and Shapley, R. M.},
  year = {2011},
  month = jun,
  volume = {31},
  pages = {9658--9664},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0660-11.2011},
  abstract = {Gamma-band (25\textendash{}90Hz) peaks in local field potential (LFP) power spectra are present throughout the cerebral cortex and have been related to perception, attention, memory, and disorders e.g. schizophrenia and autism. It has been theorized gamma oscillations provide a `clock' for precise temporal encoding and `binding' of signals about stimulus features across brain regions. For gamma to function as a `clock' it must be autocoherent: phase and frequency conserved over a period of time. We computed phase and frequency trajectories of gamma-band bursts, using timefrequency analysis of LFPs recorded in macaque primary visual cortex (V1) during visual stimulation. The data were compared with simulations of random networks and clock signals in noise. Gamma-band bursts in LFP data were statistically indistinguishable from those found in filtered broadband noise. Therefore, V1 LFP data did not contain `clock'-like gamma-band signals. We consider possible functions for stochastic gamma-band activity, such as a synchronizing pulse signal.},
  file = {/Users/qualia/Documents/Papers/2012 - Manuscript - NIH Public Access(2).pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Busemeyer2006,
  title = {Quantum Dynamics of Human Decision-Making},
  author = {Busemeyer, Jerome R. and Wang, Zheng and Townsend, James T.},
  year = {2006},
  month = jun,
  volume = {50},
  pages = {220--241},
  issn = {00222496},
  doi = {10.1016/j.jmp.2006.01.003},
  abstract = {A quantum dynamic model of decision-making is presented, and it is compared with a previously established Markov model. Both the quantum and the Markov models are formulated as random walk decision processes, but the probabilistic principles differ between the two approaches. Quantum dynamics describe the evolution of complex valued probability amplitudes over time, whereas Markov models describe the evolution of real valued probabilities over time. Quantum dynamics generate interference effects, which are not possible with Markov models. An interference effect occurs when the probability of the union of two possible paths is smaller than each individual path alone. The choice probabilities and distribution of choice response time for the quantum model are derived, and the predictions are contrasted with the Markov model.},
  file = {/Users/qualia/Documents/Papers/2006 - Busemeyer, Wang, Townsend - Quantum dynamics of human decision-making.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {3}
}

@article{Busemeyer2015,
  title = {What {{Is Quantum Cognition}}, and {{How Is It Applied}} to {{Psychology}}?},
  author = {Busemeyer, Jerome R. and Wang, Zheng},
  year = {2015},
  month = jun,
  volume = {24},
  pages = {163--169},
  issn = {0963-7214, 1467-8721},
  doi = {10.1177/0963721414568663},
  abstract = {Quantum cognition is a new research program that uses mathematical principles from quantum theory as a framework to explain human cognition, including judgment and decision making, concepts, reasoning, memory, and perception. This research is not concerned with whether the brain is a quantum computer. Instead, it uses quantum theory as a fresh conceptual framework and a coherent set of formal tools for explaining puzzling empirical findings in psychology. In this introduction, we focus on two quantum principles as examples to show why quantum cognition is an appealing new theoretical direction for psychology: complementarity, which suggests that some psychological measures have to be made sequentially and that the context generated by the first measure can influence responses to the next one, producing measurement order effects, and superposition, which suggests that some psychological states cannot be defined with respect to definite values but, instead, that all possible values within the superposition have some potential for being expressed. We present evidence showing how these two principles work together to provide a coherent explanation for many divergent and puzzling phenomena in psychology.},
  file = {/Users/qualia/Documents/Papers/2015 - Busemeyer, Wang - What Is Quantum Cognition, and How Is It Applied to Psychology.pdf;/Users/qualia/Documents/Papers/Busemeyer and Wang - 2015 - What Is Quantum Cognition, and How Is It Applied t.pdf},
  journal = {Current Directions in Psychological Science},
  language = {en},
  number = {3}
}

@article{Butson2007,
  title = {Patient-Specific Analysis of the Volume of Tissue Activated during Deep Brain Stimulation},
  author = {Butson, Christopher R. and Cooper, Scott E. and Henderson, Jaimie M. and McIntyre, Cameron C.},
  year = {2007},
  month = jan,
  volume = {34},
  pages = {661--670},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.09.034},
  file = {/Users/qualia/Documents/Papers/2007 - Butson et al. - Patient-specific analysis of the volume of tissue activated during deep brain stimulation.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Buzsaki1983,
  title = {Cellular Bases of Hippocampal {{EEG}} in the Behaving Rat},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and {Lai-Wo S.}, Leung and Vanderwolf, Cornelius H.},
  year = {1983},
  month = oct,
  volume = {6},
  pages = {139--171},
  issn = {01650173},
  doi = {10.1016/0165-0173(83)90037-1},
  file = {/Users/qualia/Documents/Papers/1983 - Buzsáki, Leung, Vanderwolf - Cellular Bases of Hippocampal EEG in the Behaving Rat.pdf},
  journal = {Brain Research Reviews},
  language = {en},
  number = {2}
}

@article{Buzsaki2012,
  title = {The Origin of Extracellular Fields and Currents \textemdash{} {{EEG}}, {{ECoG}}, {{LFP}} and Spikes},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Anastassiou, Costas A. and Koch, Christof},
  year = {2012},
  month = jun,
  volume = {13},
  pages = {407--420},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3241},
  abstract = {Neuronal activity in the brain gives rise to transmembrane currents that can be measured in the extracellular medium. Although the major contributor of the extracellular signal is the synaptic transmembrane current, other sources \textemdash{} including Na+ and Ca2+ spikes, ionic fluxes through voltage- and ligand-gated channels, and intrinsic membrane oscillations \textemdash{} can substantially shape the extracellular field. High-density recordings of field activity in animals and subdural grid recordings in humans, combined with recently developed data processing tools and computational modelling, can provide insight into the cooperative behaviour of neurons, their average synaptic input and their spiking output, and can increase our understanding of how these processes contribute to the extracellular signal.},
  file = {/Users/qualia/Documents/Papers/2012 - Buzsáki, Anastassiou, Koch - The origin of extracellular fields and currents--EEG, ECoG, LFP and spikes.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {6}
}

@article{Buzsaki2014,
  title = {The Log-Dynamic Brain: How Skewed Distributions Affect Network Operations},
  shorttitle = {The Log-Dynamic Brain},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Mizuseki, Kenji},
  year = {2014},
  month = apr,
  volume = {15},
  pages = {264--278},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3687},
  abstract = {We often assume that the variables of functional and structural brain parameters \textemdash{} such as synaptic weights, the firing rates of individual neurons, the synchronous discharge of neural populations, the number of synaptic contacts between neurons and the size of dendritic boutons \textemdash{} have a bell-shaped distribution. However, at many physiological and anatomical levels in the brain, the distribution of numerous parameters is in fact strongly skewed with a heavy tail, suggesting that skewed (typically lognormal) distributions are fundamental to structural and functional brain organization. This insight not only has implications for how we should collect and analyse data, it may also help us to understand how the different levels of skewed distributions \textemdash{} from synapses to cognition \textemdash{} are related to each other.},
  file = {/Users/qualia/Documents/Papers/Buzsáki and Mizuseki - 2014 - The log-dynamic brain how skewed distributions af.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {4}
}

@article{Buzsaki2015,
  title = {What Does Gamma Coherence Tell Us about Inter-Regional Neural Communication?},
  author = {Buzs{\'a}ki, Gy{\"o}rgy and Schomburg, Erik W},
  year = {2015},
  month = apr,
  volume = {18},
  pages = {484--489},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3952},
  file = {/Users/qualia/Documents/Papers/Buzsáki and Schomburg - 2015 - What does gamma coherence tell us about inter-regi.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Buzsaki2015a,
  title = {Hippocampal Sharp Wave-Ripple: {{A}} Cognitive Biomarker for Episodic Memory and Planning: {{HIPPOCAMPAL SHARP WAVE}}-{{RIPPLE}}},
  shorttitle = {Hippocampal Sharp Wave-Ripple},
  author = {Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2015},
  month = oct,
  volume = {25},
  pages = {1073--1188},
  issn = {10509631},
  doi = {10.1002/hipo.22488},
  abstract = {Sharp wave ripples (SPW-Rs) represent the most synchronous population pattern in the mammalian brain. Their excitatory output affects a wide area of the cortex and several subcortical nuclei. SPW-Rs occur during ``off-line'' states of the brain, associated with consummatory behaviors and non-REM sleep, and are influenced by numerous neurotransmitters and neuromodulators. They arise from the excitatory recurrent system of the CA3 region and the SPW-induced excitation brings about a fast network oscillation (ripple) in CA1. The spike content of SPW-Rs is temporally and spatially coordinated by a consortium of interneurons to replay fragments of waking neuronal sequences in a compressed format. SPW-Rs assist in transferring this compressed hippocampal representation to distributed circuits to support memory consolidation; selective disruption of SPW-Rs interferes with memory. Recently acquired and pre-existing information are combined during SPW-R replay to influence decisions, plan actions and, potentially, allow for creative thoughts. In addition to the widely studied contribution to memory, SPW-Rs may also affect endocrine function via activation of hypothalamic circuits. Alteration of the physiological mechanisms supporting SPW-Rs leads to their pathological conversion, ``p-ripples,'' which are a marker of epileptogenic tissue and can be observed in rodent models of schizophrenia and Alzheimer's Disease. Mechanisms for SPW-R genesis and function are discussed in this review. VC 2015 The Authors Hippocampus Published by Wiley Periodicals, Inc.},
  file = {/Users/qualia/Documents/Papers/Buzsáki - 2015 - Hippocampal sharp wave-ripple A cognitive biomark.pdf},
  journal = {Hippocampus},
  language = {en},
  number = {10}
}

@inproceedings{Bylander1997,
  title = {A Perceptron-like Online Algorithm for Tracking the Median},
  booktitle = {Proceedings of {{International Conference}} on {{Neural Networks}} ({{ICNN}}'97)},
  author = {Bylander, T. and Rosen, B.},
  year = {1997},
  volume = {4},
  pages = {2219--2224},
  publisher = {{IEEE}},
  address = {{Houston, TX, USA}},
  doi = {10.1109/ICNN.1997.614292},
  abstract = {We present an online algorithm for tracking the median of a series of values. The algorithm updates its current estimate of the median by incrementing or decrementing a fixed value, which is analogous to perceptron updating. The median value of a sequence minimizes the absolute loss, i.e., the sum of absolute deviations. Our analysis shows that the worst-case absolute loss of our algorithm is comparable to the absolute loss of any sequence of target medians, given restrictions on how much the target can change per trial.},
  file = {/Users/qualia/Documents/Papers/1997 - Bylander, Rosen - A perceptron-like online algorithm for tracking the median.pdf},
  isbn = {978-0-7803-4122-7},
  language = {en}
}

@article{Caballero2018,
  title = {A Probabilistic, Distributed, Recursive Mechanism for Decision-Making in the Brain},
  author = {Caballero, Javier A. and Humphries, Mark D. and Gurney, Kevin N.},
  editor = {Daunizeau, Jean},
  year = {2018},
  month = apr,
  volume = {14},
  pages = {e1006033},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1006033},
  abstract = {Decision formation recruits many brain regions, but the procedure they jointly execute is unknown. Here we characterize its essential composition, using as a framework a novel recursive Bayesian algorithm that makes decisions based on spike-trains with the statistics of those in sensory cortex (MT). Using it to simulate the random-dot-motion task, we demonstrate it quantitatively replicates the choice behaviour of monkeys, whilst predicting losses of otherwise usable information from MT. Its architecture maps to the recurrent corticobasal-ganglia-thalamo-cortical loops, whose components are all implicated in decision-making. We show that the dynamics of its mapped computations match those of neural activity in the sensorimotor cortex and striatum during decisions, and forecast those of basal ganglia output and thalamus. This also predicts which aspects of neural dynamics are and are not part of inference. Our single-equation algorithm is probabilistic, distributed, recursive, and parallel. Its success at capturing anatomy, behaviour, and electrophysiology suggests that the mechanism implemented by the brain has these same characteristics. Accepted: February 12, 2018 Published: April 3, 2018 Copyright: \textcopyright{} 2018 Caballero et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.},
  file = {/Users/qualia/Documents/Papers/Caballero et al. - 2018 - A probabilistic, distributed, recursive mechanism .pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {4}
}

@article{Caceres,
  title = {Towards a Realistic {{NNLIF}} Model: {{Analysis}} and Numerical Solver for Excitatory-Inhibitory Networks with Delay and Refractory Periods},
  author = {Caceres, Mar\i{}a J and Schneider, Ricarda},
  pages = {28},
  abstract = {The Network of Noisy Leaky Integrate and Fire (NNLIF) model describes the behavior of a neural network at mesoscopic level. It is one of the simplest self-contained mean-field models considered for that purpose. Even so, to study the mathematical properties of the model some simplifications were necessary [4, 5, 6], which disregard crucial phenomena. In this work we deal with the general NNLIF model without simplifications. It involves a network with two populations (excitatory and inhibitory), with transmission delays between the neurons and where the neurons remain in a refractory state for a certain time. We have studied the number of steady states in terms of the model parameters, the long time behaviour via the entropy method and Poincar\textasciiacute{}e's inequality, blow-up phenomena, and the importance of transmission delays between excitatory neurons to prevent blow-up and to give rise to synchronous solutions. Besides analytical results, we have presented a numerical resolutor for this model, based on high order flux-splitting WENO schemes and an explicit third order TVD Runge-Kutta method, in order to describe the wide range of phenomena exhibited by the network: blow-up, asynchronous/synchronous solutions and instability/stability of the steady states; the solver also allows us to observe the time evolution of the firing rates, refractory states and the probability distributions of the excitatory and inhibitory populations.},
  file = {/Users/qualia/Documents/Papers/Caceres and Schneider - Towards a realistic NNLIF model Analysis and nume.pdf},
  language = {en}
}

@article{Calabresi2014,
  title = {Direct and Indirect Pathways of Basal Ganglia: A Critical Reappraisal},
  shorttitle = {Direct and Indirect Pathways of Basal Ganglia},
  author = {Calabresi, Paolo and Picconi, Barbara and Tozzi, Alessandro and Ghiglieri, Veronica and Di Filippo, Massimiliano},
  year = {2014},
  month = aug,
  volume = {17},
  pages = {1022--1030},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3743},
  file = {/Users/qualia/Documents/Papers/2014 - Calabresi et al. - Direct and indirect pathways of basal ganglia a critical reappraisal.pdf;/Users/qualia/Documents/Papers/Calabresi et al. - 2014 - Direct and indirect pathways of basal ganglia a c.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {8}
}

@article{Calhoun2014,
  title = {Maximally Informative Foraging by {{Caenorhabditis}} Elegans},
  author = {Calhoun, Adam J and Chalasani, Sreekanth H and Sharpee, Tatyana O},
  year = {2014},
  month = dec,
  volume = {3},
  pages = {e04220},
  issn = {2050-084X},
  doi = {10.7554/eLife.04220},
  abstract = {Animals have evolved intricate search strategies to find new sources of food. Here, we analyze a complex food seeking behavior in the nematode Caenorhabditis elegans (C. elegans) to derive a general theory describing different searches. We show that C. elegans, like many other animals, uses a multi-stage search for food, where they initially explore a small area intensively (`local search') before switching to explore a much larger area (`global search'). We demonstrate that these search strategies as well as the transition between them can be quantitatively explained by a maximally informative search strategy, where the searcher seeks to continuously maximize information about the target. Although performing maximally informative search is computationally demanding, we show that a drift-diffusion model can approximate it successfully with just three neurons. Our study reveals how the maximally informative search strategy can be implemented and adopted to different search conditions.
          , 
            How an animal forages for food can make the difference between life and death, and there are several different searching strategies that may be adopted. Foraging could be more productive if animals could take into account any of the patterns with which food is distributed in their environment, but how much could they measure and memorize? Calhoun et al. show that a tiny worm called Caenorhabditis elegans can keep track of how its previous food finds were spread out, and uses this knowledge to optimize future searches for food.
            When C. elegans forages, it begins by performing an intensive search of where it believes food is likely to be found. This strategy, called `local search', is characterised by the worm making numerous sharp turns that keep it in its target search area. If the worm has not found food after 15 min, it abruptly switches its behavior to a so-called `global search' strategy, which features fewer sharp turns and more forays into the surrounding area.
            C. elegans is often thought to follow the smell of a food source in order to locate it. While reliable on small scale, this strategy can prove problematic when the distribution of food is patchy. Calhoun et al. show that in extreme conditions, such as when food is completely removed, the animals determine where and for how long to persist with their search based on their knowledge of what was typical of their environment. Such a strategy is called infotaxis, which literally means `guided by information'. While the neural circuits underpinning these behaviors remain to be found, Calhoun et al. propose a model that suggests that these circuits could be relatively simple, and made up of as few as three neurons.},
  file = {/Users/qualia/Documents/Papers/Calhoun et al. - 2014 - Maximally informative foraging by Caenorhabditis e.pdf},
  journal = {eLife},
  language = {en}
}

@article{Calhoun2015,
  title = {Neural {{Mechanisms}} for {{Evaluating Environmental Variability}} in {{Caenorhabditis}} Elegans},
  author = {Calhoun, Adam J. and Tong, Ada and Pokala, Navin and Fitzpatrick, James A.J. and Sharpee, Tatyana O. and Chalasani, Sreekanth H.},
  year = {2015},
  month = apr,
  volume = {86},
  pages = {428--441},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.03.026},
  abstract = {The ability to evaluate variability in the environment is vital for making optimal behavioral decisions. Here we show that Caenorhabditis elegans evaluates variability in its food environment and modifies its future behavior accordingly. We derive a behavioral model that reveals a critical period over which information about the food environment is acquired and predicts future search behavior. We also identify a pair of high-threshold sensory neurons that encode variability in food concentration and the downstream dopamine-dependent circuit that generates appropriate search behavior upon removal from food. Further, we show that CREB is required in a subset of interneurons and determines the timescale over which the variability is integrated. Interestingly, the variability circuit is a subset of a larger circuit driving search behavior, showing that learning directly modifies the very same neurons driving behavior. Our study reveals how a neural circuit decodes environmental variability to generate contextually appropriate decisions.},
  file = {/Users/qualia/Documents/Papers/Calhoun et al. - 2015 - Neural Mechanisms for Evaluating Environmental Var.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Callaway2000,
  title = {Network {{Robustness}} and {{Fragility}}: {{Percolation}} on {{Random Graphs}}},
  author = {Callaway, Duncan S and Newman, M E J and Strogatz, Steven H and Watts, Duncan J},
  year = {2000},
  volume = {85},
  pages = {4},
  file = {/Users/qualia/Documents/Papers/2000 - Callaway et al. - Network robustness and fragility percolation on random graphs.pdf},
  journal = {PHYSICAL REVIEW LETTERS},
  language = {en},
  number = {25}
}

@article{Callaway2001,
  title = {Are Randomly Grown Graphs Really Random?},
  author = {Callaway, Duncan S. and Hopcroft, John E. and Kleinberg, Jon M. and Newman, M. E. J. and Strogatz, Steven H.},
  year = {2001},
  month = sep,
  volume = {64},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/PhysRevE.64.041902},
  file = {/Users/qualia/Documents/Papers/2001 - Callaway et al. - Are randomly grown graphs really random.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {4}
}

@article{Camerer1998,
  title = {Experience-{{Weighted Attraction Learning}} in {{Coordination Games}}: {{Probability Rules}}, {{Heterogeneity}}, and {{Time}}-{{Variation}}},
  shorttitle = {Experience-{{Weighted Attraction Learning}} in {{Coordination Games}}},
  author = {Camerer, Colin and Ho, Teck-Hua},
  year = {1998},
  month = jun,
  volume = {42},
  pages = {305--326},
  issn = {00222496},
  doi = {10.1006/jmps.1998.1217},
  file = {/Users/qualia/Documents/Papers/1998 - Camerer, Ho - Experience-Weighted Attraction Learning in Coordination Games Probability Rules, Heterogeneity, and Time-Variation.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {2-3}
}

@article{Camerer1999,
  title = {Experience-Weighted {{Attraction Learning}} in {{Normal Form Games}}},
  author = {Camerer, Colin and Hua Ho, Teck},
  year = {1999},
  month = jul,
  volume = {67},
  pages = {827--874},
  issn = {0012-9682, 1468-0262},
  doi = {10.1111/1468-0262.00054},
  abstract = {In `experience-weighted attraction' {\v Z}EWA. learning, strategies have attractions that reflect initial predispositions, are updated based on payoff experience, and determine choice probabilities according to some rule {\v Z}e.g., logit.. A key feature is a parameter ? that weights the strength of hypothetical reinforcement of strategies that were not chosen according to the payoff they would have yielded, relative to reinforcement of chosen strategies according to received payoffs. The other key features are two discount rates, ␾ and ␳, which separately discount previous attractions, and an experience weight. EWA includes reinforcement learning and weighted fictitious play {\v Z}belief learning. as special cases, and hybridizes their key elements. When ? s 0 and ␳ s 0, cumulative choice reinforcement results. When ? s 1 and ␳ s ␾, levels of reinforcement of strategies are exactly the same as expected payoffs given weighted fictitious play beliefs. Using three sets of experimental data, parameter estimates of the model were calibrated on part of the data and used to predict a holdout sample. Estimates of ? are generally around .50, ␾ around .8᎐1, and ␳ varies from 0 to ␾. Reinforcement and belief-learning special cases are generally rejected in favor of EWA, though belief models do better in some constant-sum games. EWA is able to combine the best features of previous approaches, allowing attractions to begin and grow flexibly as choice reinforcement does, but reinforcing unchosen strategies substantially as belief-based models implicitly do.},
  file = {/Users/qualia/Documents/Papers/2003 - Camerer, Ho - Experience‐weighted Attraction Learning in Normal Form Games.pdf},
  journal = {Econometrica},
  language = {en},
  number = {4}
}

@article{Camerer2002,
  title = {Sophisticated {{Experience}}-{{Weighted Attraction Learning}} and {{Strategic Teaching}} in {{Repeated Games}}},
  author = {Camerer, Colin F. and Ho, Teck-Hua and Chong, Juin-Kuan},
  year = {2002},
  month = may,
  volume = {104},
  pages = {137--188},
  issn = {00220531},
  doi = {10.1006/jeth.2002.2927},
  file = {/Users/qualia/Documents/Papers/2002 - Camerer, Ho, Chong - Sophisticated experience-weighted attraction learning and strategic teaching in repeated games.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {1}
}

@article{Camerer2003,
  title = {Behavioural Studies of Strategic Thinking in Games},
  author = {Camerer, Colin F.},
  year = {2003},
  month = may,
  volume = {7},
  pages = {225--231},
  issn = {13646613},
  doi = {10.1016/S1364-6613(03)00094-9},
  file = {/Users/qualia/Documents/Papers/2003 - Camerer - Behavioural studies of strategic thinking in games.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {5}
}

@article{Camley2018,
  title = {Collective Gradient Sensing and Chemotaxis: Modeling and Recent Developments},
  shorttitle = {Collective Gradient Sensing and Chemotaxis},
  author = {Camley, Brian A},
  year = {2018},
  month = jun,
  volume = {30},
  pages = {223001},
  issn = {0953-8984, 1361-648X},
  doi = {10.1088/1361-648X/aabd9f},
  file = {/Users/qualia/Documents/Papers/Camley - 2018 - Collective gradient sensing and chemotaxis modeli.pdf},
  journal = {Journal of Physics: Condensed Matter},
  language = {en},
  number = {22}
}

@article{Campbell2004,
  title = {Delayed {{Coupling Between Two Neural Network Loops}}},
  author = {Campbell, Sue Ann and Edwards, R. and {van den Driessche}, P.},
  year = {2004},
  month = jan,
  volume = {65},
  pages = {316--335},
  issn = {0036-1399, 1095-712X},
  doi = {10.1137/S0036139903434833},
  abstract = {Coupled loops with time delays are common in physiological systems such as neural networks. We study a Hopfield-type network that consists of a pair of one-way loops each with three neurons and two-way coupling (of either excitatory or inhibitory type) between a single neuron of each loop. Time delays are introduced in the connections between loops, and the effects of coupling strengths and delays on the network dynamics are investigated. These effects depend strongly on whether the coupling is symmetric (of the same type in both directions) or asymmetric (inhibitory in one direction and excitatory in the other). The network of six delay differential equations is studied by linear stability analysis and bifurcation theory. Loops having inherently stable zero solutions cannot be destabilized by weak coupling, regardless of the delay. Asymmetric coupling is weakly stabilizing but easily upset by delays. Symmetric coupling (if not too weak) can destabilize an inherently stable zero solution, leading to nontrivial fixed points if the gain of the neuron response function is not too negative or to oscillation otherwise. In the oscillation case, intermediate delays can restabilize the zero solution. At the borderline of the weak coupling region (symmetric or asymmetric), stability can change with delay ranges. When the coupling strengths are of the same magnitude, the oscillations of corresponding neurons in the two loops can be in phase, antiphase (symmetric coupling), or one quarter period out of phase (asymmetric coupling) depending on the delay.},
  file = {/Users/qualia/Documents/Papers/2015 - Campbell, Edwards, van den Driessche - Delayed Coupling between Two Neural Network Loops.pdf;/Users/qualia/Documents/Papers/Campbell et al. - 2004 - Delayed Coupling Between Two Neural Network Loops.pdf},
  journal = {SIAM Journal on Applied Mathematics},
  language = {en},
  number = {1}
}

@article{Canavier2015,
  title = {Phase-Resetting as a Tool of Information Transmission},
  author = {Canavier, Carmen C},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {206--213},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.12.003},
  file = {/Users/qualia/Documents/Papers/Canavier - 2015 - Phase-resetting as a tool of information transmiss.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Candes2008,
  title = {An {{Introduction To Compressive Sampling}}},
  author = {Candes, E.J. and Wakin, M.B.},
  year = {2008},
  month = mar,
  volume = {25},
  pages = {21--30},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.914731},
  file = {/Users/qualia/Documents/Papers/2008 - Candes, Wakin - An Introduction To Compressive Sampling.pdf},
  journal = {IEEE Signal Processing Magazine},
  language = {en},
  number = {2}
}

@article{Cannon2014,
  title = {{{LEMS}}: A Language for Expressing Complex Biological Models in Concise and Hierarchical Form and Its Use in Underpinning {{NeuroML}} 2},
  shorttitle = {{{LEMS}}},
  author = {Cannon, Robert C. and Gleeson, Padraig and Crook, Sharon and Ganapathy, Gautham and Marin, Boris and Piasini, Eugenio and Silver, R. Angus},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5196},
  doi = {10.3389/fninf.2014.00079},
  abstract = {Computational models are increasingly important for studying complex neurophysiological systems. As scientific tools, it is essential that such models can be reproduced and critically evaluated by a range of scientists. However, published models are currently implemented using a diverse set of modeling approaches, simulation tools, and computer languages making them inaccessible and difficult to reproduce. Models also typically contain concepts that are tightly linked to domain-specific simulators, or depend on knowledge that is described exclusively in text-based documentation. To address these issues we have developed a compact, hierarchical, XML-based language called LEMS (Low Entropy Model Specification), that can define the structure and dynamics of a wide range of biological models in a fully machine readable format. We describe how LEMS underpins the latest version of NeuroML and show that this framework can define models of ion channels, synapses, neurons and networks. Unit handling, often a source of error when reusing models, is built into the core of the language by specifying physical quantities in models in terms of the base dimensions. We show how LEMS, together with the open source Java and Python based libraries we have developed, facilitates the generation of scripts for multiple neuronal simulators and provides a route for simulator free code generation. We establish that LEMS can be used to define models from systems biology and map them to neuroscience-domain specific simulators, enabling models to be shared between these traditionally separate disciplines. LEMS and NeuroML 2 provide a new, comprehensive framework for defining computational models of neuronal and other biological systems in a machine readable format, making them more reproducible and increasing the transparency and accessibility of their underlying structure and properties.},
  file = {/Users/qualia/Documents/Papers/2014 - Cannon et al. - LEMS a language for expressing complex biological models in concise and hierarchical form and its use in underpi.pdf;/Users/qualia/Documents/Papers/Cannon et al. - 2014 - LEMS a language for expressing complex biological.pdf},
  journal = {Frontiers in Neuroinformatics},
  language = {en}
}

@article{Cannon2015,
  title = {Neural {{Sequence Generation Using Spatiotemporal Patterns}} of {{Inhibition}}},
  author = {Cannon, Jonathan and Kopell, Nancy and Gardner, Timothy and Markowitz, Jeffrey},
  editor = {Sporns, Olaf},
  year = {2015},
  month = nov,
  volume = {11},
  pages = {e1004581},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004581},
  file = {/Users/qualia/Documents/Papers/2015 - Cannon et al. - Neural Sequence Generation Using Spatiotemporal Patterns of Inhibition.pdf;/Users/qualia/Documents/Papers/Cannon et al. - 2015 - Neural Sequence Generation Using Spatiotemporal Pa.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {11}
}

@article{Cannon2016,
  title = {Synaptic and Intrinsic Homeostasis Cooperate to Optimize Single Neuron Response Properties and Tune Integrator Circuits},
  author = {Cannon, Jonathan and Miller, Paul},
  year = {2016},
  month = nov,
  volume = {116},
  pages = {2004--2022},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00253.2016},
  file = {/Users/qualia/Documents/Papers/Cannon and Miller - 2016 - Synaptic and intrinsic homeostasis cooperate to op.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Cannon2017,
  title = {Stable {{Control}} of {{Firing Rate Mean}} and {{Variance}} by {{Dual Homeostatic Mechanisms}}},
  author = {Cannon, Jonathan and Miller, Paul},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2190-8567},
  doi = {10.1186/s13408-017-0043-7},
  abstract = {Homeostatic processes that provide negative feedback to regulate neuronal firing rates are essential for normal brain function. Indeed, multiple parameters of individual neurons, including the scale of afferent synapse strengths and the densities of specific ion channels, have been observed to change on homeostatic time scales to oppose the effects of chronic changes in synaptic input. This raises the question of whether these processes are controlled by a single slow feedback variable or multiple slow variables. A single homeostatic process providing negative feedback to a neuron's firing rate naturally maintains a stable homeostatic equilibrium with a characteristic mean firing rate; but the conditions under which multiple slow feedbacks produce a stable homeostatic equilibrium have not yet been explored. Here we study a highly general model of homeostatic firing rate control in which two slow variables provide negative feedback to drive a firing rate toward two different target rates. Using dynamical systems techniques, we show that such a control system can be used to stably maintain a neuron's characteristic firing rate mean and variance in the face of perturbations, and we derive conditions under which this happens. We also derive expressions that clarify the relationship between the homeostatic firing rate targets and the resulting stable firing rate mean and variance. We provide specific examples of neuronal systems that can be effectively regulated by dual homeostasis. One of these examples is a recurrent excitatory network, which a dual feedback system can robustly tune to serve as an integrator.},
  file = {/Users/qualia/Documents/Papers/Cannon and Miller - 2017 - Stable Control of Firing Rate Mean and Variance by.pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Canolty2006,
  title = {High {{Gamma Power Is Phase}}-{{Locked}} to {{Theta Oscillations}} in {{Human Neocortex}}},
  author = {Canolty, R. T. and Edwards, E. and Dalal, S. S. and Soltani, M. and Nagarajan, S. S. and Kirsch, H. E. and Berger, M. S. and Barbaro, N. M. and Knight, R. T.},
  year = {2006},
  month = sep,
  volume = {313},
  pages = {1626--1628},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1128115},
  file = {/Users/qualia/Documents/Papers/2006 - Canolty et al. - High gamma power is phase-locked to theta oscillations in human neocortex(2).pdf;/Users/qualia/Documents/Papers/2006 - Canolty et al. - High gamma power is phase-locked to theta oscillations in human neocortex(3).pdf;/Users/qualia/Documents/Papers/2009 - Canolty et al. - NIH Public Access.pdf},
  journal = {Science},
  language = {en},
  number = {5793}
}

@article{Canteras1990,
  title = {Afferent Connections of the Subthalamic Nucleus: A Combined Retrograde and Anterograde Horseradish Peroxidase Study in the Rat},
  shorttitle = {Afferent Connections of the Subthalamic Nucleus},
  author = {Canteras, Newton S. and {Shammah-Lagnado}, Sara J. and Silva, Bomfim A. and Ricardo, Juarez A.},
  year = {1990},
  month = apr,
  volume = {513},
  pages = {43--59},
  issn = {00068993},
  doi = {10.1016/0006-8993(90)91087-W},
  file = {/Users/qualia/Documents/Papers/1988 - Canteras et al. - Somatosensory Inputs To the Subthalamic Nucleus - a Combined Retrograde and Anterograde Horseradish-Peroxidase.pdf;/Users/qualia/Documents/Papers/Canteras et al. - 1990 - Afferent connections of the subthalamic nucleus a.pdf},
  journal = {Brain Research},
  language = {en},
  number = {1}
}

@article{Cantero2018,
  title = {Bundles of {{Brain Microtubules Generate Electrical Oscillations}}},
  author = {Cantero, Mar{\'i}a del Roc{\'i}o and Villa Etchegoyen, Cecilia and Perez, Paula L. and Scarinci, Noelia and Cantiello, Horacio F.},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-30453-2},
  file = {/Users/qualia/Documents/Papers/Cantero et al. - 2018 - Bundles of Brain Microtubules Generate Electrical .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Cao2016,
  title = {Collective {{Activity}} of {{Many Bistable Assemblies Reproduces Characteristic Dynamics}} of {{Multistable Perception}}},
  author = {Cao, R. and Pastukhov, A. and Mattia, M. and Braun, J.},
  year = {2016},
  month = jun,
  volume = {36},
  pages = {6957--6972},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4626-15.2016},
  file = {/Users/qualia/Documents/Papers/Cao et al. - 2016 - Collective Activity of Many Bistable Assemblies Re.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Cao2016a,
  title = {Collective {{Activity}} of {{Many Bistable Assemblies Reproduces Characteristic Dynamics}} of {{Multistable Perception}}},
  author = {Cao, R. and Pastukhov, A. and Mattia, M. and Braun, J.},
  year = {2016},
  month = jun,
  volume = {36},
  pages = {6957--6972},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4626-15.2016},
  file = {/Users/qualia/Documents/Papers/Cao et al. - 2016 - Collective Activity of Many Bistable Assemblies Re 2.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Caraballo2001,
  title = {Attractors for {{Differential Equations}} with {{Variable Delays}}},
  author = {Caraballo, Tom{\'a}s and Langa, Jos{\'e} A. and Robinson, James C.},
  year = {2001},
  month = aug,
  volume = {260},
  pages = {421--438},
  issn = {0022247X},
  doi = {10.1006/jmaa.2000.7464},
  file = {/Users/qualia/Documents/Papers/2001 - Caraballo, Langa, Robinson - Attractors for Differential Equations with Variable Delays.pdf},
  journal = {Journal of Mathematical Analysis and Applications},
  language = {en},
  number = {2}
}

@article{Carandini2004,
  title = {Amplification of {{Trial}}-to-{{Trial Response Variability}} by {{Neurons}} in {{Visual Cortex}}},
  author = {Carandini, Matteo},
  editor = {{Charles Stevens}},
  year = {2004},
  month = aug,
  volume = {2},
  pages = {e264},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0020264},
  file = {/Users/qualia/Documents/Papers/2004 - Carandini - Amplification of trial-to-trial response variability by neurons in visual cortex.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {9}
}

@article{Carandini2007,
  title = {Melting the {{Iceberg}}: {{Contrast Invariance}} in {{Visual Cortex}}},
  shorttitle = {Melting the {{Iceberg}}},
  author = {Carandini, Matteo},
  year = {2007},
  month = apr,
  volume = {54},
  pages = {11--13},
  issn = {08966273},
  doi = {10.1016/j.neuron.2007.03.019},
  file = {/Users/qualia/Documents/Papers/2007 - Carandini - Melting the Iceberg Contrast Invariance in Visual Cortex.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Cardin2005,
  title = {Stimulus-{{Dependent}} (30-50 {{Hz}}) {{Oscillations}} in {{Simple}} and {{Complex Fast Rhythmic Bursting Cells}} in {{Primary Visual Cortex}}},
  author = {Cardin, J. A.},
  year = {2005},
  month = jun,
  volume = {25},
  pages = {5339--5350},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0374-05.2005},
  file = {/Users/qualia/Documents/Papers/2005 - Cardin, Palmer, Contreras - Stimulus-Dependent gamma (30-50 Hz) Oscillations in Simple and Complex Fast Rhythmic Bursting Cell.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {22}
}

@article{Cardin2016,
  title = {Snapshots of the {{Brain}} in {{Action}}: {{Local Circuit Operations}} through the {{Lens}} of {{Oscillations}}},
  shorttitle = {Snapshots of the {{Brain}} in {{Action}}},
  author = {Cardin, J. A.},
  year = {2016},
  month = oct,
  volume = {36},
  pages = {10496--10504},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1021-16.2016},
  file = {/Users/qualia/Documents/Papers/Cardin - 2016 - Snapshots of the Brain in Action Local Circuit Op.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Carp2010,
  title = {Age {{Differences}} in the {{Neural Representation}} of {{Working Memory Revealed}} by {{Multi}}-{{Voxel Pattern Analysis}}},
  author = {Carp, Joshua and Gmeindl, Leon and {Reuter-Lorenz}, Patricia A.},
  year = {2010},
  volume = {4},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2010.00217},
  abstract = {Working memory function declines across the lifespan. Computational models of aging attribute such memory impairments to reduced distinctiveness between neural representations of different mental states in old age, a phenomenon termed dedifferentiation. These models predict that neural distinctiveness should be reduced uniformly across experimental conditions in older adults. In contrast, the Compensation-Related Utilization of Neural Circuits Hypothesis (CRUNCH) model predicts that the distinctiveness of neural representations should be increased in older adults (relative to young adults) at low levels of task demand but reduced at high levels of demand. The present study used multi-voxel pattern analysis to measure the effects of age and task demands on the distinctiveness of the neural representations of verbal and visuospatial working memory. Neural distinctiveness was estimated separately for memory encoding, maintenance, and retrieval, and for low, medium, and high memory loads. Results from sensory cortex during encoding and retrieval were consistent with the dedifferentiation hypothesis: distinctiveness of visual cortical representations during these phases was uniformly reduced in older adults, irrespective of memory load. However, maintenance-related responses in prefrontal and parietal regions yielded a strikingly different pattern of results. At low loads, older adults showed higher distinctiveness than younger adults; at high loads, this pattern reversed, such that distinctiveness was higher in young adults. This interaction between age group and memory load is at odds with the dedifferentiation hypothesis but consistent with CRUNCH. In sum, our results provide partial support for both dedifferentiation- and compensation-based models; we argue that comprehensive theories of cognitive aging must incorporate aspects of both models to fully explain complex patterns of age-related neuro-cognitive change.},
  file = {/Users/qualia/Documents/Papers/2010 - Carp, Gmeindl, Reuter-Lorenz - Age differences in the neural representation of working memory revealed by multi-voxel pattern ana.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Carroll2019,
  title = {Mutual {{Information}} and the {{Edge}} of {{Chaos}} in {{Reservoir Computers}}},
  author = {Carroll, Thomas L.},
  year = {2019},
  month = jun,
  abstract = {A reservoir computer is a dynamical system that may be used to perform computations. A reservoir computer usually consists of a set of nonlinear nodes coupled together in a network so that there are feedback paths. Training the reservoir computer consists of inputing a signal of interest and fitting the time series signals of the reservoir computer nodes to a training signal that is related to the input signal. It is believed that dynamical systems function most efficiently as computers at the "edge of chaos", the point at which the largest Lyapunov exponent of the dynamical system transitions from negative to positive. In this work I simulate several different reservoir computers and ask if the best performance really does come at this edge of chaos. I find that while it is possible to get optimum performance at the edge of chaos, there may also be parameter values where the edge of chaos regime produces poor performance. This ambiguous parameter dependance has implications for building reservoir computers from analog physical systems, where the parameter range is restricted.},
  archivePrefix = {arXiv},
  eprint = {1906.03186},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Carroll - 2019 - Mutual Information and the Edge of Chaos in Reserv.pdf},
  journal = {arXiv:1906.03186 [nlin]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Nonlinear Sciences - Adaptation and Self-Organizing Systems},
  language = {en},
  primaryClass = {nlin}
}

@article{Carvalho2009,
  title = {Differential {{Effects}} of {{Excitatory}} and {{Inhibitory Plasticity}} on {{Synaptically Driven Neuronal Input}}-{{Output Functions}}},
  author = {Carvalho, Tiago P. and Buonomano, Dean V.},
  year = {2009},
  month = mar,
  volume = {61},
  pages = {774--785},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.01.013},
  abstract = {Ultimately, whether or not a neuron produces a spike determines its contribution to local computations. In response to brief stimuli the probability a neuron will fire can be described by its input-output function, which depends on the net balance and timing of excitatory and inhibitory currents. While excitatory and inhibitory synapses are plastic, most studies examine plasticity of subthreshold events. Thus, the effects of concerted regulation of excitatory and inhibitory synaptic strength on neuronal inputoutput functions are not well understood. Here, theoretical analyses reveal that excitatory synaptic strength controls the threshold of the neuronal input-output function, while inhibitory plasticity alters the threshold and gain. Experimentally, changes in the balance of excitation and inhibition in CA1 pyramidal neurons also altered their inputoutput function as predicted by the model. These results support the existence of two functional modes of plasticity that can be used to optimize information processing: threshold and gain plasticity.},
  file = {/Users/qualia/Documents/Papers/2009 - Carvalho, Buonomano - Differential Effects of Excitatory and Inhibitory Plasticity on Synaptically Driven Neuronal Input-Output F.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Casanova2008,
  title = {The Impact of Temporal Regularization on Estimates of the {{BOLD}} Hemodynamic Response Function: {{A}} Comparative Analysis},
  shorttitle = {The Impact of Temporal Regularization on Estimates of the {{BOLD}} Hemodynamic Response Function},
  author = {Casanova, Ramon and Ryali, Srikanth and Serences, John and Yang, Lucie and Kraft, Robert and Laurienti, Paul J. and Maldjian, Joseph A.},
  year = {2008},
  month = may,
  volume = {40},
  pages = {1606--1618},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.01.011},
  file = {/Users/qualia/Documents/Papers/2008 - Casanova et al. - The impact of temporal regularization on estimates of the BOLD hemodynamic response function a comparative anal.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Cates2015,
  title = {Testing the Foundations of Classical Entropy: Colloid Experiments},
  shorttitle = {Celebrating {{{\emph{Soft Matter}}}} 's 10th Anniversary},
  author = {Cates, Michael E. and Manoharan, Vinothan N.},
  year = {2015},
  volume = {11},
  pages = {6538--6546},
  issn = {1744-683X, 1744-6848},
  doi = {10.1039/C5SM01014D},
  file = {/Users/qualia/Documents/Papers/2015 - Cates, Manoharan - Celebrating Soft Matter's 10th anniversary Testing the foundations of classical entropy colloid experiments(2).pdf;/Users/qualia/Documents/Papers/Cates and Manoharan - 2015 - Celebrating iSoft Matteri 's 10th anniversary 2.pdf;/Users/qualia/Documents/Papers/Cates and Manoharan - 2015 - Celebrating iSoft Matteri 's 10th anniversary.pdf},
  journal = {Soft Matter},
  language = {en},
  number = {33}
}

@article{Caze,
  title = {Dendrites Enable a Robust Mechanism for Neuronal Stimulus Selectivity},
  author = {Caze, Romain D and Jarvis, Sarah and Foust, Amanda J and Schultz, Simon R},
  pages = {16},
  abstract = {Hearing, vision, touch \textendash{} underlying all of these senses is stimulus selectivity, a robust information processing operation in which cortical neurons respond more to some stimuli than to others. Previous models assume that these neurons receive the highest weighted input from an ensemble encoding the preferred stimulus, but dendrites enable other possibilities. Non-linear dendritic processing can produce stimulus selectivity based on the spatial distribution of synapses, even if the total preferred stimulus weight does not exceed that of non-preferred stimuli. Using a multi-subunit non-linear model, we demonstrate that selectivity can arise from the spatial distribution of synapses.},
  file = {/Users/qualia/Documents/Papers/Caze et al. - Dendrites enable a robust mechanism for neuronal s.pdf},
  language = {en}
}

@article{Caze2013,
  title = {Passive {{Dendrites Enable Single Neurons}} to {{Compute Linearly Non}}-Separable {{Functions}}},
  author = {Caz{\'e}, Romain Daniel and Humphries, Mark and Gutkin, Boris},
  editor = {Sporns, Olaf},
  year = {2013},
  month = feb,
  volume = {9},
  pages = {e1002867},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002867},
  abstract = {Local supra-linear summation of excitatory inputs occurring in pyramidal cell dendrites, the so-called dendritic spikes, results in independent spiking dendritic sub-units, which turn pyramidal neurons into two-layer neural networks capable of computing linearly non-separable functions, such as the exclusive OR. Other neuron classes, such as interneurons, may possess only a few independent dendritic sub-units, or only passive dendrites where input summation is purely sub-linear, and where dendritic sub-units are only saturating. To determine if such neurons can also compute linearly non-separable functions, we enumerate, for a given parameter range, the Boolean functions implementable by a binary neuron model with a linear sub-unit and either a single spiking or a saturating dendritic sub-unit. We then analytically generalize these numerical results to an arbitrary number of non-linear sub-units. First, we show that a single non-linear dendritic sub-unit, in addition to the somatic non-linearity, is sufficient to compute linearly non-separable functions. Second, we analytically prove that, with a sufficient number of saturating dendritic sub-units, a neuron can compute all functions computable with purely excitatory inputs. Third, we show that these linearly non-separable functions can be implemented with at least two strategies: one where a dendritic sub-unit is sufficient to trigger a somatic spike; another where somatic spiking requires the cooperation of multiple dendritic sub-units. We formally prove that implementing the latter architecture is possible with both types of dendritic sub-units whereas the former is only possible with spiking dendrites. Finally, we show how linearly non-separable functions can be computed by a generic two-compartment biophysical model and a realistic neuron model of the cerebellar stellate cell interneuron. Taken together our results demonstrate that passive dendrites are sufficient to enable neurons to compute linearly non-separable functions.},
  file = {/Users/qualia/Documents/Papers/Cazé et al. - 2013 - Passive Dendrites Enable Single Neurons to Compute.PDF},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {2}
}

@techreport{Cesario2019,
  title = {Your {{Brain Is Not}} an {{Onion}} with a {{Tiny Reptile Inside}}},
  author = {Cesario, Joseph and Johnson, David Jeffrey and Eisthen, Heather},
  year = {2019},
  month = dec,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/x83dq},
  abstract = {A widespread misconception in much of psychology holds that (1) as vertebrate animals evolved, "newer" brain structures were added over existing "older" brain structures and (2) these newer, more complex structures endowed animals with newer and more complex psychological functions, behavioral flexibility, and language. This belief, though widely shared in our introductory textbooks, has long been discredited among neurobiologists and stands in contrast to the clear and unanimous agreement on these issues among those studying nervous system evolution. We bring psychologists up to date on this issue by describing the more accurate model of neural evolution, and we provide examples of how this inaccurate view may have impeded progress in psychology. We urge psychologists to abandon this mistaken view of human brains.},
  file = {/Users/qualia/Documents/Papers/Cesario et al. - 2019 - Your Brain Is Not an Onion with a Tiny Reptile Ins.pdf},
  language = {en},
  type = {Preprint}
}

@article{Chadwick2010,
  title = {Decoding {{Individual Episodic Memory Traces}} in the {{Human Hippocampus}}},
  author = {Chadwick, Martin J. and Hassabis, Demis and Weiskopf, Nikolaus and Maguire, Eleanor A.},
  year = {2010},
  month = mar,
  volume = {20},
  pages = {544--547},
  issn = {09609822},
  doi = {10.1016/j.cub.2010.01.053},
  abstract = {In recent years, multivariate pattern analyses have been performed on functional magnetic resonance imaging (fMRI) data, permitting prediction of mental states from local patterns of blood oxygen-level-dependent (BOLD) signal across voxels [1, 2]. We previously demonstrated that it is possible to predict the position of individuals in a virtualreality environment from the pattern of activity across voxels in the hippocampus [3]. Although this shows that spatial memories can be decoded, substantially more challenging, and arguably only possible to investigate in humans [4], is whether it is feasible to predict which complex everyday experience, or episodic memory, a person is recalling. Here we document for the first time that traces of individual rich episodic memories are detectable and distinguishable solely from the pattern of fMRI BOLD signals across voxels in the human hippocampus. In so doing, we uncovered a possible functional topography in the hippocampus, with preferential episodic processing by some hippocampal regions over others. Moreover, our results imply that the neuronal traces of episodic memories are stable (and thus predictable) even over many re-activations. Finally, our data provide further evidence for functional differentiation within the medial temporal lobe, in that we show the hippocampus contains significantly more episodic information than adjacent structures.},
  file = {/Users/qualia/Documents/Papers/2010 - Chadwick et al. - Decoding individual episodic memory traces in the human hippocampus.pdf},
  journal = {Current Biology},
  language = {en},
  number = {6}
}

@incollection{Chakravorty2014,
  title = {Multi-{{Armed Bandits}}, {{Gittins Index}}, and Its {{Calculation}}},
  booktitle = {Methods and {{Applications}} of {{Statistics}} in {{Clinical Trials}}},
  author = {Chakravorty, Jhelum and Mahajan, Aditya},
  editor = {Balakrishnan, N.},
  year = {2014},
  month = jun,
  pages = {416--435},
  publisher = {{John Wiley \& Sons, Inc.}},
  address = {{Hoboken, NJ, USA}},
  doi = {10.1002/9781118596333.ch24},
  file = {/Users/qualia/Documents/Papers/Chakravorty and Mahajan - 2014 - Multi-Armed Bandits, Gittins Index, and its Calcul.pdf},
  isbn = {978-1-118-59633-3 978-1-118-30476-1},
  language = {en}
}

@article{Chambers2012,
  title = {Parametric Computation Predicts a Multiplicative Interaction between Synaptic Strength Parameters That Control Gamma Oscillations},
  author = {Chambers, Jordan D. and Bethwaite, Blair and Diamond, Neil T. and Peachey, Tom and Abramson, David and Petrou, Steve and Thomas, Evan A.},
  year = {2012},
  volume = {6},
  issn = {1662-5188},
  doi = {10.3389/fncom.2012.00053},
  abstract = {Gamma oscillations are thought to be critical for a number of behavioral functions, they occur in many regions of the brain and through a variety of mechanisms. Fast repetitive bursting (FRB) neurons in layer 2 of the cortex are able to drive gamma oscillations over long periods of time. Even though the oscillation is driven by FRB neurons, strong feedback within the rest of the cortex must modulate properties of the oscillation such as frequency and power. We used a highly detailed model of the cortex to determine how a cohort of 33 parameters controlling synaptic drive might modulate gamma oscillation properties. We were interested in determining not just the effects of parameters individually, but we also wanted to reveal interactions between parameters beyond additive effects. To prevent a combinatorial explosion in parameter combinations that might need to be simulated, we used a fractional factorial design (FFD) that estimated the effects of individual parameters and two parameter interactions. This experiment required only 4096 model runs. We found that the largest effects on both gamma power and frequency came from a complex interaction between efficacy of synaptic connections from layer 2 inhibitory neurons to layer 2 excitatory neurons and the parameter for the reciprocal connection. As well as the effect of the individual parameters determining synaptic efficacy, there was an interaction between these parameters beyond the additive effects of the parameters alone. The magnitude of this effect was similar to that of the individual parameters, predicting that it is physiologically important in setting gamma oscillation properties.},
  file = {/Users/qualia/Documents/Papers/2012 - Chambers et al. - Parametric computation predicts a multiplicative interaction between synaptic strength parameters that control.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Chance,
  title = {Divisive Inhibition in Recurrent Networks},
  author = {Chance, Frances S and Abbott, L F},
  pages = {11},
  abstract = {Models of visual cortex suggest that response selectivity can arise from recurrent networks operating at high gain. However, such networks have a number of problematic features: (i) they operate perilously close to a point of instability, (ii) small changes in synaptic strength can dramatically modify the degree of amplification, and (iii) they respond slowly to rapidly changing stimuli. Divisive inhibition, acting through interneurons that are themselves divisively inhibited, can solve these problems without degrading the selectivity of a recurrent network.},
  file = {/Users/qualia/Documents/Papers/2000 - Chance, Abbott - Divisive inhibition in recurrent networks.pdf},
  language = {en}
}

@article{ChandranKS2016,
  title = {Comparison of {{Matching Pursuit Algorithm}} with {{Other Signal Processing Techniques}} for {{Computation}} of the {{Time}}-{{Frequency Power Spectrum}} of {{Brain Signals}}},
  author = {Chandran KS, S. and Mishra, A. and Shirhatti, V. and Ray, S.},
  year = {2016},
  month = mar,
  volume = {36},
  pages = {3399--3408},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3633-15.2016},
  file = {/Users/qualia/Documents/Papers/Chandran KS et al. - 2016 - Comparison of Matching Pursuit Algorithm with Othe.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {12}
}

@article{Chandrasekaran2018,
  title = {Brittleness in Model Selection Analysis of Single Neuron Firing Rates},
  author = {Chandrasekaran, Chandramouli and {Soldado-Magraner}, Joana and Peixoto, Diogo and Newsome, William T and Shenoy, Krishna and Sahani, Maneesh},
  year = {2018},
  month = sep,
  doi = {10.1101/430710},
  abstract = {Models of complex heterogeneous systems like the brain are inescapably incomplete, and thus always falsified with enough data. As neural data grow in volume and complexity, absolute measures of adequacy are being replaced by model selection methods that rank the relative accuracy of competing theories. Selection still depends on incomplete mathematical instantiations, but the implicit expectation is that ranking is robust to their details. Here we highlight a contrary finding of ``brittleness,'' where data matching one theory conceptually are ranked closer to an instance of another. In particular, selection between recent models of decision making is conceptually misleading when data are simulated with minor distributional mismatch, with mixed secondary signals, or with non-stationary parameters; and decision-related responses in macaque cortex show features suggesting that these effects may impact empirical results. We conclude with recommendations to mitigate such brittleness when using model selection to study neural signals.},
  file = {/Users/qualia/Documents/Papers/Chandrasekaran et al. - 2018 - Brittleness in model selection analysis of single .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Chang2017,
  title = {The {{Code}} for {{Facial Identity}} in the {{Primate Brain}}},
  author = {Chang, Le and Tsao, Doris Y.},
  year = {2017},
  month = jun,
  volume = {169},
  pages = {1013-1028.e14},
  issn = {00928674},
  doi = {10.1016/j.cell.2017.05.011},
  abstract = {Primates recognize complex objects such as faces with remarkable speed and reliability. Here, we reveal the brain's code for facial identity. Experiments in macaques demonstrate an extraordinarily simple transformation between faces and responses of cells in face patches. By formatting faces as points in a high-dimensional linear space, we discovered that each face cell's firing rate is proportional to the projection of an incoming face stimulus onto a single axis in this space, allowing a face cell ensemble to encode the location of any face in the space. Using this code, we could precisely decode faces from neural population responses and predict neural firing rates to faces. Furthermore, this code disavows the long-standing assumption that face cells encode specific facial identities, confirmed by engineering faces with drastically different appearance that elicited identical responses in single face cells. Our work suggests that other objects could be encoded by analogous metric coordinate systems.},
  file = {/Users/qualia/Documents/Papers/Chang and Tsao - 2017 - The Code for Facial Identity in the Primate Brain.pdf},
  journal = {Cell},
  language = {en},
  number = {6}
}

@article{Charness,
  title = {When {{Optimal Choices Feel Wrong}}: {{A Laboratory Study}} of {{Bayesian Updating}}, {{Complexity}}, and {{Affect}}},
  author = {Charness, Gary and Levin, Dan},
  pages = {41},
  abstract = {We examine decision-making under risk and uncertainty in a laboratory experiment. The heart of our design examines how one's propensity to use Bayes' rule is affected by whether this rule is aligned with reinforcement or clashes with it. In some cases, we create environments where Bayesian updating after a successful outcome should lead a decision-maker to make a change, while no change should be made after observing an unsuccessful outcome.},
  file = {/Users/qualia/Documents/Papers/2005 - Charness, Levin - When optimal choices feel wrong A laboratory study of bayesian updating, complexity, and affect.pdf},
  language = {en}
}

@article{Chaslot,
  title = {Monte-{{Carlo Tree Search}}: {{A New Framework}} for {{Game AI}}},
  author = {Chaslot, Guillaume and Bakkes, Sander and Szita, Istvan and Spronck, Pieter},
  pages = {2},
  abstract = {Classic approaches to game AI require either a high quality of domain knowledge, or a long time to generate effective AI behaviour. These two characteristics hamper the goal of establishing challenging game AI. In this paper, we put forward Monte-Carlo Tree Search as a novel, unified framework to game AI. In the framework, randomized explorations of the search space are used to predict the most promising game actions. We will demonstrate that Monte-Carlo Tree Search can be applied effectively to (1) classic board-games, (2) modern board-games, and (3) video games.},
  file = {/Users/qualia/Documents/Papers/Chaslot et al. - Monte-Carlo Tree Search A New Framework for Game .pdf},
  language = {en}
}

@article{Chaturvedi2012,
  title = {Current Steering to Activate Targeted Neural Pathways during Deep Brain Stimulation of the Subthalamic Region},
  author = {Chaturvedi, Ashutosh and Foutz, Thomas J. and McIntyre, Cameron C.},
  year = {2012},
  month = jul,
  volume = {5},
  pages = {369--377},
  issn = {1935861X},
  doi = {10.1016/j.brs.2011.05.002},
  file = {/Users/qualia/Documents/Papers/2012 - Chaturvedi, Foutz, McIntyre - Current steering to activate targeted neural pathways during deep brain stimulation of the subthala.pdf},
  journal = {Brain Stimulation},
  language = {en},
  number = {3}
}

@article{Chaturvedi2013,
  title = {Artificial Neural Network Based Characterization of the Volume of Tissue Activated during Deep Brain Stimulation},
  author = {Chaturvedi, Ashutosh and Luj{\'a}n, J Luis and McIntyre, Cameron C},
  year = {2013},
  month = oct,
  volume = {10},
  pages = {056023},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/10/5/056023},
  abstract = {Objective. Clinical deep brain stimulation (DBS) systems can be programmed with thousands of different stimulation parameter combinations (e.g. electrode contact(s), voltage, pulse width, frequency). Our goal was to develop novel computational tools to characterize the effects of stimulation parameter adjustment for DBS. Approach. The volume of tissue activated (VTA) represents a metric used to estimate the spatial extent of DBS for a given parameter setting. Traditional methods for calculating the VTA rely on activation function (AF)-based approaches and tend to overestimate the neural response when stimulation is applied through multiple electrode contacts. Therefore, we created a new method for VTA calculation that relied on artificial neural networks (ANNs). Main results. The ANN-based predictor provides more accurate descriptions of the spatial spread of activation compared to AF-based approaches for monopolar stimulation. In addition, the ANN was able to accurately estimate the VTA in response to multi-contact electrode configurations. Significance. The ANN-based approach may represent a useful method for fast computation of the VTA in situations with limited computational resources, such as a clinical DBS programming application on a tablet computer.},
  file = {/Users/qualia/Documents/Papers/2013 - Chaturvedi, Luján, McIntyre - Artificial neural network based characterization of the volume of tissue activated during deep bra.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {5}
}

@article{Chaudhuri2014,
  title = {A Diversity of Localized Timescales in Network Activity},
  author = {Chaudhuri, Rishidev and Bernacchia, Alberto and Wang, Xiao-Jing},
  year = {2014},
  month = jan,
  volume = {3},
  issn = {2050-084X},
  doi = {10.7554/eLife.01239},
  abstract = {Neurons show diverse timescales, so that different parts of a network respond with disparate temporal dynamics. Such diversity is observed both when comparing timescales across brain areas and among cells within local populations; the underlying circuit mechanism remains unknown. We examine conditions under which spatially local connectivity can produce such diverse temporal behavior.},
  file = {/Users/qualia/Documents/Papers/2014 - Chaudhuri, Bernacchia, Wang - A diversity of localized timescales in network activity.pdf;/Users/qualia/Documents/Papers/2014 - Chaudhuri, Bernacchia, Wang - A diversity of localized timescales in network activity(2).pdf;/Users/qualia/Documents/Papers/Chaudhuri et al. - 2014 - A diversity of localized timescales in network act 2.pdf;/Users/qualia/Documents/Papers/Chaudhuri et al. - 2014 - A diversity of localized timescales in network act.pdf},
  journal = {eLife},
  language = {en}
}

@article{Chaudhuri2015,
  title = {A {{Large}}-{{Scale Circuit Mechanism}} for {{Hierarchical Dynamical Processing}} in the {{Primate Cortex}}},
  author = {Chaudhuri, Rishidev and Knoblauch, Kenneth and Gariel, Marie-Alice and Kennedy, Henry and Wang, Xiao-Jing},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {419--431},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.008},
  abstract = {We developed a large-scale dynamical model of the macaque neocortex, which is based on recently acquired directed- and weighted-connectivity data from tract-tracing experiments, and which incorporates heterogeneity across areas. A hierarchy of timescales naturally emerges from this system: sensory areas show brief, transient responses to input (appropriate for sensory processing), whereas association areas integrate inputs over time and exhibit persistent activity (suitable for decision-making and working memory). The model displays multiple temporal hierarchies, as evidenced by contrasting responses to visual versus somatosensory stimulation. Moreover, slower prefrontal and temporal areas have a disproportionate impact on global brain dynamics. These findings establish a circuit mechanism for ``temporal receptive windows'' that are progressively enlarged along the cortical hierarchy, suggest an extension of time integration in decision making from local to large circuits, and should prompt a reevaluation of the analysis of functional connectivity (measured by fMRI or electroencephalography/magnetoencephalography) by taking into account interareal heterogeneity.},
  file = {/Users/qualia/Documents/Papers/2015 - Chaudhuri et al. - A Large-Scale Circuit Mechanism for Hierarchical Dynamical Processing in the Primate Cortex.pdf;/Users/qualia/Documents/Papers/Chaudhuri et al. - 2015 - A Large-Scale Circuit Mechanism for Hierarchical D 2.pdf;/Users/qualia/Documents/Papers/Chaudhuri et al. - 2015 - A Large-Scale Circuit Mechanism for Hierarchical D.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Chaudhuri2016,
  title = {Random Recurrent Networks near Criticality Capture the Broadband Power Distribution of Human {{ECoG}} Dynamics},
  author = {Chaudhuri, Rishidev and He, Biyu and Wang, Xiao-Jing},
  year = {2016},
  month = jan,
  doi = {10.1101/036228},
  abstract = {The power spectrum of brain electric field potential recordings is dominated by an arrhythmic broadband signal but a mechanistic account of its underlying neural network dynamics is lacking. Here we show how the broadband power spectrum of field potential recordings can be explained by a simple random network of nodes near criticality. Such a recurrent network produces activity with a combination of a fast and a slow autocorrelation time constant, with the fast mode corresponding to local dynamics and the slow mode resulting from recurrent excitatory connections across the network. These modes are combined to produce a power spectrum similar to that observed in human intracranial EEG (i.e., electrocorticography, ECoG) recordings. Moreover, such a network naturally converts input correlations across nodes into temporal autocorrelation of the network activity. Consequently, increased independence between nodes results in a reduction in low-frequency power, which offers a possible explanation for observed changes in ECoG power spectra during task performance. Lastly, changes in network coupling produce changes in network activity power spectra reminiscent of those seen in human ECoG recordings across different arousal states. This model thus links macroscopic features of the empirical ECoG power spectrum to a parsimonious underlying network structure and proposes potential mechanisms for changes in ECoG power spectra observed across behavioral and arousal states. This provides a computational framework within which to generate and test hypotheses about the cellular and network mechanisms underlying whole brain electrical dynamics, their variations across behavioral states as well as abnormalities associated with brain diseases.},
  file = {/Users/qualia/Documents/Papers/Chaudhuri et al. - 2016 - Random recurrent networks near criticality capture.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Chehelcheraghi2016,
  title = {A Neural Mass Model of Phase\textendash{}Amplitude Coupling},
  author = {Chehelcheraghi, Mojtaba and Nakatani, Chie and Steur, Erik and {van Leeuwen}, Cees},
  year = {2016},
  month = jun,
  volume = {110},
  pages = {171--192},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-016-0687-5},
  abstract = {Brain activity shows phase\textendash{}amplitude coupling between its slow and fast oscillatory components. We study phase\textendash{}amplitude coupling as recorded at individual sites, using a modified version of the well-known Wendling neural mass model. To the population of fast inhibitory interneurons of this model, we added external modulatory input and dynamic self-feedback. These two modifications together are sufficient to let the inhibitory population serve as a limit-cycle oscillator, with frequency characteristics comparable to the beta and gamma bands. The frequency and power of these oscillations can be tuned through the time constant of the dynamic and modulatory input. Alpha band activity is generated, as is usual in such models, as a result of interactions of pyramidal neurons and a population of slow inhibitory interneurons. The slow inhibitory population activity directly influences the fast oscillations via the synaptic gain between slow and fast inhibitory populations. As a result, the amplitude envelope of the fast oscillation is coupled to the phase of the slow activity; this result is consistent with the notion that phase\textendash{}amplitude coupling is effectuated by interactions between inhibitory interneurons.},
  file = {/Users/qualia/Documents/Papers/Chehelcheraghi et al. - 2016 - A neural mass model of phase–amplitude coupling.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {2-3}
}

@article{Chelaru2008,
  title = {Efficient Coding in Heterogeneous Neuronal Populations},
  author = {Chelaru, M. I. and Dragoi, V.},
  year = {2008},
  month = oct,
  volume = {105},
  pages = {16344--16349},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0807744105},
  file = {/Users/qualia/Documents/Papers/Chelaru and Dragoi - 2008 - Efficient coding in heterogeneous neuronal populat.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {42}
}

@article{Chen,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  pages = {12},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  file = {/Users/qualia/Documents/Papers/Chen et al. - Neural Ordinary Differential Equations.pdf},
  language = {en}
}

@article{Chen2013,
  title = {The {{Role}} of {{Coincidence}}-{{Detector Neurons}} in the {{Reliability}} and {{Precision}} of {{Subthreshold Signal Detection}} in {{Noise}}},
  author = {Chen, Yueling and Zhang, Hui and Wang, Hengtong and Yu, Lianchun and Chen, Yong},
  editor = {Chacron, Maurice J.},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {e56822},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0056822},
  abstract = {Subthreshold signal detection is an important task for animal survival in complex environments, where noise increases both the external signal response and the spontaneous spiking of neurons. The mechanism by which neurons process the coding of signals is not well understood. Here, we propose that coincidence detection, one of the ways to describe the functionality of a single neural cell, can improve the reliability and the precision of signal detection through detection of presynaptic input synchrony. Using a simplified neuronal network model composed of dozens of integrate-and-fire neurons and a single coincidence-detector neuron, we show how the network reads out the subthreshold noisy signals reliably and precisely. We find suitable pairing parameters, the threshold and the detection time window of the coincidence-detector neuron, that optimize the precision and reliability of the neuron. Furthermore, it is observed that the refractory period induces an oscillation in the spontaneous firing, but the neuron can inhibit this activity and improve the reliability and precision further. In the case of intermediate intrinsic states of the input neuron, the network responds to the input more efficiently. These results present the critical link between spiking synchrony and noisy signal transfer, which is utilized in coincidence detection, resulting in enhancement of temporally sensitive coding scheme.},
  file = {/Users/qualia/Documents/Papers/2013 - Chen et al. - The Role of Coincidence-Detector Neurons in the Reliability and Precision of Subthreshold Signal Detection in Noise.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {2}
}

@article{Chen2018,
  title = {This {{Looks Like That}}: {{Deep Learning}} for {{Interpretable Image Recognition}}},
  shorttitle = {This {{Looks Like That}}},
  author = {Chen, Chaofan and Li, Oscar and Tao, Chaofan and Barnett, Alina Jade and Su, Jonathan and Rudin, Cynthia},
  year = {2018},
  month = jun,
  abstract = {When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The algorithm thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, geologists, architects, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training, meaning that there are no labels for parts of images. We demonstrate the method on the CIFAR-10 dataset and 10 classes from the CUB200-2011 dataset.},
  archivePrefix = {arXiv},
  eprint = {1806.10574},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Chen et al. - 2018 - This Looks Like That Deep Learning for Interpreta.pdf},
  journal = {arXiv:1806.10574 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Chen2018a,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2018},
  month = jun,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a blackbox differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf},
  journal = {arXiv:1806.07366 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Cheng,
  title = {Polynomial {{Regression As}} an {{Alternative}} to {{Neural Nets}}},
  author = {Cheng, Xi and Khomtchouk, Bohdan and Matloff, Norman and Mohanty, Pete},
  pages = {23},
  abstract = {Despite the success of neural networks (NNs), there is still a concern among many over their ``black box'' nature. Why do they work? Here we present a simple analytic argument that NNs are in fact essentially polynomial regression models. This view will have various implications for NNs, e.g. providing an explanation for why convergence problems arise in NNs, and it gives rough guidance on avoiding overfitting. In addition, we use this phenomenon to predict and confirm a multicollinearity property of NNs not previously reported in the literature. Most importantly, given this loose correspondence, one may choose to routinely use polynomial models instead of NNs, thus avoiding some major problems of the latter, such as having to set many tuning parameters and dealing with convergence issues. We present a number of empirical results; in each case, the accuracy of the polynomial approach matches or exceeds that of NN approaches. A many-featured, open-source software package, polyreg, is available.},
  file = {/Users/qualia/Documents/Papers/Cheng et al. - Polynomial Regression As an Alternative to Neural .pdf},
  language = {en}
}

@article{Chernoff1986,
  title = {Numerical {{Solutions}} for {{Bayes Sequential Decision Problems}}},
  author = {Chernoff, Herman and Petkau, A. John},
  year = {1986},
  month = jan,
  volume = {7},
  pages = {46--59},
  issn = {0196-5204, 2168-3417},
  doi = {10.1137/0907003},
  abstract = {Certain sequential decision problems involving normal random variables reduce to optimal stopping problems which can be related to the solution of corresponding free boundary problems for the heat equation. The numerical solution of these free boundary problems can then be approximated by calculating the solution of simpler optimal stopping problems by backward induction. This approach is not well adapted for very precise results but is surprisingly effective for rough approximations. An estimate of the difference between the solutions of the related problems permits one to make continuity corrections which provide considerably improved accuracy. Further reductions in the necessary computational effort are possible by considering truncated procedures for one-sided boundaries and by exploiting monotone and symmetric boundaries.},
  file = {/Users/qualia/Documents/Papers/1986 - Chernoff, Petkaut - Numerical solutions for bayes sequential decision.pdf;/Users/qualia/Documents/Papers/Chernoff and Petkau - 1986 - Numerical Solutions for Bayes Sequential Decision .pdf},
  journal = {SIAM Journal on Scientific and Statistical Computing},
  language = {en},
  number = {1}
}

@article{Chersi2011,
  title = {Neuronal {{Chains}} for {{Actions}} in the {{Parietal Lobe}}: {{A Computational Model}}},
  shorttitle = {Neuronal {{Chains}} for {{Actions}} in the {{Parietal Lobe}}},
  author = {Chersi, Fabian and Ferrari, Pier Francesco and Fogassi, Leonardo},
  editor = {Meck, Warren H.},
  year = {2011},
  month = nov,
  volume = {6},
  pages = {e27652},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0027652},
  abstract = {The inferior part of the parietal lobe (IPL) is known to play a very important role in sensorimotor integration. Neurons in this region code goal-related motor acts performed with the mouth, with the hand and with the arm. It has been demonstrated that most IPL motor neurons coding a specific motor act (e.g., grasping) show markedly different activation patterns according to the final goal of the action sequence in which the act is embedded (grasping for eating or grasping for placing). Some of these neurons (parietal mirror neurons) show a similar selectivity also during the observation of the same action sequences when executed by others. Thus, it appears that the neuronal response occurring during the execution and the observation of a specific grasping act codes not only the executed motor act, but also the agent's final goal (intention). In this work we present a biologically inspired neural network architecture that models mechanisms of motor sequences execution and recognition. In this network, pools composed of motor and mirror neurons that encode motor acts of a sequence are arranged in form of action goal-specific neuronal chains. The execution and the recognition of actions is achieved through the propagation of activity bursts along specific chains modulated by visual and somatosensory inputs. The implemented spiking neuron network is able to reproduce the results found in neurophysiological recordings of parietal neurons during task performance and provides a biologically plausible implementation of the action selection and recognition process. Finally, the present paper proposes a mechanism for the formation of new neural chains by linking together in a sequential manner neurons that represent subsequent motor acts, thus producing goal-directed sequences.},
  file = {/Users/qualia/Documents/Papers/2011 - Chersi, Ferrari, Fogassi - Neuronal chains for actions in the parietal lobe A computational model.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {11}
}

@article{Cheung2019,
  title = {Superposition of Many Models into One},
  author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  year = {2019},
  month = jun,
  abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
  archivePrefix = {arXiv},
  eprint = {1902.05522},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Cheung et al. - 2019 - Superposition of many models into one.pdf},
  journal = {arXiv:1902.05522 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Childs2008,
  title = {Stability Diagram for the Forced {{Kuramoto}} Model},
  author = {Childs, Lauren M. and Strogatz, Steven H.},
  year = {2008},
  month = dec,
  volume = {18},
  pages = {043128},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3049136},
  file = {/Users/qualia/Documents/Papers/2008 - Childs, Strogatz - Stability diagram for the forced Kuramoto model.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {4}
}

@article{Childs2011,
  title = {From {{Inflammation}} to {{Wound Healing}}: {{Using}} a {{Simple Model}} to {{Understand}} the {{Functional Versatility}} of {{Murine Macrophages}}},
  shorttitle = {From {{Inflammation}} to {{Wound Healing}}},
  author = {Childs, Lauren M. and Paskow, Michael and Morris, Sidney M. and Hesse, Matthias and Strogatz, Steven},
  year = {2011},
  month = nov,
  volume = {73},
  pages = {2575--2604},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/s11538-011-9637-5},
  file = {/Users/qualia/Documents/Papers/2011 - Childs et al. - From Inflammation to Wound Healing Using a Simple Model to Understand the Functional Versatility of Murine Macrop.pdf},
  journal = {Bulletin of Mathematical Biology},
  language = {en},
  number = {11}
}

@article{Chklovskii2002,
  title = {Wiring {{Optimization}} in {{Cortical Circuits}}},
  author = {Chklovskii, Dmitri B. and Schikorski, Thomas and Stevens, Charles F.},
  year = {2002},
  month = apr,
  volume = {34},
  pages = {341--347},
  issn = {08966273},
  doi = {10.1016/S0896-6273(02)00679-7},
  abstract = {Wiring a brain presents a formidable problem because neural circuits require an enormous number of fast and durable connections. We propose that evolution was likely to have optimized neural circuits to minimize conduction delays in axons, passive cable attenuation in dendrites, and the length of ``wire'' used to construct circuits, and to have maximized the density of synapses. Here we ask the question: ``What fraction of the volume should be taken up by axons and dendrites (i.e., wire) when these variables are at their optimal values?'' The biophysical properties of axons and dendrites dictate that wire should occupy 3/5 of the volume in an optimally wired gray matter. We have measured the fraction of the volume occupied by each cellular component and find that the volume of wire is close to the predicted optimal value.},
  file = {/Users/qualia/Documents/Papers/Chklovskii et al. - 2002 - Wiring Optimization in Cortical Circuits.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Cho2015,
  title = {Gamma {{Rhythms Link Prefrontal Interneuron Dysfunction}} with {{Cognitive Inflexibility}} in {{Dlx5}}/6+/- {{Mice}}},
  author = {Cho, Kathleen K.A. and Hoch, Renee and Lee, Anthony T. and Patel, Tosha and Rubenstein, John L.R. and Sohal, Vikaas S.},
  year = {2015},
  month = mar,
  volume = {85},
  pages = {1332--1343},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.02.019},
  abstract = {Abnormalities in GABAergic interneurons, particularly fast-spiking interneurons (FSINs) that generate gamma (g; \$30\textendash{}120 Hz) oscillations, are hypothesized to disrupt prefrontal cortex (PFC)-dependent cognition in schizophrenia. Although g rhythms are abnormal in schizophrenia, it remains unclear whether they directly influence cognition. Mechanisms underlying schizophrenia's typical postadolescent onset also remain elusive. We addressed these issues using mice heterozygous for Dlx5/6, which regulate GABAergic interneuron development. In Dlx5/6+/{\`A} mice, FSINs become abnormal following adolescence, coinciding with the onset of cognitive inflexibility and deficient task-evoked g oscillations. Inhibiting PFC interneurons in control mice reproduced these deficits, whereas stimulating them at g-frequencies restored cognitive flexibility in adult Dlx5/6+/{\`A} mice. These pro-cognitive effects were frequency specific and persistent. These findings elucidate a mechanism whereby abnormal FSIN development may contribute to the post-adolescent onset of schizophrenia endophenotypes. Furthermore, they demonstrate a causal, potentially therapeutic, role for PFC interneuron-driven g oscillations in cognitive domains at the core of schizophrenia.},
  file = {/Users/qualia/Documents/Papers/Cho et al. - 2015 - Gamma Rhythms Link Prefrontal Interneuron Dysfunct.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Choi,
  title = {Inverse {{Reinforcement Learning}} in {{Partially Observable Environments}}},
  author = {Choi, Jaedeug and Kim, Kee-Eung and Kr, Ai Kaist Ac and Kr, Cs Kaist Ac},
  pages = {40},
  abstract = {Inverse reinforcement learning (IRL) is the problem of recovering the underlying reward function from the behavior of an expert. Most of the existing IRL algorithms assume that the environment is modeled as a Markov decision process (MDP), although it is desirable to handle partially observable settings in order to handle more realistic scenarios. In this paper, we present IRL algorithms for partially observable environments that can be modeled as a partially observable Markov decision process (POMDP). We deal with two cases according to the representation of the given expert's behavior, namely the case in which the expert's policy is explicitly given, and the case in which the expert's trajectories are available instead. The IRL in POMDPs poses a greater challenge than in MDPs since it is not only ill-posed due to the nature of IRL, but also computationally intractable due to the hardness in solving POMDPs. To overcome these obstacles, we present algorithms that exploit some of the classical results from the POMDP literature. Experimental results on several benchmark POMDP domains show that our work is useful for partially observable settings.},
  file = {/Users/qualia/Documents/Papers/2011 - Kim - Inverse Reinforcement Learning in Partially Observable Environments.pdf},
  language = {en}
}

@article{Choudhary2018,
  title = {Weak-Winner Phase Synchronization},
  author = {Choudhary, Anshul and Saha, Arindam and Krueger, Samuel and Finke, Christian and Rosa, Jr and Freund, Jan A. and Feudel, Ulrike},
  year = {2018},
  month = dec,
  abstract = {We report the observation of a novel and non-trivial synchronization state in a system consisting of three oscillators coupled in a linear chain. For certain ranges of coupling strength the weakly coupled oscillator pair exhibits phase synchronization while the strongly coupled oscillator pair does not. This intriguing "weak-winner" synchronization phenomenon can be explained by the interplay between non-isochronicity and natural frequency of the oscillator, as coupling strength is varied. Further, we present sufficient conditions under which the weak-winner phase synchronization can occur for limit cycle as well as chaotic oscillators. Employing model system from ecology as well as a paradigmatic model from physics, we demonstrate that this phenomenon is a generic feature for a large class of coupled oscillator systems. The realization of this peculiar yet quite generic weak-winner dynamics can have far reaching consequences in a wide range of scientific disciplines that deal with the phenomenon of phase synchronization. Our results also highlight the role of non-isochronicity (shear) as a fundamental feature of an oscillator in shaping the emergent dynamics.},
  archivePrefix = {arXiv},
  eprint = {1812.02642},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Choudhary et al. - 2018 - Weak-winner phase synchronization.pdf},
  journal = {arXiv:1812.02642 [nlin]},
  keywords = {Nonlinear Sciences - Chaotic Dynamics},
  language = {en},
  primaryClass = {nlin}
}

@article{Chung2014,
  title = {Empirical {{Evaluation}} of {{Gated Recurrent Neural Networks}} on {{Sequence Modeling}}},
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  year = {2014},
  month = dec,
  abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archivePrefix = {arXiv},
  eprint = {1412.3555},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2014 - Chung et al. - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf;/Users/qualia/Documents/Papers/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Net.pdf},
  journal = {arXiv:1412.3555 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Churchland2006,
  title = {A {{Central Source}} of {{Movement Variability}}},
  author = {Churchland, Mark M. and Afshar, Afsheen and Shenoy, Krishna V.},
  year = {2006},
  month = dec,
  volume = {52},
  pages = {1085--1096},
  issn = {08966273},
  doi = {10.1016/j.neuron.2006.10.034},
  abstract = {Movements are universally, sometimes frustratingly, variable. When such variability causes error, we typically assume that something went wrong during the movement. The same assumption is made by recent and influential models of motor control. These posit that the principal limit on repeatable performance is neuromuscular noise that corrupts movement as it occurs. An alternative hypothesis is that movement variability arises before movements begin, during motor preparation. We examined this possibility directly by recording the preparatory activity of single cortical neurons during a highly practiced reach task. Small variations in preparatory neural activity were predictive of small variations in the upcoming reach. Effect magnitudes were such that at least half of the observed movement variability likely had its source during motor preparation. Thus, even for a highly practiced task, the ability to repeatedly plan the same movement limits our ability to repeatedly execute the same movement.},
  file = {/Users/qualia/Documents/Papers/2006 - Baudouin-Cornu, Bragg - Analyzing proteomic, genomic and transcriptomic elemental compositions to uncover the intimate evolution.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Churchland2007,
  title = {Delay of {{Movement Caused}} by {{Disruption}} of {{Cortical Preparatory Activity}}},
  author = {Churchland, Mark M. and Shenoy, Krishna V.},
  year = {2007},
  month = jan,
  volume = {97},
  pages = {348--359},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00808.2006},
  file = {/Users/qualia/Documents/Papers/2007 - Churchland, Shenoy - Delay of movement caused by disruption of cortical preparatory activity.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Churchland2007a,
  title = {Techniques for Extracting Single-Trial Activity Patterns from Large-Scale Neural Recordings},
  author = {Churchland, Mark M and Yu, Byron M and Sahani, Maneesh and Shenoy, Krishna V},
  year = {2007},
  month = oct,
  volume = {17},
  pages = {609--618},
  issn = {09594388},
  doi = {10.1016/j.conb.2007.11.001},
  file = {/Users/qualia/Documents/Papers/2007 - Churchland et al. - Techniques for extracting single-trial activity patterns from large-scale neural recordings.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {5}
}

@article{Churchland2010,
  title = {Cortical {{Preparatory Activity}}: {{Representation}} of {{Movement}} or {{First Cog}} in a {{Dynamical Machine}}?},
  shorttitle = {Cortical {{Preparatory Activity}}},
  author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Ryu, Stephen I. and Shenoy, Krishna V.},
  year = {2010},
  month = nov,
  volume = {68},
  pages = {387--400},
  issn = {08966273},
  doi = {10.1016/j.neuron.2010.09.015},
  abstract = {The motor cortices are active during both movement and movement preparation. A common assumption is that preparatory activity constitutes a subthreshold form of movement activity: a neuron active during rightward movements becomes modestly active during preparation of a rightward movement. We asked whether this pattern of activity is, in fact, observed. We found that it was not: at the level of a single neuron, preparatory tuning was weakly correlated with movement-period tuning. Yet, somewhat paradoxically, preparatory tuning could be captured by a preferred direction in an abstract ``space'' that described the population-level pattern of movement activity. In fact, this relationship accounted for preparatory responses better than did traditional tuning models. These results are expected if preparatory activity provides the initial state of a dynamical system whose evolution produces movement activity. Our results thus suggest that preparatory activity may not represent specific factors, and may instead play a more mechanistic role.},
  file = {/Users/qualia/Documents/Papers/2010 - Churchland et al. - Cortical Preparatory Activity Representation of Movement or First Cog in a Dynamical Machine.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Churchland2010a,
  title = {Stimulus Onset Quenches Neural Variability: A Widespread Cortical Phenomenon},
  shorttitle = {Stimulus Onset Quenches Neural Variability},
  author = {Churchland, Mark M and Yu, Byron M and Cunningham, John P and Sugrue, Leo P and Cohen, Marlene R and Corrado, Greg S and Newsome, William T and Clark, Andrew M and Hosseini, Paymon and Scott, Benjamin B and Bradley, David C and Smith, Matthew A and Kohn, Adam and Movshon, J Anthony and Armstrong, Katherine M and Moore, Tirin and Chang, Steve W and Snyder, Lawrence H and Lisberger, Stephen G and Priebe, Nicholas J and Finn, Ian M and Ferster, David and Ryu, Stephen I and Santhanam, Gopal and Sahani, Maneesh and Shenoy, Krishna V},
  year = {2010},
  month = mar,
  volume = {13},
  pages = {369--378},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2501},
  file = {/Users/qualia/Documents/Papers/2010 - Churchland et al. - Stimulus onset quenches neural variability a widespread cortical phenomenon.pdf;/Users/qualia/Documents/Papers/2010 - Churchland et al. - Stimulus onset quenches neural variability a widespread cortical phenomenon(2).pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Churchland2012,
  title = {Neural Population Dynamics during Reaching},
  author = {Churchland, Mark M. and Cunningham, John P. and Kaufman, Matthew T. and Foster, Justin D. and Nuyujukian, Paul and Ryu, Stephen I. and Shenoy, Krishna V.},
  year = {2012},
  month = jul,
  volume = {487},
  pages = {51--56},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature11129},
  file = {/Users/qualia/Documents/Papers/2012 - Churchland et al. - Neural population dynamics during reaching.pdf},
  journal = {Nature},
  language = {en},
  number = {7405}
}

@article{Churchland2012a,
  title = {Two Layers of Neural Variability},
  author = {Churchland, Mark M and Abbott, L F},
  year = {2012},
  month = nov,
  volume = {15},
  pages = {1472--1474},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3247},
  file = {/Users/qualia/Documents/Papers/2012 - Churchland, Abbott - Two layers of neural variability(2).pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Cisek2009,
  title = {Decisions in {{Changing Conditions}}: {{The Urgency}}-{{Gating Model}}},
  shorttitle = {Decisions in {{Changing Conditions}}},
  author = {Cisek, P. and Puskas, G. A. and {El-Murr}, S.},
  year = {2009},
  month = sep,
  volume = {29},
  pages = {11560--11571},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1844-09.2009},
  file = {/Users/qualia/Documents/Papers/2009 - Cisek, Puskas, El-Murr - Decisions in changing conditions the urgency-gating model.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@article{Cisek2019,
  title = {Resynthesizing Behavior through Phylogenetic Refinement},
  author = {Cisek, Paul},
  year = {2019},
  month = jun,
  issn = {1943-3921, 1943-393X},
  doi = {10.3758/s13414-019-01760-1},
  abstract = {This article proposes that biologically plausible theories of behavior can be constructed by following a method of Bphylogenetic refinement,\^ whereby they are progressively elaborated from simple to complex according to phylogenetic data on the sequence of changes that occurred over the course of evolution. It is argued that sufficient data exist to make this approach possible, and that the result can more effectively delineate the true biological categories of neurophysiological mechanisms than do approaches based on definitions of putative functions inherited from psychological traditions. As an example, the approach is used to sketch a theoretical framework of how basic feedback control of interaction with the world was elaborated during vertebrate evolution, to give rise to the functional architecture of the mammalian brain. The results provide a conceptual taxonomy of mechanisms that naturally map to neurophysiological and neuroanatomical data and that offer a context for defining putative functions that, it is argued, are better grounded in biology than are some of the traditional concepts of cognitive science.},
  file = {/Users/qualia/Documents/Papers/Cisek - 2019 - Resynthesizing behavior through phylogenetic refin.pdf},
  journal = {Atten Percept Psychophys},
  language = {en}
}

@article{Claus,
  title = {The {{Dynamics}} of {{Reinforcement Learning}} in {{Cooperative Multiagent Systems}}},
  author = {Claus, Caroline and Boutilier, Craig},
  pages = {7},
  abstract = {Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.},
  file = {/Users/qualia/Documents/Papers/1998 - Claus, Boutilier - The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems.pdf},
  language = {en}
}

@article{Clithero2011,
  title = {Within- and Cross-Participant Classifiers Reveal Different Neural Coding of Information},
  author = {Clithero, John A. and Smith, David V. and Carter, R. McKell and Huettel, Scott A.},
  year = {2011},
  month = may,
  volume = {56},
  pages = {699--708},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.03.057},
  abstract = {Analyzing distributed patterns of brain activation using multivariate pattern analysis (MVPA) has become a popular approach for using functional magnetic resonance imaging (fMRI) data to predict mental states. While the majority of studies currently build separate classifiers for each participant in the sample, in principle a single classifier can be derived from and tested on data from all participants. These two approaches, within- and cross-participant classification, rely on potentially different sources of variability and thus may provide distinct information about brain function. Here, we used both approaches to identify brain regions that contain information about passively received monetary rewards (i.e., images of currency that influenced participant payment) and social rewards (i.e., images of human faces). Our withinparticipant analyses implicated regions in the ventral visual processing stream\textemdash{}including fusiform gyrus and primary visual cortex\textemdash{}and ventromedial prefrontal cortex (VMPFC). Two key results indicate these regions may contain statistically discriminable patterns that contain different informational representations. First, cross-participant analyses implicated additional brain regions, including striatum and anterior insula. The cross-participant analyses also revealed systematic changes in predictive power across brain regions, with the pattern of change consistent with the functional properties of regions. Second, individual differences in classifier performance in VMPFC were related to individual differences in preferences between our two reward modalities. We interpret these results as reflecting a distinction between patterns showing participant-specific functional organization and those indicating aspects of brain organization that generalize across individuals.},
  file = {/Users/qualia/Documents/Papers/2011 - Clithero et al. - Within- and cross-participant classifiers reveal different neural coding of information.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Cocchi2016,
  title = {A Hierarchy of Timescales Explains Distinct Effects of Local Inhibition of Primary Visual Cortex and Frontal Eye Fields},
  author = {Cocchi, Luca and Sale, Martin V and L Gollo, Leonardo and Bell, Peter T and Nguyen, Vinh T and Zalesky, Andrew and Breakspear, Michael and Mattingley, Jason B},
  year = {2016},
  month = sep,
  volume = {5},
  issn = {2050-084X},
  doi = {10.7554/eLife.15252},
  file = {/Users/qualia/Documents/Papers/Cocchi et al. - 2016 - A hierarchy of timescales explains distinct effect.pdf},
  journal = {eLife},
  language = {en}
}

@article{CogliatiDezza2017,
  title = {Learning the Value of Information and Reward over Time When Solving Exploration-Exploitation Problems},
  author = {Cogliati Dezza, Irene and Yu, Angela J. and Cleeremans, Axel and Alexander, William},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-17237-w},
  file = {/Users/qualia/Documents/Papers/Cogliati Dezza et al. - 2017 - Learning the value of information and reward over .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Cohen-KashiMalina2016,
  title = {Local and Thalamic Origins of Ongoing and Sensory Evoked Cortical Correlations},
  author = {{Cohen-Kashi Malina}, Katayun and Mohar, Boaz and Rappaport, Akiva N and Lampl, Ilan},
  year = {2016},
  month = jun,
  doi = {10.1101/058727},
  abstract = {Thalamic inputs of layer 4 (L4) cells in sensory cortices are outnumbered by local connections. Thus, it was suggested that robust sensory response in L4 emerges due to synchronized thalamic activity. In order to investigate the role of both inputs in generation of cortical synchronization, we isolated the thalamic excitatory inputs of cortical cells by optogenetically silencing cortical firing. In anesthetized mice, we measured the correlation between isolated thalamic synaptic inputs of simultaneously patched nearby L4 cells of the barrel cortex. In contrast to correlated activity of excitatory synaptic inputs in the intact cortex, isolated thalamic inputs exhibit lower variability and asynchronous spontaneous and sensory evoked inputs. These results were further supported in awake mice when we recorded the excitatory inputs of individual cortical cells simultaneously with the local field potential (LFP) in a nearby site. Our results therefore indicate that cortical synchronization emerges by intracortical coupling.},
  file = {/Users/qualia/Documents/Papers/Cohen-Kashi Malina et al. - 2016 - Local and thalamic origins of ongoing and sensory .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Cohen2014,
  title = {Fluctuations in {{Oscillation Frequency Control Spike Timing}} and {{Coordinate Neural Networks}}},
  author = {Cohen, M. X.},
  year = {2014},
  month = jul,
  volume = {34},
  pages = {8988--8998},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0261-14.2014},
  file = {/Users/qualia/Documents/Papers/2014 - Cohen - Fluctuations in Oscillation Frequency Control Spike Timing.pdf;/Users/qualia/Documents/Papers/Cohen - 2014 - Fluctuations in Oscillation Frequency Control Spik.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {27}
}

@article{Cohen2015,
  title = {Serotonergic Neurons Signal Reward and Punishment on Multiple Timescales},
  author = {Cohen, Jeremiah Y and Amoroso, Mackenzie W and Uchida, Naoshige},
  year = {2015},
  month = feb,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.06346},
  file = {/Users/qualia/Documents/Papers/2015 - Cohen, Amoroso, Uchida - Serotonergic neurons signal reward and punishment on multiple timescales.pdf;/Users/qualia/Documents/Papers/Cohen et al. - 2015 - Serotonergic neurons signal reward and punishment .pdf},
  journal = {eLife},
  language = {en}
}

@article{Cohen2017,
  title = {Computational Approaches to {{fMRI}} Analysis},
  author = {Cohen, Jonathan D and Daw, Nathaniel and Engelhardt, Barbara and Hasson, Uri and Li, Kai and Niv, Yael and Norman, Kenneth A and Pillow, Jonathan and Ramadge, Peter J and {Turk-Browne}, Nicholas B and Willke, Theodore L},
  year = {2017},
  month = mar,
  volume = {20},
  pages = {304--313},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4499},
  file = {/Users/qualia/Documents/Papers/Cohen et al. - 2017 - Computational approaches to fMRI analysis.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Colas2019,
  title = {{{CURIOUS}}: {{Intrinsically Motivated Multi}}-{{Task}}  {{Multi}}-{{Goal Reinforcement Learning}}},
  author = {Colas, C{\'e}dric and Fournier, Pierre and Sigaud, Olivier and Chetouani, Mohamed and Oudeyer, Pierre-Yves},
  year = {2019},
  volume = {1810.06284v3},
  pages = {1--15},
  abstract = {In open-ended and changing environments, agents face a wide range of potential tasks that might not come with associated reward functions. Such autonomous learning agents must set their own tasks and build their own curriculum through an intrinsically motivated exploration. Because some tasks might prove easy and some impossible, agents must actively select which task to practice at any given moment, to maximize their overall mastery on the set of learnable tasks. This paper proposes CURIOUS, an algorithm that leverages: 1) an extension of Universal Value Function Approximators to achieve, within a unique policy, multiple tasks, each parameterized by multiple goals and 2) an automated curriculum learning mechanism that biases the attention of the agent towards tasks maximizing the absolute learning progress. Agents focus on achievable tasks first, and focus back on tasks that are being forgotten. Experiments conducted in a new multi-task multigoal robotic environment show that our algorithm benefits from these two ideas and demonstrate properties of robustness to distracting tasks, forgetting and changes in body properties.},
  file = {/Users/qualia/Documents/Papers/Colas et al. - CURIOUS Intrinsically Motivated Multi-Task  Multi.pdf},
  journal = {Arxiv},
  language = {en}
}

@techreport{Cole2016,
  title = {Nonsinusoidal Oscillations Underlie Pathological Phase-Amplitude Coupling in the Motor Cortex in {{Parkinson}}'s Disease},
  author = {Cole, Scott R. and Peterson, Erik J. and {van der Meij}, Roemer and {de Hemptinne}, Coralie and Starr, Philip A. and Voytek, Bradley},
  year = {2016},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/049304},
  abstract = {Parkinson's disease (PD) is associated with abnormal beta oscillations (13-30 Hz) in the basal ganglia and motor cortex (M1). Recent reports show that M1 beta-high gamma (50-200 Hz) phase-amplitude coupling (PAC) is exaggerated in PD and is reduced following acute deep brain stimulation (DBS). Here we analyze invasive M1 electrocorticography recordings in PD patients on and off DBS, and in isolated cervical dystonia patients, and show that M1 beta oscillations are nonsinusoidal, having sharp and asymmetric features. These sharp oscillatory beta features underlie the previously reported PAC, providing an alternative to the standard interpretation of PAC as an interaction between two distinct frequency components. Specifically, the ratio between peak and trough sharpness is nearly perfectly correlated with beta-high gamma PAC (r = 0.96) and predicts PD-related motor deficit. Using a simulation of the local field potential, we demonstrate that sharp oscillatory waves can arise from synchronous synaptic activity. We propose that exaggerated beta-high gamma PAC may actually reflect such synchronous synaptic activity, manifesting as sharp beta oscillations that are ``smoothed out'' with DBS. These results support the ``desynchronization'' hypothesis of DBS wherein DBS counteracts pathological synchronization throughout the basal ganglia-thalamocortical loop. We argue that PAC can be influenced by more than one mechanism. In this case synaptic synchrony, rather than the often assumed spike-field coherence, may underlie exaggerated PAC. These often overlooked temporal features of the oscillatory waveform carry critical physiological information about neural processes and dynamics that may lead to better understanding of underlying neuropathology.},
  file = {/Users/qualia/Documents/Papers/Cole et al. - 2016 - Nonsinusoidal oscillations underlie pathological p.pdf},
  language = {en},
  type = {Preprint}
}

@article{Cole2017,
  title = {Brain {{Oscillations}} and the {{Importance}} of {{Waveform Shape}}},
  author = {Cole, Scott R. and Voytek, Bradley},
  year = {2017},
  month = feb,
  volume = {21},
  pages = {137--149},
  issn = {13646613},
  doi = {10.1016/j.tics.2016.12.008},
  file = {/Users/qualia/Documents/Papers/Cole and Voytek - 2017 - Brain Oscillations and the Importance of Waveform  2.pdf;/Users/qualia/Documents/Papers/Cole and Voytek - 2017 - Brain Oscillations and the Importance of Waveform .pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {2}
}

@article{Cole2017a,
  title = {Nonsinusoidal {{Beta Oscillations Reflect Cortical Pathophysiology}} in {{Parkinson}}'s {{Disease}}},
  author = {Cole, Scott R. and {van der Meij}, Roemer and Peterson, Erik J. and {de Hemptinne}, Coralie and Starr, Philip A. and Voytek, Bradley},
  year = {2017},
  month = may,
  volume = {37},
  pages = {4830--4840},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2208-16.2017},
  file = {/Users/qualia/Documents/Papers/Cole et al. - 2017 - Nonsinusoidal Beta Oscillations Reflect Cortical P.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {18}
}

@techreport{Cole2018,
  title = {Hippocampal Theta Bursting and Waveform Shape Reflect {{CA1}} Spiking Patterns},
  author = {Cole, Scott and Voytek, Bradley},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/452987},
  abstract = {Brain rhythms are nearly always analyzed in the spectral domain in terms of their power, phase, and frequency. While this conventional approach has uncovered spike-field coupling, as well as correlations to normal behaviors and pathological states, emerging work has highlighted the physiological and behavioral importance of multiple novel oscillation features. Oscillatory bursts, for example, uniquely index a variety of cognitive states, and the nonsinusoidal shape of oscillations relate to physiological changes, including Parkinson's disease. Open questions remain regarding how bursts and nonsinusoidal features relate to circuit-level processes, and how they interrelate. By analyzing unit and local field recordings in the rodent hippocampus, we uncover a number of significant relationships between oscillatory bursts, nonsinusoidal waveforms, and local inhibitory and excitatory spiking patterns. Bursts of theta oscillations are surprisingly related to a decrease in pyramidal neuron synchrony, and have no detectable effect on firing sequences, despite significant increases in neuronal firing rates during periods of theta bursting. Theta burst duration is predicted by the asymmetries of its first cycle, and cycle asymmetries relate to firing rate, synchrony, and sequences of pyramidal neurons and interneurons. These results provide compelling physiological evidence that time-domain features, of both nonsinusoidal hippocampal theta waveform and the theta bursting state, reflects local circuit properties. These results point to the possibility of inferring circuit states from local field potential features in the hippocampus and perhaps other brain regions with other rhythms.},
  file = {/Users/qualia/Documents/Papers/Cole and Voytek - 2018 - Hippocampal theta bursting and waveform shape refl.pdf},
  language = {en},
  type = {Preprint}
}

@article{Cole2019,
  title = {Cycle-by-Cycle Analysis of Neural Oscillations},
  author = {Cole, Scott and Voytek, Bradley},
  year = {2019},
  volume = {122},
  pages = {849--861},
  file = {/Users/qualia/Documents/Papers/Cole and Voytek - Cycle-by-cycle analysis of neural oscillations.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Colgin2009,
  title = {Frequency of Gamma Oscillations Routes Flow of Information in the Hippocampus},
  author = {Colgin, Laura Lee and Denninger, Tobias and Fyhn, Marianne and Hafting, Torkel and Bonnevie, Tora and Jensen, Ole and Moser, May-Britt and Moser, Edvard I.},
  year = {2009},
  month = nov,
  volume = {462},
  pages = {353--357},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature08573},
  file = {/Users/qualia/Documents/Papers/Colgin et al. - 2009 - Frequency of gamma oscillations routes flow of inf.pdf},
  journal = {Nature},
  language = {en},
  number = {7271}
}

@article{Colgin2016,
  title = {Rhythms of the Hippocampal Network},
  author = {Colgin, Laura Lee},
  year = {2016},
  month = apr,
  volume = {17},
  pages = {239--249},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2016.21},
  abstract = {The hippocampal local field potential (LFP) shows three major types of rhythms: theta, sharp wave\textendash{}ripples and gamma. These rhythms are defined by their frequencies, they have behavioural correlates in several species including rats and humans, and they have been proposed to carry out distinct functions in hippocampal memory processing. However, recent findings have challenged traditional views on these behavioural functions. In this Review, I discuss our current understanding of the origins and the mnemonic functions of hippocampal theta, sharp wave\textendash{}ripples and gamma rhythms on the basis of findings from rodent studies. In addition, I present an updated synthesis of their roles and interactions within the hippocampal network.},
  file = {/Users/qualia/Documents/Papers/Colgin - 2016 - Rhythms of the hippocampal network.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {4}
}

@article{Colgin2016a,
  title = {Rhythms of the Hippocampal Network},
  author = {Colgin, Laura Lee},
  year = {2016},
  month = apr,
  volume = {17},
  pages = {239--249},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2016.21},
  abstract = {The hippocampal local field potential (LFP) exhibits three major types of rhythms, theta, sharp wave-ripples and gamma. These rhythms are defined by their frequencies, have behavioral correlates in several species including rats and humans, and have been proposed to perform distinct functions in hippocampal memory processing. However, recent findings have challenged traditional views on these behavioral functions. Here I review our current understanding of the origins and mnemonic functions of hippocampal theta, sharp-wave ripples and gamma rhythms based on findings from rodent studies, and present an updated, synthesized view of their roles and interactions within the hippocampal network.},
  file = {/Users/qualia/Documents/Papers/Colgin - 2016 - Rhythms of the hippocampal network 2.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {4}
}

@article{Collins2017,
  title = {{{CAPACITY AND TRAINABILITY IN RECURRENT NEURAL NETWORKS}}},
  author = {Collins, Jasmine and {Sohl-Dickstein}, Jascha and Sussillo, David},
  year = {2017},
  pages = {16},
  abstract = {Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU.},
  file = {/Users/qualia/Documents/Papers/Collins et al. - 2017 - CAPACITY AND TRAINABILITY IN RECURRENT NEURAL NETW.pdf},
  language = {en}
}

@article{Cona2014,
  title = {A Thalamo-Cortical Neural Mass Model for the Simulation of Brain Rhythms during Sleep},
  author = {Cona, F. and Lacanna, M. and Ursino, M.},
  year = {2014},
  month = aug,
  volume = {37},
  pages = {125--148},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-013-0493-1},
  file = {/Users/qualia/Documents/Papers/2014 - Cona, Lacanna, Ursino - A thalamo-cortical neural mass model for the simulation of brain rhythms during sleep.pdf;/Users/qualia/Documents/Papers/Cona et al. - 2014 - A thalamo-cortical neural mass model for the simul.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Conaway2017,
  title = {Solving {{Nonlinearly Separable Classifications}} in a {{Single}}-{{Layer Neural Network}}},
  author = {Conaway, Nolan and Kurtz, Kenneth J.},
  year = {2017},
  month = mar,
  volume = {29},
  pages = {861--866},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00931},
  file = {/Users/qualia/Documents/Papers/Conaway and Kurtz - 2017 - Solving Nonlinearly Separable Classifications in a.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {3}
}

@article{Connolly2012,
  title = {The {{Representation}} of {{Biological Classes}} in the {{Human Brain}}},
  author = {Connolly, A. C. and Guntupalli, J. S. and Gors, J. and Hanke, M. and Halchenko, Y. O. and Wu, Y.-C. and Abdi, H. and Haxby, J. V.},
  year = {2012},
  month = feb,
  volume = {32},
  pages = {2608--2618},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5547-11.2012},
  file = {/Users/qualia/Documents/Papers/2012 - Connolly et al. - The representation of biological classes in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {8}
}

@inproceedings{Connolly2015,
  title = {Guiding Deep Brain Stimulation Contact Selection Using Local Field Potentials Sensed by a Chronically Implanted Device in {{Parkinson}}'s Disease Patients},
  booktitle = {2015 7th {{International IEEE}}/{{EMBS Conference}} on {{Neural Engineering}} ({{NER}})},
  author = {Connolly, Allison T. and Kaemmerer, William F. and Dani, Siddharth and Stanslaski, Scott R. and Panken, Eric and Johnson, Matthew D. and Denison, Timothy},
  year = {2015},
  month = apr,
  pages = {840--843},
  publisher = {{IEEE}},
  address = {{Montpellier, France}},
  doi = {10.1109/NER.2015.7146754},
  abstract = {We have found that a set of support vector machines operating upon local field potentials sensed from an implanted DBS lead can identify the contact chosen by the physician for the patient's STN DBS therapy with 91\% accuracy. The finding is based on a small data set and thus subject to change with further data collection and crossvalidation. Nevertheless, the results suggest that an algorithm for selecting an effective contact for STN DBS based on the signals sensed from the DBS lead may be feasible.},
  file = {/Users/qualia/Documents/Papers/2015 - Connolly et al. - Guiding Deep Brain Stimulation Contact Selection Using Local Field Potentials Sensed by a Chronically Implant.pdf;/Users/qualia/Documents/Papers/Connolly et al. - 2015 - Guiding deep brain stimulation contact selection u.pdf},
  isbn = {978-1-4673-6389-1},
  language = {en}
}

@article{Connolly2015a,
  title = {Modulations in {{Oscillatory Frequency}} and {{Coupling}} in {{Globus Pallidus}} with {{Increasing Parkinsonian Severity}}},
  author = {Connolly, Allison T. and Jensen, Alicia L. and Bello, Edward M. and Netoff, Theoden I. and Baker, Kenneth B. and Johnson, Matthew D. and Vitek, Jerrold L.},
  year = {2015},
  month = apr,
  volume = {35},
  pages = {6231--6240},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4137-14.2015},
  file = {/Users/qualia/Documents/Papers/Connolly et al. - 2015 - Modulations in Oscillatory Frequency and Coupling .pdf;/Users/qualia/Zotero/storage/65572R6R/Connolly et al. - 2015 - Modulations in Oscillatory Frequency and Coupling .pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {15}
}

@article{Connor1977,
  title = {Neural Repetitive Firing: Modifications of the {{Hodgkin}}-{{Huxley}} Axon Suggested by Experimental Results from Crustacean Axons},
  shorttitle = {Neural Repetitive Firing},
  author = {Connor, J.A. and Walter, D. and McKown, R.},
  year = {1977},
  month = apr,
  volume = {18},
  pages = {81--102},
  issn = {00063495},
  doi = {10.1016/S0006-3495(77)85598-7},
  abstract = {The Hodgkin-Huxley equations for space-clamped squid axon (18'C) have been modified to approximate voltage clamp data from repetitive-firing crustacean walking leg axons and activity in response to constant current stimulation has been computed. The ino and h. parameters of the sodium conductance system were shifted along the voltage axis in opposite directions so that their relative overlap was increased approximately 7 mV. Time constants, Tm and Th, were moved in a similar manner. Voltage-dependent parameters of delayed potassium conductance, n,O and T, were shifted 4.3 mV in the positive direction and Tr was uniformly increased by a factor of 2. Leakage conductance and capacitance were unchanged. Repetitive activity of this modified circuit was qualitatively similar to that of the standard model. A fifth branch was added to the circuit representing a transient potassium conductance system present in the repetitive walking leg axons and in other repetitive neurons. This model, with various parameter choices, fired repetitively down to approximately 2 spikes/s and up to 350/s. The frequency vs. stimulus current plot could be fit well by a straight line over a decade of the low frequency range and the general appearance of the spike trains was similar to that of other repetitive neurons. Stimulus intensities were of the same order as those which produce repetitive activity in the standard Hodgkin-Huxley axon. The repetitive firing rate and first spike latency (utilization time) were found to be most strongly influenced by the inactivation time constant of the transient potassium conductance (TB), the delayed potassium conductance (Tn), and the value of leakage conductance (ga. The model presents a mechanism by which stable low frequency discharge can be generated by millisecondorder membrane conductance changes.},
  file = {/Users/qualia/Documents/Papers/1977 - The - Modifications of the Hodgkin-Huxley Axon Suggested.pdf},
  journal = {Biophysical Journal},
  language = {en},
  number = {1}
}

@article{Conroy,
  title = {{{fMRI}}-{{Based Inter}}-{{Subject Cortical Alignment Using Functional Connectivity}}},
  author = {Conroy, Bryan R and Singer, Benjamin D and Haxby, James V and Ramadge, Peter J},
  pages = {9},
  abstract = {The inter-subject alignment of functional MRI (fMRI) data is important for improving the statistical power of fMRI group analyses. In contrast to existing anatomically-based methods, we propose a novel multi-subject algorithm that derives a functional correspondence by aligning spatial patterns of functional connectivity across a set of subjects. We test our method on fMRI data collected during a movie viewing experiment. By cross-validating the results of our algorithm, we show that the correspondence successfully generalizes to a secondary movie dataset not used to derive the alignment.},
  file = {/Users/qualia/Documents/Papers/2009 - Conroy, Singer - fMRI-based inter-subject cortical alignment using functional connectivity.pdf},
  language = {en}
}

@article{Contin2010,
  title = {Pharmacokinetics of Levodopa},
  author = {Contin, Manuela and Martinelli, Paolo},
  year = {2010},
  month = nov,
  volume = {257},
  pages = {253--261},
  issn = {0340-5354, 1432-1459},
  doi = {10.1007/s00415-010-5728-8},
  file = {/Users/qualia/Documents/Papers/2010 - Contin, Martinelli - Pharmacokinetics of levodopa.pdf},
  journal = {Journal of Neurology},
  language = {en},
  number = {S2}
}

@article{Contractor2015,
  title = {Altered {{Neuronal}} and {{Circuit Excitability}} in {{Fragile X Syndrome}}},
  author = {Contractor, Anis and Klyachko, Vitaly A. and {Portera-Cailliau}, Carlos},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {699--715},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.06.017},
  file = {/Users/qualia/Documents/Papers/Contractor et al. - 2015 - Altered Neuronal and Circuit Excitability in Fragi.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Cook,
  title = {It {{Takes Two Neurons To Ride}} a {{Bicycle}}},
  author = {Cook, Matthew},
  pages = {8},
  abstract = {Past attempts to get computers to ride bicycles have required an inordinate amount of learning time (1700 practice rides for a reinforcement learning approach [1], while still failing to be able to ride in a straight line), or have required an algebraic analysis of the exact equations of motion for the specific bicycle to be controlled [2, 3]. Mysteriously, humans do not need to do either of these when learning to ride a bicycle.},
  file = {/Users/qualia/Documents/Papers/2004 - Cook - It takes two neurons to ride a bicycle.pdf},
  language = {en}
}

@article{Cools2006,
  title = {Dopaminergic Modulation of Cognitive Function-Implications for l-{{DOPA}} Treatment in {{Parkinson}}'s Disease},
  author = {Cools, Roshan},
  year = {2006},
  month = jan,
  volume = {30},
  pages = {1--23},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2005.03.024},
  abstract = {It is well recognised that patients with Parkinson's disease exhibit cognitive deficits, even in the earliest disease stages. Whereas, L-DOPA therapy in early Parkinson's disease is accepted to improve the motor symptoms, the effects on cognitive performance are more complex: both positive and negative effects have been observed. The purpose of the present article is to review the effects of L-DOPA medication in Parkinson's disease on cognitive functions in the broad domains of cognitive flexibility and working memory. The review places the effects in Parkinson's disease within a framework of evidence from studies with healthy human volunteers, rodents and non-human primates as well as computational modeling work. It is suggested that beneficial or detrimental effects of L-DOPA are observed depending on task demands and basal dopamine levels in distinct parts of the striatum. The study of the beneficial and detrimental cognitive effects of L-DOPA in Parkinson's disease has substantial implications for the understanding and treatment development of cognitive abnormalities in Parkinson's disease as well as normal health.},
  file = {/Users/qualia/Documents/Papers/2006 - Cools - Dopaminergic modulation of cognitive function-implications for L-DOPA treatment in Parkinson's disease.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en},
  number = {1}
}

@article{Coombes2005,
  title = {Waves, Bumps, and Patterns in Neural Field Theories},
  author = {Coombes, S.},
  year = {2005},
  month = aug,
  volume = {93},
  pages = {91--108},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-005-0574-y},
  abstract = {Neural field models of firing rate activity have had a major impact in helping to develop an understanding of the dynamics seen in brain slice preparations. These models typically take the form of integrodifferential equations. Their non-local nature has led to the development of a set of analytical and numerical tools for the study of waves, bumps and patterns, based around natural extensions of those used for local differential equation models. In this paper we present a review of such techniques and show how recent advances have opened the way for future studies of neural fields in both one and two dimensions that can incorporate realistic forms of axo-dendritic interactions and the slow intrinsic currents that underlie bursting behaviour in single neurons.},
  file = {/Users/qualia/Documents/Papers/2005 - Coombes - Waves, bumps, and patterns in neural field theories.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {2}
}

@article{Coon2016,
  title = {Oscillatory Phase Modulates the Timing of Neuronal Activations and Resulting Behavior},
  author = {Coon, W.G. and Gunduz, A. and Brunner, P. and Ritaccio, A.L. and Pesaran, B. and Schalk, G.},
  year = {2016},
  month = jun,
  volume = {133},
  pages = {294--301},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2016.02.080},
  abstract = {Human behavioral response timing is highly variable from trial to trial. While it is generally understood that behavioral variability must be due to trial-by-trial variations in brain function, it is still largely unknown which physiological mechanisms govern the timing of neural activity as it travels through networks of neuronal populations, and how variations in the timing of neural activity relate to variations in the timing of behavior. In our study, we submitted recordings from the cortical surface to novel analytic techniques to chart the trajectory of neuronal population activity across the human cortex in single trials, and found joint modulation of the timing of this activity and of consequent behavior by neuronal oscillations in the alpha band (8\textendash{}12 Hz). Specifically, we established that the onset of population activity tends to occur during the trough of oscillatory activity, and that deviations from this preferred relationship are related to changes in the timing of population activity and the speed of the resulting behavioral response. These results indicate that neuronal activity incurs variable delays as it propagates across neuronal populations, and that the duration of each delay is a function of the instantaneous phase of oscillatory activity. We conclude that the results presented in this paper are supportive of a general model for variability in the effective speed of information transmission in the human brain and for variability in the timing of human behavior.},
  file = {/Users/qualia/Documents/Papers/Coon et al. - 2016 - Oscillatory phase modulates the timing of neuronal 2.pdf;/Users/qualia/Documents/Papers/Coon et al. - 2016 - Oscillatory phase modulates the timing of neuronal.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Cooper2003,
  title = {Paradox Lost? {{Exploring}} the Role of Alpha Oscillations during Externally vs. Internally Directed Attention and the Implications for Idling and Inhibition Hypotheses},
  shorttitle = {Paradox Lost?},
  author = {Cooper, Nicholas R and Croft, Rodney J and Dominey, Samuel J.J and Burgess, Adrian P and Gruzelier, John H},
  year = {2003},
  month = jan,
  volume = {47},
  pages = {65--74},
  issn = {01678760},
  doi = {10.1016/S0167-8760(02)00107-1},
  abstract = {Although slow waves of the electroencephalogram (EEG) have been associated with attentional processes, the functional significance of the alpha component in the EEG (8.1\textendash{}12 Hz) remains uncertain. Conventionally, synchronisation in the alpha frequency range is taken to be a marker of cognitive inactivity, i.e. `cortical idling'. However, it has been suggested that alpha may index the active inhibition of sensory information during internally directed attentional tasks such as mental imagery. More recently, this idea has been amended to encompass the notion of alpha synchronisation as a means of inhibition of non-task relevant cortical areas irrespective of the direction of attention. Here we test the adequacy of the one idling and two inhibition hypotheses about alpha. In two experiments we investigated the relation between alpha and internally vs. externally directed attention using mental imagery vs. sensory-intake paradigms. Results from both experiments showed a clear relationship between alpha and both attentional factors and increased task demands. At various scalp sites alpha amplitudes were greater during internally directed attention and during increased load, results incompatible with alpha reflecting cortical idling and more in keeping with suggestions of active inhibition necessary for internally driven mental operations.},
  file = {/Users/qualia/Documents/Papers/2003 - Cooper et al. - Paradox lost Exploring the role of alpha oscillations during externally vs. internally directed attention and the.pdf},
  journal = {International Journal of Psychophysiology},
  language = {en},
  number = {1}
}

@article{Corbit2016,
  title = {Pallidostriatal {{Projections Promote Oscillations}} in a {{Dopamine}}-{{Depleted Biophysical Network Model}}},
  author = {Corbit, V. L. and Whalen, T. C. and Zitelli, K. T. and Crilly, S. Y. and Rubin, J. E. and Gittis, A. H.},
  year = {2016},
  month = may,
  volume = {36},
  pages = {5556--5571},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0339-16.2016},
  file = {/Users/qualia/Documents/Papers/Corbit et al. - 2016 - Pallidostriatal Projections Promote Oscillations i.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {20}
}

@article{Cornell-Bell1990,
  title = {Glutamate Induces Calcium Waves in Cultured Astrocytes: Long-Range Glial Signaling},
  shorttitle = {Glutamate Induces Calcium Waves in Cultured Astrocytes},
  author = {{Cornell-Bell}, A. and Finkbeiner, S. and Cooper, M. and Smith, S.},
  year = {1990},
  month = jan,
  volume = {247},
  pages = {470--473},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1967852},
  file = {/Users/qualia/Documents/Papers/Cornell-Bell et al. - 1990 - Glutamate induces calcium waves in cultured astroc.pdf},
  journal = {Science},
  language = {en},
  number = {4941}
}

@article{Cornell-Bell1990a,
  title = {Glutamate Induces Calcium Waves in Cultured Astrocytes: Long-Range Glial Signaling},
  shorttitle = {Glutamate Induces Calcium Waves in Cultured Astrocytes},
  author = {{Cornell-Bell}, A. and Finkbeiner, S. and Cooper, M. and Smith, S.},
  year = {1990},
  month = jan,
  volume = {247},
  pages = {470--473},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1967852},
  file = {/Users/qualia/Documents/Papers/Cornell-Bell et al. - 1990 - Glutamate induces calcium waves in cultured astroc 2.pdf},
  journal = {Science},
  language = {en},
  number = {4941}
}

@article{Cossell2015,
  title = {Functional Organization of Excitatory Synaptic Strength in Primary Visual Cortex},
  author = {Cossell, Lee and Iacaruso, Maria Florencia and Muir, Dylan R. and Houlton, Rachael and Sader, Elie N. and Ko, Ho and Hofer, Sonja B. and {Mrsic-Flogel}, Thomas D.},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {399--403},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14182},
  file = {/Users/qualia/Documents/Papers/2015 - Cossell et al. - Functional organization of excitatory synaptic strength in primary visual cortex.pdf;/Users/qualia/Documents/Papers/Cossell et al. - 2015 - Functional organization of excitatory synaptic str.pdf},
  journal = {Nature},
  language = {en},
  number = {7539}
}

@article{Courbariaux2015,
  title = {{{BinaryConnect}}: {{Training Deep Neural Networks}} with Binary Weights during Propagations},
  shorttitle = {{{BinaryConnect}}},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  year = {2015},
  month = nov,
  abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
  archivePrefix = {arXiv},
  eprint = {1511.00363},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Courbariaux, Bengio, David - BinaryConnect Training Deep Neural Networks with binary weights during propagations.pdf;/Users/qualia/Documents/Papers/Courbariaux et al. - 2015 - BinaryConnect Training Deep Neural Networks with .pdf},
  journal = {arXiv:1511.00363 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Coutanche2012,
  title = {The Advantage of Brief {{fMRI}} Acquisition Runs for Multi-Voxel Pattern Detection across Runs},
  author = {Coutanche, Marc N. and {Thompson-Schill}, Sharon L.},
  year = {2012},
  month = jul,
  volume = {61},
  pages = {1113--1119},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.03.076},
  abstract = {Functional magnetic resonance imaging (fMRI) studies are broken up into runs (or `sessions'), frequently selected to be long to minimize across-run signal variations. For investigations that use multi-voxel pattern analysis (MVPA), however, employing many short runs might improve a classifier's ability to generalize across irrelevant pattern variations and detect condition-related activity patterns. We directly tested this hypothesis by scanning participants with both long and short runs and comparing MVPA performance using data from each set of runs. Every run included presentations of faces, places, man-made objects and fruit in a blocked 1-back design. MVPA performance significantly improved from using a large number of short runs, compared to several long runs, in across-run classifications with identical amounts of data. Superior classification was found across variations in the classifier employed, feature selection procedure and region of interest. Performance improvements also extended to an information brain mapping `searchlight' procedure. These results suggest that investigators looking to maximize the detection of subtle multi-voxel patterns across runs might consider employing short fMRI runs.},
  file = {/Users/qualia/Documents/Papers/2013 - Coutanche, Thompson-Schill - The advantage of brief fMRI acquisition runs for multi-voxel pattern detection across runs.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Cowen2017,
  title = {Self-Report Captures 27 Distinct Categories of Emotion Bridged by Continuous Gradients},
  author = {Cowen, Alan S. and Keltner, Dacher},
  year = {2017},
  month = sep,
  volume = {114},
  pages = {E7900-E7909},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1702247114},
  file = {/Users/qualia/Documents/Papers/Cowen and Keltner - 2017 - Self-report captures 27 distinct categories of emo.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {38}
}

@article{Cox2003,
  title = {Functional Magnetic Resonance Imaging ({{fMRI}}) ``Brain Reading'': Detecting and Classifying Distributed Patterns of {{fMRI}} Activity in Human Visual Cortex},
  shorttitle = {Functional Magnetic Resonance Imaging ({{fMRI}}) ``Brain Reading''},
  author = {Cox, David D and Savoy, Robert L},
  year = {2003},
  month = jun,
  volume = {19},
  pages = {261--270},
  issn = {10538119},
  doi = {10.1016/S1053-8119(03)00049-1},
  abstract = {Traditional (univariate) analysis of functional MRI (fMRI) data relies exclusively on the information contained in the time course of individual voxels. Multivariate analyses can take advantage of the information contained in activity patterns across space, from multiple voxels. Such analyses have the potential to greatly expand the amount of information extracted from fMRI data sets. In the present study, multivariate statistical pattern recognition methods, including linear discriminant analysis and support vector machines, were used to classify patterns of fMRI activation evoked by the visual presentation of various categories of objects. Classifiers were trained using data from voxels in predefined regions of interest during a subset of trials for each subject individually. Classification of subsequently collected fMRI data was attempted according to the similarity of activation patterns to prior training examples. Classification was done using only small amounts of data (20 s worth) at a time, so such a technique could, in principle, be used to extract information about a subject's percept on a near real-time basis. Classifiers trained on data acquired during one session were equally accurate in classifying data collected within the same session and across sessions separated by more than a week, in the same subject. Although the highest classification accuracies were obtained using patterns of activity including lower visual areas as input, classification accuracies well above chance were achieved using regions of interest restricted to higher-order object-selective visual areas. In contrast to typical fMRI data analysis, in which hours of data across many subjects are averaged to reveal slight differences in activation, the use of pattern recognition methods allows a subtle 10-way discrimination to be performed on an essentially trial-by-trial basis within individuals, demonstrating that fMRI data contain far more information than is typically appreciated.},
  file = {/Users/qualia/Documents/Papers/2003 - Cox, Savoy - Functional magnetic resonance imaging (fMRI) “brain reading” detecting and classifying distributed patterns of f.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Cox2016,
  title = {Variability and Stability of Large-Scale Cortical Oscillation Patterns},
  author = {Cox, Roy and Schapiro, Anna and Stickgold, Robert},
  year = {2016},
  month = dec,
  doi = {10.1101/093005},
  abstract = {Individual differences in brain organization exist at many spatial and temporal scales, contributing to the substantial heterogeneity underlying human thought and behavior. Oscillatory neural activity is crucial for these behaviors, but how such rhythms are expressed across the cortex within and across individuals has not been thoroughly characterized. Combining electroencephalography (EEG) with representational similarity and multivariate classification techniques, we provide a systematic characterization of brain-wide activity across frequency bands and oscillatory features during rest and task performance. Results indicate that oscillatory profiles exhibit sizable group-level correspondences, indicating the presence of common templates of oscillatory organization. At the same time, we observed well-defined subject-specific network profiles that were discernible above and beyond the structure shared across individuals. These individualized patterns were sufficiently stable over time to allow successful classification of individuals several months later. Finally, our findings indicate that the network structure of rhythmic activity varies considerably across distinct oscillatory frequencies and features, suggesting the existence of multiple, parallel information processing streams embedded in distributed electrophysiological activity. Together, these findings affirm the richness of spatiotemporal EEG signals and emphasize the utility of multivariate network analyses for understanding the role of brain oscillations in physiology and behavior.},
  file = {/Users/qualia/Documents/Papers/Cox et al. - 2016 - Variability and stability of large-scale cortical .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Cragg2004,
  title = {Synaptic Release of Dopamine in the Subthalamic Nucleus},
  author = {Cragg, Stephanie J. and Baufreton, Jerome and Xue, Yi and Bolam, J. Paul and Bevan, Mark D.},
  year = {2004},
  month = oct,
  volume = {20},
  pages = {1788--1802},
  issn = {0953-816X, 1460-9568},
  doi = {10.1111/j.1460-9568.2004.03629.x},
  abstract = {The direct modulation of subthalamic nucleus (STN) neurons by dopamine (DA) neurons of the substantia nigra (SN) is controversial owing to the thick caliber and low density of DA axons in the STN. The abnormal activity of the STN in Parkinson's disease (PD), which is central to the appearance of symptoms, is therefore thought to result from the loss of DA in the striatum. We carried out three experiments in rats to explore the function of DA in the STN: (i) light and electron microscopic analysis of tyrosine hydroxylase (TH)-, dopamine b-hydroxylase (DbH)- and DA-immunoreactive structures to determine whether DA axons form synapses; (ii) fast-scan cyclic voltammetry (FCV) to determine whether DA axons release DA; and (iii) patch clamp recording to determine whether DA, at a concentration similar to that detected by FCV, can modulate activity and synaptic transmission {$\fracslash$} integration. TH- and DAimmunoreactive axons mostly formed symmetric synapses. Because DbH-immunoreactive axons were rare and formed asymmetric synapses, they comprised the minority of TH-immunoreactive synapses. Voltammetry demonstrated that DA release was sufficient for the activation of receptors and abolished by blockade of voltage-dependent Na+ channels or removal of extracellular Ca2+. The lifetime and concentration of extracellular DA was increased by blockade of the DA transporter. Dopamine application depolarized STN neurons, increased their frequency of activity and reduced the impact of c-aminobutyric acid (GABA)-ergic inputs. These findings suggest that SN DA neurons directly modulate the activity of STN neurons and their loss may contribute to the abnormal activity of STN neurons in PD.},
  file = {/Users/qualia/Documents/Papers/2004 - Cragg et al. - Synaptic release of dopamine in the subthalamic nucleus.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {7}
}

@book{Crank1975,
  title = {The Mathematics of Diffusion},
  author = {Crank, John},
  year = {1975},
  edition = {2d ed},
  publisher = {{Clarendon Press}},
  address = {{Oxford, [Eng]}},
  file = {/Users/qualia/Documents/Papers/1975 - Crank - THE MATHEMATICS OF DIFFUSION.pdf;/Users/qualia/Documents/Papers/Crank - 1975 - The mathematics of diffusion.pdf},
  isbn = {978-0-19-853344-3},
  keywords = {Diffusion},
  language = {en},
  lccn = {QD543 .C77 1975}
}

@article{Crespo-Facorro2001,
  title = {Neural {{Mechanisms}} of {{Anhedonia}} in {{Schizophrenia}}: {{A PET Study}} of {{Response}} to {{Unpleasant}} and {{Pleasant Odors}}},
  shorttitle = {Neural {{Mechanisms}} of {{Anhedonia}} in {{Schizophrenia}}},
  author = {{Crespo-Facorro}, Benedicto and Paradiso, Sergio and Andreasen, Nancy C. and O'Leary, Daniel S. and Watkins, G. Leonard and Ponto, Laura L. B. and Hichwa, Richard D.},
  year = {2001},
  month = jul,
  volume = {286},
  pages = {427},
  issn = {0098-7484},
  doi = {10.1001/jama.286.4.427},
  abstract = {Objective To study the neural basis of emotional processing in schizophrenia by exploring the pattern of brain responses to olfactory stimuli in patients and healthy volunteers. Design Positron emission tomographic study of patients with schizophrenia and healthy volunteers. Positron emission tomographic data were collected between July 21, 1995, and September 11, 1997, and data analyses were conducted in 1999-2001. Setting The Mental Health Clinical Research Center at the University of Iowa, Iowa City. Participants Sixteen healthy volunteers with a mean age of 29.5 years and 18 patients with schizophrenia and a mean age of 30.0 years. Main Outcome Measure Areas of relative increase or decrease in regional cerebral blood flow, measured using positron emission tomography and the [15O]water method while participants performed an emotion-induction olfactory task to determine response to pleasant (vanillin) and unpleasant (4-methylvaleric acid) odors, compared between patients and healthy volunteers.
Results Patients with schizophrenia subjectively experienced unpleasant odors in a manner similar to healthy volunteers but showed impairment in the experience of pleasant odors. The analysis of the regional cerebral blood flow revealed that patients failed to activate limbic/paralimbic regions (eg, insular cortex, nucleus accumbens, and parahippocampal gyrus) during the experience of unpleasant odors, recruiting a compensatory set of frontal cortical regions instead.
Conclusion Abnormalities in the complex functional interactions between mesolimbic and frontal regions may underlie emotional disturbances in schizophrenia. JAMA. 2001;286:427-435},
  file = {/Users/qualia/Documents/Papers/2001 - Crespo-Facorro et al. - Neural Mechanisms of Anhedonia in Schizophrenia A PET Study of Response to Unpleasant and Pleasant Odors.pdf},
  journal = {JAMA},
  language = {en},
  number = {4}
}

@article{Creswell2018,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  year = {2018},
  month = jan,
  volume = {35},
  pages = {53--65},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2765202},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  file = {/Users/qualia/Documents/Papers/Creswell et al. - 2018 - Generative Adversarial Networks An Overview.pdf},
  journal = {IEEE Signal Processing Magazine},
  language = {en},
  number = {1}
}

@article{Crossley2016,
  title = {A Two-Neuron System for Adaptive Goal-Directed Decision-Making in {{Lymnaea}}},
  author = {Crossley, Michael and Staras, Kevin and Kemenes, Gy{\"o}rgy},
  year = {2016},
  month = dec,
  volume = {7},
  issn = {2041-1723},
  doi = {10.1038/ncomms11793},
  file = {/Users/qualia/Documents/Papers/Crossley et al. - 2016 - A two-neuron system for adaptive goal-directed dec.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Csaba,
  title = {Perspectives of {{Using Oscillators}} for {{Computing}} and {{Signal Processing}}},
  author = {Csaba, Gyorgy and Porod, Wolfgang},
  pages = {12},
  abstract = {It is an intriguing concept to use oscillators as fundamental building blocks of electronic computers. The idea is not new, but is currently subject to intense research as a part of the quest for 'beyond Moore' electronic devices. In this paper we give an engineering-minded survey of oscillator-based computing architectures, with the goal of understanding their promise and limitations for next-generation computing. We will mostly discuss non-Boolean, neurally-inspired computing concepts and put the emphasis on hardware and on circuits where the oscillators are realized from emerging, nanoscale building blocks. Despite all the promise that oscillatory computing holds, existing literature gives very few clear-cut arguments about the possible benefits of using oscillators in place of other analog nonlinear circuit elements. In this survey we will argue for finding the rationale of using oscillatory building blocks and call for benchmarking studies that compare oscillatory computing circuits to level-based (analog) implementations.},
  file = {/Users/qualia/Documents/Papers/Csaba and Porod - Perspectives of Using Oscillators for Computing an.pdf},
  language = {en}
}

@article{Csicsvari2003,
  title = {Mechanisms of {{Gamma Oscillations}} in the {{Hippocampus}} of the {{Behaving Rat}}},
  author = {Csicsvari, Jozsef and Jamieson, Brian and Wise, Kensall D. and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2003},
  month = jan,
  volume = {37},
  pages = {311--322},
  issn = {08966273},
  doi = {10.1016/S0896-6273(02)01169-8},
  abstract = {Gamma frequency oscillations (30\textendash{}100 Hz) have been suggested to underlie various cognitive and motor functions. Here, we examine the generation of gamma oscillation currents in the hippocampus, using twodimensional, 96-site silicon probes. Two gamma generators were identified, one in the dentate gyrus and another in the CA3-CA1 regions. The coupling strength between the two oscillators varied during both theta and nontheta states. Both pyramidal cells and interneurons were phase-locked to gamma waves. Anatomical connectivity, rather than physical distance, determined the coupling strength of the oscillating neurons. CA3 pyramidal neurons discharged CA3 and CA1 interneurons at latencies indicative of monosynaptic connections. Intrahippocampal gamma oscillation emerges in the CA3 recurrent system, which entrains the CA1 region via its interneurons.},
  file = {/Users/qualia/Documents/Papers/Csicsvari et al. - 2003 - Mechanisms of Gamma Oscillations in the Hippocampu.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Cuccu2018,
  title = {Playing {{Atari}} with {{Six Neurons}}},
  author = {Cuccu, Giuseppe and Togelius, Julian and {Cudre-Mauroux}, Philippe},
  year = {2018},
  month = jun,
  abstract = {Deep reinforcement learning on Atari games maps pixel directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. Aiming at devoting entire deep networks to decision making alone, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by a novel algorithm based on Vector Quantization and Sparse Coding, trained online along with the network, and capable of growing its dictionary size over time. We also introduce new techniques allowing both the neural network and the evolution strategy to cope with varying dimensions. This enables networks of only 6 to 18 neurons to learn to play a selection of Atari games with performance comparable\textemdash{}and occasionally superior\textemdash{}to state-of-the-art techniques using evolution strategies on deep networks two orders of magnitude larger.},
  archivePrefix = {arXiv},
  eprint = {1806.01363},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Cuccu et al. - 2018 - Playing Atari with Six Neurons.pdf},
  journal = {arXiv:1806.01363 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Cugno2018,
  title = {Geometric Principles of Second Messenger Dynamics in Dendritic Spines},
  author = {Cugno, Andrea and Bartol, Thomas M. and Sejnowski, Terrence J. and Iyengar, Ravi and Rangamani, Padmini},
  year = {2018},
  month = oct,
  doi = {10.1101/444489},
  abstract = {Dendritic spines are small, bulbous protrusions along dendrites in neurons and play a critical role in synaptic transmission. Dendritic spines come in a variety of shapes that depend on their developmental state. Additionally, roughly 14{$\sim$}19\% of mature spines have a specialized endoplasmic reticulum called the spine apparatus. How do the shape of a postsynaptic spine and its internal organization affect the spatiotemporal dynamics of short timescale signaling? This question is central for understanding the beginnings of synaptic transmission, learning, and memory formation. In this work, we used mathematical modeling using reaction-diffusion equations in idealized geometries (ellipsoids and spheres) to characterize the effect of spine and spine apparatus geometries on the spatio-temporal dynamics of second messengers. Our analyses and simulations showed that in the short timescale, spine size and shape coupled with the spine apparatus geometries govern the spatiotemporal chemical dynamics of second messengers within the cell. We showed that the curvature of the geometries gives rise to pseudoharmonic functions, which predict the locations of maximum and minimum concentrations. Furthermore, we showed that the lifetime of the chemical gradient can be fine-tuned by localization of fluxes on the spine head and varying the relative curvatures and distances between the spine apparatus and the spine head. Thus, we identified some of the key geometric determinants of how spine head and spine apparatus may regulate the short timescale chemical dynamics of small molecules.},
  file = {/Users/qualia/Documents/Papers/Cugno et al. - 2018 - Geometric principles of second messenger dynamics .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Cui2017,
  title = {The {{HTM Spatial Pooler}}: A Neocortical Algorithm for Online Sparse Distributed Coding},
  shorttitle = {The {{HTM Spatial Pooler}}},
  author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
  year = {2017},
  month = feb,
  doi = {10.1101/085035},
  abstract = {Each region in the cortex receives input through millions of axons from sensory organs and from other cortical regions. It remains a mystery how cortical neurons learn to form specific connections from this large number of unlabeled inputs in order to support further computations. Hierarchical temporal memory (HTM) provides a theoretical framework for understanding the computational principles in the neocortex. In this paper we describe an important component of HTM, the HTM spatial pooler that models how neurons learn feedforward connections. The spatial pooler converts arbitrary binary input patterns into sparse distributed representations (SDRs) using competitive Hebbian learning rules and homeostasis excitability control mechanisms. Through a series of simulations, we demonstrate the key computational properties of HTM spatial pooler, including preserving semantic similarity among inputs, fast adaptation to changing statistics of the inputs, improved noise robustness over learning, efficient use of all cells and flexibility in the event of cell death or loss of input afferents. To quantify these properties, we developed a set of metrics that can be directly measured from the spatial pooler outputs. These metrics can be used as complementary performance indicators for any sparse coding algorithm. We discuss the relationship with neuroscience and previous studies of sparse coding and competitive learning. The HTM spatial pooler represents a neurally inspired algorithm for learning SDRs from noisy data streams online.},
  file = {/Users/qualia/Documents/Papers/Cui et al. - 2017 - The HTM Spatial Pooler a neocortical algorithm fo.pdf},
  journal = {bioRxiv},
  language = {en}
}

@inproceedings{Cully2013,
  title = {Behavioral Repertoire Learning in Robotics},
  booktitle = {Proceeding of the Fifteenth Annual Conference on {{Genetic}} and Evolutionary Computation Conference - {{GECCO}} '13},
  author = {Cully, Antoine and Mouret, Jean-Baptiste},
  year = {2013},
  pages = {175},
  publisher = {{ACM Press}},
  address = {{Amsterdam, The Netherlands}},
  doi = {10.1145/2463372.2463399},
  abstract = {Learning in robotics typically involves choosing a simple goal (e.g. walking) and assessing the performance of each controller with regard to this task (e.g. walking speed). However, learning advanced, input-driven controllers (e.g. walking in each direction) requires testing each controller on a large sample of the possible input signals. This costly process makes difficult to learn useful low-level controllers in robotics.},
  file = {/Users/qualia/Documents/Papers/Cully and Mouret - 2013 - Behavioral repertoire learning in robotics.pdf},
  isbn = {978-1-4503-1963-8},
  language = {en}
}

@article{Cully2015,
  title = {Robots That Can Adapt like Animals},
  author = {Cully, Antoine and Clune, Jeff and Tarapore, Danesh and Mouret, Jean-Baptiste},
  year = {2015},
  month = may,
  volume = {521},
  pages = {503--507},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14422},
  abstract = {As robots leave the controlled environments of factories to autonomously function in more complex, natural environments, they will have to respond to the inevitable fact that they will become damaged. However, while animals can quickly adapt to a wide variety of injuries, current robots cannot "think outside the box" to find a compensatory behavior when damaged: they are limited to their pre-specified self-sensing abilities, can diagnose only anticipated failure modes, and require a pre-programmed contingency plan for every type of potential damage, an impracticality for complex robots. Here we introduce an intelligent trial and error algorithm that allows robots to adapt to damage in less than two minutes, without requiring self-diagnosis or pre-specified contingency plans. Before deployment, a robot exploits a novel algorithm to create a detailed map of the space of high-performing behaviors: This map represents the robot's intuitions about what behaviors it can perform and their value. If the robot is damaged, it uses these intuitions to guide a trial-and-error learning algorithm that conducts intelligent experiments to rapidly discover a compensatory behavior that works in spite of the damage. Experiments reveal successful adaptations for a legged robot injured in five different ways, including damaged, broken, and missing legs, and for a robotic arm with joints broken in 14 different ways. This new technique will enable more robust, effective, autonomous robots, and suggests principles that animals may use to adapt to injury.},
  archivePrefix = {arXiv},
  eprint = {1407.3501},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Cully et al. - 2015 - Robots that can adapt like animals.pdf},
  journal = {Nature},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {7553}
}

@article{Cunningham2004,
  title = {A Role for Fast Rhythmic Bursting Neurons in Cortical Gamma Oscillations in Vitro},
  author = {Cunningham, M. O. and Whittington, M. A. and Bibbig, A. and Roopun, A. and LeBeau, F. E. N. and Vogt, A. and Monyer, H. and Buhl, E. H. and Traub, R. D.},
  year = {2004},
  month = may,
  volume = {101},
  pages = {7152--7157},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0402060101},
  file = {/Users/qualia/Documents/Papers/2004 - Cunningham et al. - A role for fast rhythmic bursting neurons in cortical gamma oscillations in vitro.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {18}
}

@article{Curto2008,
  title = {Cell {{Groups Reveal Structure}} of {{Stimulus Space}}},
  author = {Curto, Carina and Itskov, Vladimir},
  editor = {Friston, Karl J.},
  year = {2008},
  month = oct,
  volume = {4},
  pages = {e1000205},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000205},
  abstract = {An important task of the brain is to represent the outside world. It is unclear how the brain may do this, however, as it can only rely on neural responses and has no independent access to external stimuli in order to ``decode'' what those responses mean. We investigate what can be learned about a space of stimuli using only the action potentials (spikes) of cells with stereotyped\textemdash{}but unknown\textemdash{}receptive fields. Using hippocampal place cells as a model system, we show that one can (1) extract global features of the environment and (2) construct an accurate representation of space, up to an overall scale factor, that can be used to track the animal's position. Unlike previous approaches to reconstructing position from place cell activity, this information is derived without knowing place fields or any other functions relating neural responses to position. We find that simply knowing which groups of cells fire together reveals a surprising amount of structure in the underlying stimulus space; this may enable the brain to construct its own internal representations.},
  file = {/Users/qualia/Documents/Papers/2008 - Curto, Itskov - Cell groups reveal structure of stimulus space.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Curto2013,
  title = {Combinatorial {{Neural Codes}} from a {{Mathematical Coding Theory Perspective}}},
  author = {Curto, Carina and Itskov, Vladimir and Morrison, Katherine and Roth, Zachary and Walker, Judy L.},
  year = {2013},
  month = jul,
  volume = {25},
  pages = {1891--1925},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00459},
  file = {/Users/qualia/Documents/Papers/2013 - Curto et al. - Combinatorial Neural Codes from a Mathematical Coding Theory Perspective.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Curto2013a,
  title = {Encoding {{Binary Neural Codes}} in {{Networks}} of {{Threshold}}-{{Linear Neurons}}},
  author = {Curto, Carina and Degeratu, Anda and Itskov, Vladimir},
  year = {2013},
  month = nov,
  volume = {25},
  pages = {2858--2903},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00504},
  file = {/Users/qualia/Documents/Papers/2014 - Lehky et al. - Encoding Binary Neural Codes in Networks of Threshold-Linear Neurons.pdf;/Users/qualia/Documents/Papers/Curto et al. - 2013 - Encoding Binary Neural Codes in Networks of Thresh.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@article{Curto2013b,
  title = {The {{Neural Ring}}: {{An Algebraic Tool}} for {{Analyzing}} the {{Intrinsic Structure}} of {{Neural Codes}}},
  shorttitle = {The {{Neural Ring}}},
  author = {Curto, Carina and Itskov, Vladimir and {Veliz-Cuba}, Alan and Youngs, Nora},
  year = {2013},
  month = sep,
  volume = {75},
  pages = {1571--1611},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/s11538-013-9860-3},
  abstract = {Neurons in the brain represent external stimuli via neural codes. These codes often arise from stereotyped stimulus-response maps, associating to each neuron a convex receptive field. An important problem confronted by the brain is to infer properties of a represented stimulus space without knowledge of the receptive fields, using only the intrinsic structure of the neural code. How does the brain do this? To address this question, it is important to determine what stimulus space features can\textemdash{}in principle\textemdash{}be extracted from neural codes. This motivates us to define the neural ring and a related neural ideal, algebraic objects that encode the full combinatorial data of a neural code. Our main finding is that these objects can be expressed in a ``canonical form'' that directly translates to a minimal description of the receptive field structure intrinsic to the code. We also find connections to Stanley\textendash{}Reisner rings, and use ideas similar to those in the theory of monomial ideals to obtain an algorithm for computing the primary decomposition of pseudo-monomial ideals. This allows us to algorithmically extract the canonical form associated to any neural code, providing the groundwork for inferring stimulus space features from neural activity alone.},
  file = {/Users/qualia/Documents/Papers/2013 - Curto et al. - The Neural Ring An Algebraic Tool for Analyzing the Intrinsic Structure of Neural Codes.pdf},
  journal = {Bulletin of Mathematical Biology},
  language = {en},
  number = {9}
}

@article{Curto2015,
  title = {Neural Ring Homomorphisms and Maps between Neural Codes},
  author = {Curto, Carina and Youngs, Nora},
  year = {2015},
  month = nov,
  abstract = {Understanding how the brain stores and processes information is central to mathematical neuroscience. Neural data is often represented as a neural code: a set of binary firing patterns C {$\subset$} \{0, 1\}n. We have previously introduced the neural ring, an algebraic object which encodes combinatorial information, in order to analyze the structure of neural codes. We now relate maps between neural codes to notions of homomorphism between the corresponding neural rings. Using three natural operations on neural codes (permutation, inclusion, deletion) as motivation, we search for a restricted class of homomorphisms which correspond to these natural operations. We choose the framework of linear-monomial module homomorphisms, and find that the class of associated code maps neatly captures these three operations, and necessarily includes two others - repetition and adding trivial neurons - which are also meaningful in a neural coding context.},
  archivePrefix = {arXiv},
  eprint = {1511.00255},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Curto, Youngs - Neural ring homomorphisms and maps between neural codes.pdf;/Users/qualia/Documents/Papers/Curto and Youngs - 2015 - Neural ring homomorphisms and maps between neural .pdf},
  journal = {arXiv:1511.00255 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {q-bio}
}

@article{Curto2015a,
  title = {Pattern Completion in Symmetric Threshold-Linear Networks},
  author = {Curto, Carina and Morrison, Katherine},
  year = {2015},
  month = dec,
  abstract = {Threshold-linear networks are a common class of firing rate models that describe recurrent interactions among neurons. Unlike their linear counterparts, these networks generically possess multiple stable fixed points (steady states), making them viable candidates for memory encoding and retrieval. In this work, we characterize stable fixed points of general threshold-linear networks with constant external drive, and discover constraints on the co-existence of fixed points involving different subsets of active neurons. In the case of symmetric networks, we prove the following antichain property: if a set of neurons {$\tau$} is the support of a stable fixed point, then no proper subset or superset of {$\tau$} can support a stable fixed point. Symmetric threshold-linear networks thus appear to be well suited for pattern completion, since the dynamics are guaranteed not to get ``stuck'' in a subset or superset of a stored pattern. We also show that for any graph G, we can construct a network whose stable fixed points correspond precisely to the maximal cliques of G. As an application, we design network decoders for place field codes, and demonstrate their efficacy for error correction and pattern completion. The proofs of our main results build on the theory of permitted sets in threshold-linear networks, including recently-developed connections to classical distance geometry.},
  archivePrefix = {arXiv},
  eprint = {1512.00897},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Curto, Morrison - Pattern completion in symmetric threshold-linear networks.pdf;/Users/qualia/Documents/Papers/Curto and Morrison - 2015 - Pattern completion in symmetric threshold-linear n.pdf},
  journal = {arXiv:1512.00897 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {q-bio}
}

@article{Curto2015b,
  title = {What Makes a Neural Code Convex?},
  author = {Curto, Carina and Gross, Elizabeth and Jeffries, Jack and Morrison, Katherine and Omar, Mohamed and Rosen, Zvi and Shiu, Anne and Youngs, Nora},
  year = {2015},
  month = aug,
  abstract = {Neural codes allow the brain to represent, process, and store information about the world. Combinatorial codes, comprised of binary patterns of neural activity, encode information via the collective behavior of populations of neurons. A code is called convex if its codewords correspond to regions defined by an arrangement of convex open sets in Euclidean space. Convex codes have been observed experimentally in many brain areas, including sensory cortices and the hippocampus, where neurons exhibit convex receptive fields. What makes a neural code convex? That is, how can we tell from the intrinsic structure of a code if there exists a corresponding arrangement of convex open sets? Using tools from combinatorics and commutative algebra, we uncover key signatures of convex and non-convex codes. In many cases, these signatures are sufficient to determine convexity, and reveal bounds on the minimal dimension of the underlying Euclidean space.},
  archivePrefix = {arXiv},
  eprint = {1508.00150},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Curto et al. - What makes a neural code convex Introduction Convex neural codes.pdf;/Users/qualia/Documents/Papers/Curto et al. - 2015 - What makes a neural code convex.pdf},
  journal = {arXiv:1508.00150 [math, q-bio]},
  keywords = {Mathematics - Combinatorics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {math, q-bio}
}

@article{Curto2016,
  title = {What Can Topology Tell Us about the Neural Code?},
  author = {Curto, Carina},
  year = {2016},
  month = may,
  abstract = {Neuroscience is undergoing a period of rapid experimental progress and expansion. New mathematical tools, previously unknown in the neuroscience community, are now being used to tackle fundamental questions and analyze emerging data sets. Consistent with this trend, the last decade has seen an uptick in the use of topological ideas and methods in neuroscience. In this talk I will survey recent applications of topology in neuroscience, and explain why topology is an especially natural tool for understanding neural codes. Note: This is a write-up of my talk for the Current Events Bulletin, held at the 2016 Joint Math Meetings in Seattle, WA.},
  archivePrefix = {arXiv},
  eprint = {1605.01905},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Curto - What can topology tell us about the neural code.pdf;/Users/qualia/Documents/Papers/Curto - 2016 - What can topology tell us about the neural code.pdf},
  journal = {arXiv:1605.01905 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {q-bio}
}

@article{Cybenkot1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenkot, G},
  year = {1989},
  volume = {2},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/Cybenkot - Approximation by superpositions of a sigmoidal fun.pdf},
  journal = {Math. Control Signals System},
  language = {en}
}

@article{Dabney2020,
  title = {A Distributional Code for Value in Dopamine-Based Reinforcement Learning},
  author = {Dabney, Will and {Kurth-Nelson}, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, R{\'e}mi and Botvinick, Matthew},
  year = {2020},
  month = jan,
  volume = {577},
  pages = {671--675},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1924-6},
  file = {/Users/qualia/Documents/Papers/Dabney et al. - 2020 - A distributional code for value in dopamine-based .pdf},
  journal = {Nature},
  language = {en},
  number = {7792}
}

@article{Dale1997,
  title = {Selective Averaging of Rapidly Presented Individual Trials Using {{fMRI}}},
  author = {Dale, Anders M. and Buckner, Randy L.},
  year = {1997},
  volume = {5},
  pages = {329--340},
  issn = {10659471},
  doi = {10.1002/(SICI)1097-0193(1997)5:5<329::AID-HBM1>3.0.CO;2-5},
  abstract = {A major limitation in conducting functional neuroimaging studies, particularly for cognitive experiments, has been the use of blocked task paradigms. Here we explored whether selective averaging techniques similar to those applied in event-related potential (ERP) experiments could be used to demonstrate functional magnetic resonance imaging (fMRI) responses to rapidly intermixed trials. In the first two experiments, we observed that for 1-sec trials of full-field visual checkerboard stimulation, the fMRI blood oxygenation level-dependent (BOLD) signal summated in a roughly linear fashion across successive trials even at very short (2 sec and 5 sec) intertrial intervals, although subtle departures from linearity were observed. In experiments 3 and 4, we observed that it is possible to obtain robust activation maps for rapidly presented randomly mixed trial types (left- and right-hemifield visual checkerboard stimulation) spaced as little as 2 sec apart. Taken collectively, these results suggest that selective averaging may enable fMRI experimental designs identical to those used in typical behavioral and ERP studies. The ability to analyze closely spaced single-trial, or event-related, signals provides for a class of experiments which cannot be conducted using blocked designs. Trial types can be randomly intermixed, and selective averaging based upon trial type and/or subject performance is possible. Hum. Brain Mapping 5:329\textendash{}340, 1997.},
  file = {/Users/qualia/Documents/Papers/1997 - Dale, Buckner - Selective averaging of rapidly presented individual trials using fMRI.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {5}
}

@article{Dam2013,
  title = {Credit {{Assignment}} during {{Movement Reinforcement Learning}}},
  author = {Dam, Gregory and Kording, Konrad and Wei, Kunlin},
  editor = {Gribble, Paul L.},
  year = {2013},
  month = feb,
  volume = {8},
  pages = {e55352},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0055352},
  abstract = {We often need to learn how to move based on a single performance measure that reflects the overall success of our movements. However, movements have many properties, such as their trajectories, speeds and timing of end-points, thus the brain needs to decide which properties of movements should be improved; it needs to solve the credit assignment problem. Currently, little is known about how humans solve credit assignment problems in the context of reinforcement learning. Here we tested how human participants solve such problems during a trajectory-learning task. Without an explicitly-defined target movement, participants made hand reaches and received monetary rewards as feedback on a trialby-trial basis. The curvature and direction of the attempted reach trajectories determined the monetary rewards received in a manner that can be manipulated experimentally. Based on the history of action-reward pairs, participants quickly solved the credit assignment problem and learned the implicit payoff function. A Bayesian credit-assignment model with built-in forgetting accurately predicts their trial-by-trial learning.},
  file = {/Users/qualia/Documents/Papers/Dam et al. - 2013 - Credit Assignment during Movement Reinforcement Le.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {2}
}

@article{Damodaran2014,
  title = {Synchronized Firing of Fast-Spiking Interneurons Is Critical to Maintain Balanced Firing between Direct and Indirect Pathway Neurons of the Striatum},
  author = {Damodaran, Sriraman and Evans, Rebekah C. and Blackwell, Kim T.},
  year = {2014},
  month = feb,
  volume = {111},
  pages = {836--848},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00382.2013},
  file = {/Users/qualia/Documents/Papers/2014 - Damodaran, Evans, Blackwell - Synchronized firing of fast-spiking interneurons is critical to maintain balanced firing between di.pdf;/Users/qualia/Documents/Papers/Damodaran et al. - 2014 - Synchronized firing of fast-spiking interneurons i.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Danziger2015,
  title = {A Neuron Model of Stochastic Resonance Using Rectangular Pulse Trains},
  author = {Danziger, Zachary and Grill, Warren M.},
  year = {2015},
  month = feb,
  volume = {38},
  pages = {53--66},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-014-0526-4},
  abstract = {Stochastic resonance (SR) is the enhanced representation of a weak input signal by the addition of an optimal level of broadband noise to a nonlinear (threshold) system. Since its discovery in the 1980s the domain of input signals shown to be applicable to SR has greatly expanded, from strictly periodic inputs to now nearly any aperiodic forcing function. The perturbations (noise) used to generate SR have also expanded, from white noise to now colored noise or vibrational forcing. This study demonstrates that a new class of perturbations can achieve SR, namely, series of stochastically generated biphasic pulse trains. Using these pulse trains as `noise' we show that a Hodgkin Huxley model neuron exhibits SR behavior when detecting weak input signals. This result is of particular interest to neuroscience because nearly all artificial neural stimulation is implemented with square current or voltage pulses rather than broadband noise, and this new method may facilitate the translation of the performance gains achievable through SR to neural prosthetics.},
  file = {/Users/qualia/Documents/Papers/2015 - Danzinger, Grill - A Neuron Model of Stochastic Resonance Using Rectangular Pulse Trains.pdf;/Users/qualia/Documents/Papers/Danziger and Grill - 2015 - A neuron model of stochastic resonance using recta.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Dasgupta2017,
  title = {A Neural Algorithm for a Fundamental Computing Problem},
  author = {Dasgupta, Sanjoy and Stevens, Charles F and Navlakha, Saket},
  year = {2017},
  pages = {5},
  file = {/Users/qualia/Documents/Papers/Dasgupta et al. - 2017 - A neural algorithm for a fundamental computing pro.pdf},
  language = {en}
}

@article{Dauphin,
  title = {{{MetaInit}}: {{Initializing}} Learning by Learning to Initialize},
  author = {Dauphin, Yann N and Schoenholz, Samuel},
  pages = {13},
  abstract = {Deep learning models frequently trade handcrafted features for deep features learned with much less human intervention using gradient descent. While this paradigm has been enormously successful, deep networks are often difficult to train and performance can depend crucially on the initial choice of parameters. In this work, we introduce an algorithm called MetaInit as a step towards automating the search for good initializations using meta-learning. Our approach is based on a hypothesis that good initializations make gradient descent easier by starting in regions that look locally linear with minimal second order effects. We formalize this notion via a quantity that we call the gradient quotient, which can be computed with any architecture or dataset. MetaInit minimizes this quantity efficiently by using gradient descent to tune the norms of the initial weight matrices. We conduct experiments on plain and residual networks and show that the algorithm can automatically recover from a class of bad initializations. MetaInit allows us to train networks and achieve performance competitive with the state-of-the-art without batch normalization or residual connections. In particular, we find that this approach outperforms normalization for networks without skip connections on CIFAR-10 and can scale to Resnet-50 models on Imagenet.},
  file = {/Users/qualia/Documents/Papers/Dauphin and Schoenholz - MetaInit Initializing learning by learning to ini.pdf},
  language = {en}
}

@article{dAutume2019,
  title = {Episodic {{Memory}} in {{Lifelong Language Learning}}},
  author = {{d'Autume}, Cyprien de Masson and Ruder, Sebastian and Kong, Lingpeng and Yogatama, Dani},
  year = {2019},
  month = oct,
  abstract = {We introduce a lifelong language learning setup where a model needs to learn from a stream of text examples without any dataset identifier. We propose an episodic memory model that performs sparse experience replay and local adaptation to mitigate catastrophic forgetting in this setup. Experiments on text classification and question answering demonstrate the complementary benefits of sparse experience replay and local adaptation to allow the model to continuously learn from new datasets. We also show that the space complexity of the episodic memory module can be reduced significantly ({$\sim$}50-90\%) by randomly choosing which examples to store in memory with a minimal decrease in performance. We consider an episodic memory component as a crucial building block of general linguistic intelligence and see our model as a first step in that direction.},
  archivePrefix = {arXiv},
  eprint = {1906.01076},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/d'Autume et al. - 2019 - Episodic Memory in Lifelong Language Learning.pdf},
  journal = {arXiv:1906.01076 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{David2003,
  title = {A Neural Mass Model for {{MEG}}/{{EEG}}:},
  shorttitle = {A Neural Mass Model for {{MEG}}/{{EEG}}},
  author = {David, Olivier and Friston, Karl J.},
  year = {2003},
  month = nov,
  volume = {20},
  pages = {1743--1755},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2003.07.015},
  abstract = {Although MEG/EEG signals are highly variable, systematic changes in distinct frequency bands are commonly encountered. These frequency-specific changes represent robust neural correlates of cognitive or perceptual processes (for example, alpha rhythms emerge on closing the eyes). However, their functional significance remains a matter of debate. Some of the mechanisms that generate these signals are known at the cellular level and rest on a balance of excitatory and inhibitory interactions within and between populations of neurons. The kinetics of the ensuing population dynamics determine the frequency of oscillations. In this work we extended the classical nonlinear lumped-parameter model of alpha rhythms, initially developed by Lopes da Silva and colleagues [Kybernetik 15 (1974) 27], to generate more complex dynamics. We show that the whole spectrum of MEG/EEG signals can be reproduced within the oscillatory regime of this model by simply changing the population kinetics. We used the model to examine the influence of coupling strength and propagation delay on the rhythms generated by coupled cortical areas. The main findings were that (1) coupling induces phase-locked activity, with a phase shift of 0 or ␲ when the coupling is bidirectional, and (2) both coupling and propagation delay are critical determinants of the MEG/EEG spectrum. In forthcoming articles, we will use this model to (1) estimate how neuronal interactions are expressed in MEG/EEG oscillations and establish the construct validity of various indices of nonlinear coupling, and (2) generate event-related transients to derive physiologically informed basis functions for statistical modelling of average evoked responses.},
  file = {/Users/qualia/Documents/Papers/2003 - David, Friston - A neural mass model for MEGEEG.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@techreport{David2019,
  title = {Single {{Cortical Neurons}} as {{Deep Artificial Neural Networks}}},
  author = {David, Beniaguev and Idan, Segev and Michael, London},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/613141},
  abstract = {We propose a novel approach based on modern deep artificial neural networks (DNNs) for understanding how the morpho-electrical complexity of neurons shapes their input/output (I/O) properties at the millisecond resolution in response to massive synaptic input. The I/O of integrate and fire point neuron is accurately captured by a DNN with a single unit and one hidden layer. A fully connected DNN with one hidden layer faithfully replicated the I/O relationship of a detailed model of Layer 5 cortical pyramidal cell (L5PC) receiving AMPA and GABAA synapses. However, when adding voltage-gated NMDA-conductances, a temporally-convolutional DNN with seven layers was required. Analysis of the DNN filters provides new insights into dendritic processing shaping the I/O properties of neurons. This work proposes a systematic approach for characterizing the functional ``depth'' of a biological neurons, suggesting that cortical pyramidal neurons and the networks they form are computationally much more powerful than previously assumed.},
  file = {/Users/qualia/Documents/Papers/David et al. - 2019 - Single Cortical Neurons as Deep Artificial Neural .pdf},
  language = {en},
  type = {Preprint}
}

@article{Davis2014,
  title = {What Do Differences between Multi-Voxel and Univariate Analysis Mean? {{How}} Subject-, Voxel-, and Trial-Level Variance Impact {{fMRI}} Analysis},
  shorttitle = {What Do Differences between Multi-Voxel and Univariate Analysis Mean?},
  author = {Davis, Tyler and LaRocque, Karen F. and Mumford, Jeanette A. and Norman, Kenneth A. and Wagner, Anthony D. and Poldrack, Russell A.},
  year = {2014},
  month = aug,
  volume = {97},
  pages = {271--283},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2014.04.037},
  abstract = {Multi-voxel pattern analysis (MVPA) has led to major changes in how fMRI data are analyzed and interpreted. Many studies now report both MVPA results and results from standard univariate voxel-wise analysis, often with the goal of drawing different conclusions from each. Because MVPA results can be sensitive to latent multidimensional representations and processes whereas univariate voxel-wise analysis cannot, one conclusion that is often drawn when MVPA and univariate results differ is that the activation patterns underlying MVPA results contain a multidimensional code. In the current study, we conducted simulations to formally test this assumption. Our findings reveal that MVPA tests are sensitive to the magnitude of voxel-level variability in the effect of a condition within subjects, even when the same linear relationship is coded in all voxels. We also find that MVPA is insensitive to subject-level variability in mean activation across an ROI, which is the primary variance component of interest in many standard univariate tests. Together, these results illustrate that differences between MVPA and univariate tests do not afford conclusions about the nature or dimensionality of the neural code. Instead, targeted tests of the informational content and/or dimensionality of activation patterns are critical for drawing strong conclusions about the representational codes that are indicated by significant MVPA results.},
  file = {/Users/qualia/Documents/Papers/2014 - Davis1 et al. - What Do Differences Between Multi-voxel and Univariate Analysis Mean How Subject-, Voxel-, and Trial-level Varian.pdf;/Users/qualia/Documents/Papers/Davis et al. - 2014 - What do differences between multi-voxel and univar.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Daw,
  title = {The Pigeon as Particle Filter},
  author = {Daw, Nathaniel D and Courville, Aaron C},
  pages = {8},
  abstract = {Although theorists have interpreted classical conditioning as a laboratory model of Bayesian belief updating, a recent reanalysis showed that the key features that theoretical models capture about learning are artifacts of averaging over subjects. Rather than learning smoothly to asymptote (reflecting, according to Bayesian models, the gradual tradeoff from prior to posterior as data accumulate), subjects learn suddenly, and their predictions fluctuate perpetually. We suggest that abrupt and unstable learning can be modeled by assuming subjects are conducting inference using sequential Monte Carlo sampling with a small number of samples \textemdash{} one, in our simulations. Ensemble behavior resembles exact Bayesian models since, as in particle filters, it averages over many samples. Further, the model is capable of exhibiting sophisticated behaviors like retrospective revaluation at the ensemble level, even given minimally sophisticated individuals that do not track uncertainty in their beliefs over trials.},
  file = {/Users/qualia/Documents/Papers/2008 - Daw, Courville - The pigeon as particle filter.pdf},
  language = {en}
}

@article{Daw2002,
  title = {Opponent Interactions between Serotonin and Dopamine},
  author = {Daw, Nathaniel D and Kakade, Sham and Dayan, Peter},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {603--616},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00052-7},
  abstract = {Anatomical and pharmacological evidence suggests that the dorsal raphe serotonin system and the ventral tegmental and substantia nigra dopamine system may act as mutual opponents. In the light of the temporal difference model of the involvement of the dopamine system in reward learning, we consider three aspects of motivational opponency involving dopamine and serotonin. We suggest that a tonic serotonergic signal reports the long-run average reward rate as part of an average-case reinforcement learning model; that a tonic dopaminergic signal reports the long-run average punishment rate in a similar context; and finally speculate that a phasic serotonin signal might report an ongoing prediction error for future punishment. q 2002 Elsevier Science Ltd. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2002 - Daw, Kakade, Dayan - Opponent interactions between serotonin and dopamine.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Dayan1993,
  title = {Improving {{Generalisation}} for {{Temporal Difference Learning}}: {{The Successor Representation}}},
  author = {Dayan, Peter},
  year = {1993},
  volume = {5},
  pages = {613--624},
  abstract = {Estimation of returns over time, the focus of temporal difference (TD) algorithms, imposes particular constraints on good function approximators or representations. Appropriate generalisation between states is determined by how similar their successors are, and representations should follow suit. This paper shows how TD machinery can be used to learn such representations, and illustrates, using a navigation task, the appropriately distributed nature of the result.},
  file = {/Users/qualia/Documents/Papers/1993 - Dayan - Improving Generalization for Temporal Difference Learning The Successor Representation.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{Dayan1995,
  title = {The {{Helmholtz Machine}}},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  year = {1995},
  month = sep,
  volume = {7},
  pages = {889--904},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1995.7.5.889},
  file = {/Users/qualia/Documents/Papers/1995 - Dayan et al. - The helmholtz machine.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Dayan1996,
  title = {Exploration Bonuses and Dual Control},
  author = {Dayan, Peter and Sejnowski, Terrence J.},
  year = {1996},
  month = oct,
  volume = {25},
  pages = {5--22},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00115298},
  abstract = {Finding the Bayesian balance between exploration and exploitation in adaptive optimal control is in general intractable. This paper shows how to compute suboptimal estimates based on a certainty equivalence approximation (Cozzolino, Gonzalez-Zubieta \& Miller, 1965) arising from a form of dual control. This systematizes and extends existing uses of exploration bonuses in reinforcement learning (Sutton, 1990). The approach has two components: a statistical model of uncertainty in the world and a way of turning this into exploratory behavior. This general approach is applied to two-dimensional mazes with moveable barriers and its performance is compared with Sutton's DYNA system.},
  file = {/Users/qualia/Documents/Papers/Dayan and Sejnowski - 1996 - Exploration bonuses and dual control.pdf},
  journal = {Mach Learn},
  language = {en},
  number = {1}
}

@inproceedings{deAbril2018,
  title = {Curiosity-{{Driven Reinforcement Learning}} with {{Homeostatic Regulation}}},
  booktitle = {2018 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {{de Abril}, Ildefons Magrans and Kanai, Ryota},
  year = {2018},
  month = jul,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Rio de Janeiro}},
  doi = {10.1109/IJCNN.2018.8489075},
  abstract = {We propose a curiosity reward based on information theory principles and consistent with the animal instinct to maintain certain critical parameters within a bounded range. Our experimental validation shows the added value of the additional homeostatic drive to enhance the overall information gain of a reinforcement learning agent interacting with a complex environment using continuous actions. Our method builds upon two ideas: i) To take advantage of a new Bellman-like equation of information gain and ii) to simplify the computation of the local rewards by avoiding the approximation of complex distributions over continuous states and actions.},
  file = {/Users/qualia/Documents/Papers/de Abril and Kanai - 2018 - Curiosity-Driven Reinforcement Learning with Homeo.pdf},
  isbn = {978-1-5090-6014-6},
  language = {en}
}

@article{Deacon2008,
  title = {Shannon - {{Boltzmann}} - {{Darwin}}: {{Redefining}} Information ({{Part II}})},
  shorttitle = {Shannon - {{Boltzmann}} - {{Darwin}}},
  author = {Deacon, Terrence W.},
  year = {2008},
  month = jan,
  volume = {2008},
  pages = {169--196},
  issn = {1662-1425},
  doi = {10.3726/81605_169},
  abstract = {A scientifically adequate theory of semiotic processes must ultimately be founded on a theory of information that can unify the physical, biological, cognitive, and computational uses of the concept. Unfortunately, no such unification exists, and more importantly, the causal status of informational content remains ambiguous as a result. Lacking this grounding, semiotic theories have tended to be predominantly phenomenological taxonomies rather than dynamical explanations of the representational processes of natural systems. This paper argues that the problem of information that prevents the development of a scientific semiotic theory is the necessity of analyzing it as a negative relationship: defined with respect to absence. This is cryptically implicit in concepts of design and function in biology, acknowledged in psychological and philosophical accounts of intentionality and content, and is explicitly formulated in the mathematical theory of communication (aka ``information theory''). Beginning from the base established by Claude Shannon, which otherwise ignores issues of content, reference, and evaluation, this two part essay explores its relationship to two other higher-order theories that are also explicitly based on an analysis of absence: Boltzmann's theory of thermodynamic entropy (in Part 1) and Darwin's theory of natural selection (in Part 2). This comparison demonstrates that these theories are both formally homologous and hierarchically interdependent. Their synthesis into a general theory of entropy and information provides the necessary grounding for theories of function and semiosis.},
  file = {/Users/qualia/Documents/Papers/2015 - Deacon - Shannon - Boltzmann — Darwin Redefining information ( Part II ).pdf;/Users/qualia/Documents/Papers/Deacon - 2008 - Shannon - Boltzmann - Darwin Redefining informati.pdf},
  journal = {Cognitive Semiotics},
  language = {en},
  number = {2}
}

@article{Deb2002,
  title = {A Fast and Elitist Multiobjective Genetic Algorithm: {{NSGA}}-{{II}}},
  shorttitle = {A Fast and Elitist Multiobjective Genetic Algorithm},
  author = {Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  year = {2002},
  month = apr,
  volume = {6},
  pages = {182--197},
  issn = {1089778X},
  doi = {10.1109/4235.996017},
  file = {/Users/qualia/Documents/Papers/2002 - Deb et al. - A fast and elitist multiobjective genetic algorithm NSGA-II.pdf},
  journal = {IEEE Transactions on Evolutionary Computation},
  language = {en},
  number = {2}
}

@article{DeDeo2011,
  title = {Effective Theories for Circuits and Automata},
  author = {DeDeo, Simon},
  year = {2011},
  month = sep,
  volume = {21},
  pages = {037106},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.3640747},
  file = {/Users/qualia/Documents/Papers/2011 - DeDeo - Effective theories for circuits and automata.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {3}
}

@article{Deemyad2018,
  title = {Astrocytes Integrate and Drive Action Potential Firing in Inhibitory Subnetworks},
  author = {Deemyad, Tara and L{\"u}thi, Joel and Spruston, Nelson},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {4336},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-06338-3},
  file = {/Users/qualia/Documents/Papers/Deemyad et al. - 2018 - Astrocytes integrate and drive action potential fi.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Deger2014,
  title = {Fluctuations and Information Filtering in Coupled Populations of Spiking Neurons with Adaptation},
  author = {Deger, Moritz and Schwalger, Tilo and Naud, Richard and Gerstner, Wulfram},
  year = {2014},
  month = dec,
  volume = {90},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.90.062704},
  file = {/Users/qualia/Documents/Papers/2014 - Deger et al. - Dynamics of interacting finite-sized networks of spiking neurons with adaptation.pdf;/Users/qualia/Documents/Papers/2014 - Deger et al. - Dynamics of interacting finite-sized networks of spiking neurons with adaptation(2).pdf;/Users/qualia/Documents/Papers/Deger et al. - 2014 - Fluctuations and information filtering in coupled  2.pdf;/Users/qualia/Documents/Papers/Deger et al. - 2014 - Fluctuations and information filtering in coupled .pdf},
  journal = {Physical Review E},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {6}
}

@article{Degris2012,
  title = {Off-{{Policy Actor}}-{{Critic}}},
  author = {Degris, Thomas and White, Martha and Sutton, Richard S.},
  year = {2012},
  month = may,
  abstract = {This paper presents the first actor-critic algorithm for off-policy reinforcement learning. Our algorithm is online and incremental, and its per-time-step complexity scales linearly with the number of learned weights. Previous work on actor-critic algorithms is limited to the on-policy setting and does not take advantage of the recent advances in offpolicy gradient temporal-difference learning. Off-policy techniques, such as Greedy-GQ, enable a target policy to be learned while following and obtaining data from another (behavior) policy. For many problems, however, actor-critic methods are more practical than action value methods (like Greedy-GQ) because they explicitly represent the policy; consequently, the policy can be stochastic and utilize a large action space. In this paper, we illustrate how to practically combine the generality and learning potential of off-policy learning with the flexibility in action selection given by actor-critic methods. We derive an incremental, linear time and space complexity algorithm that includes eligibility traces, prove convergence under assumptions similar to previous off-policy algorithms1, and empirically show better or comparable performance to existing algorithms on standard reinforcement-learning benchmark problems.},
  archivePrefix = {arXiv},
  eprint = {1205.4839},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2012 - Degris, White, Sutton - Off-Policy Actor-Critic.pdf},
  journal = {arXiv:1205.4839 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{deHemptinne2015,
  title = {Therapeutic Deep Brain Stimulation Reduces Cortical Phase-Amplitude Coupling in {{Parkinson}}'s Disease},
  author = {{de Hemptinne}, Coralie and Swann, Nicole C and Ostrem, Jill L and {Ryapolova-Webb}, Elena S and San Luciano, Marta and Galifianakis, Nicholas B and Starr, Philip A},
  year = {2015},
  month = may,
  volume = {18},
  pages = {779--786},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3997},
  file = {/Users/qualia/Documents/Papers/de Hemptinne et al. - 2015 - Therapeutic deep brain stimulation reduces cortica.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Dehghani2016,
  title = {Dynamic {{Balance}} of {{Excitation}} and {{Inhibition}} in {{Human}} and {{Monkey Neocortex}}},
  author = {Dehghani, Nima and Peyrache, Adrien and Telenczuk, Bartosz and Le Van Quyen, Michel and Halgren, Eric and Cash, Sydney S. and Hatsopoulos, Nicholas G. and Destexhe, Alain},
  year = {2016},
  month = sep,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep23176},
  file = {/Users/qualia/Documents/Papers/2014 - Dehghani et al. - Multiscale Balance of Excitation and Inhibition in Single-Unit ensemble Recordings in Human and Monkey Neocorte.pdf;/Users/qualia/Documents/Papers/Dehghani et al. - 2016 - Dynamic Balance of Excitation and Inhibition in Hu 2.pdf;/Users/qualia/Documents/Papers/Dehghani et al. - 2016 - Dynamic Balance of Excitation and Inhibition in Hu.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Dehmamy2018,
  title = {A Structural Transition in Physical Networks},
  author = {Dehmamy, Nima and Milanlouei, Soodabeh and Barab{\'a}si, Albert-L{\'a}szl{\'o}},
  year = {2018},
  month = nov,
  volume = {563},
  pages = {676--680},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0726-6},
  file = {/Users/qualia/Documents/Papers/Dehmamy et al. - 2018 - A structural transition in physical networks.pdf},
  journal = {Nature},
  language = {en},
  number = {7733}
}

@article{delaFuente-Fernandez2004,
  title = {Presynaptic Mechanisms of Motor Fluctuations in {{Parkinson}}'s Disease: A Probabilistic Model},
  shorttitle = {Presynaptic Mechanisms of Motor Fluctuations in {{Parkinson}}'s Disease},
  author = {{de la Fuente-Fern{\'a}ndez}, Ra{\'u}l and Schulzer, Michael and Mak, Edwin and Calne, Donald B. and Stoessl, A. Jon},
  year = {2004},
  month = apr,
  volume = {127},
  pages = {888--899},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awh102},
  file = {/Users/qualia/Documents/Papers/2004 - De La Fuente-Fernández et al. - Presynaptic mechanisms of motor fluctuations in Parkinson's disease A probabilistic model.pdf},
  journal = {Brain},
  language = {en},
  number = {4}
}

@incollection{DeLaPava2015,
  title = {A {{Gaussian Process Emulator}} for {{Estimating}} the {{Volume}} of {{Tissue Activated During Deep Brain Stimulation}}},
  booktitle = {Pattern {{Recognition}} and {{Image Analysis}}},
  author = {De La Pava, Iv{\'a}n and G{\'o}mez, Viviana and {\'A}lvarez, Mauricio A. and Henao, {\'O}scar A. and {Daza-Santacoloma}, Genaro and Orozco, {\'A}lvaro A.},
  editor = {Paredes, Roberto and Cardoso, Jaime S. and Pardo, Xos{\'e} M.},
  year = {2015},
  volume = {9117},
  pages = {691--699},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-19390-8_77},
  file = {/Users/qualia/Documents/Papers/2015 - Ángel et al. - A Gaussian Process Emulator for Estimating the Volume of Tissue Activated During Deep Brain Stimulation.pdf;/Users/qualia/Documents/Papers/De La Pava et al. - 2015 - A Gaussian Process Emulator for Estimating the Vol.pdf},
  isbn = {978-3-319-19389-2 978-3-319-19390-8},
  language = {en}
}

@article{Delevich2016,
  title = {Parvalbumin Interneuron Dysfunction in a Thalamus - Prefrontal Cortex Circuit in {{Disc1}} Deficiency Mice},
  author = {Delevich, Kristen and {Jaaro-Peled}, Hanna and Penzo, Mario and Sawa, Akira and Li, Bo},
  year = {2016},
  month = may,
  doi = {10.1101/054759},
  abstract = {Two of the most consistent findings across disrupted-in-schizophrenia-1 (DISC1) mouse models are impaired working memory and reduced number or function of parvalbumin interneurons within the prefrontal cortex. While these findings suggest parvalbumin interneuron dysfunction in DISC1-related pathophysiology, to date, cortical inhibitory circuit function has not been investigated in depth in DISC1 deficiency mouse models. Here we assessed the function of a feedforward circuit between the mediodorsal thalamus (MD) and the medial prefrontal cortex (mPFC) in mice harboring a deletion in one allele of the Disc1 gene. We found that the inhibitory drive onto layer 3 pyramidal neurons in the mPFC was significantly reduced in the Disc1 deficient mice. This reduced inhibition was accompanied by decreased GABA release from local parvalbumin, but not somatostatin, inhibitory interneurons, and by impaired feedforward inhibition in the MD-mPFC circuit. Our results reveal a cellular mechanism by which deficiency in DISC1 causes neural circuit dysfunction frequently implicated in psychiatric disorders.},
  file = {/Users/qualia/Documents/Papers/Delevich et al. - 2016 - Parvalbumin interneuron dysfunction in a thalamus .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{DeLuca2014,
  title = {Statistically Rigorous Calculations Do Not Support Common Input and Long-Term Synchronization of Motor-Unit Firings},
  author = {De Luca, Carlo J. and Kline, Joshua C.},
  year = {2014},
  month = dec,
  volume = {112},
  pages = {2729--2744},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00725.2013},
  file = {/Users/qualia/Documents/Papers/De Luca and Kline - 2014 - Statistically rigorous calculations do not support.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {11}
}

@article{DeMartino2008,
  title = {Combining Multivariate Voxel Selection and Support Vector Machines for Mapping and Classification of {{fMRI}} Spatial Patterns},
  author = {De Martino, Federico and Valente, Giancarlo and Staeren, No{\"e}l and Ashburner, John and Goebel, Rainer and Formisano, Elia},
  year = {2008},
  month = oct,
  volume = {43},
  pages = {44--58},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.06.037},
  abstract = {In functional brain mapping, pattern recognition methods allow detecting multivoxel patterns of brain activation which are informative with respect to a subject's perceptual or cognitive state. The sensitivity of these methods, however, is greatly reduced when the proportion of voxels that convey the discriminative information is small compared to the total number of measured voxels. To reduce this dimensionality problem, previous studies employed univariate voxel selection or region-of-interest-based strategies as a preceding step to the application of machine learning algorithms.},
  file = {/Users/qualia/Documents/Papers/2008 - De Martino et al. - Combining multivariate voxel selection and support vector machines for mapping and classification of fMRI spa.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Deneve,
  title = {Bayesian Inference in Spiking Neurons},
  author = {Deneve, Sophie},
  pages = {8},
  abstract = {We propose a new interpretation of spiking neurons as Bayesian integrators accumulating evidence over time about events in the external world or the body, and communicating to other neurons their certainties about these events. In this model, spikes signal the occurrence of new information, i.e. what cannot be predicted from the past activity. As a result, firing statistics are close to Poisson, albeit providing a deterministic representation of probabilities. We proceed to develop a theory of Bayesian inference in spiking neural networks, recurrent interactions implementing a variant of belief propagation.},
  file = {/Users/qualia/Documents/Papers/2005 - Deneve - Bayesian Inference in Spiking Neurons.pdf},
  language = {en}
}

@article{Deneve2016,
  title = {Efficient Codes and Balanced Networks},
  author = {Den{\`e}ve, Sophie and Machens, Christian K},
  year = {2016},
  month = mar,
  volume = {19},
  pages = {375--382},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4243},
  file = {/Users/qualia/Documents/Papers/Denève and Machens - 2016 - Efficient codes and balanced networks.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Denker2011,
  title = {The {{Local Field Potential Reflects Surplus Spike Synchrony}}},
  author = {Denker, Michael and Roux, S{\'e}bastien and Lind{\'e}n, Henrik and Diesmann, Markus and Riehle, Alexa and Gr{\"u}n, Sonja},
  year = {2011},
  month = dec,
  volume = {21},
  pages = {2681--2695},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhr040},
  file = {/Users/qualia/Documents/Papers/Denker et al. - 2011 - The Local Field Potential Reflects Surplus Spike S.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {12}
}

@article{DePasquale,
  title = {Using {{Firing}}-{{Rate Dynamics}} to {{Train Recurrent Networks}} of {{Spiking Model Neurons}}},
  author = {DePasquale, Brian and Churchland, Mark M and Abbott, L F},
  pages = {17},
  abstract = {Recurrent neural networks are powerful tools for understanding and modeling computation and representation by populations of neurons. Continuous-variable or ``rate'' model networks have been analyzed and applied extensively for these purposes. However, neurons fire action potentials, and the discrete nature of spiking is an important feature of neural circuit dynamics. Despite significant advances, training recurrently connected spiking neural networks remains a challenge. We present a procedure for training recurrently connected spiking networks to generate dynamical patterns autonomously, to produce complex temporal outputs based on integrating network input, and to model physiological data. Our procedure makes use of a continuous-variable network to identify targets for training the inputs to the spiking model neurons. Surprisingly, we are able to construct spiking networks that duplicate tasks performed by continuous-variable networks with only a relatively minor expansion in the number of neurons. Our approach provides a novel view of the significance and appropriate use of ``firing rate'' models, and it is a useful approach for building model spiking networks that can be used to address important questions about representation and computation in neural systems.},
  file = {/Users/qualia/Documents/Papers/DePasquale et al. - Using Firing-Rate Dynamics to Train Recurrent Netw.pdf},
  language = {en}
}

@article{DePasquale2018,
  title = {Full-{{FORCE}}: {{A}} Target-Based Method for Training Recurrent Networks},
  shorttitle = {Full-{{FORCE}}},
  author = {DePasquale, Brian and Cueva, Christopher J. and Rajan, Kanaka and Escola, G. Sean and Abbott, L. F.},
  editor = {Chacron, Maurice J.},
  year = {2018},
  month = feb,
  volume = {13},
  pages = {e0191527},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0191527},
  abstract = {Trained recurrent networks are powerful tools for modeling dynamic neural computations. We present a target-based method for modifying the full connectivity matrix of a recurrent network to train it to perform tasks involving temporally complex input/output transformations. The method introduces a second network during training to provide suitable ``target'' dynamics useful for performing the task. Because it exploits the full recurrent connectivity, the method produces networks that perform tasks with fewer neurons and greater noise robustness than traditional least-squares (FORCE) approaches. In addition, we show how introducing additional input signals into the target-generating network, which act as task hints, greatly extends the range of tasks that can be learned and provides control over the complexity and nature of the dynamics of the trained, task-performing network.},
  file = {/Users/qualia/Documents/Papers/DePasquale et al. - 2018 - full-FORCE A target-based method for training rec.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {2}
}

@article{Destexhe,
  title = {Kinetic {{Models}} of {{Synaptic Transmission}}},
  author = {Destexhe, A and Mainen, Z F and Sejnowski, T J and Koch, C (EDITOR) and Segev, I (EDITOR)},
  pages = {26},
  file = {/Users/qualia/Documents/Papers/1998 - Destexhe et al. - Kinetic models of synaptic transmission.pdf},
  language = {en}
}

@article{Destexhe2003,
  title = {The High-Conductance State of Neocortical Neurons in Vivo},
  author = {Destexhe, Alain and Rudolph, Michael and Par{\'e}, Denis},
  year = {2003},
  month = sep,
  volume = {4},
  pages = {739--751},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1198},
  file = {/Users/qualia/Documents/Papers/2003 - Destexhe, Rudolph, Paré - The high-conductance state of neocortical neurons in vivo.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {9}
}

@article{Dhawale,
  title = {1 {{Automated}} Long-Term Recording and Analysis of Neural Activity in Behaving Animals},
  author = {Dhawale, Ashesh K and Poddar, Rajesh and Wolff, Steffen B E and Normand, Valentin A and {\"O}lveczky, Bence P},
  pages = {108},
  file = {/Users/qualia/Documents/Papers/Dhawale et al. - 1 Automated long-term recording and analysis of ne.pdf},
  language = {en}
}

@article{Diedrichsen2011,
  title = {Comparing the Similarity and Spatial Structure of Neural Representations: {{A}} Pattern-Component Model},
  shorttitle = {Comparing the Similarity and Spatial Structure of Neural Representations},
  author = {Diedrichsen, J{\"o}rn and Ridgway, Gerard R. and Friston, Karl J. and Wiestler, Tobias},
  year = {2011},
  month = apr,
  volume = {55},
  pages = {1665--1678},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.01.044},
  abstract = {In recent years there has been growing interest in multivariate analyses of neuroimaging data, which can be used to detect distributed patterns of activity that encode an experimental factor of interest. In this setting, it has become common practice to study the correlations between patterns to make inferences about the way a brain region represents stimuli or tasks (known as representational similarity analysis). Although it would be of great interest to compare these correlations from different regions, direct comparisons are currently not possible. This is because sample correlations are strongly influenced by voxel-selection, fMRI noise, and nonspecific activation patterns, all of which can differ widely between regions. Here, we present a multivariate modeling framework in which the measured patterns are decomposed into their constituent parts. The model is based on a standard linear mixed model, in which pattern components are considered to be randomly distributed over voxels. The model allows one to estimate the true correlations of the underlying neuronal pattern components, thereby enabling comparisons between different regions or individuals. The pattern estimates also allow us to make inferences about the spatial structure of different response components. Thus, the new model provides a theoretical and analytical framework to study the structure of distributed neural representations.},
  file = {/Users/qualia/Documents/Papers/2011 - Diedrichsen et al. - Comparing the similarity and spatial structure of neural representations a pattern-component model.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@inproceedings{Diehl2015,
  title = {Fast-Classifying, High-Accuracy Spiking Deep Networks through Weight and Threshold Balancing},
  booktitle = {2015 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Diehl, Peter U. and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  year = {2015},
  month = jul,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Killarney, Ireland}},
  doi = {10.1109/IJCNN.2015.7280696},
  abstract = {Deep neural networks such as Convolutional Networks (ConvNets) and Deep Belief Networks (DBNs) represent the state-of-the-art for many machine learning and computer vision classification problems. To overcome the large computational cost of deep networks, spiking deep networks have recently been proposed, given the specialized hardware now available for spiking neural networks (SNNs). However, this has come at the cost of performance losses due to the conversion from analog neural networks (ANNs) without a notion of time, to sparsely firing, event-driven SNNs. Here we analyze the effects of converting deep ANNs into SNNs with respect to the choice of parameters for spiking neurons such as firing rates and thresholds. We present a set of optimization techniques to minimize performance loss in the conversion process for ConvNets and fully connected deep networks. These techniques yield networks that outperform all previous SNNs on the MNIST database to date, and many networks here are close to maximum performance after only 20 ms of simulated time. The techniques include using rectified linear units (ReLUs) with zero bias during training, and using a new weight normalization method to help regulate firing rates. Our method for converting an ANN into an SNN enables lowlatency classification with high accuracies already after the first output spike, and compared with previous SNN approaches it yields improved performance without increased training time. The presented analysis and optimization techniques boost the value of spiking deep networks as an attractive framework for neuromorphic computing platforms aimed at fast and efficient pattern recognition.},
  file = {/Users/qualia/Documents/Papers/2015 - Diehl et al. - Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing.pdf;/Users/qualia/Documents/Papers/Diehl et al. - 2015 - Fast-classifying, high-accuracy spiking deep netwo.pdf},
  isbn = {978-1-4799-1960-4},
  language = {en}
}

@article{Ding2016,
  title = {Cortical Tracking of Hierarchical Linguistic Structures in Connected Speech},
  author = {Ding, Nai and Melloni, Lucia and Zhang, Hang and Tian, Xing and Poeppel, David},
  year = {2016},
  month = jan,
  volume = {19},
  pages = {158--164},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4186},
  file = {/Users/qualia/Documents/Papers/2015 - Ding et al. - Cortical tracking of hierarchical linguistic structures in connected speech.pdf;/Users/qualia/Documents/Papers/Ding et al. - 2016 - Cortical tracking of hierarchical linguistic struc.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {1}
}

@article{Doelling2019,
  title = {An Oscillator Model Better Predicts Cortical Entrainment to Music},
  author = {Doelling, Keith B. and Assaneo, M. Florencia and Bevilacqua, Dana and Pesaran, Bijan and Poeppel, David},
  year = {2019},
  month = apr,
  pages = {201816414},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1816414116},
  abstract = {A body of research demonstrates convincingly a role for synchronization of auditory cortex to rhythmic structure in sounds including speech and music. Some studies hypothesize that an oscillator in auditory cortex could underlie important temporal processes such as segmentation and prediction. An important critique of these findings raises the plausible concern that what is measured is perhaps not an oscillator but is instead a sequence of evoked responses. The two distinct mechanisms could look very similar in the case of rhythmic input, but an oscillator might better provide the computational roles mentioned above (i.e., segmentation and prediction). We advance an approach to adjudicate between the two models: analyzing the phase lag between stimulus and neural signal across different stimulation rates. We ran numerical simulations of evoked and oscillatory computational models, showing that in the evoked case,phase lag is heavily rate-dependent, while the oscillatory model displays marked phase concentration across stimulation rates. Next, we compared these model predictions with magnetoencephalography data recorded while participants listened to music of varying note rates. Our results show that the phase concentration of the experimental data is more in line with the oscillatory model than with the evoked model. This finding supports an auditory cortical signal that (
              i
              ) contains components of both bottom-up evoked responses and internal oscillatory synchronization whose strengths are weighted by their appropriateness for particular stimulus types and (
              ii
              ) cannot be explained by evoked responses alone.},
  file = {/Users/qualia/Documents/Papers/Doelling et al. - 2019 - An oscillator model better predicts cortical entra.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en}
}

@article{Doi2014,
  title = {A {{Simple Model}} of {{Optimal Population Coding}} for {{Sensory Systems}}},
  author = {Doi, Eizaburo and Lewicki, Michael S.},
  editor = {Bethge, Matthias},
  year = {2014},
  month = aug,
  volume = {10},
  pages = {e1003761},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003761},
  abstract = {A fundamental task of a sensory system is to infer information about the environment. It has long been suggested that an important goal of the first stage of this process is to encode the raw sensory signal efficiently by reducing its redundancy in the neural representation. Some redundancy, however, would be expected because it can provide robustness to noise inherent in the system. Encoding the raw sensory signal itself is also problematic, because it contains distortion and noise. The optimal solution would be constrained further by limited biological resources. Here, we analyze a simple theoretical model that incorporates these key aspects of sensory coding, and apply it to conditions in the retina. The model specifies the optimal way to incorporate redundancy in a population of noisy neurons, while also optimally compensating for sensory distortion and noise. Importantly, it allows an arbitrary input-to-output cell ratio between sensory units (photoreceptors) and encoding units (retinal ganglion cells), providing predictions of retinal codes at different eccentricities. Compared to earlier models based on redundancy reduction, the proposed model conveys more information about the original signal. Interestingly, redundancy reduction can be near-optimal when the number of encoding units is limited, such as in the peripheral retina. We show that there exist multiple, equally-optimal solutions whose receptive field structure and organization vary significantly. Among these, the one which maximizes the spatial locality of the computation, but not the sparsity of either synaptic weights or neural responses, is consistent with known basic properties of retinal receptive fields. The model further predicts that receptive field structure changes less with light adaptation at higher input-to-output cell ratios, such as in the periphery.},
  file = {/Users/qualia/Documents/Papers/Doi and Lewicki - 2014 - A Simple Model of Optimal Population Coding for Se.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {8}
}

@article{Doiron2001,
  title = {Subtractive and {{Divisive Inhibition}}: {{Effect}} of {{Voltage}}-{{Dependent Inhibitory Conductances}} and {{Noise}}},
  shorttitle = {Subtractive and {{Divisive Inhibition}}},
  author = {Doiron, Brent and Longtin, Andr{\'e} and Berman, Neil and Maler, Leonard},
  year = {2001},
  month = jan,
  volume = {13},
  pages = {227--248},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976601300014691},
  file = {/Users/qualia/Documents/Papers/2001 - Doiron et al. - Subtractive and divisive inhibition effect of voltage-dependent inhibitory conductances and noise.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {1}
}

@article{Doll2015,
  title = {Model-Based Choices Involve Prospective Neural Activity},
  author = {Doll, Bradley B and Duncan, Katherine D and Simon, Dylan A and Shohamy, Daphna and Daw, Nathaniel D},
  year = {2015},
  month = may,
  volume = {18},
  pages = {767--772},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3981},
  file = {/Users/qualia/Documents/Papers/2015 - Doll et al. - Model-based choices involve prospective neural activity.pdf;/Users/qualia/Documents/Papers/Doll et al. - 2015 - Model-based choices involve prospective neural act.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@incollection{Douglas2010,
  title = {Canonical {{Cortical Circuits}}},
  booktitle = {Handbook of {{Brain Microcircuits}}},
  author = {Douglas, Rodney J. and Martin, Kevan A. C.},
  editor = {Shepherd, MD, DPhil, Gordon and Grillner, MD, Sten},
  year = {2010},
  month = aug,
  pages = {15--21},
  publisher = {{Oxford University Press}},
  doi = {10.1093/med/9780195389883.003.0002},
  file = {/Users/qualia/Documents/Papers/2010 - Douglas - Canonical cortical circuits.pdf;/Users/qualia/Documents/Papers/2010 - Douglas - Canonical cortical circuits(2).pdf},
  isbn = {978-0-19-538988-3},
  language = {en}
}

@article{Doya2000,
  title = {Complementary Roles of Basal Ganglia and Cerebellum in Learning and Motor Control},
  author = {Doya, K},
  year = {2000},
  month = dec,
  volume = {10},
  pages = {732--739},
  issn = {09594388},
  doi = {10.1016/S0959-4388(00)00153-7},
  file = {/Users/qualia/Documents/Papers/Doya - 2000 - Complementary roles of basal ganglia and cerebellu.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {6}
}

@article{Dreyer2014,
  title = {Three {{Mechanisms}} by Which {{Striatal Denervation Causes Breakdown}} of {{Dopamine Signaling}}},
  author = {Dreyer, J. K.},
  year = {2014},
  month = sep,
  volume = {34},
  pages = {12444--12456},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1458-14.2014},
  file = {/Users/qualia/Documents/Papers/2014 - Dreyer - Three Mechanisms by which Striatal Denervation Causes Breakdown of Dopamine Signaling.pdf;/Users/qualia/Documents/Papers/Dreyer - 2014 - Three Mechanisms by which Striatal Denervation Cau.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@article{Drion2015,
  title = {Ion Channel Degeneracy Enables Robust and Tunable Neuronal Firing Rates},
  author = {Drion, Guillaume and O'Leary, Timothy and Marder, Eve},
  year = {2015},
  month = sep,
  volume = {112},
  pages = {E5361-E5370},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1516400112},
  file = {/Users/qualia/Documents/Papers/2015 - Drion, O’Leary, Marder - Ion channel degeneracy enables robust and tunable neuronal firing rates.pdf;/Users/qualia/Documents/Papers/Drion et al. - 2015 - Ion channel degeneracy enables robust and tunable .pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {38}
}

@article{Druckmann2007,
  title = {A Novel Multiple Objective Optimization Framework for Constraining Conductance-Based Neuron Models by Experimental Data},
  author = {Druckmann, Shaul},
  year = {2007},
  month = nov,
  volume = {1},
  pages = {7--18},
  issn = {16624548},
  doi = {10.3389/neuro.01.1.1.001.2007},
  file = {/Users/qualia/Documents/Papers/2007 - Druckmann et al. - A novel multiple objective optimization framework for constraining conductance-based neuron models by experime.pdf},
  journal = {Frontiers in Neuroscience},
  language = {en},
  number = {1}
}

@article{Druckmann2008,
  title = {Evaluating Automated Parameter Constraining Procedures of Neuron Models by Experimental and Surrogate Data},
  author = {Druckmann, Shaul and Berger, Thomas K. and Hill, Sean and Sch{\"u}rmann, Felix and Markram, Henry and Segev, Idan},
  year = {2008},
  month = nov,
  volume = {99},
  pages = {371--379},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-008-0269-2},
  file = {/Users/qualia/Documents/Papers/2008 - Druckmann et al. - Evaluating automated parameter constraining procedures of neuron models by experimental and surrogate data.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {4-5}
}

@article{Druckmann2011,
  title = {Effective {{Stimuli}} for {{Constructing Reliable Neuron Models}}},
  author = {Druckmann, Shaul and Berger, Thomas K. and Sch{\"u}rmann, Felix and Hill, Sean and Markram, Henry and Segev, Idan},
  editor = {Graham, Lyle J.},
  year = {2011},
  month = aug,
  volume = {7},
  pages = {e1002133},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002133},
  abstract = {The rich dynamical nature of neurons poses major conceptual and technical challenges for unraveling their nonlinear membrane properties. Traditionally, various current waveforms have been injected at the soma to probe neuron dynamics, but the rationale for selecting specific stimuli has never been rigorously justified. The present experimental and theoretical study proposes a novel framework, inspired by learning theory, for objectively selecting the stimuli that best unravel the neuron's dynamics. The efficacy of stimuli is assessed in terms of their ability to constrain the parameter space of biophysically detailed conductance-based models that faithfully replicate the neuron's dynamics as attested by their ability to generalize well to the neuron's response to novel experimental stimuli. We used this framework to evaluate a variety of stimuli in different types of cortical neurons, ages and animals. Despite their simplicity, a set of stimuli consisting of step and ramp current pulses outperforms synaptic-like noisy stimuli in revealing the dynamics of these neurons. The general framework that we propose paves a new way for defining, evaluating and standardizing effective electrical probing of neurons and will thus lay the foundation for a much deeper understanding of the electrical nature of these highly sophisticated and non-linear devices and of the neuronal networks that they compose.},
  file = {/Users/qualia/Documents/Papers/2011 - Druckmann et al. - Effective stimuli for constructing reliable neuron models.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {8}
}

@article{Druckmann2012,
  title = {Neuronal {{Circuits Underlying Persistent Representations Despite Time Varying Activity}}},
  author = {Druckmann, Shaul and Chklovskii, Dmitri B.},
  year = {2012},
  month = nov,
  volume = {22},
  pages = {2095--2103},
  issn = {09609822},
  doi = {10.1016/j.cub.2012.08.058},
  abstract = {Background: Our brains are capable of remarkably stable stimulus representations despite time-varying neural activity. For instance, during delay periods in working memory tasks, while stimuli are represented in working memory, neurons in the prefrontal cortex, thought to support the memory representation, exhibit time-varying neuronal activity. Since neuronal activity encodes the stimulus, its time-varying dynamics appears to be paradoxical and incompatible with stable network stimulus representations. Indeed, this finding raises a fundamental question: can stable representations only be encoded with stable neural activity, or, its corollary, is every change in activity a sign of change in stimulus representation?
Results: Here we explain how different time-varying representations offered by individual neurons can be woven together to form a coherent, time-invariant, representation. Motivated by two ubiquitous features of the neocortex\textemdash{}redundancy of neural representation and sparse intracortical connections\textemdash{}we derive a network architecture that resolves the apparent contradiction between representation stability and changing neural activity. Unexpectedly, this network architecture exhibits many structural properties that have been measured in cortical sensory areas. In particular, we can account for few-neuron motifs, synapse weight distribution, and the relations between neuronal functional properties and connection probability.
Conclusions: We show that the intuition regarding network stimulus representation, typically derived from considering single neurons, may be misleading and that time-varying activity of distributed representation in cortical circuits does not necessarily imply that the network explicitly encodes time-varying properties.},
  file = {/Users/qualia/Documents/Papers/2012 - Druckmann, Chklovskii - Neuronal circuits underlying persistent representations despite time varying activity.pdf;/Users/qualia/Documents/Papers/2012 - Druckmann, Chklovskii - Neuronal circuits underlying persistent representations despite time varying activity(2).pdf},
  journal = {Current Biology},
  language = {en},
  number = {22}
}

@article{Duann2002,
  title = {Single-{{Trial Variability}} in {{Event}}-{{Related BOLD Signals}}},
  author = {Duann, Jeng-Ren and Jung, Tzyy-Ping and Kuo, Wen-Jui and Yeh, Tzu-Chen and Makeig, Scott and Hsieh, Jen-Chuen and Sejnowski, Terrence J.},
  year = {2002},
  month = apr,
  volume = {15},
  pages = {823--835},
  issn = {10538119},
  doi = {10.1006/nimg.2001.1049},
  file = {/Users/qualia/Documents/Papers/2002 - Duann et al. - Single-trial variability in event-related BOLD signals.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Duarte2017,
  title = {Synaptic Patterning and the Timescales of Cortical Dynamics},
  author = {Duarte, Renato and Seeholzer, Alexander and Zilles, Karl and Morrison, Abigail},
  year = {2017},
  month = apr,
  volume = {43},
  pages = {156--165},
  issn = {09594388},
  doi = {10.1016/j.conb.2017.02.007},
  file = {/Users/qualia/Documents/Papers/Duarte et al. - 2017 - Synaptic patterning and the timescales of cortical.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Dummer2014,
  title = {Self-Consistent Determination of the Spike-Train Power Spectrum in a Neural Network with Sparse Connectivity},
  author = {Dummer, Benjamin and Wieland, Stefan and Lindner, Benjamin},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00104},
  abstract = {A major source of random variability in cortical networks is the quasi-random arrival of presynaptic action potentials from many other cells. In network studies as well as in the study of the response properties of single cells embedded in a network, synaptic background input is often approximated by Poissonian spike trains. However, the output statistics of the cells is in most cases far from being Poisson. This is inconsistent with the assumption of similar spike-train statistics for pre- and postsynaptic cells in a recurrent network. Here we tackle this problem for the popular class of integrate-and-fire neurons and study a self-consistent statistics of input and output spectra of neural spike trains. Instead of actually using a large network, we use an iterative scheme, in which we simulate a single neuron over several generations. In each of these generations, the neuron is stimulated with surrogate stochastic input that has a similar statistics as the output of the previous generation. For the surrogate input, we employ two distinct approximations: (i) a superposition of renewal spike trains with the same interspike interval density as observed in the previous generation and (ii) a Gaussian current with a power spectrum proportional to that observed in the previous generation. For input parameters that correspond to balanced input in the network, both the renewal and the Gaussian iteration procedure converge quickly and yield comparable results for the self-consistent spike-train power spectrum. We compare our results to large-scale simulations of a random sparsely connected network of leaky integrate-and-fire neurons (Brunel, 2000) and show that in the asynchronous regime close to a state of balanced synaptic input from the network, our iterative schemes provide an excellent approximations to the autocorrelation of spike trains in the recurrent network.},
  file = {/Users/qualia/Documents/Papers/Dummer et al. - 2014 - Self-consistent determination of the spike-train p.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Dumoulin2018,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  archivePrefix = {arXiv},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Dumoulin and Visin - 2018 - A guide to convolution arithmetic for deep learnin.pdf},
  journal = {arXiv:1603.07285 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Dunsmoor2015,
  title = {Categories, Concepts, and Conditioning: How Humans Generalize Fear},
  shorttitle = {Categories, Concepts, and Conditioning},
  author = {Dunsmoor, Joseph E. and Murphy, Gregory L.},
  year = {2015},
  month = feb,
  volume = {19},
  pages = {73--77},
  issn = {13646613},
  doi = {10.1016/j.tics.2014.12.003},
  file = {/Users/qualia/Documents/Papers/2015 - Dunsmoor, Murphy - Categories, concepts, and conditioning How humans generalize fear.pdf;/Users/qualia/Documents/Papers/Dunsmoor and Murphy - 2015 - Categories, concepts, and conditioning how humans.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {2}
}

@article{Duval2016,
  title = {A Brain Network Model Explaining Tremor in {{Parkinson}}'s Disease},
  author = {Duval, Christian and Daneault, Jean-Francois and Hutchison, William D. and Sadikot, Abbas F.},
  year = {2016},
  month = jan,
  volume = {85},
  pages = {49--59},
  issn = {09699961},
  doi = {10.1016/j.nbd.2015.10.009},
  abstract = {This paper presents a novel model of tremor in Parkinson's disease (PD) based on extensive literature review as well as novel results stemming from functional stereotactic neurosurgery for the alleviation of tremor in PD. Specifically, evidence that suggests the basal ganglia induces PD tremor via excessive inhibitory output to the thalamus and altered firing patterns which in turn generate rhythmic bursting activity of thalamic cells is presented. Then, evidence that the thalamus generates PD tremor by facilitating the generation and consolidation of rhythmic bursting activity of neurons within its nuclei is also offered. Finally, evidence that the cerebellum may modulate characteristics of PD tremor by treating it as if it was a voluntary motor behavior is presented. Accordingly, the current paper proposes that PD tremor is induced by abnormal basal ganglia activity; it is generated by the thalamus, and modulated or reinforced by the cerebellum.},
  file = {/Users/qualia/Documents/Papers/2015 - Duval et al. - A brain network model explaining tremor in Parkinson's disease.pdf;/Users/qualia/Documents/Papers/Duval et al. - 2016 - A brain network model explaining tremor in Parkins.pdf},
  journal = {Neurobiology of Disease},
  language = {en}
}

@article{Dwork2013,
  title = {The {{Algorithmic Foundations}} of {{Differential Privacy}}},
  author = {Dwork, Cynthia and Roth, Aaron},
  year = {2013},
  volume = {9},
  pages = {211--407},
  issn = {1551-305X, 1551-3068},
  doi = {10.1561/0400000042},
  file = {/Users/qualia/Documents/Papers/2013 - Dwork, Roth - The Algorithmic Foundations of Differential Privacy.pdf},
  journal = {Foundations and Trends\textregistered{} in Theoretical Computer Science},
  language = {en},
  number = {3-4}
}

@article{Ebert2014,
  title = {Coordinated Reset Stimulation in a Large-Scale Model of the {{STN}}-{{GPe}} Circuit},
  author = {Ebert, Martin and Hauptmann, Christian and Tass, Peter A.},
  year = {2014},
  month = nov,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00154},
  abstract = {Synchronization of populations of neurons is a hallmark of several brain diseases. Coordinated reset (CR) stimulation is a model-based stimulation technique which specifically counteracts abnormal synchrony by desynchronization. Electrical CR stimulation, e.g., for the treatment of Parkinson's disease (PD), is administered via depth electrodes. In order to get a deeper understanding of this technique, we extended the top-down approach of previous studies and constructed a large-scale computational model of the respective brain areas. Furthermore, we took into account the spatial anatomical properties of the simulated brain structures and incorporated a detailed numerical representation of 2 {$\cdot$} 104 simulated neurons. We simulated the subthalamic nucleus (STN) and the globus pallidus externus (GPe). Connections within the STN were governed by spike-timing dependent plasticity (STDP). In this way, we modeled the physiological and pathological activity of the considered brain structures. In particular, we investigated how plasticity could be exploited and how the model could be shifted from strongly synchronized (pathological) activity to strongly desynchronized (healthy) activity of the neuronal populations via CR stimulation of the STN neurons. Furthermore, we investigated the impact of specific stimulation parameters especially the electrode position on the stimulation outcome. Our model provides a step forward toward a biophysically realistic model of the brain areas relevant to the emergence of pathological neuronal activity in PD. Furthermore, our model constitutes a test bench for the optimization of both stimulation parameters and novel electrode geometries for efficient CR stimulation.},
  file = {/Users/qualia/Documents/Papers/2014 - Ebert, Hauptmann, Tass - Coordinated reset stimulation in a large-scale model of the STN-GPe circuit.pdf;/Users/qualia/Documents/Papers/Ebert et al. - 2014 - Coordinated reset stimulation in a large-scale mod.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Eckhardt2007,
  title = {Modeling Walker Synchronization on the {{Millennium Bridge}}},
  author = {Eckhardt, Bruno and Ott, Edward and Strogatz, Steven H. and Abrams, Daniel M. and McRobie, Allan},
  year = {2007},
  month = feb,
  volume = {75},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.021110},
  file = {/Users/qualia/Documents/Papers/2007 - Eckhardt et al. - Modeling walker synchronization on the millennium bridge.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {2}
}

@article{Ecoffet2019,
  title = {Go-{{Explore}}: A {{New Approach}} for {{Hard}}-{{Exploration Problems}}},
  shorttitle = {Go-{{Explore}}},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2019},
  month = may,
  abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to encourage exploration and improve performance on hardexploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember states that have previously been visited, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through exploiting any available means (including by introducing determinism), then robustify (create a policy that can reliably perform the solution) via imitation learning. The combined effect of these principles generates dramatic performance improvements on hardexploration problems. On Montezuma's Revenge, without being provided any domain knowledge, Go-Explore scores over 43,000 points, almost 4 times the previous state of the art. Go-Explore can also easily harness human-provided domain knowledge, and when augmented with it Go-Explore scores a mean of over 650,000 points on Montezuma's Revenge. Its max performance of 18 million surpasses the human world record by an order of magnitude, thus meeting even the strictest definition of ``superhuman'' performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean performance of almost 60,000 points also exceeds expert human performance. Because GoExplore can produce many high-performing demonstrations automatically and cheaply, it also outperforms previous imitation learning work in which the solution was provided in the form of a human demonstration. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in a variety of domains, especially the many that often harness a simulator during training (e.g. robotics).},
  archivePrefix = {arXiv},
  eprint = {1901.10995},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Ecoffet et al. - 2019 - Go-Explore a New Approach for Hard-Exploration Pr.pdf},
  journal = {arXiv:1901.10995 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Effenberger2015,
  title = {Self-Organization in {{Balanced State Networks}} by {{STDP}} and {{Homeostatic Plasticity}}},
  author = {Effenberger, Felix and Jost, J{\"u}rgen and Levina, Anna},
  editor = {Morrison, Abigail},
  year = {2015},
  month = sep,
  volume = {11},
  pages = {e1004420},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004420},
  file = {/Users/qualia/Documents/Papers/Effenberger et al. - 2015 - Self-organization in Balanced State Networks by ST.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {9}
}

@article{Eichler2017,
  title = {The Complete Connectome of a Learning and Memory Centre in an Insect Brain},
  author = {Eichler, Katharina and Li, Feng and {Litwin-Kumar}, Ashok and Park, Youngser and Andrade, Ingrid and {Schneider-Mizell}, Casey M. and Saumweber, Timo and Huser, Annina and Eschbach, Claire and Gerber, Bertram and Fetter, Richard D. and Truman, James W. and Priebe, Carey E. and Abbott, L. F. and Thum, Andreas S. and Zlatic, Marta and Cardona, Albert},
  year = {2017},
  month = aug,
  volume = {548},
  pages = {175--182},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature23455},
  file = {/Users/qualia/Documents/Papers/Eichler et al. - 2017 - The complete connectome of a learning and memory c.pdf},
  journal = {Nature},
  language = {en},
  number = {7666}
}

@article{Eklund2016,
  title = {Cluster Failure: {{Why fMRI}} Inferences for Spatial Extent Have Inflated False-Positive Rates},
  shorttitle = {Cluster Failure},
  author = {Eklund, Anders and Nichols, Thomas E. and Knutsson, Hans},
  year = {2016},
  month = jul,
  volume = {113},
  pages = {7900--7905},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1602413113},
  file = {/Users/qualia/Documents/Papers/Eklund et al. - 2016 - Cluster failure Why fMRI inferences for spatial e 2.pdf;/Users/qualia/Documents/Papers/Eklund et al. - 2016 - Cluster failure Why fMRI inferences for spatial e.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {28}
}

@article{Ekstrom2005,
  title = {Human Hippocampal Theta Activity during Virtual Navigation},
  author = {Ekstrom, Arne D. and Caplan, Jeremy B. and Ho, Emily and Shattuck, Kirk and Fried, Itzhak and Kahana, Michael J.},
  year = {2005},
  volume = {15},
  pages = {881--889},
  issn = {1050-9631, 1098-1063},
  doi = {10.1002/hipo.20109},
  abstract = {This study examines whether 4\textendash{}8-Hz theta oscillations can be seen in the human hippocampus, and whether these oscillations increase during virtual movement and searching, as they do in rodents. Recordings from both hippocampal and neocortical depth electrodes were analyzed while six epileptic patients played a virtual taxi-driver game. During the game, the patients alternated between searching for passengers, whose locations were random, and delivering them to stores, whose locations remained constant. In both hippocampus and neocortex, theta increased during virtual movement in all phases of the game. Hippocampal and neocortical theta activity were also significantly correlated with each other, but this correlation did not differ between neocortex and hippocampus and within disparate neocortical electrodes. Our findings demonstrate the existence of movement-related theta oscillations in human hippocampus, and suggest that both cortical and hippocampal oscillations play a role in attention and sensorimotor integration. VC 2005 Wiley-Liss, Inc.},
  file = {/Users/qualia/Documents/Papers/2005 - Ekstrom et al. - Human hippocampal theta activity during virtual navigation.pdf},
  journal = {Hippocampus},
  language = {en},
  number = {7}
}

@article{ElBoustani2007,
  title = {Activated Cortical States: {{Experiments}}, Analyses and Models},
  shorttitle = {Activated Cortical States},
  author = {El Boustani, Sami and Pospischil, Martin and {Rudolph-Lilith}, Michelle and Destexhe, Alain},
  year = {2007},
  month = jan,
  volume = {101},
  pages = {99--109},
  issn = {09284257},
  doi = {10.1016/j.jphysparis.2007.10.001},
  abstract = {In awake animals, the cerebral cortex displays an ``activated'' state, with distinct characteristics compared to other states like slow-wave sleep or anesthesia. These characteristics include a sustained depolarized membrane potential (Vm) and irregular firing activity. In the present paper, we evaluate our understanding of cortical activated states from a computational neuroscience point of view. We start by reviewing the electrophysiological characteristics of activated cortical states based on recordings and analysis performed in awake cat association cortex. These analyses show that cortical activity is characterized by an apparent Poisson-distributed stochastic dynamics, both at the single-cell and population levels, and that single cells display a high-conductance state dominated by inhibition. We next overview computational models of the ``awake'' cortex, and perform the same analyses as in the experiments. Many properties identified experimentally are indeed reproduced by models, such as depolarized Vm, irregular firing with apparent Poisson statistics, and the determinant role of inhibitory fluctuations on spiking. However, other features are not well reproduced, such as firing statistics and the conductance state of the membrane, suggesting that the network state displayed by models is not entirely correct. We also show how networks can approach a correct conductance state, suggesting ways by which future models will generate activity fully consistent with experimental data.},
  file = {/Users/qualia/Documents/Papers/2007 - El Boustani et al. - Activated cortical states Experiments, analyses and models.pdf},
  journal = {Journal of Physiology-Paris},
  language = {en},
  number = {1-3}
}

@article{Eliasmith2014,
  title = {The Use and Abuse of Large-Scale Brain Models},
  author = {Eliasmith, Chris and Trujillo, Oliver},
  year = {2014},
  month = apr,
  volume = {25},
  pages = {1--6},
  issn = {09594388},
  doi = {10.1016/j.conb.2013.09.009},
  file = {/Users/qualia/Documents/Papers/Eliasmith and Trujillo - 2014 - The use and abuse of large-scale brain models.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Enel2016,
  title = {Reservoir {{Computing Properties}} of {{Neural Dynamics}} in {{Prefrontal Cortex}}},
  author = {Enel, Pierre and Procyk, Emmanuel and Quilodran, Ren{\'e} and Dominey, Peter Ford},
  editor = {O'Reilly, Jill X},
  year = {2016},
  month = jun,
  volume = {12},
  pages = {e1004967},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004967},
  file = {/Users/qualia/Documents/Papers/Enel et al. - 2016 - Reservoir Computing Properties of Neural Dynamics .pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {6}
}

@article{Engelken2015,
  title = {Comment on ``{{Two}} Types of Asynchronous Activity in Networks of Excitatory and Inhibitory Spiking Neurons''},
  author = {Engelken, Rainer and Farkhooi, Farzad and Hansel, David and {van Vreeswijk}, Carl and Wolf, Fred},
  year = {2015},
  month = apr,
  doi = {10.1101/017798},
  abstract = {Slow neural dynamics are believed to be important for behavior, learning and memory. Rate models operating in the chaotic regime show a rich dynamics at the scale of hundreds of milliseconds and provide remarkable learning capabilities. However, neurons in the brain communicate via spikes and it is a major challenge in computational neuroscience to obtain similar slow rate dynamics in networks of spiking neuron models. This question was addressed in a recent paper by Ostojic. The central claim of that paper is the existence of two states of asynchronous activity separated by a phase transition in spiking networks with fast synapses. We found that the analysis presented in the paper is factually incorrect and conceptually misleading. We provide compelling evidence that there is no such phase transition.},
  file = {/Users/qualia/Documents/Papers/2014 - Unknown - Comment on Two types of asynchronous activity in networks of excitatory and inhibitory spiking neurons.pdf;/Users/qualia/Documents/Papers/Engelken et al. - 2015 - Comment on “Two types of asynchronous activity in .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Engle-Warnick2006,
  title = {Learning to Trust in Indefinitely Repeated Games},
  author = {{Engle-Warnick}, J. and Slonim, Robert L.},
  year = {2006},
  month = jan,
  volume = {54},
  pages = {95--114},
  issn = {08998256},
  doi = {10.1016/j.geb.2004.10.009},
  abstract = {Although it is well known that trust and trustworthiness (i.e., the fulfillment of trust) are important behaviors for the fulfillment of incomplete contracts, less is known about how the economic environment influences them. In this paper we design an experiment to examine how exogenously determined (stochastic) past relationship lengths affect trust and trustworthiness in new relationships. We find that shorter-lasting relationships have an immediate negative impact on both behaviors in the relationships that immediately follow, while longer-lasting relationships have the opposite effect. The effect of stochastic end-points declines for trustworthiness but not for trust as subjects gain experience, indicating that trust is able to rebound when longer-lasting relationships follow shorter-lasting ones.},
  file = {/Users/qualia/Documents/Papers/2006 - Engle-Warnick, Slonim - Learning to trust in indefinitely repeated games.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1}
}

@article{Ennis1988,
  title = {A Multidimensional Stochastic Theory of Similarity},
  author = {Ennis, Daniel M. and Palen, Joseph J. and Mullen, Kenneth},
  year = {1988},
  month = dec,
  volume = {32},
  pages = {449--465},
  issn = {00222496},
  doi = {10.1016/0022-2496(88)90023-5},
  file = {/Users/qualia/Documents/Papers/1988 - Ennis - Theory of Similarity.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {4}
}

@inproceedings{Epshteyn2008,
  title = {Active Reinforcement Learning},
  booktitle = {Proceedings of the 25th International Conference on {{Machine}} Learning - {{ICML}} '08},
  author = {Epshteyn, Arkady and Vogel, Adam and DeJong, Gerald},
  year = {2008},
  pages = {296--303},
  publisher = {{ACM Press}},
  address = {{Helsinki, Finland}},
  doi = {10.1145/1390156.1390194},
  abstract = {When the transition probabilities and rewards of a Markov Decision Process (MDP) are known, an agent can obtain the optimal policy without any interaction with the environment. However, exact transition probabilities are difficult for experts to specify. One option left to an agent is a long and potentially costly exploration of the environment. In this paper, we propose another alternative: given initial (possibly inaccurate) specification of the MDP, the agent determines the sensitivity of the optimal policy to changes in transitions and rewards. It then focuses its exploration on the regions of space to which the optimal policy is most sensitive. We show that the proposed exploration strategy performs well on several control and planning problems.},
  file = {/Users/qualia/Documents/Papers/Epshteyn et al. - 2008 - Active reinforcement learning.pdf},
  isbn = {978-1-60558-205-4},
  language = {en}
}

@article{Erev1998,
  title = {Predicting {{How People Play Games}}: {{Reinforcement Learning}} in {{Experimental Games}} with {{Unique}} , {{Mixed Strategy Equilibria}}},
  author = {Erev, Ido and Roth, Alvin E.},
  year = {1998},
  volume = {88},
  pages = {848--881},
  file = {/Users/qualia/Documents/Papers/1998 - Erev, Roth - Predicting how people play games Reinforcement learning in games with unique strategy equilibrium.pdf},
  journal = {The American Economic Review},
  language = {en},
  number = {4}
}

@article{Erev1998a,
  title = {Signal Detection by Human Observers: {{A}} Cutoff Reinforcement Learning Model of Categorization Decisions under Uncertainty.},
  shorttitle = {Signal Detection by Human Observers},
  author = {Erev, Ido},
  year = {1998},
  volume = {105},
  pages = {280--298},
  issn = {0033-295X},
  doi = {10.1037//0033-295X.105.2.280},
  file = {/Users/qualia/Documents/Papers/Erev - 1998 - Signal detection by human observers A cutoff rein.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {2}
}

@article{Eriksson2019,
  title = {Slow and Fast Cortical Dynamics Distinguish Motor Planning and Execution},
  author = {Eriksson, David},
  year = {2019},
  volume = {857300v1},
  pages = {31},
  file = {/Users/qualia/Documents/Papers/Eriksson - Slow and fast cortical dynamics distinguish motor .pdf},
  journal = {bioRxiv},
  language = {en}
}

@book{Ermentrout2010,
  title = {Mathematical Foundations of Neuroscience},
  author = {Ermentrout, Bard and Terman, David H.},
  year = {2010},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/Users/qualia/Documents/Papers/2010 - Ermentrout, Terman - Mathematical foundations of neuroscience.pdf},
  isbn = {978-0-387-87707-5},
  keywords = {Computational neuroscience,Mathematics,Neurosciences},
  language = {en},
  lccn = {QP357.5 .E76 2010},
  number = {v. 35},
  series = {Interdisciplinary Applied Mathematics}
}

@article{Ester2015,
  title = {Parietal and {{Frontal Cortex Encode Stimulus}}-{{Specific Mnemonic Representations}} during {{Visual Working Memory}}},
  author = {Ester, Edward F. and Sprague, Thomas C. and Serences, John T.},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {893--905},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.07.013},
  abstract = {Working memory (WM) enables the storage and manipulation of information in an active state. WM storage has long been associated with sustained increases in activation across a network of frontal and parietal cortical regions. However, recent evidence suggests that these regions primarily encode information related to general task goals rather than featureselective representations of specific memoranda. These goal-related representations are thought to provide top-down feedback that coordinates the representation of fine-grained details in early sensory areas. Here, we test this model using fMRI-based reconstructions of remembered visual details from region-level activation patterns. We could reconstruct high-fidelity representations of a remembered orientation based on activation patterns in occipital visual cortex and in several sub-regions of frontal and parietal cortex, independent of sustained increases in mean activation. These results challenge models of WM that postulate disjoint frontoparietal ``top-down control'' and posterior sensory ``feature storage'' networks.},
  file = {/Users/qualia/Documents/Papers/2015 - Ester, Sprague, Serences - Parietal and Frontal Cortex Encode Stimulus- Specific Mnemonic Representations during Visual Working M.pdf;/Users/qualia/Documents/Papers/Ester et al. - 2015 - Parietal and Frontal Cortex Encode Stimulus-Specif.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Ethofer2009,
  title = {Decoding of {{Emotional Information}} in {{Voice}}-{{Sensitive Cortices}}},
  author = {Ethofer, Thomas and Van De Ville, Dimitri and Scherer, Klaus and Vuilleumier, Patrik},
  year = {2009},
  month = jun,
  volume = {19},
  pages = {1028--1033},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.04.054},
  abstract = {The ability to correctly interpret emotional signals from others is crucial for successful social interaction. Previous neuroimaging studies showed that voice-sensitive auditory areas [1\textendash{}3] activate to a broad spectrum of vocally expressed emotions more than to neutral speech melody (prosody). However, this enhanced response occurs irrespective of the specific emotion category, making it impossible to distinguish different vocal emotions with conventional analyses [4\textendash{}8]. Here, we presented pseudowords spoken in five prosodic categories (anger, sadness, neutral, relief, joy) during event-related functional magnetic resonance imaging (fMRI), then employed multivariate pattern analysis [9, 10] to discriminate between these categories on the basis of the spatial response pattern within the auditory cortex. Our results demonstrate successful decoding of vocal emotions from fMRI responses in bilateral voicesensitive areas, which could not be obtained by using averaged response amplitudes only. Pairwise comparisons showed that each category could be classified against all other alternatives, indicating for each emotion a specific spatial signature that generalized across speakers. These results demonstrate for the first time that emotional information is represented by distinct spatial patterns that can be decoded from brain activity in modality-specific cortical areas.},
  file = {/Users/qualia/Documents/Papers/2009 - Ethofer et al. - Decoding of emotional information in voice-sensitive cortices.pdf},
  journal = {Current Biology},
  language = {en},
  number = {12}
}

@incollection{Even-Dar2002,
  title = {{{PAC Bounds}} for {{Multi}}-Armed {{Bandit}} and {{Markov Decision Processes}}},
  booktitle = {Computational {{Learning Theory}}},
  author = {{Even-Dar}, Eyal and Mannor, Shie and Mansour, Yishay},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Kivinen, Jyrki and Sloan, Robert H.},
  year = {2002},
  volume = {2375},
  pages = {255--270},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-45435-7_18},
  file = {/Users/qualia/Documents/Papers/Even-Dar et al. - 2002 - PAC Bounds for Multi-armed Bandit and Markov Decis.pdf},
  isbn = {978-3-540-43836-6 978-3-540-45435-9},
  language = {en}
}

@article{Even-Dar2006,
  title = {Action {{Elimination}} and {{Stopping Conditions}} for the {{Multi}}-{{Armed Bandit}} and {{Reinforcement Learning Problems}}},
  author = {{Even-Dar}, Eyal and Mannor, Shie and Mansour, Yishay},
  year = {2006},
  volume = {7},
  pages = {1--27},
  abstract = {We incorporate statistical confidence intervals in both the multi-armed bandit and the reinforcement learning problems. In the bandit problem we show that given n arms, it suffices to pull the arms a total of O (n/{$\epsilon$}2) log(1/{$\delta$}) times to find an {$\epsilon$}-optimal arm with probability of at least 1 - {$\delta$}. This bound matches the lower bound of Mannor and Tsitsiklis (2004) up to constants. We also devise action elimination procedures in reinforcement learning algorithms. We describe a framework that is based on learning the confidence interval around the value function or the Q-function and eliminating actions that are not optimal (with high probability). We provide a model-based and a model-free variants of the elimination method. We further derive stopping conditions guaranteeing that the learned policy is approximately optimal with high probability. Simulations demonstrate a considerable speedup and added robustness over {$\epsilon$}-greedy Q-learning.},
  file = {/Users/qualia/Documents/Papers/Even-Dar et al. - Action Elimination and Stopping Conditions for the.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{Evensen2004,
  title = {Sampling Strategies and Square Root Analysis Schemes for the {{EnKF}}},
  author = {Evensen, Geir},
  year = {2004},
  month = dec,
  volume = {54},
  pages = {539--560},
  issn = {1616-7341, 1616-7228},
  doi = {10.1007/s10236-004-0099-2},
  abstract = {The purpose of this paper is to examine how different sampling strategies and implementations of the analysis scheme influence the quality of the results in the EnKF. It is shown that by selecting the initial ensemble, the model noise and the measurement perturbations wisely, it is possible to achieve a significant improvement in the EnKF results, using the same number of members in the ensemble. The results are also compared with a square root implementation of the EnKF analysis scheme where the analyzed ensemble is computed without the perturbation of measurements. It is shown that the measurement perturbations introduce sampling errors which can be reduced using improved sampling schemes in the standard EnKF or fully eliminated when the square root analysis algorithm is used. Further, a new computationally efficient square root algorithm is proposed which allows for the use of a low-rank representation of the measurement error covariance matrix. It is shown that this algorithm in fact solves the full problem at a low cost without introducing any new approximations.},
  file = {/Users/qualia/Documents/Papers/2004 - Evensen - Sampling strategies and square root analysis schemes for the EnKF.pdf},
  journal = {Ocean Dynamics},
  language = {en},
  number = {6}
}

@article{Fadlallah,
  title = {Weighted-{{Permutation Entropy}}: {{An Improved Complexity Measure}} for {{Time Series}}},
  author = {Fadlallah, Bilal and Pr\i{}ncipe, Jose and Chen, Badong and Keil, Andreas},
  pages = {8},
  file = {/Users/qualia/Documents/Papers/2013 - Fadlallah, Keil - Weighted-Permutation Entropy An Improved Complexity Measure for Time Series.pdf},
  language = {en}
}

@article{Faisal2008,
  title = {Noise in the Nervous System},
  author = {Faisal, A. Aldo and Selen, Luc P. J. and Wolpert, Daniel M.},
  year = {2008},
  month = apr,
  volume = {9},
  pages = {292--303},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2258},
  abstract = {Random disturbances of signals, termed `noise', pose a fundamental problem for information processing and affect all aspects of nervous-system function. However, the nature, amount and impact of noise in the nervous system have only recently been addressed in a quantitative manner. Experimental and computational methods have shown that multiple noise sources contribute to cellular and behavioural trial-to-trial variability. We review the sources of noise in the nervous system, from the molecular to the behavioural level, and show how noise contributes to trial-totrial variability. We highlight how noise affects neuronal networks and the principles the nervous system applies to counter detrimental effects of noise, and briefly discuss noise's potential benefits.},
  file = {/Users/qualia/Documents/Papers/2008 - Faisal, Selen, Wolpert - Noise in the nervous system.pdf;/Users/qualia/Documents/Papers/2008 - Faisal, Selen, Wolpert - Noise in the nervous system(2).pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {4}
}

@article{Fan,
  title = {{{LIBLINEAR}}: {{A Library}} for {{Large Linear Classification}}},
  author = {Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
  pages = {29},
  abstract = {LIBLINEAR is an open source library for large-scale linear classification. It supports logistic regression and linear support vector machines. We provide easy-to-use command-line tools and library calls for users and developers. Comprehensive documents are available for both beginners and advanced users. Experiments demonstrate that LIBLINEAR is very efficient on large sparse data sets.},
  file = {/Users/qualia/Documents/Papers/Fan et al. - LIBLINEAR A Library for Large Linear Classiﬁcatio.pdf},
  language = {en}
}

@techreport{Fan2019,
  title = {All-Optical Electrophysiology Reveals Excitation, Inhibition, and Neuromodulation in Cortical Layer 1},
  author = {Fan, Linlin Z. and Kheifets, Simon and B{\"o}hm, Urs L. and Piatkevich, Kiryl D. and Wu, Hao and Parot, Vicente and Xie, Michael E. and Boyden, Edward S. and Takesian, Anne E. and Cohen, Adam E.},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/614172},
  abstract = {Abstract
          
            The stability of neural dynamics arises through a tight coupling of excitatory (E) and inhibitory (I) signals. Genetically encoded voltage indicators (GEVIs) can report both spikes and subthreshold dynamics
            in vivo
            , but voltage alone only reveals the combined effects of E and I synaptic inputs, not their separate contributions individually. Here we combine optical recording of membrane voltage with simultaneous optogenetic manipulation to probe E and I individually in barrel cortex Layer 1 (L1) neurons in awake mice. Our studies show that L1 neurons integrate thalamocortical excitation and lateral inhibition to produce precisely timed responses to whisker stimuli. Top-down neuromodulatory inputs drive additional excitation in L1. Together, these results suggest a model for computation in L1 consistent with its hypothesized role in attentional gating of the underlying cortex.
          
          
            One Sentence Summary
            All-optical electrophysiology revealed the function in awake mice of an inhibitory microcircuit in barrel cortex Layer 1.},
  file = {/Users/qualia/Documents/Papers/Fan et al. - 2019 - All-optical electrophysiology reveals excitation, .pdf},
  language = {en},
  type = {Preprint}
}

@article{Fang1991,
  title = {A Method to Effect Physiological Recruitment Order in Electrically Activated Muscle},
  author = {Fang, Z.-P. and Mortimer, J.T.},
  year = {Feb./1991},
  volume = {38},
  pages = {175--179},
  issn = {00189294},
  doi = {10.1109/10.76384},
  abstract = {A new stimulation method has been utilized to achieve physiological recruitment order of small-to-large motor units in electrically activated muscles. The use of quasitrapezoidal-shaped pulses and a tripolar cuff electrode made selective activation of small motor axons possible, thus recruiting slow-twitch, fatigue-resistant muscle units before fast-twitch, fatigable units in a heterogeneous muscle. Isometric contraction force from the medial gastrocnemius muscle was measured in five cats. The physiological recruitment order was evidenced by larger twitch widths at lower force levels and smaller twitch widths at higher force levels in the muscles tested. In addition, force modulation process was more gradual and fused contractions were obtained at lower stimulation frequencies when the new stimulation method was employed. Furthermore, muscles activated by the new method were more fatigue-resistant under repetitive activation at low force levels. This stimulation method is simpler to implement and has fewer adverse effects on the neuromuscular system than previous blocking methods. Therefore, it may have applications in future functional neuromuscular stimulation systems.},
  file = {/Users/qualia/Documents/Papers/1991 - Fang, Mortimer - A Method to Effect Physiological Recruitment Order in Electrically Activated Muscle.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {2}
}

@article{Farhoodi2019,
  title = {On Functions Computed on Trees},
  author = {Farhoodi, Roozbeh and Filom, Khashayar and Jones, Ilenna Simone and Kording, Konrad Paul},
  year = {2019},
  month = apr,
  abstract = {Any function can be constructed using a hierarchy of simpler functions through compositions. Such a hierarchy can be characterized by a binary rooted tree. Each node of this tree is associated with a function which takes as inputs two numbers from its children and produces one output. Since thinking about functions in terms of computation graphs is getting popular we may want to know which functions can be implemented on a given tree. Here, we describe a set of necessary constraints in the form of a system of non-linear partial differential equations that must be satisfied. Moreover, we prove that these conditions are sufficient in both contexts of analytic and bit-value functions. In the latter case, we explicitly enumerate discrete functions and observe that there are relatively few. Our point of view allows us to compare different neural network architectures in regard to their function spaces. Our work connects the structure of computation graphs with the functions they can implement and has potential applications to neuroscience and computer science.},
  archivePrefix = {arXiv},
  eprint = {1904.02309},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Farhoodi et al. - 2019 - On functions computed on trees.pdf},
  journal = {arXiv:1904.02309 [cs, math, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Combinatorics,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, q-bio, stat}
}

@inproceedings{Favaretto2017,
  title = {Bode Meets {{Kuramoto}}: {{Synchronized}} Clusters in Oscillatory Networks},
  shorttitle = {Bode Meets {{Kuramoto}}},
  booktitle = {2017 {{American Control Conference}} ({{ACC}})},
  author = {Favaretto, Chiara and Bassett, Danielle S. and Cenedese, Angelo and Pasqualetti, Fabio},
  year = {2017},
  month = may,
  pages = {2799--2804},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.23919/ACC.2017.7963375},
  abstract = {In this paper we study cluster synchronization in a network of Kuramoto oscillators, where groups of oscillators evolve cohesively and at different frequencies from the neighboring oscillators. Synchronization is critical in a variety of systems, where it enables complex functionalities and behaviors. Synchronization over networks depends on the oscillators' dynamics, the interaction topology, and coupling strengths, and the relationship between these different factors can be quite intricate. In this work we formally show that three network properties enable the emergence of cluster synchronization. Specifically, weak inter-cluster connections, strong intra-cluster connections, and sufficiently diverse natural frequencies among oscillators belonging to different groups. Our approach relies on system-theoretic tools, and is validated with numerical studies.},
  file = {/Users/qualia/Documents/Papers/Favaretto et al. - 2017 - Bode meets Kuramoto Synchronized clusters in osci.pdf},
  isbn = {978-1-5090-5992-8},
  language = {en}
}

@article{Feingold2015,
  title = {Bursts of Beta Oscillation Differentiate Postperformance Activity in the Striatum and Motor Cortex of Monkeys Performing Movement Tasks},
  author = {Feingold, Joseph and Gibson, Daniel J. and DePasquale, Brian and Graybiel, Ann M.},
  year = {2015},
  month = nov,
  volume = {112},
  pages = {13687--13692},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1517629112},
  file = {/Users/qualia/Documents/Papers/2015 - Feingold et al. - Bursts of beta oscillation differentiate postperformance activity in the striatum and motor cortex of monkey(2).pdf;/Users/qualia/Documents/Papers/Feingold et al. - 2015 - Bursts of beta oscillation differentiate postperfo 2.pdf;/Users/qualia/Documents/Papers/Feingold et al. - 2015 - Bursts of beta oscillation differentiate postperfo.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {44}
}

@article{Fellous,
  title = {Computational {{Models}} of {{Neuromodulation}}},
  author = {Fellous, Jean-Marc and Linster, Christiane},
  pages = {35},
  file = {/Users/qualia/Documents/Papers/1998 - Fellous, Linser - Computational Models of Neuromodulation.pdf},
  language = {en}
}

@article{Feltovich,
  title = {John {{Du}} Y {{Department}} of {{Economics University}} of {{Pittsburgh Pittsburgh}}, {{PA}} 15260, {{USA}} Jdu Y+@pitt.Edu},
  author = {Feltovich, Nick},
  pages = {50},
  abstract = {How do individuals achieve good outcomes" in one shot strategic situations? One much explored possibility is that they engage in some kind of preplay communication|cheap talk|in which they endeavor to convince one another of the actions they intend to play. However, there may be no incentive for such communication to be truthful, or even informative. Another, less explored, possibility is that individuals take account of their knowledge of the past behavior of others when deciding which actions to play. While these two possibilities have been considered separately, there has been little research comparing the importance of these two devices as aids in achieving good outcomes. We design and run an experiment with human subjects that allows for such a comparison. The e ects of cheap talk and observation of past actions are compared with each other, and with the standard control case where neither cheap talk nor observation is allowed. We consider three di erent 2  2 games and explain why cheap talk or observation is likely to be the more e ective device for achieving good outcomes in each game. The experimental evidence suggests that both devices|cheap talk and observation| make cooperation and successful coordination more likely and increase payo s relative to the control. The relative success of cheap talk versus observation in achieving such good outcomes depends on the game played, in accordance with our predictions. We also nd that the signals players send are informative in the sense that they are cor related with their eventual actions, and that receivers of signals take this fact into account by conditioning their actions on the signal they receive. The results of this experiment can be used to extend game theoretic models of how individuals make use of the di erent types of information available in strategic environments. As a rst step in this direction, we construct a learning model in which individuals can condition their behavior on cheap talk or observed past actions, and we show that this model provides a good quantitative as well as qualitative t to the experimental data.},
  file = {/Users/qualia/Documents/Papers/2002 - Duffy, Feltovich - Do actions speak louder than words An experimental comparison of observation and cheap talk.pdf},
  language = {en}
}

@article{Ferrario2016,
  title = {Homeostasis {{Meets Motivation}} in the {{Battle}} to {{Control Food Intake}}},
  author = {Ferrario, Carrie R. and Labou{\`e}be, Gwena{\"e}l and Liu, Shuai and Nieh, Edward H. and Routh, Vanessa H. and Xu, Shengjin and O'Connor, Eoin C.},
  year = {2016},
  month = nov,
  volume = {36},
  pages = {11469--11481},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2338-16.2016},
  file = {/Users/qualia/Documents/Papers/Ferrario et al. - 2016 - Homeostasis Meets Motivation in the Battle to Cont.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {45}
}

@article{Ferre1997,
  title = {Adenosine\textendash{}Dopamine Receptor\textendash{}Receptor Interactions as an Integrative Mechanism in the Basal Ganglia},
  author = {Ferr{\'e}, Sergi and Fuxe, Kjell and B. Fredholm, Bertil and Morelli, Micaela and Popoli, Patrizia},
  year = {1997},
  month = oct,
  volume = {20},
  pages = {482--487},
  issn = {01662236},
  doi = {10.1016/S0166-2236(97)01096-5},
  file = {/Users/qualia/Documents/Papers/1997 - Ferré et al. - Adenosine-dopamine receptor-receptor interactions as an integrative mechanism in the basal ganglia.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {10}
}

@article{Fields2015,
  title = {A New Mechanism of Nervous System Plasticity: Activity-Dependent Myelination},
  shorttitle = {A New Mechanism of Nervous System Plasticity},
  author = {Fields, R. Douglas},
  year = {2015},
  month = dec,
  volume = {16},
  pages = {756--767},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn4023},
  abstract = {The synapse is the focus of experimental research and theory on the cellular mechanisms of nervous system plasticity and learning, but recent research is expanding the consideration of plasticity into new mechanisms beyond the synapse, notably including the possibility that conduction velocity could be modifiable through changes in myelin to optimize the timing of information transmission through neural circuits. This concept emerges from a confluence of brain imaging that reveals changes in white matter in the human brain during learning, together with cellular studies showing that the process of myelination can be influenced by action potential firing in axons. This Opinion article summarizes the new research on activity-dependent myelination, explores the possible implications of these studies and outlines the potential for new research.},
  file = {/Users/qualia/Documents/Papers/2015 - Fields - A new mechanism of nervous system plasticity activity-dependent myelination.pdf;/Users/qualia/Documents/Papers/Fields - 2015 - A new mechanism of nervous system plasticity acti.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {12}
}

@article{Fiete2006,
  title = {Gradient {{Learning}} in {{Spiking Neural Networks}} by {{Dynamic Perturbation}} of {{Conductances}}},
  author = {Fiete, Ila R. and Seung, H. Sebastian},
  year = {2006},
  month = jul,
  volume = {97},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.048104},
  file = {/Users/qualia/Documents/Papers/2006 - Fiete, Seung - Gradient learning in spiking neural networks by dynamic perturbation of conductances.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {4}
}

@article{Filatrella2007,
  title = {Generalized Coupling in the {{Kuramoto}} Model},
  author = {Filatrella, G. and Pedersen, N. F. and Wiesenfeld, K.},
  year = {2007},
  month = jan,
  volume = {75},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.017201},
  file = {/Users/qualia/Documents/Papers/2007 - Filatrella, Pedersen, Wiesenfeld - Generalized coupling in the Kuramoto model.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {1}
}

@techreport{Filipowicz2020,
  title = {The Complexity of Model-Free and Model-Based Learning Strategies},
  author = {Filipowicz, Alexandre L. S. and Levine, Jonathan and Piasini, Eugenio and Tavoni, Gaia and Kable, Joseph W. and Gold, Joshua I.},
  year = {2020},
  month = jan,
  institution = {{Neuroscience}},
  doi = {10.1101/2019.12.28.879965},
  abstract = {A proposed continuum of learning strategies, from model-free to model-based, is thought to progress systematically in complexity and therefore flexibility. Here we distinguish different forms of complexity to show that, contrary to this idea, strategies at both ends of this continuum can be equally flexible. Using a canonical learning task, we first simulated behavior to show that computational complexity, a measure of implementation demands, is higher for a standard modelbased versus model-free algorithm, but information complexity, a measure of flexibility, is not. We then analyzed human behavior to show that information complexity, which unlike computational complexity can be estimated from behavior, tended to increase for strategies that were increasingly either model-free or model-based, resulting in similar accuracy, suboptimal use of information, and increased response times. Thus, model-free and model-based strategies can have similar overall flexibility and instead are better distinguished by the specific task features from which they learn.},
  file = {/Users/qualia/Documents/Papers/Filipowicz et al. - 2020 - The complexity of model-free and model-based learn.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Findling2018,
  title = {Computational Noise in Reward-Guided Learning Drives Behavioral Variability in Volatile Environments},
  author = {Findling, Charles and Skvortsova, Vasilisa and Dromnelle, Remi and Palminteri, Stefano and Wyart, Valentin},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/439885},
  abstract = {When learning the value of actions in volatile environments, humans often make seemingly irrational decisions which fail to maximize expected value. We reasoned that these 'non-greedy' decisions, instead of reflecting information seeking during choice, may be caused by computational noise in the learning of action values. Here, using reinforcement learning (RL) models of behavior and multimodal neurophysiological data, we show that the majority of non-greedy decisions stems from this learning noise. The trial-to-trial variability of sequential learning steps and their impact on behavior could be predicted both by BOLD responses to obtained rewards in the dorsal anterior cingulate cortex (dACC) and by phasic pupillary dilation - suggestive of neuromodulatory fluctuations driven by the locus coeruleus-norepinephrine (LC-NE) system. Together, these findings indicate that most of behavioral variability, rather than reflecting human exploration, is due to the limited computational precision of reward-guided learning.},
  file = {/Users/qualia/Documents/Papers/Findling et al. - 2018 - Computational noise in reward-guided learning driv.pdf},
  language = {en},
  type = {Preprint}
}

@article{Finkbeiner1992,
  title = {Calcium Waves in Astrocytes-Filling in the Gaps},
  author = {Finkbeiner, Steven},
  year = {1992},
  month = jun,
  volume = {8},
  pages = {1101--1108},
  issn = {08966273},
  doi = {10.1016/0896-6273(92)90131-V},
  file = {/Users/qualia/Documents/Papers/Finkbeiner - 1992 - Calcium waves in astrocytes-filling in the gaps.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Flavell2013,
  title = {Serotonin and the {{Neuropeptide PDF Initiate}} and {{Extend Opposing Behavioral States}} in {{C}}. Elegans},
  author = {Flavell, Steven W. and Pokala, Navin and Macosko, Evan Z. and Albrecht, Dirk R. and Larsch, Johannes and Bargmann, Cornelia I.},
  year = {2013},
  month = aug,
  volume = {154},
  pages = {1023--1035},
  issn = {00928674},
  doi = {10.1016/j.cell.2013.08.001},
  abstract = {Foraging animals have distinct exploration and exploitation behaviors that are organized into discrete behavioral states. Here, we characterize a neuromodulatory circuit that generates long-lasting roaming and dwelling states in Caenorhabditis elegans. We find that two opposing neuromodulators, serotonin and the neuropeptide pigment dispersing factor (PDF), each initiate and extend one behavioral state. Serotonin promotes dwelling states through the MOD-1 serotonin-gated chloride channel. The spontaneous activity of serotonergic neurons correlates with dwelling behavior, and optogenetic modulation of the critical MOD-1-expressing targets induces prolonged dwelling states. PDF promotes roaming states through a Gas-coupled PDF receptor; optogenetic activation of cAMP production in PDF receptor-expressing cells induces prolonged roaming states. The neurons that produce and respond to each neuromodulator form a distributed circuit orthogonal to the classical wiring diagram, with several essential neurons that express each molecule. The slow temporal dynamics of this neuromodulatory circuit supplement fast motor circuits to organize long-lasting behavioral states.},
  file = {/Users/qualia/Documents/Papers/Flavell et al. - 2013 - Serotonin and the Neuropeptide PDF Initiate and Ex.pdf},
  journal = {Cell},
  language = {en},
  number = {5}
}

@article{Fletcher2019,
  title = {Neocortical {{Topology Governs}} the {{Dendritic Integrative Capacity}} of {{Layer}} 5 {{Pyramidal Neurons}}},
  author = {Fletcher, Lee N. and Williams, Stephen R.},
  year = {2019},
  month = jan,
  volume = {101},
  pages = {76-90.e4},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.10.048},
  abstract = {The structure of the neocortex varies across the neocortical mantle to govern the physical size of principal neurons. What impact such anatomical variation has on the computational operations of principal neurons remains unknown. Here, we demonstrate within a functionally defined area that neocortical thickness governs the anatomical, electrophysiological, and computational properties of the principal output neurons of the neocortex. We find that neocortical thickness and the size of layer 5B pyramidal neurons changes as a gradient across the rostro-caudal axis of the rat primary visual cortex. Simultaneous somato-dendritic whole-cell recordings and compartmental modeling revealed that the electrical architecture of principal neurons was not preserved; rather, primary visual cortex site-dependent differences in intracellular resistivity accentuated a gradient of the electrical behavior of layer 5B pyramidal neurons to influence the emergence of active dendritic computations. Our findings therefore reveal an exquisite relationship between neocortical structure and neuronal computation.},
  file = {/Users/qualia/Documents/Papers/Fletcher and Williams - 2019 - Neocortical Topology Governs the Dendritic Integra.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Forger2011,
  title = {Optimal {{Stimulus Shapes}} for {{Neuronal Excitation}}},
  author = {Forger, Daniel B. and Paydarfar, David and Clay, John R.},
  editor = {Morrison, Abigail},
  year = {2011},
  month = jul,
  volume = {7},
  pages = {e1002089},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002089},
  abstract = {An important problem in neuronal computation is to discern how features of stimuli control the timing of action potentials. One aspect of this problem is to determine how an action potential, or spike, can be elicited with the least energy cost, e.g., a minimal amount of applied current. Here we show in the Hodgkin \& Huxley model of the action potential and in experiments on squid giant axons that: 1) spike generation in a neuron can be highly discriminatory for stimulus shape and 2) the optimal stimulus shape is dependent upon inputs to the neuron. We show how polarity and time course of postsynaptic currents determine which of these optimal stimulus shapes best excites the neuron. These results are obtained mathematically using the calculus of variations and experimentally using a stochastic search methodology. Our findings reveal a surprising complexity of computation at the single cell level that may be relevant for understanding optimization of signaling in neurons and neuronal networks.},
  file = {/Users/qualia/Documents/Papers/2011 - Forger, Paydarfar, Clay - Optimal Stimulus Shapes for Neuronal Excitation.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {7}
}

@article{Forkman1991,
  title = {Some {{Problems}} with {{Current Patch}}-{{Choice Theory}}: {{A Study}} on the {{Mongolian Gerbil}}},
  author = {Forkman, Bj{\"o}rn},
  year = {1991},
  volume = {117},
  pages = {243--254},
  file = {/Users/qualia/Documents/Papers/Forkman - 1991 - Some Problems with Current Patch-Choice Theory A .pdf},
  journal = {Behaviour},
  language = {en},
  number = {3/4}
}

@article{Forkman1991a,
  title = {Some {{Problems}} with {{Current Patch}}-{{Choice Theory}}: {{A Study}} on the {{Mongolian Gerbil}}},
  author = {Forkman, Bj{\"o}rn},
  year = {1991},
  volume = {117},
  pages = {243--254},
  file = {/Users/qualia/Documents/Papers/Forkman - 1991 - Some Problems with Current Patch-Choice Theory A .pdf},
  journal = {Behaviour},
  language = {en},
  number = {3/4}
}

@article{Forkman1991b,
  title = {Some {{Problems}} with {{Current Patch}}-{{Choice Theory}}: {{A Study}} on the {{Mongolian Gerbil}}},
  author = {Forkman, Bj{\"o}rn},
  year = {1991},
  volume = {117},
  pages = {243--254},
  file = {/Users/qualia/Documents/Papers/Forkman - 1991 - Some Problems with Current Patch-Choice Theory A  2.pdf},
  journal = {Behaviour},
  language = {en},
  number = {3/4}
}

@article{Forkman1996,
  title = {The {{Foraging Behaviour}} of {{Mongolian Gerbils}}: {{A Behavioural Need}} or a {{Need}} to {{Know}}?},
  author = {Forkman, Bj{\"o}rn},
  year = {1996},
  volume = {133},
  pages = {129--143},
  abstract = {In the present paper Mongolian gerbils (Meriones unguiculatus) were shown to prefer to forage from an unprofitable food source when it contained hidden food, but not when the food was clearly visible. Four experiments were performed, in each experiment the animal could forage from either a food source with easily accessible food or from a food source which required more work. In the first experiment the animal could choose between seeds with husks and those without, and in the second experiment between seeds glued to a stick and seeds in a bowl. In both these experiments the animals could see the food of both food sources. The animals chose to forage from the most profitable food source, i.e. the seeds without husks and the seeds in a bowl respectively. In the third experiment the animals could choose between eating seeds hidden under lids or seeds in a bowl, and finally in the fourth experiment they could forage for seeds on a camouflaging surface or on a surface where the seeds were clearly visible. In these last two experiments it was impossible to see the food in the unprofitable food sources without working for it. In these situations the animals choose to forage from the unprofitable food source, i.e. from underneath the lids and on the camouflaging surface. These results are in accordance with exploration being the driving force behind contrafreeloading (learned industrioussness). The results cannot be explained by classical optimal foraging theory.},
  file = {/Users/qualia/Documents/Papers/Forkman - 1996 - The Foraging Behaviour of Mongolian Gerbils A Beh.pdf},
  journal = {Behaviour},
  language = {en},
  number = {1/2}
}

@article{Forkman1996a,
  title = {The {{Foraging Behaviour}} of {{Mongolian Gerbils}}: {{A Behavioural Need}} or a {{Need}} to {{Know}}?},
  author = {Forkman, Bj{\"o}rn},
  year = {1996},
  volume = {133},
  pages = {129--143},
  abstract = {In the present paper Mongolian gerbils (Meriones unguiculatus) were shown to prefer to forage from an unprofitable food source when it contained hidden food, but not when the food was clearly visible. Four experiments were performed, in each experiment the animal could forage from either a food source with easily accessible food or from a food source which required more work. In the first experiment the animal could choose between seeds with husks and those without, and in the second experiment between seeds glued to a stick and seeds in a bowl. In both these experiments the animals could see the food of both food sources. The animals chose to forage from the most profitable food source, i.e. the seeds without husks and the seeds in a bowl respectively. In the third experiment the animals could choose between eating seeds hidden under lids or seeds in a bowl, and finally in the fourth experiment they could forage for seeds on a camouflaging surface or on a surface where the seeds were clearly visible. In these last two experiments it was impossible to see the food in the unprofitable food sources without working for it. In these situations the animals choose to forage from the unprofitable food source, i.e. from underneath the lids and on the camouflaging surface. These results are in accordance with exploration being the driving force behind contrafreeloading (learned industrioussness). The results cannot be explained by classical optimal foraging theory.},
  file = {/Users/qualia/Documents/Papers/Forkman - 1996 - The Foraging Behaviour of Mongolian Gerbils A Beh 2.pdf},
  journal = {Behaviour},
  language = {en},
  number = {1/2}
}

@article{Formisano2008,
  title = {Multivariate Analysis of {{fMRI}} Time Series: Classification and Regression of Brain Responses Using Machine Learning},
  shorttitle = {Multivariate Analysis of {{fMRI}} Time Series},
  author = {Formisano, Elia and De Martino, Federico and Valente, Giancarlo},
  year = {2008},
  month = sep,
  volume = {26},
  pages = {921--934},
  issn = {0730725X},
  doi = {10.1016/j.mri.2008.01.052},
  abstract = {Machine learning and pattern recognition techniques are being increasingly employed in functional magnetic resonance imaging (fMRI) data analysis. By taking into account the full spatial pattern of brain activity measured simultaneously at many locations, these methods allow detecting subtle, non-strictly localized effects that may remain invisible to the conventional analysis with univariate statistical methods. In typical fMRI applications, pattern recognition algorithms ``learn'' a functional relationship between brain response patterns and a perceptual, cognitive or behavioral state of a subject expressed in terms of a label, which may assume discrete (classification) or continuous (regression) values. This learned functional relationship is then used to predict the unseen labels from a new data set (``brain reading''). In this article, we describe the mathematical foundations of machine learning applications in fMRI. We focus on two methods, support vector machines and relevance vector machines, which are respectively suited for the classification and regression of fMRI patterns. Furthermore, by means of several examples and applications, we illustrate and discuss the methodological challenges of using machine learning algorithms in the context of fMRI data analysis.},
  file = {/Users/qualia/Documents/Papers/2008 - Formisano, De Martino, Valente - Multivariate analysis of fMRI time series classification and regression of brain responses using.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {7}
}

@article{Forster2000,
  title = {Key {{Concepts}} in {{Model Selection}}: {{Performance}} and {{Generalizability}}},
  shorttitle = {Key {{Concepts}} in {{Model Selection}}},
  author = {Forster, Malcolm R},
  year = {2000},
  month = mar,
  volume = {44},
  pages = {205--231},
  issn = {00222496},
  doi = {10.1006/jmps.1999.1284},
  file = {/Users/qualia/Documents/Papers/2000 - Forster - Key Concepts in Model Selection Performance and Generalizability.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en},
  number = {1}
}

@article{Foster,
  title = {Regret {{Testing}}: {{Learning}} to {{Play Nash Equilibrium Without Knowing You Have}} an {{Opponent}}},
  author = {Foster, Dean P and Young, H Peyton},
  pages = {29},
  abstract = {A learning rule is uncoupled if a player does not condition his strategy on the opponent's payoffs. It is radically uncoupled if a player does not condition his strategy on the opponent's actions or payoffs. We demonstrate a family of simple, radically uncoupled learning rules whose period-by-period behavior comes arbitrarily close to Nash equilibrium behavior in any finite two-person game.},
  file = {/Users/qualia/Documents/Papers/2006 - Foster, Young - Regret testing learning to play Nash equilibrium without knowing you have an opponent.pdf},
  language = {en}
}

@article{Foster2016,
  title = {The Topography of Alpha-Band Activity Tracks the Content of Spatial Working Memory},
  author = {Foster, Joshua J. and Sutterer, David W. and Serences, John T. and Vogel, Edward K. and Awh, Edward},
  year = {2016},
  month = jan,
  volume = {115},
  pages = {168--177},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00860.2015},
  file = {/Users/qualia/Documents/Papers/2015 - Foster et al. - The topography of alpha-band activity tracks the content of spatial working memory.pdf;/Users/qualia/Documents/Papers/Foster et al. - 2016 - The topography of alpha-band activity tracks the c 2.pdf;/Users/qualia/Documents/Papers/Foster et al. - 2016 - The topography of alpha-band activity tracks the c.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Fourcaud2002,
  title = {Dynamics of the {{Firing Probability}} of {{Noisy Integrate}}-and-{{Fire Neurons}}},
  author = {Fourcaud, Nicolas and Brunel, Nicolas},
  year = {2002},
  month = sep,
  volume = {14},
  pages = {2057--2110},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602320264015},
  file = {/Users/qualia/Documents/Papers/2002 - Fourcaud, Brunel - Dynamics of the firing probability of noisy integrate-and-fire neurons.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {9}
}

@article{Foutz2010,
  title = {Evaluation of Novel Stimulus Waveforms for Deep Brain Stimulation},
  author = {Foutz, Thomas J and McIntyre, Cameron C},
  year = {2010},
  month = dec,
  volume = {7},
  pages = {066008},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/7/6/066008},
  abstract = {Deep brain stimulation (DBS) is an established therapy for the treatment of a wide range of neurological disorders. Historically, DBS and other neurostimulation technologies have relied on rectangular stimulation waveforms to impose their effects on the nervous system. Recent work has suggested that non-rectangular waveforms may have advantages over the traditional rectangular pulse. Therefore, we used detailed computer models to compare a range of charge-balanced biphasic waveforms with rectangular, exponential, triangular, Gaussian and sinusoidal stimulus pulse shapes. We explored the neural activation energy of these waveforms for both intracellular and extracellular current-controlled stimulation conditions. In the context of extracellular stimulation, we compared their effects on both axonal fibers of passage and projection neurons. Finally, we evaluated the impact of delivering the waveforms through a clinical DBS electrode, as opposed to a theoretical point source. Our results suggest that DBS with a 1 ms centered-triangular pulse can decrease energy consumption by 64\% when compared with the standard 100 {$\mu$}s rectangular pulse (energy cost of 48 and 133 nJ, respectively, to stimulate 50\% of a distributed population of axons) and can decrease energy consumption by 10\% when compared with the most energy efficient rectangular pulse (1.25 ms duration). In turn, there may be measureable energy savings when using appropriately designed non-rectangular pulses in clinical DBS applications, thereby warranting further experimental investigation.},
  file = {/Users/qualia/Documents/Papers/2010 - Foutz, McIntyre - Evaluation of novel stimulus waveforms for deep brain stimulation.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {6}
}

@article{Fowler,
  title = {Data {{Assimilation}} Tutorial on the {{Kalman}} Filter},
  author = {Fowler, A},
  pages = {14},
  file = {/Users/qualia/Documents/Papers/1963 - Fowler - Data Assimilation tutorial on the Kalman filter.pdf;/Users/qualia/Documents/Papers/Fowler - Data Assimilation tutorial on the Kalman ﬁlter.pdf},
  language = {en}
}

@article{Foxe2011,
  title = {The {{Role}} of {{Alpha}}-{{Band Brain Oscillations}} as a {{Sensory Suppression Mechanism}} during {{Selective Attention}}},
  author = {Foxe, John J. and Snyder, Adam C.},
  year = {2011},
  volume = {2},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2011.00154},
  abstract = {Evidence has amassed from both animal intracranial recordings and human electrophysiology that neural oscillatory mechanisms play a critical role in a number of cognitive functions such as learning, memory, feature binding and sensory gating. The wide availability of high-density electrical and magnetic recordings (64\textendash{}256 channels) over the past two decades has allowed for renewed efforts in the characterization and localization of these rhythms. A variety of cognitive effects that are associated with specific brain oscillations have been reported, which range in spectral, temporal, and spatial characteristics depending on the context. Our laboratory has focused on investigating the role of alpha-band oscillatory activity (8\textendash{}14 Hz) as a potential attentional suppression mechanism, and this particular oscillatory attention mechanism will be the focus of the current review. We discuss findings in the context of intersensory selective attention as well as intrasensory spatial and feature-based attention in the visual, auditory, and tactile domains. The weight of evidence suggests that alpha-band oscillations can be actively invoked within cortical regions across multiple sensory systems, particularly when these regions are involved in processing irrelevant or distracting information. That is, a central role for alpha seems to be as an attentional suppression mechanism when objects or features need to be specifically ignored or selected against.},
  file = {/Users/qualia/Documents/Papers/2011 - Foxe, Snyder - The role of alpha-band brain oscillations as a sensory suppression mechanism during selective attention.pdf},
  journal = {Frontiers in Psychology},
  language = {en}
}

@article{Francois-Lavet2015,
  title = {How to {{Discount Deep Reinforcement Learning}}: {{Towards New Dynamic Strategies}}},
  shorttitle = {How to {{Discount Deep Reinforcement Learning}}},
  author = {{Fran{\c c}ois-Lavet}, Vincent and Fonteneau, Raphael and Ernst, Damien},
  year = {2015},
  month = dec,
  abstract = {Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1]. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network (DQN). When the discount factor progressively increases up to its final value, we empirically show that it is possible to significantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original DQN on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.},
  archivePrefix = {arXiv},
  eprint = {1512.02011},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - François-Lavet, Fonteneau, Ernst - How to Discount Deep Reinforcement Learning Towards New Dynamic Strategies.pdf;/Users/qualia/Documents/Papers/François-Lavet et al. - 2015 - How to Discount Deep Reinforcement Learning Towar.pdf},
  journal = {arXiv:1512.02011 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Frank2006,
  title = {Hold Your Horses: {{A}} Dynamic Computational Role for the Subthalamic Nucleus in Decision Making},
  shorttitle = {Hold Your Horses},
  author = {Frank, Michael J.},
  year = {2006},
  month = oct,
  volume = {19},
  pages = {1120--1136},
  issn = {08936080},
  doi = {10.1016/j.neunet.2006.03.006},
  abstract = {The basal ganglia (BG) coordinate decision making processes by facilitating adaptive frontal motor commands while suppressing others. In previous work, neural network simulations accounted for response selection deficits associated with BG dopamine depletion in Parkinson's disease. Novel predictions from this model have been subsequently confirmed in Parkinson patients and in healthy participants under pharmacological challenge. Nevertheless, one clear limitation of that model is in its omission of the subthalamic nucleus (STN), a key BG structure that participates in both motor and cognitive processes. The present model incorporates the STN and shows that by modulating when a response is executed, the STN reduces premature responding and therefore has substantial effects on which response is ultimately selected, particularly when there are multiple competing responses. Increased cortical response conflict leads to dynamic adjustments in response thresholds via cortico-subthalamicpallidal pathways. The model accurately captures the dynamics of activity in various BG areas during response selection. Simulated dopamine depletion results in emergent oscillatory activity in BG structures, which has been linked with Parkinson's tremor. Finally, the model accounts for the beneficial effects of STN lesions on these oscillations, but suggests that this benefit may come at the expense of impaired decision making.},
  file = {/Users/qualia/Documents/Papers/2006 - Frank - Hold your horses A dynamic computational role for the subthalamic nucleus in decision making.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {8}
}

@article{Frank2006a,
  title = {Mechanisms {{Underlying}} the {{Rapid Induction}} and {{Sustained Expression}} of {{Synaptic Homeostasis}}},
  author = {Frank, C. Andrew and Kennedy, Matthew J. and Goold, Carleton P. and Marek, Kurt W. and Davis, Graeme W.},
  year = {2006},
  month = nov,
  volume = {52},
  pages = {663--677},
  issn = {08966273},
  doi = {10.1016/j.neuron.2006.09.029},
  abstract = {Homeostatic signaling systems are thought to interface with the mechanisms of neural plasticity to achieve stable yet flexible neural circuitry. However, the time course, molecular design, and implementation of homeostatic signaling remain poorly defined. Here we demonstrate that a homeostatic increase in presynaptic neurotransmitter release can be induced within minutes following postsynaptic glutamate receptor blockade. The rapid induction of synaptic homeostasis is independent of new protein synthesis and does not require evoked neurotransmission, indicating that a change in the efficacy of spontaneous quantal release events is sufficient to trigger the induction of synaptic homeostasis. Finally, both the rapid induction and the sustained expression of synaptic homeostasis are blocked by mutations that disrupt the pore-forming subunit of the presynaptic CaV2.1 calcium channel encoded by cacophony. These data confirm the presynaptic expression of synaptic homeostasis and implicate presynaptic CaV2.1 in a homeostatic retrograde signaling system.},
  file = {/Users/qualia/Documents/Papers/2006 - Frank et al. - Mechanisms Underlying the Rapid Induction and Sustained Expression of Synaptic Homeostasis.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Frank2007,
  title = {Hold {{Your Horses}}: {{Impulsivity}}, {{Deep Brain Stimulation}}, and {{Medication}} in {{Parkinsonism}}},
  shorttitle = {Hold {{Your Horses}}},
  author = {Frank, M. J. and Samanta, J. and Moustafa, A. A. and Sherman, S. J.},
  year = {2007},
  month = nov,
  volume = {318},
  pages = {1309--1312},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1146157},
  file = {/Users/qualia/Documents/Papers/2007 - Frank et al. - Hold Your Horses Impulsivity, Deep Brain Stimulation, and Medication in Parkinsonism.pdf},
  journal = {Science},
  language = {en},
  number = {5854}
}

@article{Frank2012,
  title = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Corticostriatal Circuits}} 1: {{Computational Analysis}}},
  shorttitle = {Mechanisms of {{Hierarchical Reinforcement Learning}} in {{Corticostriatal Circuits}} 1},
  author = {Frank, Michael J. and Badre, David},
  year = {2012},
  month = mar,
  volume = {22},
  pages = {509--526},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhr114},
  abstract = {Growing evidence suggests that the prefrontal cortex (PFC) is organized hierarchically, with more anterior regions having increasingly abstract representations. How does this organization support hierarchical cognitive control and the rapid discovery of abstract action rules? We present computational models at different levels of description. A neural circuit model simulates interacting corticostriatal circuits organized hierarchically. In each circuit the basal ganglia (BG) gate frontal actions, with some striatal units gating the inputs to PFC, and others gating the outputs to influence response selection. Learning at all of these levels is accomplished via dopaminergic reward prediction error signals in each corticostriatal circuit. This functionality allows the system to exhibit conditional if-then hypothesis testing and to learn rapidly in environments with hierarchical structure. We also develop a hybrid Bayesian-RL mixture of experts (MoE) model, which can estimate the most likely hypothesis state of individual participants based on their observed sequence of choices and rewards. This model yields accurate probabilistic estimates about which hypotheses are attended by manipulating attentional states in the generative neural model and recovering them with the MoE model. This two-pronged modeling approach leads to multiple quantitative predictions that are tested with fMRI in the companion paper.},
  file = {/Users/qualia/Documents/Papers/2011 - Frank, Badre - Mechanisms of hierarchical reinforcement learning in corticostriatal circuits 1 computational analysis.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Frankle2018,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2018},
  month = mar,
  abstract = {Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the lottery ticket hypothesis, proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these ``lottery tickets,'' meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization.},
  archivePrefix = {arXiv},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra 2.pdf;/Users/qualia/Documents/Papers/Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf},
  journal = {arXiv:1803.03635 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Fransen2015,
  title = {Identifying Neuronal Oscillations Using Rhythmicity},
  author = {Fransen, Anne M.M. and {van Ede}, Freek and Maris, Eric},
  year = {2015},
  month = sep,
  volume = {118},
  pages = {256--267},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2015.06.003},
  abstract = {Neuronal oscillations are a characteristic feature of neuronal activity and are typically investigated through measures of power and coherence. However, neither of these measures directly reflects the distinctive feature of oscillations: their rhythmicity. Rhythmicity is the extent to which future phases can be predicted from the present one. Here, we present lagged coherence, a frequency-indexed measure that quantifies the rhythmicity of neuronal activity. We use this method to identify the sensorimotor alpha and beta rhythms in ongoing magnetoencephalographic (MEG) data, and to study their attentional modulation. Using lagged coherence, the sensorimotor rhythms become visible in ongoing activity as local rhythmicity peaks that are separated from the strong posterior activity in the same frequency bands. In contrast, using conventional power analyses, the sensorimotor rhythms cannot be identified in ongoing data, nor can they be separated from the posterior activity. We go on to show that the attentional modulation of these rhythms is also evident in lagged coherence and moreover, that in contrast to power, it can be visualised even without an experimental contrast. These findings suggest that the rhythmicity of neuronal activity is better suited to identify neuronal oscillations than the power in the same frequency band.},
  file = {/Users/qualia/Documents/Papers/2015 - Fransen, van Ede, Maris - Identifying neuronal oscillations using rhythmicity.pdf;/Users/qualia/Documents/Papers/Fransen et al. - 2015 - Identifying neuronal oscillations using rhythmicit.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Freeman1972,
  title = {Linear {{Analysis}} of the {{Dynamics}} of {{Neural Masses}}},
  author = {Freeman, W J},
  year = {1972},
  month = jun,
  volume = {1},
  pages = {225--256},
  issn = {0084-6589, 0084-6589},
  doi = {10.1146/annurev.bb.01.060172.001301},
  file = {/Users/qualia/Documents/Papers/1972 - Freeman - Linear analysis of the dynamics of neural masses.pdf;/Users/qualia/Documents/Papers/Freeman - 1972 - Linear Analysis of the Dynamics of Neural Masses.pdf},
  journal = {Annual Review of Biophysics and Bioengineering},
  language = {en},
  number = {1}
}

@article{Freeman2015,
  title = {Mapping Nonlinear Receptive Field Structure in Primate Retina at Single Cone Resolution},
  author = {Freeman, Jeremy and Field, Greg D and Li, Peter H and Greschner, Martin and Gunning, Deborah E and Mathieson, Keith and Sher, Alexander and Litke, Alan M and Paninski, Liam and Simoncelli, Eero P and Chichilnisky, Ej},
  year = {2015},
  month = oct,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.05241},
  file = {/Users/qualia/Documents/Papers/2015 - Freeman et al. - Mapping nonlinear receptive field structure in primate retina at single cone resolution.pdf;/Users/qualia/Documents/Papers/Freeman et al. - 2015 - Mapping nonlinear receptive field structure in pri.pdf},
  journal = {eLife},
  language = {en}
}

@article{Fregnac2017,
  title = {Big Data and the Industrialization of Neuroscience: {{A}} Safe Roadmap for Understanding the Brain?},
  shorttitle = {Big Data and the Industrialization of Neuroscience},
  author = {Fr{\'e}gnac, Yves},
  year = {2017},
  month = oct,
  volume = {358},
  pages = {470--477},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aan8866},
  file = {/Users/qualia/Documents/Papers/Frégnac - 2017 - Big data and the industrialization of neuroscience.pdf},
  journal = {Science},
  language = {en},
  number = {6362}
}

@article{Freund1999,
  title = {Adaptive {{Game Playing Using Multiplicative Weights}}},
  author = {Freund, Yoav and Schapire, Robert E.},
  year = {1999},
  month = oct,
  volume = {29},
  pages = {79--103},
  issn = {08998256},
  doi = {10.1006/game.1999.0738},
  abstract = {We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are non-asymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback-Liebler divergence. This analysis yields a new, simple proof of the minmax theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense.},
  file = {/Users/qualia/Documents/Papers/1999 - Freund, Schapire - Adaptive game playing using multiplicative weights.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1-2}
}

@article{Fries2001,
  title = {Modulation of {{Oscillatory Neuronal Synchronization}} by {{Selective Visual Attention}}},
  author = {Fries, P.},
  year = {2001},
  month = feb,
  volume = {291},
  pages = {1560--1563},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1055465},
  file = {/Users/qualia/Documents/Papers/2001 - Fries - Modulation of Oscillatory Neuronal Synchronization by Selective Visual Attention.pdf},
  journal = {Science},
  language = {en},
  number = {5508}
}

@article{Fries2015,
  title = {Rhythms for {{Cognition}}: {{Communication}} through {{Coherence}}},
  shorttitle = {Rhythms for {{Cognition}}},
  author = {Fries, Pascal},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {220--235},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.034},
  file = {/Users/qualia/Documents/Papers/2015 - Fries - Rhythms for Cognition Communication through Coherence.pdf;/Users/qualia/Documents/Papers/Fries - 2015 - Rhythms for Cognition Communication through Coher.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Fries2015a,
  title = {Rhythms for {{Cognition}}: {{Communication}} through {{Coherence}}},
  shorttitle = {Rhythms for {{Cognition}}},
  author = {Fries, Pascal},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {220--235},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.034},
  abstract = {I propose that synchronization affects communication between neuronal groups. Gamma-band (30-90 Hz) synchronization modulates excitation rapidly enough so it escapes the following inhibition and activates postsynaptic neurons effectively. Synchronization also ensures that a presynaptic activation pattern arrives at postsynaptic neurons in a temporally coordinated manner. At a postsynaptic neuron, multiple presynaptic groups converge, e.g. representing different stimuli. If a stimulus is selected by attention, its neuronal representation shows stronger and higher-frequency gamma-band synchronization. Thereby, the attended stimulus representation selectively entrains postsynaptic neurons. The entrainment creates sequences of short excitation and longer inhibition that are coordinated between pre- and postsynaptic groups to transmit the attended representation and shut out competing inputs. The predominantly bottom-up directed gamma-band influences are controlled by predominantly top-down directed alpha-beta band (8-20 Hz) influences. Attention itself samples stimuli at a 7-8 Hz theta rhythm. Thus, several rhythms and their interplay render neuronal communication effective, precise and selective.},
  file = {/Users/qualia/Documents/Papers/Fries - 2015 - Rhythms for Cognition Communication through Coher 2.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Fries2015b,
  title = {Rhythms for {{Cognition}}: {{Communication}} through {{Coherence}}},
  shorttitle = {Rhythms for {{Cognition}}},
  author = {Fries, Pascal},
  year = {2015},
  month = oct,
  volume = {88},
  pages = {220--235},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.034},
  abstract = {I propose that synchronization affects communication between neuronal groups. Gamma-band (30-90 Hz) synchronization modulates excitation rapidly enough so it escapes the following inhibition and activates postsynaptic neurons effectively. Synchronization also ensures that a presynaptic activation pattern arrives at postsynaptic neurons in a temporally coordinated manner. At a postsynaptic neuron, multiple presynaptic groups converge, e.g. representing different stimuli. If a stimulus is selected by attention, its neuronal representation shows stronger and higher-frequency gamma-band synchronization. Thereby, the attended stimulus representation selectively entrains postsynaptic neurons. The entrainment creates sequences of short excitation and longer inhibition that are coordinated between pre- and postsynaptic groups to transmit the attended representation and shut out competing inputs. The predominantly bottom-up directed gamma-band influences are controlled by predominantly top-down directed alpha-beta band (8-20 Hz) influences. Attention itself samples stimuli at a 7-8 Hz theta rhythm. Thus, several rhythms and their interplay render neuronal communication effective, precise and selective.},
  file = {/Users/qualia/Documents/Papers/Fries - 2015 - Rhythms for Cognition Communication through Coher 3.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Friston2014,
  title = {The Anatomy of Choice: Dopamine and Decision-Making},
  shorttitle = {The Anatomy of Choice},
  author = {Friston, K. and Schwartenbeck, P. and FitzGerald, T. and Moutoussis, M. and Behrens, T. and Dolan, R. J.},
  year = {2014},
  month = sep,
  volume = {369},
  pages = {20130481--20130481},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2013.0481},
  file = {/Users/qualia/Documents/Papers/Friston et al. - 2014 - The anatomy of choice dopamine and decision-makin.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1655}
}

@article{Friston2016,
  title = {Active Inference and Learning},
  author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and O⿿Doherty, John and Pezzulo, Giovanni},
  year = {2016},
  month = sep,
  volume = {68},
  pages = {862--879},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2016.06.022},
  abstract = {This paper offers an active inference account of choice behaviour and learning. It focuses on the distinction between goal-directed and habitual behaviour and how they contextualise each other. We show that habits emerge naturally (and autodidactically) from sequential policy optimisation when agents are equipped with state-action policies. In active inference, behaviour has explorative (epistemic) and exploitative (pragmatic) aspects that are sensitive to ambiguity and risk respectively, where epistemic (ambiguity-resolving) behaviour enables pragmatic (reward-seeking) behaviour and the subsequent emergence of habits. Although goal-directed and habitual policies are usually associated with model-based and model-free schemes, we find the more important distinction is between belief-free and belief-based schemes. The underlying (variational) belief updating provides a comprehensive (if metaphorical) process theory for several phenomena, including the transfer of dopamine responses, reversal learning, habit formation and devaluation. Finally, we show that active inference reduces to a classical (Bellman) scheme, in the absence of ambiguity.},
  file = {/Users/qualia/Documents/Papers/Friston et al. - 2016 - Active inference and learning.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en}
}

@article{Fritzke,
  title = {A {{Growing Neural Gas Network Learns Topologies}}},
  author = {Fritzke, Bernd},
  pages = {8},
  abstract = {An incremental network model is introduced which is able to learn the important topological relations in a given set of input vectors by means of a simple Hebb-like learning rule. In contrast to previous approaches like the "neural gas" method of Martinetz and Schulten (1991, 1994), this model has no parameters which change over time and is able to continue learning, adding units and connections, until a performance criterion has been met. Applications of the model include vector quantization, clustering, and interpolation.},
  file = {/Users/qualia/Documents/Papers/Fritzke - A Growing Neural Gas Network Learns Topologies.pdf},
  language = {en}
}

@article{Fukunaga2014,
  title = {Independent Control of Gamma and Theta Activity by Distinct Interneuron Networks in the Olfactory Bulb},
  author = {Fukunaga, Izumi and Herb, Jan T and Kollo, Mihaly and Boyden, Edward S and Schaefer, Andreas T},
  year = {2014},
  month = sep,
  volume = {17},
  pages = {1208--1216},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3760},
  file = {/Users/qualia/Documents/Papers/Fukunaga et al. - 2014 - Independent control of gamma and theta activity by.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Furlanello,
  title = {Born-{{Again Neural Networks}}},
  author = {Furlanello, Tommaso and Lipton, Zachary C and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  pages = {10},
  abstract = {Knowledge Distillation (KD) consists of transferring ``knowledge'' from one machine learning model (the teacher) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness, without sacrificing too much performance. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5\%) and CIFAR-100 (15.5\%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating the effect of the teacher outputs on both predicted and nonpredicted classes.},
  file = {/Users/qualia/Documents/Papers/Furlanello et al. - Born-Again Neural Networks.pdf},
  language = {en}
}

@article{Fusi2016,
  title = {Why Neurons Mix: High Dimensionality for Higher Cognition},
  shorttitle = {Why Neurons Mix},
  author = {Fusi, Stefano and Miller, Earl K and Rigotti, Mattia},
  year = {2016},
  month = apr,
  volume = {37},
  pages = {66--74},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.01.010},
  file = {/Users/qualia/Documents/Papers/Fusi et al. - 2016 - Why neurons mix high dimensionality for higher co.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Fyodorov2016,
  title = {Nonlinear Analogue of the {{May}}-{{Wigner}} Instability Transition},
  author = {Fyodorov, Yan V. and Khoruzhenko, Boris A.},
  year = {2016},
  month = jun,
  volume = {113},
  pages = {6827--6832},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1601136113},
  file = {/Users/qualia/Documents/Papers/Fyodorov and Khoruzhenko - 2016 - Nonlinear analogue of the May−Wigner instability t.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {25}
}

@article{Gabaix,
  title = {Bounded {{Rationality}} and {{Directed Cognition}}},
  author = {Gabaix, Xavier and Laibson, David},
  pages = {46},
  abstract = {This paper proposes a psychological bounded rationality algorithm that uses partially myopic option value calculations to allocate scarce cognitive resources. The model can be operationalized even when decision problems require an arbitrarily large number of state variables. We evaluate the model using experimental data on a class of complex one-person games with full information. The model explains the experimental data better than the rational actor model with zero cognition costs.},
  file = {/Users/qualia/Documents/Papers/2005 - Gabaix, Laibson - Bounded Rationality and Directed Cognition.pdf},
  language = {en}
}

@article{Gabaix2000,
  title = {A {{Boundedly Rational Decision Algorithm}}},
  author = {Gabaix, Xavier and Laibson, David},
  year = {2000},
  volume = {90},
  pages = {433--438},
  file = {/Users/qualia/Documents/Papers/2000 - Gabaix, Laibson - A boundedly rational decision algorithm.pdf},
  journal = {The American Economic Review},
  language = {en},
  number = {2,}
}

@article{Gabaix2006,
  title = {Costly {{Information Acquisition}}: {{Experimental Analysis}} of a {{Boundedly Rational Model}}},
  author = {Gabaix, Xavier and Laibson, David and Moloche, Guillermo and Weinberg, Stephen},
  year = {2006},
  volume = {96},
  pages = {26},
  file = {/Users/qualia/Documents/Papers/2006 - Gabaix et al. - Costly Information Acquisition Experimental Analysis of a Boundedly Rational Model Costly Information Acquisition.pdf},
  journal = {THE AMERICAN ECONOMIC REVIEW},
  language = {en},
  number = {4}
}

@article{Gabalda-Sagarra2017,
  title = {Recurrence-{{Based Information Processing}} in {{Gene Regulatory Networks}}},
  author = {{Gabalda-Sagarra}, Marcal and Carey, Lucas and {Garcia-Ojalvo}, Jordi},
  year = {2017},
  month = sep,
  doi = {10.1101/010124},
  abstract = {Cellular information processing is generally attributed to the complex networks of genes and proteins that regulate cell behavior. It is still unclear, however, what are the main features of those networks that allow a cell to encode and interpret its ever changing environment. Here we address this question by studying the computational capabilities of the transcriptional regulatory networks of five evolutionary distant organisms. We identify in all cases a cyclic recurrent structure, formed by a small core of genes, that is essential for dynamical encoding and information integration. The recent history of the cell is encoded by the transient dynamics of this recurrent reservoir of nodes, while the rest of the network forms a readout layer devoted to decode and interpret the highdimensional dynamical state of the recurrent core. This separation of roles allows for the integration of temporal information, while facilitating the learning of new environmental conditions and preventing catastrophic interference between those new inputs and the previously stored information. This resembles the reservoir-computing paradigm recently proposed in computational neuroscience and machine learning. Our results reveal that gene regulatory networks act as echo-state networks that perform optimally in standard memory-demanding tasks, and confirms that most of their memory resides in the recurrent reservoir. We also show that the readout layer can learn to decode the information stored in the reservoir via standard evolutionary strategies. Our work thus suggests that recurrent dynamics is a key element for the processing of complex time-dependent information by cells.},
  file = {/Users/qualia/Documents/Papers/Gabalda-Sagarra et al. - 2017 - Recurrence-Based Information Processing in Gene Re.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Ganguli2008,
  title = {Memory Traces in Dynamical Systems},
  author = {Ganguli, S. and Huh, D. and Sompolinsky, H.},
  year = {2008},
  month = dec,
  volume = {105},
  pages = {18970--18975},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0804451105},
  file = {/Users/qualia/Documents/Papers/2008 - Ganguli, Huh, Sompolinsky - Memory traces in dynamical systems.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}

@article{Ganguli2010,
  title = {Short-Term Memory in Neuronal Networks through Dynamical Compressed Sensing},
  author = {Ganguli, Surya and Sompolinsky, Haim},
  year = {2010},
  pages = {9},
  abstract = {Recent proposals suggest that large, generic neuronal networks could store memory traces of past input sequences in their instantaneous state. Such a proposal raises important theoretical questions about the duration of these memory traces and their dependence on network size, connectivity and signal statistics. Prior work, in the case of gaussian input sequences and linear neuronal networks, shows that the duration of memory traces in a network cannot exceed the number of neurons (in units of the neuronal time constant), and that no network can out-perform an equivalent feedforward network. However a more ethologically relevant scenario is that of sparse input sequences. In this scenario, we show how linear neural networks can essentially perform compressed sensing (CS) of past inputs, thereby attaining a memory capacity that exceeds the number of neurons. This enhanced capacity is achieved by a class of ``orthogonal'' recurrent networks and not by feedforward networks or generic recurrent networks. We exploit techniques from the statistical physics of disordered systems to analytically compute the decay of memory traces in such networks as a function of network size, signal sparsity and integration time. Alternately, viewed purely from the perspective of CS, this work introduces a new ensemble of measurement matrices derived from dynamical systems, and provides a theoretical analysis of their asymptotic performance.},
  file = {/Users/qualia/Documents/Papers/2010 - Ganguli, Sompolinsky - Short-term memory in neuronal networks through dynamical compressed sensing.pdf},
  language = {en}
}

@article{Gao2003,
  title = {Dopamine {{Modulation}} of {{Perisomatic}} and {{Peridendritic Inhibition}} in {{Prefrontal Cortex}}},
  author = {Gao, Wen-Jun and Wang, Yun and {Goldman-Rakic}, Patricia S.},
  year = {2003},
  month = mar,
  volume = {23},
  pages = {1622--1630},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-05-01622.2003},
  file = {/Users/qualia/Documents/Papers/2003 - Gao, Wang, Goldman-Rakic - Dopamine modulation of perisomatic and peridendritic inhibition in prefrontal cortex.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {5}
}

@article{Gao2016,
  title = {Inferring {{Synaptic Excitation}}/{{Inhibition Balance}} from {{Field Potentials}}},
  author = {Gao, Richard D and Peterson, Erik J and Voytek, Bradley},
  year = {2016},
  month = oct,
  doi = {10.1101/081125},
  abstract = {Neural circuits sit in a dynamic balance between excitation (E) and inhibition (I). Fluctuations in this E:I balance have been shown to influence neural computation, working memory, and information processing. While more drastic shifts and aberrant E:I patterns are implicated in numerous neurological and psychiatric disorders, current methods for measuring E:I dynamics require invasive procedures that are difficult to perform in behaving animals, and nearly impossible in humans. This has limited the ability to examine the full impact that E:I shifts have in neural computation and disease. In this study, we develop a computational model to show that E:I ratio can be estimated from the power law exponent (slope) of the electrophysiological power spectrum, and validate this relationship using previously published datasets from two species (rat local field potential and macaque electrocorticography). This simple method--one that can be applied retrospectively to existing data--removes a major hurdle in understanding a currently difficult to measure, yet fundamental, aspect of neural computation.},
  file = {/Users/qualia/Documents/Papers/Gao et al. - 2016 - Inferring Synaptic ExcitationInhibition Balance f.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Gao2016a,
  title = {Inferring {{Synaptic Excitation}}/{{Inhibition Balance}} from {{Field Potentials}}},
  author = {Gao, Richard D. and Peterson, Erik J. and Voytek, Bradley},
  year = {2016},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/081125},
  abstract = {Neural circuits sit in a dynamic balance between excitation (E) and inhibition (I). Fluctuations in this E:I balance have been shown to influence neural computation, working memory, and information processing. While more drastic shifts and aberrant E:I patterns are implicated in numerous neurological and psychiatric disorders, current methods for measuring E:I dynamics require invasive procedures that are difficult to perform in behaving animals, and nearly impossible in humans. This has limited the ability to examine the full impact that E:I shifts have in neural computation and disease. In this study, we develop a computational model to show that E:I ratio can be estimated from the power law exponent (slope) of the electrophysiological power spectrum, and validate this relationship using previously published datasets from two species (rat local field potential and macaque electrocorticography). This simple method--one that can be applied retrospectively to existing data--removes a major hurdle in understanding a currently difficult to measure, yet fundamental, aspect of neural computation.},
  file = {/Users/qualia/Documents/Papers/Gao et al. - 2016 - Inferring Synaptic ExcitationInhibition Balance f 2.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Gao2017,
  title = {A Theory of Multineuronal Dimensionality, Dynamics and Measurement},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron M. and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  year = {2017},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/214262},
  abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Dimensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  file = {/Users/qualia/Documents/Papers/Gao et al. - 2017 - A theory of multineuronal dimensionality, dynamics.pdf},
  language = {en},
  type = {Preprint}
}

@article{Garcia-Ojalvo2004,
  title = {Modeling a Synthetic Multicellular Clock: {{Repressilators}} Coupled by Quorum Sensing},
  shorttitle = {Modeling a Synthetic Multicellular Clock},
  author = {{Garcia-Ojalvo}, J. and Elowitz, M. B. and Strogatz, S. H.},
  year = {2004},
  month = jul,
  volume = {101},
  pages = {10955--10960},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0307095101},
  file = {/Users/qualia/Documents/Papers/2004 - Garcia-Ojalvo, Elowitz, Strogatz - Modeling a synthetic multicellular clock Repressilators coupled by quorum sensing.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {30}
}

@article{Geirhos,
  title = {Comparing Deep Neural Networks against Humans: Object Recognition When the Signal Gets Weaker},
  author = {Geirhos, Robert and Janssen, David H J and Schutt, Heiko H and Rauber, Jonas and Bethge, Matthias and Wichmann, Felix A},
  pages = {31},
  abstract = {Human visual object recognition is typically rapid and seemingly effortless, as well as largely independent of viewpoint and object orientation. Until very recently, animate visual systems were the only ones capable of this remarkable computational feat. This has changed with the rise of a class of computer vision algorithms called deep neural networks (DNNs) that achieve human-level classification performance on object recognition tasks. Furthermore, a growing number of studies report similarities in the way DNNs and the human visual system process objects, suggesting that current DNNs may be good models of human visual object recognition. Yet there clearly exist important architectural and processing differences between stateof-the-art DNNs and the primate visual system. The potential behavioural consequences of these differences are not well understood. We aim to address this issue by comparing human and DNN generalisation abilities towards image degradations. We find the human visual system to be more robust to image manipulations like contrast reduction, additive noise or novel eidolon-distortions. In addition, we find progressively diverging classification error-patterns between man and DNNs when the signal gets weaker, indicating that there may still be marked differences in the way humans and current DNNs perform visual object recognition. We envision that our findings as well as our carefully measured and freely available behavioural datasets1 provide a new useful benchmark for the computer vision community to improve the robustness of DNNs and a motivation for neuroscientists to search for mechanisms in the brain that could facilitate this robustness.},
  file = {/Users/qualia/Documents/Papers/Geirhos et al. - Comparing deep neural networks against humans obj.pdf},
  language = {en}
}

@article{Germain2015,
  title = {{{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}},
  shorttitle = {{{MADE}}},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2015},
  month = feb,
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with stateof-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  archivePrefix = {arXiv},
  eprint = {1502.03509},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Germain et al. - MADE Masked Autoencoder for Distribution Estimation.pdf;/Users/qualia/Documents/Papers/Germain et al. - 2015 - MADE Masked Autoencoder for Distribution Estimati.pdf},
  journal = {arXiv:1502.03509 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Gershman2018,
  title = {Deconstructing the Human Algorithms for Exploration},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = apr,
  volume = {173},
  pages = {34--42},
  issn = {00100277},
  doi = {10.1016/j.cognition.2017.12.014},
  abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
  file = {/Users/qualia/Documents/Papers/Gershman - 2018 - Deconstructing the human algorithms for exploratio.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Gershman2018a,
  title = {Deconstructing the Human Algorithms for Exploration},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = apr,
  volume = {173},
  pages = {34--42},
  issn = {00100277},
  doi = {10.1016/j.cognition.2017.12.014},
  abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
  file = {/Users/qualia/Documents/Papers/Gershman - 2018 - Deconstructing the human algorithms for exploratio 2.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Gershman2018b,
  title = {Deconstructing the Human Algorithms for Exploration},
  author = {Gershman, Samuel J.},
  year = {2018},
  month = apr,
  volume = {173},
  pages = {34--42},
  issn = {00100277},
  doi = {10.1016/j.cognition.2017.12.014},
  abstract = {The dilemma between information gathering (exploration) and reward seeking (exploitation) is a fundamental problem for reinforcement learning agents. How humans resolve this dilemma is still an open question, because experiments have provided equivocal evidence about the underlying algorithms used by humans. We show that two families of algorithms can be distinguished in terms of how uncertainty affects exploration. Algorithms based on uncertainty bonuses predict a change in response bias as a function of uncertainty, whereas algorithms based on sampling predict a change in response slope. Two experiments provide evidence for both bias and slope changes, and computational modeling confirms that a hybrid model is the best quantitative account of the data.},
  file = {/Users/qualia/Documents/Papers/Gershman - 2018 - Deconstructing the human algorithms for exploratio 3.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Ghuman2010,
  title = {Face {{Adaptation}} without a {{Face}}},
  author = {Ghuman, Avniel Singh and McDaniel, Jonathan R. and Martin, Alex},
  year = {2010},
  month = jan,
  volume = {20},
  pages = {32--36},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.10.077},
  abstract = {Prolonged viewing of a stimulus results in a subsequent perceptual bias [1\textendash{}3]. This perceptual adaptation and the resulting aftereffect reveal important characteristics regarding how perceptual systems are tuned [2, 4\textendash{}6]. These aftereffects occur not only for simple stimulus features but also for high-level stimulus properties [7\textendash{}10]. Here we report a novel cross-category adaptation aftereffect demonstrating that prolonged viewing of a human body without a face shifts the perceptual tuning curve for face gender and face identity. This contradicts a central assumption underlying perceptual adaptation: that adaptation depends on physical similarity between how the adapting and the adapted features are perceived [5]. Additionally, this aftereffect was not due to response bias, because its dependence on adaptation duration resembled traditional perceptual aftereffects. These body-to-face adaptation results demonstrate that bodies alone can alter the tuning properties of neurons that code for the gender and identity of faces. More generally, these results reveal that high-level perceptual adaptation can occur when the property or features being adapted are automatically inferred rather than perceived in the adapting stimulus.},
  file = {/Users/qualia/Documents/Papers/2010 - Ghuman, McDaniel, Martin - Face adaptation without a face.pdf},
  journal = {Current Biology},
  language = {en},
  number = {1}
}

@article{Giannicola2010,
  title = {The Effects of Levodopa and Ongoing Deep Brain Stimulation on Subthalamic Beta Oscillations in {{Parkinson}}'s Disease},
  author = {Giannicola, Gaia and Marceglia, Sara and Rossi, Lorenzo and {Mrakic-Sposta}, Simona and Rampini, Paolo and Tamma, Filippo and Cogiamanian, Filippo and Barbieri, Sergio and Priori, Alberto},
  year = {2010},
  month = nov,
  volume = {226},
  pages = {120--127},
  issn = {00144886},
  doi = {10.1016/j.expneurol.2010.08.011},
  abstract = {Local field potentials (LFPs) recorded through electrodes implanted in the subthalamic nucleus (STN) for deep brain stimulation (DBS) in patients with Parkinson's disease (PD) show that oscillations in the beta frequency range (8\textendash{}20 Hz) decrease after levodopa intake. Whether and how DBS influences the beta oscillations and whether levodopa- and DBS-induced changes interact remains unclear. We examined the combined effect of levodopa and DBS on subthalamic beta LFP oscillations, recorded in nine patients with PD under four experimental conditions: without levodopa with DBS turned off; without levodopa with DBS turned on; with levodopa with DBS turned on; and with levodopa with DBS turned off. The analysis of STNLFP oscillations showed that whereas levodopa abolished beta STN oscillations in all the patients (p = 0.026), DBS significantly decreased the beta oscillation only in five of the nine patients studied (p = 0.043). Another difference was that whereas levodopa completely suppressed beta oscillations, DBS merely decreased them. When we combined levodopa and DBS, the levodopa-induced beta disruption prevailed and combining levodopa and DBS induced no significant additive effect (p = 0.500). Our observations suggest that levodopa and DBS both modulate LFP beta oscillations.},
  file = {/Users/qualia/Documents/Papers/2010 - Giannicola et al. - The effects of levodopa and ongoing deep brain stimulation on subthalamic beta oscillations in Parkinson's di.pdf},
  journal = {Experimental Neurology},
  language = {en},
  number = {1}
}

@article{Gidon2020,
  title = {Dendritic Action Potentials and Computation in Human Layer 2/3 Cortical Neurons},
  author = {Gidon, Albert and Zolnik, Timothy Adam and Fidzinski, Pawel and Bolduan, Felix and Papoutsi, Athanasia and Poirazi, Panayiota and Holtkamp, Martin and Vida, Imre and Larkum, Matthew Evan},
  year = {2020},
  month = jan,
  volume = {367},
  pages = {83--87},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax6239},
  abstract = {The active electrical properties of dendrites shape neuronal input and output and are fundamental to brain function. However, our knowledge of active dendrites has been almost entirely acquired from studies of rodents. In this work, we investigated the dendrites of layer 2 and 3 (L2/3) pyramidal neurons of the human cerebral cortex ex vivo. In these neurons, we discovered a class of calcium-mediated dendritic action potentials (dCaAPs) whose waveform and effects on neuronal output have not been previously described. In contrast to typical all-or-none action potentials, dCaAPs were graded; their amplitudes were maximal for threshold-level stimuli but dampened for stronger stimuli. These dCaAPs enabled the dendrites of individual human neocortical pyramidal neurons to classify linearly nonseparable inputs\textemdash{}a computation conventionally thought to require multilayered networks.},
  file = {/Users/qualia/Documents/Papers/Gidon et al. - 2020 - Dendritic action potentials and computation in hum.pdf},
  journal = {Science},
  language = {en},
  number = {6473}
}

@article{Gigerenzer2008,
  title = {Why {{Heuristics Work}}},
  author = {Gigerenzer, Gerd},
  year = {2008},
  month = jan,
  volume = {3},
  pages = {20--29},
  issn = {1745-6916, 1745-6924},
  doi = {10.1111/j.1745-6916.2008.00058.x},
  abstract = {The adaptive toolbox is a Darwinian-inspired theory that conceives of the mind as a modular system that is composed of heuristics, their building blocks, and evolved capacities. The study of the adaptive toolbox is descriptive and analyzes the selection and structure of heuristics in social and physical environments. The study of ecological rationality is prescriptive and identifies the structure of environments in which specific heuristics either succeed or fail. Results have been used for designing heuristics and environments to improve professional decision making in the real world.},
  file = {/Users/qualia/Documents/Papers/2014 - Gigerenzer - Work Why Heuristics.pdf;/Users/qualia/Documents/Papers/Gigerenzer - 2008 - Why Heuristics Work.pdf},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {1}
}

@article{Gilbert2011,
  title = {Decoding the {{Content}} of {{Delayed Intentions}}},
  author = {Gilbert, S. J.},
  year = {2011},
  month = feb,
  volume = {31},
  pages = {2888--2894},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5336-10.2011},
  file = {/Users/qualia/Documents/Papers/2011 - Gilbert - Decoding the content of delayed intentions.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {8}
}

@article{Gillary2016,
  title = {The {{Edge}} of {{Stability}}: {{Response Times}} and {{Delta Oscillations}} in {{Balanced Networks}}},
  shorttitle = {The {{Edge}} of {{Stability}}},
  author = {Gillary, Grant and Niebur, Ernst},
  editor = {Battaglia, Francesco P.},
  year = {2016},
  month = sep,
  volume = {12},
  pages = {e1005121},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005121},
  file = {/Users/qualia/Documents/Papers/Gillary and Niebur - 2016 - The Edge of Stability Response Times and Delta Os.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {9}
}

@article{Gillies1998,
  title = {A Massively Connected Subthalamic Nucleus Leads to the Generation of Widespread Pulses},
  author = {Gillies, A. J. and Willshaw, D. J.},
  year = {1998},
  month = nov,
  volume = {265},
  pages = {2101--2109},
  issn = {1471-2954},
  doi = {10.1098/rspb.1998.0546},
  file = {/Users/qualia/Documents/Papers/1998 - Gillies, Willshaw - A massively connected subthalamic nucleus leads to the generation of widespread pulses.pdf},
  journal = {Proceedings of the Royal Society of London. Series B: Biological Sciences},
  language = {en},
  number = {1410}
}

@article{Gillies2004,
  title = {Models of the Subthalamic Nucleus},
  author = {Gillies, A. and Willshaw, D.},
  year = {2004},
  month = nov,
  volume = {26},
  pages = {723--732},
  issn = {13504533},
  doi = {10.1016/j.medengphy.2004.06.003},
  abstract = {A coherent set of models is presented that provide novel and testable predictions about the functional role of the subthalamic nucleus (STN) in the basal ganglia. The STN is emerging as an important target for novel therapeutic strategies for the alleviation of Parkinsonian type symptoms [Lancet 345 (1995) 91; Science 249 (1990) 1436]. Computational and mathematical models based on the properties of the STN and its interactions are reviewed. These models focus on core anatomical and physiological data that span many levels. By assessing models of anatomy, dynamic network models, and a detailed model of a recent pharmacological experiment, we can expose the primary modes of STN function and highlight their underlying properties. We show that the presence of functional interactions between STN projection neurons is critical in defining its behaviour and how it interacts with other basal ganglia nuclei. Pulses or switch-like activity patterns emerge in the models as a consequence of these local interactions. Furthermore, the models demonstrate that this behaviour can break down under abnormal conditions resulting in low frequency bursting oscillations. Such oscillations may play a role in symptoms of Parkinson's disease.},
  file = {/Users/qualia/Documents/Papers/2004 - Gillies, Willshaw - Models of the subthalamic nucleus The importance of intranuclear connectivity.pdf},
  journal = {Medical Engineering \& Physics},
  language = {en},
  number = {9}
}

@article{Gips2017,
  title = {Discovering Recurring Patterns in Electrophysiological Recordings},
  author = {Gips, Bart and Bahramisharif, Ali and Lowet, Eric and Roberts, Mark J. and {de Weerd}, Peter and Jensen, Ole and {van der Eerden}, Jan},
  year = {2017},
  month = jan,
  volume = {275},
  pages = {66--79},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2016.11.001},
  abstract = {Background: Fourier-based techniques are used abundantly in the analysis of electrophysiological data. However, these techniques are of limited value when the signal of interest is non-sinusoidal or nonperiodic. New method: We present sliding window matching (SWM): a new data-driven method for discovering recurring temporal patterns in electrophysiological data. SWM is effective in detecting recurring but unknown patterns even when they appear non-periodically.
Results: To demonstrate this, we used SWM on oscillations in local field potential (LFP) recordings from the rat hippocampus and monkey V1. The application of SWM yielded two interesting findings. We could show that rat hippocampal theta and monkey V1 gamma oscillations were both skewed (i.e. asymmetric in time), rather than being sinusoidal. Furthermore, gamma oscillations in monkey V1 were skewed differently in the superficial compared to the deeper cortical layers. Second, we used SWM to analyze responses evoked by stimuli or microsaccades even when the onset timing of stimulus or microsaccades was unknown.
Comparison with existing methods: We first validated the method on simulated datasets, and we checked that for recordings with a sufficiently low noise level the SWM results were consistent with results from the widely used phase alignment (PA) method.
Conclusions: We conclude that the proposed method has wide applicability in the exploration of noisy time series data where the onset times of particular events are unknown by the experimenter such as in resting state and sleep recordings.},
  file = {/Users/qualia/Documents/Papers/Gips et al. - 2017 - Discovering recurring patterns in electrophysiolog.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en}
}

@article{Girard2008,
  title = {Where Neuroscience and Dynamic System Theory Meet Autonomous Robotics: {{A}} Contracting Basal Ganglia Model for Action Selection},
  shorttitle = {Where Neuroscience and Dynamic System Theory Meet Autonomous Robotics},
  author = {Girard, B. and Tabareau, N. and Pham, Q.C. and Berthoz, A. and Slotine, J.-J.},
  year = {2008},
  month = may,
  volume = {21},
  pages = {628--641},
  issn = {08936080},
  doi = {10.1016/j.neunet.2008.03.009},
  abstract = {Action selection, the problem of choosing what to do next, is central to any autonomous agent architecture. We use here a multi-disciplinary approach at the convergence of neuroscience, dynamical system theory and autonomous robotics, in order to propose an efficient action selection mechanism based on a new model of the basal ganglia. We first describe new developments of contraction theory regarding locally projected dynamical systems. We exploit these results to design a stable computational model of the cortico-baso-thalamo-cortical loops. Based on recent anatomical data, we include usually neglected neural projections, which participate in performing accurate selection. Finally, the efficiency of this model as an autonomous robot action selection mechanism is assessed in a standard survival task. The model exhibits valuable dithering avoidance and energy-saving properties, when compared with a simple if-then-else decision rule.},
  file = {/Users/qualia/Documents/Papers/2008 - Girard et al. - Where neuroscience and dynamic system theory meet autonomous robotics A contracting basal ganglia model for actio.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4}
}

@article{Giraud2012,
  title = {Cortical Oscillations and Speech Processing: Emerging Computational Principles and Operations},
  shorttitle = {Cortical Oscillations and Speech Processing},
  author = {Giraud, Anne-Lise and Poeppel, David},
  year = {2012},
  month = apr,
  volume = {15},
  pages = {511--517},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3063},
  file = {/Users/qualia/Documents/Papers/2012 - Giraud, Poeppel - Cortical oscillations and speech processing emerging computational principles and operations.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Giusti2015,
  title = {Clique Topology Reveals Intrinsic Geometric Structure in Neural Correlations},
  author = {Giusti, Chad and Pastalkova, Eva and Curto, Carina and Itskov, Vladimir},
  year = {2015},
  month = nov,
  volume = {112},
  pages = {13455--13460},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1506407112},
  file = {/Users/qualia/Documents/Papers/Giusti et al. - 2015 - Clique topology reveals intrinsic geometric struct.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {44}
}

@article{Gjorgjieva2011,
  title = {A Triplet Spike-Timing-Dependent Plasticity Model Generalizes the {{Bienenstock}}-{{Cooper}}-{{Munro}} Rule to Higher-Order Spatiotemporal Correlations},
  author = {Gjorgjieva, J. and Clopath, C. and Audet, J. and Pfister, J.-P.},
  year = {2011},
  month = nov,
  volume = {108},
  pages = {19383--19388},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1105933108},
  file = {/Users/qualia/Documents/Papers/2011 - Gjorgjieva et al. - A triplet spike-timing-dependent plasticity model generalizes the Bienenstock-Cooper-Munro rule to higher-ord.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}

@article{Glasser2016,
  title = {The {{Human Connectome Project}}'s Neuroimaging Approach},
  author = {Glasser, Matthew F and Smith, Stephen M and Marcus, Daniel S and Andersson, Jesper L R and Auerbach, Edward J and Behrens, Timothy E J and Coalson, Timothy S and Harms, Michael P and Jenkinson, Mark and Moeller, Steen and Robinson, Emma C and Sotiropoulos, Stamatios N and Xu, Junqian and Yacoub, Essa and Ugurbil, Kamil and Van Essen, David C},
  year = {2016},
  month = sep,
  volume = {19},
  pages = {1175--1187},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4361},
  file = {/Users/qualia/Documents/Papers/Glasser et al. - 2016 - The Human Connectome Project's neuroimaging approa.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Gliske2015,
  title = {Narrowband Oscillations from Asynchronous Neural Activity},
  author = {Gliske, Stephen V. and Lim, Eugene and Holman, Katherine A. and Stacey, William C. and Fink, Christian G.},
  year = {2015},
  month = dec,
  abstract = {We investigate the possibility that narrowband oscillations may emerge from completely asynchronous, independent neural firing. We find that a population of asynchronous neurons may produce narrowband oscillations if each neuron fires quasi-periodically, and we deduce bounds on the degree of variability in neural spike-timing which will permit the emergence of such oscillations. These results suggest a novel mechanism of neural rhythmogenesis, and they help to explain recent experimental reports of large-amplitude local field potential oscillations in the absence of neural spike-timing synchrony. Simply put, although synchrony can produce oscillations, oscillations do not always imply the existence of synchrony.},
  archivePrefix = {arXiv},
  eprint = {1512.01622},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Gliske et al. - Narrowband oscillations from asynchronous neural activity.pdf;/Users/qualia/Documents/Papers/Gliske et al. - 2015 - Narrowband oscillations from asynchronous neural a.pdf},
  journal = {arXiv:1512.01622 [q-bio]},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {q-bio}
}

@article{Globerson2009,
  title = {The Minimum Information Principle and Its Application to Neural Code Analysis},
  author = {Globerson, A. and Stark, E. and Vaadia, E. and Tishby, N.},
  year = {2009},
  month = mar,
  volume = {106},
  pages = {3490--3495},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0806782106},
  file = {/Users/qualia/Documents/Papers/2009 - Globerson et al. - The minimum information principle and its application to neural code analysis.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {9}
}

@article{Gluth2012,
  title = {Deciding {{When}} to {{Decide}}: {{Time}}-{{Variant Sequential Sampling Models Explain}} the {{Emergence}} of {{Value}}-{{Based Decisions}} in the {{Human Brain}}},
  shorttitle = {Deciding {{When}} to {{Decide}}},
  author = {Gluth, S. and Rieskamp, J. and Buchel, C.},
  year = {2012},
  month = aug,
  volume = {32},
  pages = {10686--10698},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0727-12.2012},
  file = {/Users/qualia/Documents/Papers/2012 - Gluth, Rieskamp, Büchel - Deciding when to decide time-variant sequential sampling models explain the emergence of value-based d.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {31}
}

@article{Goense2016,
  title = {{{fMRI}} at {{High Spatial Resolution}}: {{Implications}} for {{BOLD}}-{{Models}}},
  shorttitle = {{{fMRI}} at {{High Spatial Resolution}}},
  author = {Goense, Jozien and Bohraus, Yvette and Logothetis, Nikos K.},
  year = {2016},
  month = jun,
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00066},
  file = {/Users/qualia/Documents/Papers/Goense et al. - 2016 - fMRI at High Spatial Resolution Implications for .pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Goeree1999,
  title = {Stochastic Game Theory: {{For}} Playing Games, Not Just for Doing Theory},
  shorttitle = {Stochastic Game Theory},
  author = {Goeree, J. K. and Holt, C. A.},
  year = {1999},
  month = sep,
  volume = {96},
  pages = {10564--10567},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.96.19.10564},
  file = {/Users/qualia/Documents/Papers/1999 - Goeree, Holt - Stochastic game theory For playing games, not just for doing theory.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {19}
}

@article{Goeree2001,
  title = {Ten {{Little Treasures}} of {{Game Theory}} and {{Ten Intuitive Contradictions}}},
  author = {Goeree, Jacob K and Holt, Charles A},
  year = {2001},
  month = dec,
  volume = {91},
  pages = {1402--1422},
  issn = {0002-8282},
  doi = {10.1257/aer.91.5.1402},
  abstract = {This paper reports data for a series of two-person games that are played only once. These games span the standard categories: static and dynamic games with complete and incomplete information. For each game, the treasure is a treatment for which behavior conforms quite nicely to the predictions of the Nash equilibrium or relevant refinement. In each case we change a key payoff parameter in a manner that does not alter the equilibrium predictions, but this theoretically neutral payoff change has a major (often dramatic) effect on observed behavior. These contradictions are typically consistent with simple economic intuition and with some recent theoretical work that incorporates bounded rationality.},
  file = {/Users/qualia/Documents/Papers/2001 - Goeree, Holt - Ten little treasure of game theory and ten intuitive contradictions.pdf},
  journal = {American Economic Review},
  language = {en},
  number = {5}
}

@article{Gollo2014,
  title = {Mechanisms of {{Zero}}-{{Lag Synchronization}} in {{Cortical Motifs}}},
  author = {Gollo, Leonardo L. and Mirasso, Claudio and Sporns, Olaf and Breakspear, Michael},
  editor = {Gutkin, Boris S.},
  year = {2014},
  month = apr,
  volume = {10},
  pages = {e1003548},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003548},
  abstract = {Zero-lag synchronization between distant cortical areas has been observed in a diversity of experimental data sets and between many different regions of the brain. Several computational mechanisms have been proposed to account for such isochronous synchronization in the presence of long conduction delays: Of these, the phenomenon of ``dynamical relaying'' \textendash{}a mechanism that relies on a specific network motif \textendash{} has proven to be the most robust with respect to parameter mismatch and system noise. Surprisingly, despite a contrary belief in the community, the common driving motif is an unreliable means of establishing zero-lag synchrony. Although dynamical relaying has been validated in empirical and computational studies, the deeper dynamical mechanisms and comparison to dynamics on other motifs is lacking. By systematically comparing synchronization on a variety of small motifs, we establish that the presence of a single reciprocally connected pair \textendash{} a ``resonance pair'' \textendash{} plays a crucial role in disambiguating those motifs that foster zero-lag synchrony in the presence of conduction delays (such as dynamical relaying) from those that do not (such as the common driving triad). Remarkably, minor structural changes to the common driving motif that incorporate a reciprocal pair recover robust zerolag synchrony. The findings are observed in computational models of spiking neurons, populations of spiking neurons and neural mass models, and arise whether the oscillatory systems are periodic, chaotic, noise-free or driven by stochastic inputs. The influence of the resonance pair is also robust to parameter mismatch and asymmetrical time delays amongst the elements of the motif. We call this manner of facilitating zero-lag synchrony resonance-induced synchronization, outline the conditions for its occurrence, and propose that it may be a general mechanism to promote zero-lag synchrony in the brain.},
  file = {/Users/qualia/Documents/Papers/2014 - Gollo et al. - Mechanisms of Zero-Lag Synchronization in Cortical Motifs.pdf;/Users/qualia/Documents/Papers/Gollo et al. - 2014 - Mechanisms of Zero-Lag Synchronization in Cortical.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {4}
}

@article{Golowasch1999,
  title = {Network {{Stability}} from {{Activity}}-{{Dependent Regulation}} of {{Neuronal Conductances}}},
  author = {Golowasch, Jorge and Casey, Michael and Abbott, L. F. and Marder, Eve},
  year = {1999},
  month = jul,
  volume = {11},
  pages = {1079--1096},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300016359},
  file = {/Users/qualia/Documents/Papers/1999 - Golowasch et al. - Network Stability from Activity-Dependent Regulation of Neuronal Conductances.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Gomez-Marin2019,
  title = {The {{Life}} of {{Behavior}}},
  author = {{Gomez-Marin}, Alex and Ghazanfar, Asif A.},
  year = {2019},
  month = oct,
  volume = {104},
  pages = {25--36},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.09.017},
  file = {/Users/qualia/Documents/Papers/Gomez-Marin and Ghazanfar - 2019 - The Life of Behavior.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Goodman2014,
  title = {Brian 2: Neural Simulations on a Variety of Computational Hardware},
  shorttitle = {Brian 2},
  author = {Goodman, Dan FM and Stimberg, Marcel and Yger, Pierre and Brette, Romain},
  year = {2014},
  volume = {15},
  pages = {P199},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-15-S1-P199},
  file = {/Users/qualia/Documents/Papers/Goodman et al. - 2014 - Brian 2 neural simulations on a variety of comput.pdf},
  journal = {BMC Neuroscience},
  language = {en},
  number = {Suppl 1}
}

@article{Gordus2015,
  title = {Feedback from {{Network States Generates Variability}} in a {{Probabilistic Olfactory Circuit}}},
  author = {Gordus, Andrew and Pokala, Navin and Levy, Sagi and Flavell, Steven W. and Bargmann, Cornelia I.},
  year = {2015},
  month = apr,
  volume = {161},
  pages = {215--227},
  issn = {00928674},
  doi = {10.1016/j.cell.2015.02.018},
  abstract = {Variability is a prominent feature of behavior and is an active element of certain behavioral strategies. To understand how neuronal circuits control variability, we examined the propagation of sensory information in a chemotaxis circuit of C. elegans where discrete sensory inputs can drive a probabilistic behavioral response. Olfactory neurons respond to odor stimuli with rapid and reliable changes in activity, but downstream AIB interneurons respond with a probabilistic delay. The interneuron response to odor depends on the collective activity of multiple neurons\textemdash{}AIB, RIM, and AVA\textemdash{}when the odor stimulus arrives. Certain activity states of the network correlate with reliable responses to odor stimuli. Artificially generating these activity states by modifying neuronal activity increases the reliability of odor responses in interneurons and the reliability of the behavioral response to odor. The integration of sensory information with network states may represent a general mechanism for generating variability in behavior.},
  file = {/Users/qualia/Documents/Papers/Gordus et al. - 2015 - Feedback from Network States Generates Variability.pdf},
  journal = {Cell},
  language = {en},
  number = {2}
}

@article{Gordus2015a,
  title = {Feedback from {{Network States Generates Variability}} in a {{Probabilistic Olfactory Circuit}}},
  author = {Gordus, Andrew and Pokala, Navin and Levy, Sagi and Flavell, Steven W. and Bargmann, Cornelia I.},
  year = {2015},
  month = apr,
  volume = {161},
  pages = {215--227},
  issn = {00928674},
  doi = {10.1016/j.cell.2015.02.018},
  abstract = {Variability is a prominent feature of behavior and is an active element of certain behavioral strategies. To understand how neuronal circuits control variability, we examined the propagation of sensory information in a chemotaxis circuit of C. elegans where discrete sensory inputs can drive a probabilistic behavioral response. Olfactory neurons respond to odor stimuli with rapid and reliable changes in activity, but downstream AIB interneurons respond with a probabilistic delay. The interneuron response to odor depends on the collective activity of multiple neurons\textemdash{}AIB, RIM, and AVA\textemdash{}when the odor stimulus arrives. Certain activity states of the network correlate with reliable responses to odor stimuli. Artificially generating these activity states by modifying neuronal activity increases the reliability of odor responses in interneurons and the reliability of the behavioral response to odor. The integration of sensory information with network states may represent a general mechanism for generating variability in behavior.},
  file = {/Users/qualia/Documents/Papers/Gordus et al. - 2015 - Feedback from Network States Generates Variability 2.pdf},
  journal = {Cell},
  language = {en},
  number = {2}
}

@article{Goris2014,
  title = {Partitioning Neuronal Variability},
  author = {Goris, Robbe L T and Movshon, J Anthony and Simoncelli, Eero P},
  year = {2014},
  month = jun,
  volume = {17},
  pages = {858--865},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3711},
  file = {/Users/qualia/Documents/Papers/Goris et al. - 2014 - Partitioning neuronal variability.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@techreport{Gornet2017,
  title = {Simulating Extracted Connectomes},
  author = {Gornet, Jonathan and Scheffer, Louis K.},
  year = {2017},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/177113},
  abstract = {Connectomes derived from volume EM imaging of the brain can generate detailed physical models of every neuron, and simulators such as NEURON or GENESIS are designed to work with such models. In principal, combining these technologies, plus transmitter and channel models, should allow detailed and accurate simulation of real neural circuits. Here we experiment with this combination, using a well-studied system (motion detection in
            Drosophila
            ). Since simulation requires both the physical geometry (which we have) and the models of the synapses (which are not currently available), we built approximate synapses corresponding to their known and estimated function. Once we did so, we reproduced direction selectivity in T4 cells, one of the main functions of this neural circuit. This verified the basic functionality of both extraction and simulations, and provided a biologically relevant computation we could use in further experiments. We then compared models with different degrees of physical realism, from full detailed models down to models consisting of a single node, to examine the tradeoff of simulation resources required versus accuracy achieved.
          
          
            Our results show that much simpler models may be adequate, at least in the case of medulla neurons in
            Drosophila
            . Such models can be easily derived from fully detailed models, and result in simulations that are much smaller, much faster, and accurate enough for many purposes. Biologically, we show that a lumped neuron model reproduces the main motion detector operation, confirming the result of Gruntman[1], that dendritic compution is not required for this function.},
  file = {/Users/qualia/Documents/Papers/Gornet and Scheffer - 2017 - Simulating extracted connectomes.pdf},
  language = {en},
  type = {Preprint}
}

@article{Gottlieb2018,
  title = {Towards a Neuroscience of Active Sampling and Curiosity},
  author = {Gottlieb, Jacqueline and Oudeyer, Pierre-Yves},
  year = {2018},
  month = dec,
  volume = {19},
  pages = {758--770},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-018-0078-0},
  abstract = {In natural behaviour, animals actively interrogate their environments using endogenously generated `question-and-answer' strategies. However, in laboratory settings participants typically engage with externally imposed stimuli and tasks, and the mechanisms of active sampling remain poorly understood. We review a nascent neuroscientific literature that examines active-sampling policies and their relation to attention and curiosity. We distinguish between information sampling, in which organisms reduce uncertainty relevant to a familiar task{$\mkern1mu$}, and information search, in which they investigate in an open-ended fashion to discover new tasks. We review evidence that both sampling and search depend on individual preferences over cognitive states, including attitudes towards uncertainty{$\mkern1mu$}, learning progress and types of information. We propose that, although these preferences are non-instrumental and can on occasion interfere with external goals, they are important heuristics that allow organisms to cope with the high complexity of both sampling and search, and generate curiosity-driven investigations in large, open environments in which rewards are sparse and ex ante unknown.},
  file = {/Users/qualia/Documents/Papers/Gottlieb and Oudeyer - 2018 - Towards a neuroscience of active sampling and curi.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {12}
}

@techreport{Goyal2018,
  title = {Functionally Distinct High and Low Theta Oscillations in the Human Hippocampus},
  author = {Goyal, Abhinav and Miller, Jonathan and Qasim, Salman E. and Watrous, Andrew J. and Stein, Joel M. and Inman, Cory S. and Gross, Robert E. and Willie, Jon T. and Lega, Bradley and Lin, Jui-Jui and Sharan, Ashwini and Wu, Chengyuan and Sperling, Michael R. and Sheth, Sameer A. and McKhann, Guy M. and Smith, Elliot H. and Schevon, Catherine and Jacobs, Joshua},
  year = {2018},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/498055},
  abstract = {Abstract
          Based on rodent models, researchers have theorized that the hippocampus supports episodic memory and navigation via the theta oscillation, a \textasciitilde{}4\textendash{}10-Hz rhythm that coordinates brain-wide neural activity. However, recordings from humans have indicated that hippocampal theta oscillations are lower in frequency and less prevalent than in rodents, suggesting interspecies differences in theta's function. To characterize human hippocampal theta, we examined the properties of theta oscillations throughout the anterior\textendash{}posterior length of the hippocampus as neurosurgical subjects performed a virtual spatial navigation task. During virtual movement, we observed hippocampal oscillations at multiple frequencies from 2 to 14 Hz. The posterior hippocampus prominently displayed oscillations at \textasciitilde{}8-Hz and the precise frequency of these oscillations correlated with the speed of movement, implicating these signals in spatial navigation. We also observed slower \textasciitilde{}3-Hz oscillations, but these signals were more prevalent in the anterior hippocampus and their frequency did not vary with movement speed. Our results converge with recent findings to suggest an updated view of human hippocampal electrophysiology. Rather than one hippocampal theta oscillation with a single general role, high-and low-theta oscillations, respectively, may reflect spatial and non-spatial cognitive processes.},
  file = {/Users/qualia/Documents/Papers/Goyal et al. - 2018 - Functionally distinct high and low theta oscillati.pdf},
  language = {en},
  type = {Preprint}
}

@article{Grabska-Barwinska2014,
  title = {How Well Do Mean Field Theories of Spiking Quadratic-Integrate-and-Fire Networks Work in Realistic Parameter Regimes?},
  author = {{Grabska-Barwi{\'n}ska}, Agnieszka and Latham, Peter E.},
  year = {2014},
  month = jun,
  volume = {36},
  pages = {469--481},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-013-0481-5},
  abstract = {We use mean field techniques to compute the distribution of excitatory and inhibitory firing rates in large networks of randomly connected spiking quadratic integrate and fire neurons. These techniques are based on the assumption that activity is asynchronous and Poisson. For most parameter settings these assumptions are strongly violated; nevertheless, so long as the networks are not too synchronous, we find good agreement between mean field prediction and network simulations. Thus, much of the intuition developed for randomly connected networks in the asynchronous regime applies to mildly synchronous networks.},
  file = {/Users/qualia/Documents/Papers/2014 - Grabska-Barwiska, Latham - How well do mean field theories of spiking quadratic-integrate-and-fire networks work in realistic par.pdf;/Users/qualia/Documents/Papers/Grabska-Barwińska and Latham - 2014 - How well do mean field theories of spiking quadrat.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@book{Grady2010,
  title = {Discrete Calculus: Applied Analysis on Graphs for Computational Science},
  shorttitle = {Discrete Calculus},
  author = {Grady, Leo J. and Polimeni, Jonathan},
  year = {2010},
  publisher = {{Springer}},
  address = {{London ; New York}},
  file = {/Users/qualia/Documents/Papers/Grady and Polimeni - 2010 - Discrete calculus applied analysis on graphs for .pdf},
  isbn = {978-1-84996-289-6 978-1-84996-290-2},
  keywords = {Calculus,Computer science,Digital techniques Mathematics,Graph algorithms,Graphic methods,Image processing,Mathematics},
  language = {en},
  lccn = {QA76.9.M35 G73 2010},
  note = {OCLC: ocn651077688}
}

@article{Grant2013,
  title = {Simulation of {{Cortico}}-{{Basal Ganglia Oscillations}} and {{Their Suppression}} by {{Closed Loop Deep Brain Stimulation}}},
  author = {Grant, Peadar F. and Lowery, Madeleine M.},
  year = {2013},
  month = jul,
  volume = {21},
  pages = {584--594},
  issn = {1534-4320, 1558-0210},
  doi = {10.1109/TNSRE.2012.2202403},
  abstract = {A new model of deep brain stimulation (DBS) is presented that integrates volume conduction effects with a neural model of pathological beta-band oscillations in the cortico-basal ganglia network. The model is used to test the clinical hypothesis that closed-loop control of the amplitude of DBS may be possible, based on the average rectified value of beta-band oscillations in the local field potential. Simulation of closed-loop high-frequency DBS was shown to yield energy savings, with the magnitude of the energy saved dependent on the strength of coupling between the subthalamic nucleus and the remainder of the cortico-basal ganglia network. When closed-loop DBS was applied to a strongly coupled cortico-basal ganglia network, the stimulation energy delivered over a 480 s period was reduced by up to 42\%. Greater energy reductions were observed for weakly coupled networks, as the stimulation amplitude reduced to zero once the initial desynchronization had occurred. The results provide support for the application of closed-loop high-frequency DBS based on electrophysiological biomarkers.},
  file = {/Users/qualia/Documents/Papers/2013 - Grant, Lowery - Simulation of cortico-basal ganglia oscillations and their suppression by closed loop deep brain stimulation.pdf},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  language = {en},
  number = {4}
}

@article{Graves2014,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = oct,
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  archivePrefix = {arXiv},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2014 - Graves, Wayne, Danihelka - Neural Turing Machines.pdf;/Users/qualia/Documents/Papers/Graves et al. - 2014 - Neural Turing Machines.pdf},
  journal = {arXiv:1410.5401 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Graves2017,
  title = {Automated {{Curriculum Learning}} for {{Neural Networks}}},
  author = {Graves, Alex and Bellemare, Marc G. and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  year = {2017},
  month = apr,
  abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
  archivePrefix = {arXiv},
  eprint = {1704.03003},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Graves et al. - 2017 - Automated Curriculum Learning for Neural Networks.pdf},
  journal = {arXiv:1704.03003 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Graves2017a,
  title = {Automated {{Curriculum Learning}} for {{Neural Networks}}},
  author = {Graves, Alex and Bellemare, Marc G. and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  year = {2017},
  month = apr,
  abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multiarmed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.},
  archivePrefix = {arXiv},
  eprint = {1704.03003},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Graves et al. - 2017 - Automated Curriculum Learning for Neural Networks 2.pdf},
  journal = {arXiv:1704.03003 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Gray1995,
  title = {A {{Perceptron Reveals}} the {{Face}} of {{Sex}}},
  author = {Gray, Michael S. and Lawrence, David T. and Golomb, Beatrice A. and Sejnowski, Terrence J.},
  year = {1995},
  month = nov,
  volume = {7},
  pages = {1160--1164},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1995.7.6.1160},
  file = {/Users/qualia/Documents/Papers/1995 - Gray et al. - A perception reveals the face of sex.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@article{GrEGoire1997,
  title = {Effect of Age on Forward and Backward Digit Spans},
  author = {Gr{\'E}Goire, Jacques and Van Der Linden, Martial},
  year = {1997},
  month = jun,
  volume = {4},
  pages = {140--149},
  issn = {1382-5585, 1744-4128},
  doi = {10.1080/13825589708256642},
  abstract = {A number of studies has suggested that aging is characterized by a decline in the central executive while the automatic processes (in particular operations by the phonological loop) remain intact. According to interpretation, age differences should be minimal in verbal forward digit span while they should be more important in backward verbal digit span. A sample of 1,000 subjects with ages ranging from 16 years to 79 years was used to test this hypothesis. The results show no significant effect of age on the difference between digit span forward and backward. The theoretical implications of these results are discussed.},
  file = {/Users/qualia/Documents/Papers/2004 - Gregoire, Van Der Linden - Effect of age on forward and backward span tasks.pdf},
  journal = {Aging, Neuropsychology, and Cognition},
  language = {en},
  number = {2}
}

@article{Grewe2017,
  title = {Synchronous Spikes Are Necessary but Not Sufficient for a Synchrony Code in Populations of Spiking Neurons},
  author = {Grewe, Jan and Kruscha, Alexandra and Lindner, Benjamin and Benda, Jan},
  year = {2017},
  month = mar,
  volume = {114},
  pages = {E1977-E1985},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1615561114},
  file = {/Users/qualia/Documents/Papers/Grewe et al. - 2017 - Synchronous spikes are necessary but not sufficien.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {10}
}

@article{Greydanus2019,
  title = {Hamiltonian {{Neural Networks}}},
  author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
  year = {2019},
  month = jun,
  abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
  archivePrefix = {arXiv},
  eprint = {1906.01563},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Greydanus et al. - 2019 - Hamiltonian Neural Networks.pdf},
  journal = {arXiv:1906.01563 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Grill1995,
  title = {Stimulus Waveforms for Selective Neural Stimulation},
  author = {Grill, W.M. and Mortimer, J.T.},
  year = {July-Aug./1995},
  volume = {14},
  pages = {375--385},
  issn = {07395175},
  doi = {10.1109/51.395310},
  file = {/Users/qualia/Documents/Papers/1995 - Grill, Mortimer - Stimulus Waveforms for Selective Neural Stimulation.pdf},
  journal = {IEEE Engineering in Medicine and Biology Magazine},
  language = {en},
  number = {4}
}

@article{Grill1996,
  title = {The Effect of Stimulus Pulse Duration on Selectivity of Neural Stimulation},
  author = {Grill, W.M. and Mortimer, J.T.},
  year = {Feb./1996},
  volume = {43},
  pages = {161--166},
  issn = {00189294},
  doi = {10.1109/10.481985},
  file = {/Users/qualia/Documents/Papers/1996 - Grill, Mortimer - The Effect of Stimulus Pulse Duration on Selectivity of Neural Stimulation.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {2}
}

@incollection{Grill2015,
  title = {Model-Based Analysis and Design of Waveforms for Efficient Neural Stimulation},
  booktitle = {Progress in {{Brain Research}}},
  author = {Grill, Warren M.},
  year = {2015},
  volume = {222},
  pages = {147--162},
  publisher = {{Elsevier}},
  doi = {10.1016/bs.pbr.2015.07.031},
  abstract = {The design space for electrical stimulation of the nervous system is extremely large, and because the response to stimulation is highly non-linear, the selection of stimulation parameters to achieve a desired response is a challenging problem. Computational models of the response of neurons to extracellular stimulation allow analysis of the effects of stimulation parameters on neural excitation and provide an approach to select or design optimal parameters of stimulation. Here, I review the use of computational models to understand the effects of stimulation waveform on the energy efficiency of neural excitation and to design novel stimulation waveforms to increase the efficiency of neural stimulation.},
  file = {/Users/qualia/Documents/Papers/2015 - Grill - Model-Based Analysis and Design of Waveforms for Efficient Neural Stimulation.pdf;/Users/qualia/Documents/Papers/Grill - 2015 - Model-based analysis and design of waveforms for e.pdf},
  isbn = {978-0-444-63546-4},
  language = {en}
}

@article{Grillner2016,
  title = {The {{Basal Ganglia Over}} 500 {{Million Years}}},
  author = {Grillner, Sten and Robertson, Brita},
  year = {2016},
  month = oct,
  volume = {26},
  pages = {R1088-R1100},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.06.041},
  file = {/Users/qualia/Documents/Papers/Grillner and Robertson - 2016 - The Basal Ganglia Over 500 Million Years.pdf},
  journal = {Current Biology},
  language = {en},
  number = {20}
}

@article{Grillner2016a,
  title = {The {{Basal Ganglia Over}} 500 {{Million Years}}},
  author = {Grillner, Sten and Robertson, Brita},
  year = {2016},
  month = oct,
  volume = {26},
  pages = {R1088-R1100},
  issn = {09609822},
  doi = {10.1016/j.cub.2016.06.041},
  file = {/Users/qualia/Documents/Papers/Grillner and Robertson - 2016 - The Basal Ganglia Over 500 Million Years 2.pdf},
  journal = {Current Biology},
  language = {en},
  number = {20}
}

@article{Grimbert2006,
  title = {Bifurcation {{Analysis}} of {{Jansen}}'s {{Neural Mass Model}}},
  author = {Grimbert, Fran{\c c}ois and Faugeras, Olivier},
  year = {2006},
  month = dec,
  volume = {18},
  pages = {3052--3068},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2006.18.12.3052},
  file = {/Users/qualia/Documents/Papers/2006 - Grimbert, Faugeras - Bifurcation analysis of Jansen's neural mass model.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {12}
}

@article{Grozinger2019,
  title = {Pathways to Cellular Supremacy in Biocomputing},
  author = {Grozinger, Lewis and Amos, Martyn and Gorochowski, Thomas E. and Carbonell, Pablo and Oyarz{\'u}n, Diego A. and Stoof, Ruud and Fellermann, Harold and Zuliani, Paolo and Tas, Huseyin and {Go{\~n}i-Moreno}, Angel},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {5250},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13232-z},
  file = {/Users/qualia/Documents/Papers/Grozinger et al. - 2019 - Pathways to cellular supremacy in biocomputing.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@inproceedings{Guckelsberger2016,
  title = {Intrinsically Motivated General Companion {{NPCs}} via {{Coupled Empowerment Maximisation}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computational Intelligence}} and {{Games}} ({{CIG}})},
  author = {Guckelsberger, Christian and Salge, Christoph and Colton, Simon},
  year = {2016},
  month = sep,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Santorini, Greece}},
  doi = {10.1109/CIG.2016.7860406},
  abstract = {Non-player characters (NPCs) in games are traditionally hard-coded or dependent on pre-specified goals, and consequently struggle to behave sensibly in ever-changing and possibly unpredictable game worlds. To make them fit for new developments in procedural content generation, we introduce the principle of Coupled Empowerment Maximisation as an intrinsic motivation for game NPCs. We focus on the development of a general game companion, designed to support the player in achieving their goals. We evaluate our approach against three intuitive and abstract companion duties. We develop dedicated scenarios for each duty in a dungeon-crawler game testbed, and provide qualitative evidence that the emergent NPC behaviour fulfils these duties. We argue that this generic approach can speed up NPC AI development, improve automatic game evolution and introduce NPCs to full game-generation systems.},
  file = {/Users/qualia/Documents/Papers/Guckelsberger et al. - 2016 - Intrinsically motivated general companion NPCs via.pdf},
  isbn = {978-1-5090-1883-3},
  language = {en}
}

@article{Guckelsberger2018,
  title = {New {{And Surprising Ways}} to {{Be Mean}}. {{Adversarial NPCs}} with {{Coupled Empowerment Minimisation}}},
  author = {Guckelsberger, Christian and Salge, Christoph and Togelius, Julian},
  year = {2018},
  month = jun,
  abstract = {Creating Non-Player Characters (NPCs) that can react robustly to unforeseen player behaviour or novel game content is difficult and time-consuming. This hinders the design of believable characters, and the inclusion of NPCs in games that rely heavily on procedural content generation. We have previously addressed this challenge by means of empowerment, a model of intrinsic motivation, and demonstrated how a coupled empowerment maximisation (CEM) policy can yield generic, companion-like behaviour. In this paper, we extend the CEM framework with a minimisation policy to give rise to adversarial behaviour. We conduct a qualitative, exploratory study in a dungeon-crawler game, demonstrating that CEM can exploit the affordances of different content facets in adaptive adversarial behaviour without modifications to the policy. Changes to the level design, underlying mechanics and our character's actions do not threaten our NPC's robustness, but yield new and surprising ways to be mean.},
  archivePrefix = {arXiv},
  eprint = {1806.01387},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Guckelsberger et al. - 2018 - New And Surprising Ways to Be Mean. Adversarial NP.pdf},
  journal = {arXiv:1806.01387 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{Guckelsberger2018a,
  title = {New {{And Surprising Ways}} to {{Be Mean}}. {{Adversarial NPCs}} with {{Coupled Empowerment Minimisation}}},
  author = {Guckelsberger, Christian and Salge, Christoph and Togelius, Julian},
  year = {2018},
  month = jun,
  abstract = {Creating Non-Player Characters (NPCs) that can react robustly to unforeseen player behaviour or novel game content is difficult and time-consuming. This hinders the design of believable characters, and the inclusion of NPCs in games that rely heavily on procedural content generation. We have previously addressed this challenge by means of empowerment, a model of intrinsic motivation, and demonstrated how a coupled empowerment maximisation (CEM) policy can yield generic, companion-like behaviour. In this paper, we extend the CEM framework with a minimisation policy to give rise to adversarial behaviour. We conduct a qualitative, exploratory study in a dungeon-crawler game, demonstrating that CEM can exploit the affordances of different content facets in adaptive adversarial behaviour without modifications to the policy. Changes to the level design, underlying mechanics and our character's actions do not threaten our NPC's robustness, but yield new and surprising ways to be mean.},
  archivePrefix = {arXiv},
  eprint = {1806.01387},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Guckelsberger et al. - 2018 - New And Surprising Ways to Be Mean. Adversarial NP 2.pdf},
  journal = {arXiv:1806.01387 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{Guclu2017,
  title = {Modeling the {{Dynamics}} of {{Human Brain Activity}} with {{Recurrent Neural Networks}}},
  author = {G{\"u}{\c c}l{\"u}, Umut and {van Gerven}, Marcel A. J.},
  year = {2017},
  month = feb,
  volume = {11},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00007},
  abstract = {Encoding models are used for predicting brain activity in response to sensory stimuli with the objective of elucidating how sensory information is represented in the brain. Encoding models typically comprise a nonlinear transformation of stimuli to features (feature model) and a linear transformation of features to responses (response model). While there has been extensive work on developing better feature models, the work on developing better response models has been rather limited. Here, we investigate the extent to which recurrent neural network models can use their internal memories for nonlinear processing of arbitrary feature sequences to predict feature-evoked response sequences as measured by functional magnetic resonance imaging. We show that the proposed recurrent neural network models can significantly outperform established response models by accurately estimating long-term dependencies that drive hemodynamic responses. The results open a new window into modeling the dynamics of brain activity in response to sensory stimuli.},
  file = {/Users/qualia/Documents/Papers/Güçlü and van Gerven - 2017 - Modeling the Dynamics of Human Brain Activity with.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Guergiuev,
  title = {Deep Learning with Segregated Dendrites},
  author = {Guergiuev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  pages = {29},
  abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that deep learning can be achieved by moving away from point neuron models and towards multi-compartment neurons. Like neocortical pyramidal neurons, neurons in our model receive feedforward sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, the network can calculate local synaptic weight updates that allow it to categorize images from the MNIST data-set with good accuracy. We show that our learning algorithm can take advantage of multilayer architectures to identify abstract categories\textemdash{}the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments for feedforward and feedback information, which may help to explain the dendritic morphology of neocortical pyramidal neurons.},
  file = {/Users/qualia/Documents/Papers/Guergiuev et al. - Deep learning with segregated dendrites.pdf},
  language = {en}
}

@article{Guerguiev2017,
  title = {Towards Deep Learning with Segregated Dendrites},
  author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
  year = {2017},
  month = dec,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.22901},
  file = {/Users/qualia/Documents/Papers/Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites 2.pdf;/Users/qualia/Documents/Papers/Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites.pdf},
  journal = {eLife},
  language = {en}
}

@article{Guest2016,
  title = {What the {{Success}} of {{Brain Imaging Implies}} about the {{Neural Code}}},
  author = {Guest, Olivia and Love, Bradley C},
  year = {2016},
  month = sep,
  doi = {10.1101/071076},
  abstract = {The success of fMRI places constraints on the nature of the neural code. The fact that researchers can infer similarities between neural representations, despite limitations in what fMRI measures, implies that certain neural coding schemes are more likely than others. For fMRI to be successful given its low temporal and spatial resolution, the neural code must smooth at the subvoxel and functional level such that similar stimuli engender similar internal representations. Through proof and simulation, we evaluate a number of reasonable coding schemes and demonstrate that only a subset are plausible given both fMRI's successes and its limitations in measuring neural activity. Deep neural network approaches, which have been forwarded as computational accounts of the ventral stream, are consistent with the success of fMRI, though functional smoothness breaks down in the later network layers. These results have implications for the nature of neural code and ventral stream, as well as what can be successfully investigated with fMRI.},
  file = {/Users/qualia/Documents/Papers/Guest and Love - 2016 - What the Success of Brain Imaging Implies about th.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Guise2014,
  title = {A {{Bayesian Model}} of {{Polychronicity}}},
  author = {Guise, Mira and Knott, Alistair and Benuskova, Lubica},
  year = {2014},
  month = sep,
  volume = {26},
  pages = {2052--2073},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00620},
  file = {/Users/qualia/Documents/Papers/2014 - Lehky et al. - Dimensionality of object representations in monkey inferotemporal cortex.pdf;/Users/qualia/Documents/Papers/Guise et al. - 2014 - A Bayesian Model of Polychronicity.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {9}
}

@article{Gulyas2020,
  title = {The Role of Detours in Individual Human Navigation Patterns of Complex Networks},
  author = {Guly{\'a}s, Andr{\'a}s and B{\'i}r{\'o}, J{\'o}zsef and R{\'e}tv{\'a}ri, G{\'a}bor and Nov{\'a}k, M{\'a}rton and K{\H o}r{\"o}si, Attila and Sl{\'i}z, Mariann and Heszberger, Zal{\'a}n},
  year = {2020},
  month = dec,
  volume = {10},
  pages = {1098},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-57856-4},
  file = {/Users/qualia/Documents/Papers/Gulyás et al. - 2020 - The role of detours in individual human navigation.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Gunawardena2014,
  title = {Models in Biology: `Accurate Descriptions of Our Pathetic Thinking'},
  shorttitle = {Models in Biology},
  author = {Gunawardena, Jeremy},
  year = {2014},
  month = dec,
  volume = {12},
  issn = {1741-7007},
  doi = {10.1186/1741-7007-12-29},
  abstract = {In this essay I will sketch some ideas for how to think about models in biology. I will begin by trying to dispel the myth that quantitative modeling is somehow foreign to biology. I will then point out the distinction between forward and reverse modeling and focus thereafter on the former. Instead of going into mathematical technicalities about different varieties of models, I will focus on their logical structure, in terms of assumptions and conclusions. A model is a logical machine for deducing the latter from the former. If the model is correct, then, if you believe its assumptions, you must, as a matter of logic, also believe its conclusions. This leads to consideration of the assumptions underlying models. If these are based on fundamental physical laws, then it may be reasonable to treat the model as `predictive', in the sense that it is not subject to falsification and we can rely on its conclusions. However, at the molecular level, models are more often derived from phenomenology and guesswork. In this case, the model is a test of its assumptions and must be falsifiable. I will discuss three models from this perspective, each of which yields biological insights, and this will lead to some guidelines for prospective model builders.},
  file = {/Users/qualia/Documents/Papers/Gunawardena - 2014 - Models in biology ‘accurate descriptions of our p.pdf},
  journal = {BMC Biology},
  language = {en},
  number = {1}
}

@article{Gunnthorsdottir2002,
  title = {Using the {{Machiavellianism}} Instrument to Predict Trustworthiness in a Bargaining Game},
  author = {Gunnthorsdottir, Anna and McCabe, Kevin and Smith, Vernon},
  year = {2002},
  month = feb,
  volume = {23},
  pages = {49--66},
  issn = {01674870},
  doi = {10.1016/S0167-4870(01)00067-8},
  abstract = {Game-theoretic experiments have revealed substantial individual differences where the game allows for off-equilibrium behavior such as trust and reciprocity. We explore the personality psychology and decision making literatures and conclude that these individual differences are likely to be mediated by differential emotional arousal. We argue that Christie and Geis's Machiavellianism scale (Mach-IV) is an instrument that allows the identification of types who vary in cooperativeness. We use that test to predict the behavior of participants in a two-person one-shot constituent game in which subjects face a choice between trust and distrust, and between reciprocation (trustworthiness) and defection. We find that the Mach-IV scale does not predict trusting behavior. It does, however, predict reciprocity. Over one half of those who score low to average on the scale reciprocate trust. High scorers overwhelmingly defect when it is to their advantage to do so. {\'O} 2002 Elsevier Science B.V. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2002 - Gunnthorsdottir, McCabe, Smith - Using the Machiavellianism instrument to predict trustworthiness in a bargaining game.pdf},
  journal = {Journal of Economic Psychology},
  language = {en},
  number = {1}
}

@article{Guntupalli2016,
  title = {A {{Model}} of {{Representational Spaces}} in {{Human Cortex}}},
  author = {Guntupalli, J. Swaroop and Hanke, Michael and Halchenko, Yaroslav O. and Connolly, Andrew C. and Ramadge, Peter J. and Haxby, James V.},
  year = {2016},
  month = jun,
  volume = {26},
  pages = {2919--2934},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhw068},
  abstract = {Current models of the functional architecture of human cortex emphasize areas that capture coarse-scale features of cortical topography but provide no account for population responses that encode information in fine-scale patterns of activity. Here, we present a linear model of shared representational spaces in human cortex that captures fine-scale distinctions among population responses with response-tuning basis functions that are common across brains and models cortical patterns of neural responses with individual-specific topographic basis functions. We derive a common model space for the whole cortex using a new algorithm, searchlight hyperalignment, and complex, dynamic stimuli that provide a broad sampling of visual, auditory, and social percepts. The model aligns representations across brains in occipital, temporal, parietal, and prefrontal cortices, as shown by between-subject multivariate pattern classification and intersubject correlation of representational geometry, indicating that structural principles for shared neural representations apply across widely divergent domains of information. The model provides a rigorous account for individual variability of well-known coarse-scale topographies, such as retinotopy and category selectivity, and goes further to account for fine-scale patterns that are multiplexed with coarse-scale topographies and carry finer distinctions.},
  file = {/Users/qualia/Documents/Papers/Guntupalli et al. - 2016 - A Model of Representational Spaces in Human Cortex.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {6}
}

@article{Gupta2006,
  title = {The {{Interplay}} between {{Exploration}} and {{Exploitation}}},
  author = {Gupta, Anil A. and Smith, Ken and Shalley, Christina},
  year = {2006},
  volume = {49},
  pages = {693--706},
  file = {/Users/qualia/Documents/Papers/[No title found].pdf},
  journal = {The Academy of Management Journal},
  language = {en},
  number = {4}
}

@article{Gupta2006a,
  title = {The {{Interplay}} between {{Exploration}} and {{Exploitation}}},
  author = {Gupta, Anil and Smith, Ken and Shalley, Christina},
  year = {2006},
  volume = {49},
  file = {/Users/qualia/Documents/Papers/Gupta et al. - 2006 - The Interplay between Exploration and Exploitation.pdf},
  journal = {The Academy of Management Journal},
  language = {en},
  number = {4}
}

@article{Gurney,
  title = {Testing Computational Hypotheses of Brain Systems Function: A Case Study with the Basal Ganglia},
  author = {Gurney, K N and Humphries, M and Wood, R and Prescott, T J and Redgrave, P},
  pages = {29},
  abstract = {We develop a methodology for testing computational hypotheses about neural functionality articulated in models at the systems level of description. In this approach, the first step is to attempt the construction of a model of the underlying brain system which is consistent with the known anatomy and physiology, but which is also able to exhibit functional properties consistent with a putative computational hypothesis. If this is successful, the second step consists of including additional known pathways into the model and testing the new models to see whether they show an improvement in functional performance (using appropriate performance metrics). A positive outcome is taken as evidence in support of the hypothesis. A final step is to construct `control' models by including pathways that are not consistent with biological data. In this case a performance detriment is taken as support for the hypothesis. The methodology is applied to the basal ganglia, and builds on a previously published model of this system (Gurney et al 2001 Biol. Cybern. 84 401\textendash{}23) which was based on the hypothesis that the basal ganglia perform action selection. The realistically constrained models show a selection benefit, while control models show a decrement in selection ability. These results, taken together, provide further validation of our selection hypothesis of basal ganglia function.},
  file = {/Users/qualia/Documents/Papers/2004 - Gurney et al. - Testing computational hypotheses of brain systems function a case study with the basal ganglia K.pdf},
  language = {en}
}

@article{Gurney2001,
  title = {A Computational Model of Action Selection in the Basal Ganglia. {{I}}. {{A}} New Functional Anatomy},
  author = {Gurney, K. and Prescott, T. J. and Redgrave, P.},
  year = {2001},
  month = may,
  volume = {84},
  pages = {401--410},
  issn = {0340-1200},
  doi = {10.1007/PL00007984},
  abstract = {We present a biologically plausible model of processing intrinsic to the basal ganglia based on the computational premise that action selection is a primary role of these central brain structures. By encoding the propensity for selecting a given action in a scalar value (the salience), it is shown that action selection may be recast in terms of signal selection. The generic properties of signal selection are de\textregistered{}ned and neural networks for this type of computation examined. A comparison between these networks and basal ganglia anatomy leads to a novel functional decomposition of the basal ganglia architecture into `selection' and `control' pathways. The former pathway performs the selection per se via a feedforward o-centre on-surround network. The control pathway regulates the action of the selection pathway to ensure its eective operation, and synergistically complements its dopaminergic modulation. The model contrasts with the prevailing functional segregation of basal ganglia into `direct' and `indirect' pathways.},
  file = {/Users/qualia/Documents/Papers/2001 - Gurney, Prescott, Redgrave - A computational model of action selection in the basal ganglia. I. A new functional anatomy.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {6}
}

@article{Gurney2001a,
  title = {A Computational Model of Action Selection in the Basal Ganglia. {{II}}. {{Analysis}} and Simulation of Behaviour},
  author = {Gurney, K. and Prescott, T. J. and Redgrave, P.},
  year = {2001},
  month = may,
  volume = {84},
  pages = {411--423},
  issn = {0340-1200},
  doi = {10.1007/PL00007985},
  abstract = {In a companion paper a new functional architecture was proposed for the basal ganglia based on the premise that these brain structures play a central role in behavioural action selection. The current paper quantitatively describes the properties of the model using analysis and simulation. The decomposition of the basal ganglia into selection and control pathways is supported in several ways. First, several elegant features are exposed {$\pm$} capacity scaling, enhanced selectivity and synergistic dopamine modulation {$\pm$} which might be expected to exist in a well designed action selection mechanism. The discovery of these features also lends support to the computational premise of selection that underpins our model. Second, good matches between model globus pallidus external segment output and globus pallidus internal segment and substantia nigra reticulata area output, and neurophysiological data, have been found which are indicative of common architectural features in the model and biological basal ganglia. Third, the behaviour of the model as a signal selection mechanism has parallels with some kinds of action selection observed in animals under various levels of dopaminergic modulation.},
  file = {/Users/qualia/Documents/Papers/2001 - Gurney, Prescott, Redgrave - A computational model of action selection in the basal ganglia. II. Analysis and simulation of behav.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {6}
}

@article{Gurney2015,
  title = {A {{New Framework}} for {{Cortico}}-{{Striatal Plasticity}}: {{Behavioural Theory Meets In Vitro Data}} at the {{Reinforcement}}-{{Action Interface}}},
  shorttitle = {A {{New Framework}} for {{Cortico}}-{{Striatal Plasticity}}},
  author = {Gurney, Kevin N. and Humphries, Mark D. and Redgrave, Peter},
  editor = {Dayan, Peter},
  year = {2015},
  month = jan,
  volume = {13},
  pages = {e1002034},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002034},
  abstract = {Operant learning requires that reinforcement signals interact with action representations at a suitable neural interface. Much evidence suggests that this occurs when phasic dopamine, acting as a reinforcement prediction error, gates plasticity at cortico-striatal synapses, and thereby changes the future likelihood of selecting the action(s) coded by striatal neurons. But this hypothesis faces serious challenges. First, cortico-striatal plasticity is inexplicably complex, depending on spike timing, dopamine level, and dopamine receptor type. Second, there is a credit assignment problem\textemdash{}action selection signals occur long before the consequent dopamine reinforcement signal. Third, the two types of striatal output neuron have apparently opposite effects on action selection. Whether these factors rule out the interface hypothesis and how they interact to produce reinforcement learning is unknown. We present a computational framework that addresses these challenges. We first predict the expected activity changes over an operant task for both types of action-coding striatal neuron, and show they co-operate to promote action selection in learning and compete to promote action suppression in extinction. Separately, we derive a complete model of dopamine and spike-timing dependent cortico-striatal plasticity from in vitro data. We then show this model produces the predicted activity changes necessary for learning and extinction in an operant task, a remarkable convergence of a bottom-up data-driven plasticity model with the top-down behavioural requirements of learning theory. Moreover, we show the complex dependencies of cortico-striatal plasticity are not only sufficient but necessary for learning and extinction. Validating the model, we show it can account for behavioural data describing extinction, renewal, and reacquisition, and replicate in vitro experimental data on cortico-striatal plasticity. By bridging the levels between the single synapse and behaviour, our model shows how striatum acts as the action-reinforcement interface.},
  file = {/Users/qualia/Documents/Papers/2015 - Gurney, Humphries, Redgrave - A New Framework for Cortico-Striatal Plasticity Behavioural Theory Meets In Vitro Data at the Reinf.pdf;/Users/qualia/Documents/Papers/Gurney et al. - 2015 - A New Framework for Cortico-Striatal Plasticity B.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {1}
}

@article{Gutfreund1995,
  title = {Subthreshold Oscillations and Resonant Frequency in Guinea-Pig Cortical Neurons: Physiology and Modelling.},
  shorttitle = {Subthreshold Oscillations and Resonant Frequency in Guinea-Pig Cortical Neurons},
  author = {Gutfreund, Y and {yarom}, Y and Segev, I},
  year = {1995},
  month = mar,
  volume = {483},
  pages = {621--640},
  issn = {00223751},
  doi = {10.1113/jphysiol.1995.sp020611},
  file = {/Users/qualia/Documents/Papers/Gutfreund et al. - 1995 - Subthreshold oscillations and resonant frequency i.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {3}
}

@article{Guthrie1999,
  title = {{{ATP Released}} from {{Astrocytes Mediates Glial Calcium Waves}}},
  author = {Guthrie, Peter B. and Knappenberger, Joshua and Segal, Menahem and Bennett, Michael V. L. and Charles, Andrew C. and Kater, S. B.},
  year = {1999},
  month = jan,
  volume = {19},
  pages = {520--528},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.19-02-00520.1999},
  file = {/Users/qualia/Documents/Papers/Guthrie et al. - 1999 - ATP Released from Astrocytes Mediates Glial Calciu.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {2}
}

@article{Guthrie1999a,
  title = {{{ATP Released}} from {{Astrocytes Mediates Glial Calcium Waves}}},
  author = {Guthrie, Peter B. and Knappenberger, Joshua and Segal, Menahem and Bennett, Michael V. L. and Charles, Andrew C. and Kater, S. B.},
  year = {1999},
  month = jan,
  volume = {19},
  pages = {520--528},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.19-02-00520.1999},
  file = {/Users/qualia/Documents/Papers/Guthrie et al. - 1999 - ATP Released from Astrocytes Mediates Glial Calciu 2.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {2}
}

@article{Gutierrez2013,
  title = {Multiple {{Mechanisms Switch}} an {{Electrically Coupled}}, {{Synaptically Inhibited Neuron}} between {{Competing Rhythmic Oscillators}}},
  author = {Gutierrez, Gabrielle J. and O'Leary, Timothy and Marder, Eve},
  year = {2013},
  month = mar,
  volume = {77},
  pages = {845--858},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.01.016},
  abstract = {Rhythmic oscillations are common features of nervous systems. One of the fundamental questions posed by these rhythms is how individual neurons or groups of neurons are recruited into different network oscillations. We modeled competing fast and slow oscillators connected to a hub neuron with electrical and inhibitory synapses. We explore the patterns of coordination shown in the network as a function of the electrical coupling and inhibitory synapse strengths with the help of a novel visualization method that we call the ``parameterscape.'' The hub neuron can be switched between the fast and slow oscillators by multiple network mechanisms, indicating that a given change in network state can be achieved by degenerate cellular mechanisms. These results have importance for interpreting experiments employing optogenetic, genetic, and pharmacological manipulations to understand circuit dynamics.},
  file = {/Users/qualia/Documents/Papers/2013 - Gutierrez, O'Leary, Marder - Multiple mechanisms switch an electrically coupled, synaptically inhibited neuron between competing.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Gutig2016,
  title = {Spiking Neurons Can Discover Predictive Features by Aggregate-Label Learning},
  author = {Gutig, R.},
  year = {2016},
  month = mar,
  volume = {351},
  pages = {aab4113-aab4113},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab4113},
  file = {/Users/qualia/Documents/Papers/Gutig - 2016 - Spiking neurons can discover predictive features b.pdf},
  journal = {Science},
  language = {en},
  number = {6277}
}

@article{Guyon,
  title = {An {{Introduction}} to {{Variable}} and {{Feature Selection}}},
  author = {Guyon, Isabelle and Elisseeff, Andre},
  pages = {26},
  abstract = {Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
  file = {/Users/qualia/Documents/Papers/2003 - Guyon, Elisseeff - An introduction to variable and feature selection.pdf},
  language = {en}
}

@article{Ha2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  doi = {10.5281/zenodo.1207631},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment.},
  archivePrefix = {arXiv},
  eprint = {1803.10122},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Ha and Schmidhuber - 2018 - World Models.pdf},
  journal = {arXiv:1803.10122 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Haarnoja,
  title = {Reinforcement {{Learning}} with {{Deep Energy}}-{{Based Policies}}},
  author = {Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  pages = {16},
  abstract = {We propose a method for learning expressive energy-based policies for continuous states and actions, which has been feasible only in tabular domains before. We apply our method to learning maximum entropy policies, resulting into a new algorithm, called soft Q-learning, that expresses the optimal policy via a Boltzmann distribution. We use the recently proposed amortized Stein variational gradient descent to learn a stochastic sampling network that approximates samples from this distribution. The benefits of the proposed algorithm include improved exploration and compositionality that allows transferring skills between tasks, which we confirm in simulated experiments with swimming and walking robots. We also draw a connection to actorcritic methods, which can be viewed performing approximate inference on the corresponding energy-based model.},
  file = {/Users/qualia/Documents/Papers/Haarnoja et al. - Reinforcement Learning with Deep Energy-Based Poli.pdf},
  language = {en}
}

@article{Haarnoja2018,
  title = {Soft {{Actor}}-{{Critic Algorithms}} and {{Applications}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = dec,
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been successfully applied to a range of challenging sequential decision making and control tasks. However, these methods typically suffer from two major challenges: high sample complexity and brittleness to hyperparameters. Both of these challenges limit the applicability of such methods to real-world domains. In this paper, we describe Soft Actor-Critic (SAC), our recently introduced off-policy actor-critic algorithm based on the maximum entropy RL framework. In this framework, the actor aims to simultaneously maximize expected return and entropy. That is, to succeed at the task while acting as randomly as possible. We extend SAC to incorporate a number of modifications that accelerate training and improve stability with respect to the hyperparameters, including a constrained formulation that automatically tunes the temperature hyperparameter. We systematically evaluate SAC on a range of benchmark tasks, as well as real-world challenging tasks such as locomotion for a quadrupedal robot and robotic manipulation with a dexterous hand. With these improvements, SAC achieves state-of-the-art performance, outperforming prior on-policy and off-policy methods in sample-efficiency and asymptotic performance. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving similar performance across different random seeds. These results suggest that SAC is a promising candidate for learning in real-world robotics tasks.},
  archivePrefix = {arXiv},
  eprint = {1812.05905},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Haarnoja et al. - Soft Actor-Critic Algorithms and Applications.pdf;/Users/qualia/Documents/Papers/Haarnoja et al. - 2018 - Soft Actor-Critic Algorithms and Applications.pdf},
  journal = {arXiv:1812.05905 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Haarnojaa,
  title = {Soft {{Actor}}-{{Critic}}:  {{Off}}-{{Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  pages = {14},
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an offpolicy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  file = {/Users/qualia/Documents/Papers/Haarnoja et al. - Soft Actor-Critic  Off-Policy Maximum Entropy Dee.pdf},
  language = {en}
}

@article{Haegens2011,
  title = {-{{Oscillations}} in the Monkey Sensorimotor Network Influence Discrimination Performance by Rhythmical Inhibition of Neuronal Spiking},
  author = {Haegens, S. and Nacher, V. and Luna, R. and Romo, R. and Jensen, O.},
  year = {2011},
  month = nov,
  volume = {108},
  pages = {19377--19382},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1117190108},
  file = {/Users/qualia/Documents/Papers/2011 - Haegens et al. - α-Oscillations in the monkey sensorimotor network influence discrimination performance by rhythmical inhibition.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {48}
}

@article{Haeri2005,
  title = {Modeling the {{Parkinson}}'s Tremor and Its Treatments},
  author = {Haeri, Mohammad and Sarbaz, Yashar and Gharibzadeh, Shahriar},
  year = {2005},
  month = oct,
  volume = {236},
  pages = {311--322},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2005.03.014},
  abstract = {In this paper, we discuss modeling issues of the Parkinson's tremor. Through the work we have employed physiological structure as well as functioning of the parts in brain that are involved in the disease. To obtain more practical similarity, random behaviors of the connection paths are also considered. Medication or treatment of the disease both by drug prescription and electrical signal stimulation are modeled based on the same model introduced for the disease itself. Two new medication strategies are proposed based on the model to reduce the side effects caused by the present drug prescription.},
  file = {/Users/qualia/Documents/Papers/2005 - Haeri, Sarbaz, Gharibzadeh - Modeling the Parkinson's tremor and its treatments.pdf},
  journal = {Journal of Theoretical Biology},
  language = {en},
  number = {3}
}

@article{Hagen2016,
  title = {Hybrid {{Scheme}} for {{Modeling Local Field Potentials}} from {{Point}}-{{Neuron Networks}}},
  author = {Hagen, Espen and Dahmen, David and Stavrinou, Maria L. and Lind{\'e}n, Henrik and Tetzlaff, Tom and {van Albada}, Sacha J. and Gr{\"u}n, Sonja and Diesmann, Markus and Einevoll, Gaute T.},
  year = {2016},
  month = dec,
  volume = {26},
  pages = {4461--4496},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhw237},
  abstract = {With rapidly advancing multi-electrode recording technology, the local field potential (LFP) has again become a popular measure of neuronal activity in both research and clinical applications. Proper understanding of the LFP requires detailed mathematical modeling incorporating the anatomical and electrophysiological features of neurons near the recording electrode, as well as synaptic inputs from the entire network. Here we propose a hybrid modeling scheme combining efficient point-neuron network models with biophysical principles underlying LFP generation by real neurons. The LFP predictions rely on populations of network-equivalent multicompartment neuron models with layer-specific synaptic connectivity, can be used with an arbitrary number of point-neuron network populations, and allows for a full separation of simulated network dynamics and LFPs. We apply the scheme to a full-scale cortical network model for a {$\sim$}1 mm2 patch of primary visual cortex, predict laminar LFPs for different network states, assess the relative LFP contribution from different laminar populations, and investigate effects of input correlations and neuron density on the LFP. The generic nature of the hybrid scheme and its public implementation in hybridLFPy form the basis for LFP predictions from other and larger pointneuron network models, as well as extensions of the current application with additional biological detail.},
  file = {/Users/qualia/Documents/Papers/Hagen et al. - 2016 - Hybrid Scheme for Modeling Local Field Potentials .pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {12}
}

@article{Hahn2010,
  title = {Modeling Shifts in the Rate and Pattern of Subthalamopallidal Network Activity during Deep Brain Stimulation},
  author = {Hahn, Philip J. and McIntyre, Cameron C.},
  year = {2010},
  month = jun,
  volume = {28},
  pages = {425--441},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-010-0225-8},
  abstract = {Deep brain stimulation (DBS) of the subthlamic nucleus (STN) represents an effective treatment for medically refractory Parkinson's disease; however, understanding of its effects on basal ganglia network activity remains limited. We constructed a computational model of the subthalamopallidal network, trained it to fit in vivo recordings from parkinsonian monkeys, and evaluated its response to STN DBS. The network model was created with synaptically connected single compartment biophysical models of STN and pallidal neurons, and stochastically defined inputs driven by cortical beta rhythms. A least mean square error training algorithm was developed to parameterize network connections and minimize error when compared to experimental spike and burst rates in the parkinsonian condition. The output of the trained network was then compared to experimental data not used in the training process. We found that reducing the influence of the cortical beta input on the model generated activity that agreed well with recordings from normal monkeys. Further, during STN DBS in the parkinsonian condition the simulations reproduced the reduction in GPi bursting found in existing experimental data. The model also provided the opportunity to greatly expand analysis of GPi bursting activity, generating three major predictions. First, its reduction was proportional to the volume of STN activated by DBS. Second, GPi bursting decreased in a stimulation frequency dependent manner, saturating at values consistent with clinically therapeutic DBS. And third, ablating STN neurons, reported to generate similar therapeutic outcomes as STN DBS, also reduced GPi bursting. Our theoretical analysis of stimulation induced network activity suggests that regularization of GPi firing is dependent on the volume of STN tissue activated and a threshold level of burst reduction may be necessary for therapeutic effect.},
  file = {/Users/qualia/Documents/Papers/2010 - Hahn, McIntyre - Modeling shifts in the rate and pattern of subthalamopallidal network activity during deep brain stimulation.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Halgren2018,
  title = {The {{Generation}} and {{Propagation}} of the {{Human Alpha Rhythm}}},
  author = {Halgren, Milan and Ulbert, Istvan and Bastuji, Helene and Fabo, Daniel and Eross, Lorand and Rey, Marc and Devinsky, Orrin and Doyle, Werner K and {Mak-McCully}, Rachel and Halgren, Eric and Wittner, Lucia and Chauvel, Patrick and Heit, Gary and Eskandar, Emad and Mandell, Arnold and Cash, Sydney S},
  year = {2018},
  month = jun,
  doi = {10.1101/202564},
  abstract = {The alpha rhythm is the longest studied brain oscillation and has been theorized to play a key role in cognition. Still, its physiology is poorly understood. In this study, we used micro and macro electrodes in surgical epilepsy patients to measure the intracortical and thalamic generators of the alpha rhythm during quiet wakefulness. We first found that alpha in posterior cortex propagates from higher-order anterosuperior areas towards the occipital pole, consistent with alpha effecting top-down processing. This cortical alpha leads pulvinar alpha, complicating prevailing theories of a thalamic pacemaker. Finally, alpha is dominated by currents and firing in supragranular cortical layers. Together, these results suggest that the alpha rhythm likely reflects short-range supragranular feedback which propagates from higher to lower-order cortex and cortex to thalamus. These physiological insights suggest how alpha could mediate feedback throughout the thalamocortical system.},
  file = {/Users/qualia/Documents/Papers/Halgren et al. - 2018 - The Generation and Propagation of the Human Alpha .pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Haller2018,
  title = {Parameterizing Neural Power Spectra},
  author = {Haller, Matar and Donoghue, Thomas and Peterson, Erik and Varma, Paroma and Sebastian, Priyadarshini and Gao, Richard and Noto, Torben and Knight, Robert T. and Shestyuk, Avgusta and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/299859},
  abstract = {Electrophysiological signals across species and recording scales exhibit both periodic and aperiodic features. Periodic oscillations have been widely studied and linked to numerous physiological, cognitive, behavioral, and disease states, while the aperiodic ``background'' 1/f component of neural power spectra has received far less attention. Most analyses of oscillations are conducted on a priori, canonically-defined frequency bands without consideration of the underlying aperiodic structure, or verification that a periodic signal even exists in addition to the aperiodic signal. This is problematic, as recent evidence shows that the aperiodic signal is dynamic, changing with age, task demands, and cognitive state. It has also been linked to the relative excitation/inhibition of the underlying neuronal population. This means that standard analytic approaches easily conflate changes in the periodic and aperiodic signals with one another because the aperiodic parameters\textemdash{}along with oscillation center frequency, power, and bandwidth\textemdash{}are all dynamic in physiologically meaningful, but likely different, ways. In order to overcome the limitations of traditional narrowband analyses and to reduce the potentially deleterious effects of conflating these features, we introduce a novel algorithm for automatic parameterization of neural power spectral densities (PSDs) as a combination of the aperiodic signal and putative periodic oscillations. Notably, this algorithm requires no a priori specification of band limits and accounts for potentially-overlapping oscillations while minimizing the degree to which they are confounded with one another. This algorithm is amenable to large-scale data exploration and analysis, providing researchers with a tool to quickly and accurately parameterize neural power spectra.},
  file = {/Users/qualia/Documents/Papers/Haller et al. - 2018 - Parameterizing neural power spectra.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Haller2018a,
  title = {Parameterizing Neural Power Spectra},
  author = {Haller, Matar and Donoghue, Thomas and Peterson, Erik and Varma, Paroma and Sebastian, Priyadarshini and Gao, Richard and Noto, Torben and Knight, Robert T. and Shestyuk, Avgusta and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/299859},
  abstract = {Electrophysiological signals across species and recording scales exhibit both periodic and aperiodic features. Periodic oscillations have been widely studied and linked to numerous physiological, cognitive, behavioral, and disease states, while the aperiodic ``background'' 1/f component of neural power spectra has received far less attention. Most analyses of oscillations are conducted on a priori, canonically-defined frequency bands without consideration of the underlying aperiodic structure, or verification that a periodic signal even exists in addition to the aperiodic signal. This is problematic, as recent evidence shows that the aperiodic signal is dynamic, changing with age, task demands, and cognitive state. It has also been linked to the relative excitation/inhibition of the underlying neuronal population. This means that standard analytic approaches easily conflate changes in the periodic and aperiodic signals with one another because the aperiodic parameters\textemdash{}along with oscillation center frequency, power, and bandwidth\textemdash{}are all dynamic in physiologically meaningful, but likely different, ways. In order to overcome the limitations of traditional narrowband analyses and to reduce the potentially deleterious effects of conflating these features, we introduce a novel algorithm for automatic parameterization of neural power spectral densities (PSDs) as a combination of the aperiodic signal and putative periodic oscillations. Notably, this algorithm requires no a priori specification of band limits and accounts for potentially-overlapping oscillations while minimizing the degree to which they are confounded with one another. This algorithm is amenable to large-scale data exploration and analysis, providing researchers with a tool to quickly and accurately parameterize neural power spectra.},
  file = {/Users/qualia/Documents/Papers/Haller et al. - 2018 - Parameterizing neural power spectra 2.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Haller2018b,
  title = {Parameterizing Neural Power Spectra},
  author = {Haller, Matar and Donoghue, Thomas and Peterson, Erik and Varma, Paroma and Sebastian, Priyadarshini and Gao, Richard and Noto, Torben and Knight, Robert T. and Shestyuk, Avgusta and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/299859},
  abstract = {Electrophysiological signals across species and recording scales exhibit both periodic and aperiodic features. Periodic oscillations have been widely studied and linked to numerous physiological, cognitive, behavioral, and disease states, while the aperiodic ``background'' 1/f component of neural power spectra has received far less attention. Most analyses of oscillations are conducted on a priori, canonically-defined frequency bands without consideration of the underlying aperiodic structure, or verification that a periodic signal even exists in addition to the aperiodic signal. This is problematic, as recent evidence shows that the aperiodic signal is dynamic, changing with age, task demands, and cognitive state. It has also been linked to the relative excitation/inhibition of the underlying neuronal population. This means that standard analytic approaches easily conflate changes in the periodic and aperiodic signals with one another because the aperiodic parameters\textemdash{}along with oscillation center frequency, power, and bandwidth\textemdash{}are all dynamic in physiologically meaningful, but likely different, ways. In order to overcome the limitations of traditional narrowband analyses and to reduce the potentially deleterious effects of conflating these features, we introduce a novel algorithm for automatic parameterization of neural power spectral densities (PSDs) as a combination of the aperiodic signal and putative periodic oscillations. Notably, this algorithm requires no a priori specification of band limits and accounts for potentially-overlapping oscillations while minimizing the degree to which they are confounded with one another. This algorithm is amenable to large-scale data exploration and analysis, providing researchers with a tool to quickly and accurately parameterize neural power spectra.},
  file = {/Users/qualia/Documents/Papers/Haller et al. - 2018 - Parameterizing neural power spectra 3.pdf},
  language = {en},
  type = {Preprint}
}

@article{Hammond2007,
  title = {Pathological Synchronization in {{Parkinson}}'s Disease: Networks, Models and Treatments},
  shorttitle = {Pathological Synchronization in {{Parkinson}}'s Disease},
  author = {Hammond, Constance and Bergman, Hagai and Brown, Peter},
  year = {2007},
  month = jul,
  volume = {30},
  pages = {357--364},
  issn = {01662236},
  doi = {10.1016/j.tins.2007.05.004},
  file = {/Users/qualia/Documents/Papers/2007 - Hammond, Bergman, Brown - Pathological synchronization in Parkinson's disease networks, models and treatments.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {7}
}

@article{Hansel1992,
  title = {Synchronization and Computation in a Chaotic Neural Network},
  author = {Hansel, D. and Sompolinsky, H.},
  year = {1992},
  month = feb,
  volume = {68},
  pages = {718--721},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.68.718},
  file = {/Users/qualia/Documents/Papers/1992 - Hansel, Sompolinsky - Synchronization and computation in a chaotic neural network.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {5}
}

@article{Hansel2002,
  title = {How {{Noise Contributes}} to {{Contrast Invariance}} of {{Orientation Tuning}} in {{Cat Visual Cortex}}},
  author = {Hansel, D. and {van Vreeswijk}, C.},
  year = {2002},
  month = jun,
  volume = {22},
  pages = {5118--5128},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.22-12-05118.2002},
  file = {/Users/qualia/Documents/Papers/2002 - Hansel, van Vreeswijk - How noise contributes to contrast invariance of orientation tuning in cat visual cortex.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {12}
}

@article{Hanson2008,
  title = {Brain {{Reading Using Full Brain Support Vector Machines}} for {{Object Recognition}}: {{There Is No}} ``{{Face}}'' {{Identification Area}}},
  shorttitle = {Brain {{Reading Using Full Brain Support Vector Machines}} for {{Object Recognition}}},
  author = {Hanson, Stephen Jos{\'e} and Halchenko, Yaroslav O.},
  year = {2008},
  month = feb,
  volume = {20},
  pages = {486--503},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.09-06-340},
  file = {/Users/qualia/Documents/Papers/2008 - Hanson, Halchenko - Brain reading using full brain support vector machines for object recognition there is no face identification.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Harlow2020,
  title = {Characterizing the {{University}} of {{California}}'s Tenure-Track Teaching Position from the Faculty and Administrator Perspectives},
  author = {Harlow, Ashley and Lo, Stanley M. and Saichaie, Kem and Sato, Brian K.},
  editor = {Bianchi, Cesario},
  year = {2020},
  month = jan,
  volume = {15},
  pages = {e0227633},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0227633},
  file = {/Users/qualia/Documents/Papers/Harlow et al. - 2020 - Characterizing the University of California’s tenu.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {1}
}

@article{Harnack2015,
  title = {Stability of {{Neuronal Networks}} with {{Homeostatic Regulation}}},
  author = {Harnack, Daniel and Pelko, Miha and Chaillet, Antoine and Chitour, Yacine and {van Rossum}, Mark C.W.},
  editor = {Gutkin, Boris S.},
  year = {2015},
  month = jul,
  volume = {11},
  pages = {e1004357},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004357},
  file = {/Users/qualia/Documents/Papers/Harnack et al. - 2015 - Stability of Neuronal Networks with Homeostatic Re 2.PDF},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {7}
}

@article{Harper2009,
  title = {The {{Replicator Equation}} as an {{Inference Dynamic}}},
  author = {Harper, Marc},
  year = {2009},
  month = nov,
  abstract = {The replicator equation is interpreted as a continuous inference equation and a formal similarity between the discrete replicator equation and Bayesian inference is described. Further connections between inference and the replicator equation are given including a discussion of information divergences, evolutionary stability, and exponential families as solutions for the replicator dynamic, using Fisher information and information geometry.},
  archivePrefix = {arXiv},
  eprint = {0911.1763},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Harper - 2009 - The Replicator Equation as an Inference Dynamic.pdf},
  journal = {arXiv:0911.1763 [cs, math]},
  keywords = {37N25; Secondary: 62F15,Computer Science - Information Theory,Mathematics - Dynamical Systems},
  language = {en},
  primaryClass = {cs, math}
}

@article{Harris-White1998,
  title = {Spiral {{Intercellular Calcium Waves}} in {{Hippocampal Slice Cultures}}},
  author = {{Harris-White}, Marni E. and Zanotti, Stephen A. and Frautschy, Sally A. and Charles, Andrew C.},
  year = {1998},
  month = feb,
  volume = {79},
  pages = {1045--1052},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.79.2.1045},
  abstract = {Harris-White, Marni E., Stephen A. Zanotti, Sally A. Frautschy, and Andrew C. Charles. Spiral intercellular calcium waves in hippocampal slice cultures. J. Neurophysiol. 79: 1045\textendash{}1052, 1998. Complex patterns of intercellular calcium signaling occur in the CA1 and CA2 regions of hippocampal slice organotypic cultures from neonatal mice. Spontaneous localized intercellular Ca
              2+
              waves involving 5\textendash{}15 cells propagate concentrically from multiple foci in the stratum oriens and s. radiatum. In these same regions, extensive Ca
              2+
              waves involving hundreds of cells propagate as curvilinear and spiral wavefronts across broad areas of CA1 and CA2. Ca
              2+
              waves travel at rates of 5\textendash{}10 {$\mu$}m/s, are abolished by thapsigargin, and do not require extracellular Ca
              2+
              . Staining for astrocytes and neurons indicates that these intercellular waves occur primarily in astrocytes. The frequency and amplitude of Ca
              2+
              waves increase in response to bath application of N-methyl-d-aspartate (NMDA) and decrease in response to removal of extracellular Ca
              2+
              or application of tetrodotoxin. This novel pattern of intercellular Ca
              2+
              signaling is characteristic of the behavior of an excitable medium. Networks of glial cells in the hippocampus may behave as an excitable medium whose spatial and temporal signaling properties are modulated by neuronal activity.},
  file = {/Users/qualia/Documents/Papers/Harris-White et al. - 1998 - Spiral Intercellular Calcium Waves in Hippocampal .pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@incollection{Hart2001,
  title = {A {{Reinforcement Procedure Leading}} to {{Correlated Equilibrium}}},
  booktitle = {Economics {{Essays}}},
  author = {Hart, Sergiu and {Mas-Colell}, Andreu},
  editor = {Debreu, G{\'e}rard and Neuefeind, Wilhelm and Trockel, Walter},
  year = {2001},
  pages = {181--200},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-04623-4_12},
  abstract = {We consider repeated games where at any period each player knows only his set of actions and the stream of payoffs that he has received in the past. He knows neither his own payoff function, nor the characteristics of the other players (how many there are, their strategies and payoffs). In this context, we present an adaptive procedure for play \textemdash{} called ``modified-regret-matching'' \textemdash{} which is interpretable as a stimulus-response or reinforcement procedure, and which has the property that any limit point of the empirical distribution of play is a correlated equilibrium of the stage game.},
  file = {/Users/qualia/Documents/Papers/2001 - Hart, Mas-colell - Economics Essays.pdf},
  isbn = {978-3-642-07539-1 978-3-662-04623-4},
  language = {en}
}

@article{Hassabis2009,
  title = {Decoding {{Neuronal Ensembles}} in the {{Human Hippocampus}}},
  author = {Hassabis, Demis and Chu, Carlton and Rees, Geraint and Weiskopf, Nikolaus and Molyneux, Peter D. and Maguire, Eleanor A.},
  year = {2009},
  month = apr,
  volume = {19},
  pages = {546--554},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.02.033},
  abstract = {Background: The hippocampus underpins our ability to navigate, to form and recollect memories, and to imagine future experiences. How activity across millions of hippocampal neurons supports these functions is a fundamental question in neuroscience, wherein the size, sparseness, and organization of the hippocampal neural code are debated.
Results: Here, by using multivariate pattern classification and high spatial resolution functional MRI, we decoded activity across the population of neurons in the human medial temporal lobe while participants navigated in a virtual reality environment. Remarkably, we could accurately predict the position of an individual within this environment solely from the pattern of activity in his hippocampus even when visual input and task were held constant. Moreover, we observed a dissociation between responses in the hippocampus and parahippocampal gyrus, suggesting that they play differing roles in navigation.
Conclusions: These results show that highly abstracted representations of space are expressed in the human hippocampus. Furthermore, our findings have implications for understanding the hippocampal population code and suggest that, contrary to current consensus, neuronal ensembles representing place memories must be large and have an anisotropic structure.},
  file = {/Users/qualia/Documents/Papers/2009 - Hassabis et al. - Decoding neuronal ensembles in the human hippocampus.pdf},
  journal = {Current Biology},
  language = {en},
  number = {7}
}

@article{Hassabis2017,
  title = {Neuroscience-{{Inspired Artificial Intelligence}}},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  year = {2017},
  month = jul,
  volume = {95},
  pages = {245--258},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.06.011},
  file = {/Users/qualia/Documents/Papers/Hassabis et al. - 2017 - Neuroscience-Inspired Artificial Intelligence.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Hassinger1995,
  title = {Evidence for Glutamate-Mediated Activation of Hippocampal Neurons by Glial Calcium Waves},
  author = {Hassinger, Tim D. and Atkinson, Paul B. and Strecker, George J. and Whalen, L. Ray and Dudek, F. Edward and Kossel, Albrecht H. and Kater, S. B.},
  year = {1995},
  month = oct,
  volume = {28},
  pages = {159--170},
  issn = {0022-3034, 1097-4695},
  doi = {10.1002/neu.480280204},
  abstract = {Communication from astrocytes to neurons has recently been reported by two laboratories, but different mechanisms were thought to underlie glial calcium wave activation of associated neurons. Neuronal calcium elevation by glia observed in the present report is similar to that reported previously, where an increase in neuronal calcium was demonstrated in response to glial stimulation. In the present study hippocampal neurons plated on a confluent glial monolayer displayed a transient increase in intracellular calcium following a short delay after the passage of a wave of increased calcium in underlying glia. Activated cells displayed action potentials in response to glial waves and showed antineurofilament immunoreactivity. Finally, the N-methyl+-aspartate glutamate receptor antagonist \textasciitilde{}r,-2-amino-5-phosphonovalericacid and the non-NMDA glutamate receptor antagonist 6,7dinitroquinoxaline-2,3-dione significantly reduced the responsiveness of neurons to glial calcium waves. O u r results indicate that hippocampal neurons growing on hippocampal or cortical astrocytes respond to glial calcium waves with elevations in calcium and increased electrical activity. Furthermore, we show that in most cases this communication appears to be mediated by ionotropic glutamate receptor channels. 01995 John Wiley \& Sons, Inc.},
  file = {/Users/qualia/Documents/Papers/Hassinger et al. - 1995 - Evidence for glutamate-mediated activation of hipp.pdf},
  journal = {J. Neurobiol.},
  language = {en},
  number = {2}
}

@article{Hassinger1996,
  title = {An Extracellular Signaling Component in Propagation of Astrocytic Calcium Waves},
  author = {Hassinger, T. D. and Guthrie, P. B. and Atkinson, P. B. and Bennett, M. V. L. and Kater, S. B.},
  year = {1996},
  month = nov,
  volume = {93},
  pages = {13268--13273},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.93.23.13268},
  abstract = {Focally evoked calcium waves in astrocyte cultures have been thought to propagate by gap-junctionmediated intercellular passage of chemical signal(s). In contrast to this mechanism we observed isolated astrocytes, which had no physical contact with other astrocytes in the culture, participating in a calcium wave. This observation requires an extracellular route of astrocyte signaling. To directly test for extracellular signaling we made cell-free lanes 10\textendash{}300 ␮m wide in conf luent cultures by deleting astrocytes with a glass pipette. After 4\textendash{}8 hr of recovery, regions of conf luent astrocytes separated by lanes devoid of cells were easily located. Electrical stimulation was used to initiate calcium waves. Waves crossed narrow ({$<$}120 ␮m) cell-free lanes in 15 of 36 cases, but failed to cross lanes wider than 120 ␮m in eight of eight cases. The probability of crossing narrow lanes was not correlated with the distance from the stimulation site, suggesting that cells along the path of the calcium wave release the extracellular messenger(s). Calculated velocity across the acellular lanes was not significantly different from velocity through regions of conf luent astrocytes. Focal superfusion altered both the extent and the direction of calcium waves in conf luent regions. These data indicate that extracellular signals may play a role in astrocyte\textendash{}astrocyte communication in situ.},
  file = {/Users/qualia/Documents/Papers/Hassinger et al. - 1996 - An extracellular signaling component in propagatio 2.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {23}
}

@article{Hasson2004,
  title = {Intersubject {{Synchronization}} of {{Cortical Activity During Natural Vision}}},
  author = {Hasson, U.},
  year = {2004},
  month = mar,
  volume = {303},
  pages = {1634--1640},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1089506},
  file = {/Users/qualia/Documents/Papers/2004 - Hasson et al. - Intersubject synchronization of cortical activity during natural vision.pdf},
  journal = {Science},
  language = {en},
  number = {5664}
}

@article{Hausler2003,
  title = {Perspectives of the High-Dimensional Dynamics of Neural Microcircuits from the Point of View of Low-Dimensional Readouts},
  author = {H{\"a}usler, Stefan and Markram, Henry and Maass, Wolfgang},
  year = {2003},
  month = mar,
  volume = {8},
  pages = {39--50},
  issn = {10762787},
  doi = {10.1002/cplx.10089},
  file = {/Users/qualia/Documents/Papers/2003 - Hausler, Markram, Maass - Perspectives of the high-dimensional dynamics of neural microcircuits from the point of view of low-dim.pdf},
  journal = {Complexity},
  language = {en},
  number = {4}
}

@article{Haxby2001,
  title = {Distributed and {{Overlapping Representations}} of {{Faces}} and {{Objects}} in {{Ventral Temporal Cortex}}},
  author = {Haxby, J. V.},
  year = {2001},
  month = sep,
  volume = {293},
  pages = {2425--2430},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1063736},
  file = {/Users/qualia/Documents/Papers/2001 - Haxby et al. - Distributed and overlapping representations of faces and objects in ventral temporal cortex.pdf},
  journal = {Science},
  language = {en},
  number = {5539}
}

@article{Haxby2011,
  title = {A {{Common}}, {{High}}-{{Dimensional Model}} of the {{Representational Space}} in {{Human Ventral Temporal Cortex}}},
  author = {Haxby, James V. and Guntupalli, J. Swaroop and Connolly, Andrew C. and Halchenko, Yaroslav O. and Conroy, Bryan R. and Gobbini, M. Ida and Hanke, Michael and Ramadge, Peter J.},
  year = {2011},
  month = oct,
  volume = {72},
  pages = {404--416},
  issn = {08966273},
  doi = {10.1016/j.neuron.2011.08.026},
  abstract = {We present a high-dimensional model of the representational space in human ventral temporal (VT) cortex in which dimensions are response-tuning functions that are common across individuals and patterns of response are modeled as weighted sums of basis patterns associated with these response tunings. We map response-pattern vectors, measured with fMRI, from individual subjects' voxel spaces into this common model space using a new method, ``hyperalignment.'' Hyperalignment parameters based on responses during one experiment\textemdash{}movie viewing\textemdash{}identified 35 common response-tuning functions that captured fine-grained distinctions among a wide range of stimuli in the movie and in two category perception experiments. Between-subject classification (BSC, multivariate pattern classification based on other subjects' data) of response-pattern vectors in common model space greatly exceeded BSC of anatomically aligned responses and matched withinsubject classification. Results indicate that population codes for complex visual stimuli in VT cortex are based on response-tuning functions that are common across individuals.},
  file = {/Users/qualia/Documents/Papers/2011 - Haxby et al. - A common, high-dimensional model of the representational space in human ventral temporal cortex.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Haynes2005,
  title = {Predicting the Orientation of Invisible Stimuli from Activity in Human Primary Visual Cortex},
  author = {Haynes, John-Dylan and Rees, Geraint},
  year = {2005},
  month = may,
  volume = {8},
  pages = {686--691},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1445},
  file = {/Users/qualia/Documents/Papers/2005 - Haynes, Rees - Predicting the orientation of invisible stimuli from activity in human primary visual cortex.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Haynes2006,
  title = {Decoding Mental States from Brain Activity in Humans},
  author = {Haynes, John-Dylan and Rees, Geraint},
  year = {2006},
  month = jul,
  volume = {7},
  pages = {523--534},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn1931},
  abstract = {Recent advances in human neuroimaging have shown that it is possible to accurately decode a person's conscious experience based only on non-invasive measurements of their brain activity. Such `brain reading' has mostly been studied in the domain of visual perception, where it helps reveal the way in which individual experiences are encoded in the human brain. The same approach can also be extended to other types of mental state, such as covert attitudes and lie detection. Such applications raise important ethical issues concerning the privacy of personal thought.},
  file = {/Users/qualia/Documents/Papers/2006 - Haynes, Rees - Decoding mental states from brain activity in humans.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {7}
}

@techreport{He2019,
  title = {Co-{{Increasing Neuronal Noise}} and {{Beta Power}} in the {{Developing Brain}}},
  author = {He, Wei and Donoghue, Thomas and Sowman, Paul F and Seymour, Robert A and Brock, Jon and Crain, Stephen and Voytek, Bradley and Hillebrand, Arjan},
  year = {2019},
  month = nov,
  institution = {{Neuroscience}},
  doi = {10.1101/839258},
  abstract = {Accumulating evidence across species indicates that brain oscillations are superimposed upon an aperiodic 1/f - like power spectrum. Maturational changes in neuronal oscillations have not been assessed in tandem with this underlying aperiodic spectrum. The current study uncovers co-maturation of the aperiodic component alongside the periodic components (oscillations) in spontaneous magnetoencephalography (MEG) data. Beamformer-reconstructed MEG time-series allowed a direct comparison of power in the source domain between 24 children (8.0 {$\pm$} 2.5 years, 17 males) and 24 adults (40.6 {$\pm$} 17.4 years, 16 males). Our results suggest that the redistribution of oscillatory power from lower to higher frequencies that is observed in childhood does not hold once the age-related changes in the aperiodic signal are controlled for. When estimating both the periodic and aperiodic components, we found that power increases with age in the beta band only, and that the 1/f signal is flattened in adults compared to children. These results suggest a pattern of co-maturing beta oscillatory power with the aperiodic 1/f signal in typical childhood development.},
  file = {/Users/qualia/Documents/Papers/He et al. - 2019 - Co-Increasing Neuronal Noise and Beta Power in the.pdf},
  language = {en},
  type = {Preprint}
}

@article{Hebbink,
  title = {Activity Types in a Neural Mass Model},
  author = {Hebbink, Jurgen},
  pages = {50},
  file = {/Users/qualia/Documents/Papers/2014 - Hebbink - Activity types in a neural mass model.pdf;/Users/qualia/Documents/Papers/Hebbink - Activity types in a neural mass model.pdf;/Users/qualia/Zotero/storage/HJHQ5CAE/1995 - Gray et al. - A perception reveals the face of sex.pdf},
  language = {en}
}

@article{Heeger1992,
  title = {Half-Squaring in Responses of Cat Striate Cells},
  author = {Heeger, David J.},
  year = {1992},
  month = nov,
  volume = {9},
  pages = {427--443},
  issn = {0952-5238, 1469-8714},
  doi = {10.1017/S095252380001124X},
  abstract = {Simple cells in striate cortex have been depicted as rectified linear operators, and complex cells have been depicted as energy mechanisms (constructed from the squared sums of linear operator outputs). This paper discusses two essential hypotheses of the linear/energy model: (1) that a cell's selectivity is due to an underlying (spatiotemporal and binocular) linear stage; and (2) that a cell's firing rate depends on the squared output of the underlying linear stage. This paper reviews physiological measurements of cat striate cell responses, and concludes that both of these hypotheses are supported by the data.},
  file = {/Users/qualia/Documents/Papers/1992 - Heeger - Half-squaring in responses of cat striate cells.pdf},
  journal = {Visual Neuroscience},
  language = {en},
  number = {05}
}

@article{Heeger2017,
  title = {Theory of Cortical Function},
  author = {Heeger, David J.},
  year = {2017},
  month = feb,
  volume = {114},
  pages = {1773--1782},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1619788114},
  file = {/Users/qualia/Documents/Papers/Heeger - 2017 - Theory of cortical function.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {8}
}

@article{Heekeren2004,
  title = {A General Mechanism for Perceptual Decision-Making in the Human Brain},
  author = {Heekeren, H. R. and Marrett, S. and Bandettini, P. A. and Ungerleider, L. G.},
  year = {2004},
  month = oct,
  volume = {431},
  pages = {859--862},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature02966},
  file = {/Users/qualia/Documents/Papers/2004 - Heekeren et al. - A general mechanism for perceptual decision-making in the human brain.pdf},
  journal = {Nature},
  language = {en},
  number = {7010}
}

@article{Helfrich2014,
  title = {Selective {{Modulation}} of {{Interhemispheric Functional Connectivity}} by {{HD}}-{{tACS Shapes Perception}}},
  author = {Helfrich, Randolph F. and Knepper, Hannah and Nolte, Guido and Str{\"u}ber, Daniel and Rach, Stefan and Herrmann, Christoph S. and Schneider, Till R. and Engel, Andreas K.},
  editor = {Jensen, Ole},
  year = {2014},
  month = dec,
  volume = {12},
  pages = {e1002031},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002031},
  abstract = {Oscillatory neuronal synchronization between cortical areas has been suggested to constitute a flexible mechanism to coordinate information flow in the human cerebral cortex. However, it remains unclear whether synchronized neuronal activity merely represents an epiphenomenon or whether it is causally involved in the selective gating of information. Here, we combined bilateral high-density transcranial alternating current stimulation (HD-tACS) at 40 Hz with simultaneous electroencephalographic (EEG) recordings to study immediate electrophysiological effects during the selective entrainment of oscillatory gamma-band signatures. We found that interhemispheric functional connectivity was modulated in a predictable, phase-specific way: In-phase stimulation enhanced synchronization, anti-phase stimulation impaired functional coupling. Perceptual correlates of these connectivity changes were found in an ambiguous motion task, which strongly support the functional relevance of long-range neuronal coupling. Additionally, our results revealed a decrease in oscillatory alpha power in response to the entrainment of gamma band signatures. This finding provides causal evidence for the antagonistic role of alpha and gamma oscillations in the parieto-occipital cortex and confirms that the observed gamma band modulations were physiological in nature. Our results demonstrate that synchronized cortical network activity across several spatiotemporal scales is essential for conscious perception and cognition.},
  file = {/Users/qualia/Documents/Papers/Helfrich et al. - 2014 - Selective Modulation of Interhemispheric Functiona.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {12}
}

@article{Helmstaedter2007,
  title = {Reconstruction of an Average Cortical Column in Silico},
  author = {Helmstaedter, M. and {de Kock}, C.P.J. and Feldmeyer, D. and Bruno, R.M. and Sakmann, B.},
  year = {2007},
  month = oct,
  volume = {55},
  pages = {193--203},
  issn = {01650173},
  doi = {10.1016/j.brainresrev.2007.07.011},
  abstract = {The characterization of individual neurons by Golgi and Cajal has been the basis of neuroanatomy for a century. A new challenge is to anatomically describe, at cellular resolution, complete local circuits that can drive behavior. In this essay, we review the possibilities to obtain a model cortical column by using in vitro and in vivo pair recordings, followed by anatomical reconstructions of the projecting and target cells. These pairs establish connection modules that eventually may be useful to synthesize an average cortical column in silico. Together with data on sensory evoked neuronal activity measured in vivo, this will allow to model the anatomical and functional cellular basis of behavior based on more realistic assumptions than previously attempted.},
  file = {/Users/qualia/Documents/Papers/2007 - Helmstaedter et al. - Reconstruction of an average cortical column in silico.pdf},
  journal = {Brain Research Reviews},
  language = {en},
  number = {2}
}

@article{Henderson,
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  pages = {26},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  file = {/Users/qualia/Documents/Papers/Henderson et al. - Deep Reinforcement Learning that Matters.pdf},
  language = {en}
}

@article{Hendricks2015,
  title = {Neuroecology: {{Tuning Foraging Strategies}} to {{Environmental Variability}}},
  shorttitle = {Neuroecology},
  author = {Hendricks, Michael},
  year = {2015},
  month = jun,
  volume = {25},
  pages = {R498-R500},
  issn = {09609822},
  doi = {10.1016/j.cub.2015.04.042},
  file = {/Users/qualia/Documents/Papers/Hendricks - 2015 - Neuroecology Tuning Foraging Strategies to Enviro.pdf},
  journal = {Current Biology},
  language = {en},
  number = {12}
}

@article{Hennequin,
  title = {Characterizing Variability in Nonlinear, Recurrent Neuronal Networks},
  author = {Hennequin, Guillaume and Lengyel, Mate},
  pages = {17},
  abstract = {In this note, we develop semi-analytical techniques to obtain the full correlational structure of a stochastic network of nonlinear neurons described by rate variables. Under the assumption that pairs of membrane potentials are jointly Gaussian \textendash{} which they tend to be in large networks \textendash{} we obtain deterministic equations for the temporal evolution of the mean firing rates and the noise covariance matrix that can be solved straightforwardly given the network connectivity. We also obtain spike count statistics such as Fano factors and pairwise correlations, assuming doubly-stochastic action potential firing. Importantly, our theory does not require fluctuations to be small, and works for several biologically motivated, convex single-neuron nonlinearities.},
  file = {/Users/qualia/Documents/Papers/Hennequin and Lengyel - Characterizing variability in nonlinear, recurrent.pdf},
  language = {en}
}

@article{Hennequina,
  title = {Fast {{Sampling}}-{{Based Inference}} in {{Balanced Neuronal Networks}}},
  author = {Hennequin, Guillaume and Aitchison, Laurence and Lengyel, Mate},
  pages = {9},
  abstract = {Multiple lines of evidence support the notion that the brain performs probabilistic inference in multiple cognitive domains, including perception and decision making. There is also evidence that probabilistic inference may be implemented in the brain through the (quasi-)stochastic activity of neural circuits, producing samples from the appropriate posterior distributions, effectively implementing a Markov chain Monte Carlo algorithm. However, time becomes a fundamental bottleneck in such sampling-based probabilistic representations: the quality of inferences depends on how fast the neural circuit generates new, uncorrelated samples from its stationary distribution (the posterior). We explore this bottleneck in a simple, linear-Gaussian latent variable model, in which posterior sampling can be achieved by stochastic neural networks with linear dynamics. The well-known Langevin sampling (LS) recipe, so far the only sampling algorithm for continuous variables of which a neural implementation has been suggested, naturally fits into this dynamical framework. However, we first show analytically and through simulations that the symmetry of the synaptic weight matrix implied by LS yields critically slow mixing when the posterior is high-dimensional. Next, using methods from control theory, we construct and inspect networks that are optimally fast, and hence orders of magnitude faster than LS, while being far more biologically plausible. In these networks, strong \textendash{} but transient \textendash{} selective amplification of external noise generates the spatially correlated activity fluctuations prescribed by the posterior. Intriguingly, although a detailed balance of excitation and inhibition is dynamically maintained, detailed balance of Markov chain steps in the resulting sampler is violated, consistent with recent findings on how statistical irreversibility can overcome the speed limitation of random walks in other domains.},
  file = {/Users/qualia/Documents/Papers/2014 - Hennequin, Aitchison, Lengyel - Fast Sampling-Based Inference in Balanced Neuronal Networks.pdf;/Users/qualia/Documents/Papers/Hennequin et al. - Fast Sampling-Based Inference in Balanced Neuronal.pdf},
  language = {en}
}

@article{Hermes2015,
  title = {Stimulus {{Dependence}} of {{Gamma Oscillations}} in {{Human Visual Cortex}}},
  author = {Hermes, D. and Miller, K.J. and Wandell, B.A. and Winawer, J.},
  year = {2015},
  month = sep,
  volume = {25},
  pages = {2951--2959},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhu091},
  abstract = {A striking feature of some field potential recordings in visual cortex is a rhythmic oscillation within the gamma band (30\textendash{}80 Hz). These oscillations have been proposed to underlie computations in perception, attention, and information transmission. Recent studies of cortical field potentials, including human electrocorticography (ECoG), have emphasized another signal within the gamma band, a nonoscillatory, broadband signal, spanning 80\textendash{}200 Hz. It remains unclear under what conditions gamma oscillations are elicited in visual cortex, whether they are necessary and ubiquitous in visual encoding, and what relationship they have to nonoscillatory, broadband field potentials. We demonstrate that ECoG responses in human visual cortex (V1/V2/V3) can include robust narrowband gamma oscillations, and that these oscillations are reliably elicited by some spatial contrast patterns (luminance gratings) but not by others (noise patterns and many natural images). The gamma oscillations can be conspicuous and robust, but because they are absent for many stimuli, which observers can see and recognize, the oscillations are not necessary for seeing. In contrast, all visual stimuli induced broadband spectral changes in ECoG responses. Asynchronous neural signals in visual cortex, reflected in the broadband ECoG response, can support transmission of information for perception and recognition in the absence of pronounced gamma oscillations.},
  file = {/Users/qualia/Documents/Papers/Hermes et al. - 2015 - Stimulus Dependence of Gamma Oscillations in Human.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {9}
}

@article{Hermes2015a,
  title = {Gamma Oscillations in Visual Cortex: The Stimulus Matters},
  shorttitle = {Gamma Oscillations in Visual Cortex},
  author = {Hermes, Dora and Miller, Kai J. and Wandell, Brian A. and Winawer, Jonathan},
  year = {2015},
  month = feb,
  volume = {19},
  pages = {57--58},
  issn = {13646613},
  doi = {10.1016/j.tics.2014.12.009},
  file = {/Users/qualia/Documents/Papers/Hermes et al. - 2015 - Gamma oscillations in visual cortex the stimulus .pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {2}
}

@article{Hermundstad2011,
  title = {Learning, {{Memory}}, and the {{Role}} of {{Neural Network Architecture}}},
  author = {Hermundstad, Ann M. and Brown, Kevin S. and Bassett, Danielle S. and Carlson, Jean M.},
  editor = {Sporns, Olaf},
  year = {2011},
  month = jun,
  volume = {7},
  pages = {e1002063},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002063},
  abstract = {The performance of information processing systems, from artificial neural networks to natural neuronal ensembles, depends heavily on the underlying system architecture. In this study, we compare the performance of parallel and layered network architectures during sequential tasks that require both acquisition and retention of information, thereby identifying tradeoffs between learning and memory processes. During the task of supervised, sequential function approximation, networks produce and adapt representations of external information. Performance is evaluated by statistically analyzing the error in these representations while varying the initial network state, the structure of the external information, and the time given to learn the information. We link performance to complexity in network architecture by characterizing local error landscape curvature. We find that variations in error landscape structure give rise to tradeoffs in performance; these include the ability of the network to maximize accuracy versus minimize inaccuracy and produce specific versus generalizable representations of information. Parallel networks generate smooth error landscapes with deep, narrow minima, enabling them to find highly specific representations given sufficient time. While accurate, however, these representations are difficult to generalize. In contrast, layered networks generate rough error landscapes with a variety of local minima, allowing them to quickly find coarse representations. Although less accurate, these representations are easily adaptable. The presence of measurable performance tradeoffs in both layered and parallel networks has implications for understanding the behavior of a wide variety of natural and artificial learning systems.},
  file = {/Users/qualia/Documents/Papers/Hermundstad et al. - 2011 - Learning, Memory, and the Role of Neural Network A.PDF},
  journal = {PLoS Comput Biol},
  language = {en},
  number = {6}
}

@article{HertA$g2014,
  title = {Analytical Approximations of the Firing Rate of an Adaptive Exponential Integrate-and-Fire Neuron in the Presence of Synaptic Noise},
  author = {Hert{\~A}\textcurrency{}g, Loreen and Durstewitz, Daniel and Brunel, Nicolas},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00116},
  file = {/Users/qualia/Documents/Papers/HertÃ¤g et al. - 2014 - Analytical approximations of the firing rate of an.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Hertag2012,
  title = {An {{Approximation}} to the {{Adaptive Exponential Integrate}}-and-{{Fire Neuron Model Allows Fast}} and {{Predictive Fitting}} to {{Physiological Data}}},
  author = {Hert{\"a}g, Loreen and Hass, Joachim and Golovko, Tatiana and Durstewitz, Daniel},
  year = {2012},
  volume = {6},
  issn = {1662-5188},
  doi = {10.3389/fncom.2012.00062},
  file = {/Users/qualia/Documents/Papers/2012 - Hertäg et al. - An Approximation to the Adaptive Exponential Integrate-and-Fire Neuron Model Allows Fast and Predictive Fitting.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Hessel,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg},
  pages = {14},
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  file = {/Users/qualia/Documents/Papers/Hessel et al. - Rainbow Combining Improvements in Deep Reinforcem.pdf},
  language = {en}
}

@article{Hewitt1994,
  title = {A Computer Model of Amplitude-modulation Sensitivity of Single Units in the Inferior Colliculus},
  author = {Hewitt, Michael J. and Meddis, Ray},
  year = {1994},
  month = apr,
  volume = {95},
  pages = {2145--2159},
  issn = {0001-4966},
  doi = {10.1121/1.408676},
  file = {/Users/qualia/Documents/Papers/1994 - Hewitt, Meddis - A computer model of amplitude‐modulation sensitivity of single units in the inferior colliculus.pdf},
  journal = {The Journal of the Acoustical Society of America},
  language = {en},
  number = {4}
}

@article{Higgins2014,
  title = {Memory {{Maintenance}} in {{Synapses}} with {{Calcium}}-{{Based Plasticity}} in the {{Presence}} of {{Background Activity}}},
  author = {Higgins, David and Graupner, Michael and Brunel, Nicolas},
  editor = {Latham, Peter E.},
  year = {2014},
  month = oct,
  volume = {10},
  pages = {e1003834},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003834},
  abstract = {Most models of learning and memory assume that memories are maintained in neuronal circuits by persistent synaptic modifications induced by specific patterns of pre- and postsynaptic activity. For this scenario to be viable, synaptic modifications must survive the ubiquitous ongoing activity present in neural circuits in vivo. In this paper, we investigate the time scales of memory maintenance in a calcium-based synaptic plasticity model that has been shown recently to be able to fit different experimental data-sets from hippocampal and neocortical preparations. We find that in the presence of background activity on the order of 1 Hz parameters that fit pyramidal layer 5 neocortical data lead to a very fast decay of synaptic efficacy, with time scales of minutes. We then identify two ways in which this memory time scale can be extended: (i) the extracellular calcium concentration in the experiments used to fit the model are larger than estimated concentrations in vivo. Lowering extracellular calcium concentration to in vivo levels leads to an increase in memory time scales of several orders of magnitude; (ii) adding a bistability mechanism so that each synapse has two stable states at sufficiently low background activity leads to a further boost in memory time scale, since memory decay is no longer described by an exponential decay from an initial state, but by an escape from a potential well. We argue that both features are expected to be present in synapses in vivo. These results are obtained first in a single synapse connecting two independent Poisson neurons, and then in simulations of a large network of excitatory and inhibitory integrate-and-fire neurons. Our results emphasise the need for studying plasticity at physiological extracellular calcium concentration, and highlight the role of synaptic bi- or multistability in the stability of learned synaptic structures.},
  file = {/Users/qualia/Documents/Papers/2014 - Higgins, Graupner, Brunel - Memory maintenance in synapses with calcium-based plasticity in the presence of background activity.pdf;/Users/qualia/Documents/Papers/Higgins et al. - 2014 - Memory Maintenance in Synapses with Calcium-Based .pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Hinde1956,
  title = {Ethological {{Models}} and the {{Concept}} of '{{Drive}}'},
  author = {Hinde, R. A.},
  year = {1956},
  volume = {6},
  pages = {321--331},
  file = {/Users/qualia/Documents/Papers/Hinde - 1956 - Ethological Models and the Concept of 'Drive'.pdf},
  journal = {The British Journal for the Philosophy of Science},
  language = {en},
  number = {24}
}

@article{Hirase2004,
  title = {Calcium {{Dynamics}} of {{Cortical Astrocytic Networks In Vivo}}},
  author = {Hirase, Hajime and Qian, Lifen and Barth{\'o}, Peter and Buzs{\'a}ki, Gy{\"o}rgy},
  editor = {{Winfred Denk}},
  year = {2004},
  month = apr,
  volume = {2},
  pages = {e96},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0020096},
  file = {/Users/qualia/Documents/Papers/Hirase et al. - 2004 - Calcium Dynamics of Cortical Astrocytic Networks I.PDF},
  journal = {PLoS Biol},
  language = {en},
  number = {4}
}

@article{Histed2017,
  title = {Feedforward Inhibition Allows Input Summation to Vary in Recurrent Cortical Networks},
  author = {Histed, Mark H},
  year = {2017},
  month = jun,
  doi = {10.1101/109736},
  abstract = {Brain computations depend on how neurons transform inputs to spike outputs. Here, to understand input-output transformations in cortical networks, we recorded spiking responses from visual cortex (V1) of awake mice of either sex while pairing sensory stimuli with optogenetic perturbation of excitatory and parvalbumin-positive inhibitory neurons.  We found V1 neurons' average responses were primarily additive (linear). We used a recurrent cortical network model to determine if these data, as well as past observations of nonlinearity, could be described by a common circuit architecture.  The model showed cortical input-output transformations can be changed from linear to sublinear with moderate (\textasciitilde{}20\%) strengthening of connections between inhibitory neurons, but this change depends on the presence of feedforward inhibition.  Thus, feedforward inhibition, a common feature of cortical circuitry, enables networks to flexibly change their spiking responses via changes in recurrent connectivity.},
  file = {/Users/qualia/Documents/Papers/Histed - 2017 - Feedforward inhibition allows input summation to v.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Ho2007,
  title = {Self-Tuning Experience Weighted Attraction Learning in Games},
  author = {Ho, Teck H. and Camerer, Colin F. and Chong, Juin-Kuan},
  year = {2007},
  month = mar,
  volume = {133},
  pages = {177--198},
  issn = {00220531},
  doi = {10.1016/j.jet.2005.12.008},
  file = {/Users/qualia/Documents/Papers/2007 - Ho, Camerer, Chong - Self-tuning Experience-Weighted Attraction Learning in Games.pdf},
  journal = {Journal of Economic Theory},
  language = {en},
  number = {1}
}

@article{Hocker2019,
  title = {Myopic Control of Neural Dynamics},
  author = {Hocker, David and Park, Il Memming},
  year = {2019},
  volume = {15},
  pages = {24},
  abstract = {Manipulating the dynamics of neural systems through targeted stimulation is a frontier of research and clinical neuroscience; however, the control schemes considered for neural systems are mismatched for the unique needs of manipulating neural dynamics. An appropriate control method should respect the variability in neural systems, incorporating moment to moment ``input'' to the neural dynamics and behaving based on the current neural state, irrespective of the past trajectory. We propose such a controller under a nonlinear statespace feedback framework that steers one dynamical system to function as through it were another dynamical system entirely. This ``myopic'' controller is formulated through a novel variant of a model reference control cost that manipulates dynamics in a short-sighted manner that only sets a target trajectory of a single time step into the future (hence its myopic nature), which omits the need to pre-calculate a rigid and computationally costly neural feedback control solution. To demonstrate the breadth of this control's utility, two examples with distinctly different applications in neuroscience are studied. First, we show the myopic control's utility to probe the causal link between dynamics and behavior for cognitive processes by transforming a winner-take-all decision-making system to operate as a robust neural integrator of evidence. Second, an unhealthy motor-like system containing an unwanted betaoscillation spiral attractor is controlled to function as a healthy motor system, a relevant clinical example for neurological disorders.},
  file = {/Users/qualia/Documents/Papers/Hocker and Park - Myopic control of neural dynamics.pdf},
  language = {en},
  number = {3}
}

@article{Hodgkin1952,
  title = {A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve},
  author = {Hodgkin, A. L. and Huxley, A. F.},
  year = {1952},
  volume = {117},
  pages = {500--544},
  file = {/Users/qualia/Documents/Papers/jphysiol01442-0106.pdf},
  journal = {J Physiol},
  number = {4}
}

@article{Hofbauer,
  title = {{{EVOLUTIONARY GAME DYNAMICS}}},
  author = {Hofbauer, Josef and Sigmund, Karl},
  pages = {41},
  abstract = {Evolutionary game dynamics is the application of population dynamical methods to game theory. It has been introduced by evolutionary biologists, anticipated in part by classical game theorists. In this survey, we present an overview of the many brands of deterministic dynamical systems motivated by evolutionary game theory, including ordinary differential equations (and, in particular, the replicator equation), differential inclusions (the best response dynamics), difference equations (as, for instance, fictitious play) and reaction-diffusion systems. A recurrent theme (the so-called `folk theorem of evolutionary game theory') is the close connection of the dynamical approach with the Nash equilibrium, but we show that a static, equilibriumbased viewpoint is, on principle, unable to always account for the long-term behaviour of players adjusting their behaviour to maximise their payoff.},
  file = {/Users/qualia/Documents/Papers/2003 - Hofbauer, Sigmund - Evolutionary Game Dynamics.pdf},
  language = {en}
}

@article{Hofer2002,
  title = {Control and {{Plasticity}} of {{Intercellular Calcium Waves}} in {{Astrocytes}}: {{A Modeling Approach}}},
  shorttitle = {Control and {{Plasticity}} of {{Intercellular Calcium Waves}} in {{Astrocytes}}},
  author = {H{\"o}fer, Thomas and Venance, Laurent and Giaume, Christian},
  year = {2002},
  month = jun,
  volume = {22},
  pages = {4850--4859},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.22-12-04850.2002},
  file = {/Users/qualia/Documents/Papers/Höfer et al. - 2002 - Control and Plasticity of Intercellular Calcium Wa.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {12}
}

@article{Holgado2010,
  title = {Conditions for the {{Generation}} of {{Beta Oscillations}} in the {{Subthalamic Nucleus}}-{{Globus Pallidus Network}}},
  author = {Holgado, A. J. N. and Terry, J. R. and Bogacz, R.},
  year = {2010},
  month = sep,
  volume = {30},
  pages = {12340--12352},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0817-10.2010},
  file = {/Users/qualia/Documents/Papers/2010 - Holgado, Terry, Bogacz - Conditions for the Generation of Beta Oscillations in the Subthalamic Nucleus-Globus Pallidus Network.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@techreport{Holler-Rickauer2019,
  title = {Structure and Function of a Neocortical Synapse},
  author = {{Holler-Rickauer}, Simone and K{\"o}stinger, German and Martin, Kevan A.C. and Schuhknecht, Gregor F.P. and Stratford, Ken J.},
  year = {2019},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/2019.12.13.875971},
  abstract = {Thirty-four years since the small nervous system of the nematode
            C. elegans
            was manually reconstructed in the electron microscope (EM)
            1
            , `high-throughput' EM techniques now enable the dense reconstruction of neural circuits within increasingly large brain volumes at synaptic resolution
            2\textendash{}6
            . As with
            C. elegans
            , however, a key limitation for inferring brain function from neuronal wiring diagrams is that it remains unknown how the structure of a synapse seen in EM relates to its physiological transmission strength. Here, we related structure and function of the same synapses to bridge this gap: we combined paired whole-cell recordings of synaptically connected pyramidal neurons in slices of mouse somatosensory cortex with correlated light microscopy and high-resolution EM of all putative synaptic contacts between the neurons. We discovered a linear relationship between synapse size (postsynaptic density area) and synapse strength (excitatory postsynaptic potential amplitude), which provides an experimental foundation for assigning the actual physiological weights to synaptic connections seen in the EM. Furthermore, quantal analysis revealed that the number of vesicle release sites exceeded the number of anatomical synapses formed by a connection by a factor of at least 2.6, which challenges the current understanding of synaptic release in neocortex and suggests that neocortical synapses operate with multivesicular release, like hippocampal synapses
            7\textendash{}11
            . Thus, neocortical synapses are more complex computational devices and may modulate their strength more flexibly than previously thought, with the corollary that the canonical neocortical microcircuitry possesses significantly higher computational power than estimated by current models.},
  file = {/Users/qualia/Documents/Papers/Holler-Rickauer et al. - 2019 - Structure and function of a neocortical synapse.pdf},
  language = {en},
  type = {Preprint}
}

@article{Holmgren2001,
  title = {Coincident {{Spiking Activity Induces Long}}-{{Term Changes}} in {{Inhibition}} of {{Neocortical Pyramidal Cells}}},
  author = {Holmgren, Carl D. and Zilberter, Yuri},
  year = {2001},
  month = oct,
  volume = {21},
  pages = {8270--8277},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.21-20-08270.2001},
  file = {/Users/qualia/Documents/Papers/Holmgren and Zilberter - 2001 - Coincident Spiking Activity Induces Long-Term Chan.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {20}
}

@article{Holt1996,
  title = {Comparison of Discharge Variability in Vitro and in Vivo in Cat Visual Cortex Neurons},
  author = {Holt, G. R. and Softky, W. R. and Koch, C. and Douglas, R. J.},
  year = {1996},
  month = may,
  volume = {75},
  pages = {1806--1814},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1996.75.5.1806},
  file = {/Users/qualia/Documents/Papers/1996 - Holt et al. - Comparison of discharge variability in vitro and in vivo in cat visual cortex neurons.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Holt1997,
  title = {Shunting {{Inhibition Does Not Have}} a {{Divisive Effect}} on {{Firing Rates}}},
  author = {Holt, Gary R. and Koch, Christof},
  year = {1997},
  month = jul,
  volume = {9},
  pages = {1001--1013},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.5.1001},
  file = {/Users/qualia/Documents/Papers/1997 - Holt, Koch - Shunting inhibition does not have a divisive effect on firing rates.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Holt2014,
  title = {Origins and Suppression of Oscillations in a Computational Model of {{Parkinson}}'s Disease},
  author = {Holt, Abbey B. and Netoff, Theoden I.},
  year = {2014},
  month = dec,
  volume = {37},
  pages = {505--521},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-014-0523-7},
  abstract = {Efficacy of deep brain stimulation (DBS) for motor signs of Parkinson's disease (PD) depends in part on post-operative programming of stimulus parameters. There is a need for a systematic approach to tuning parameters based on patient physiology. We used a physiologically realistic computational model of the basal ganglia network to investigate the emergence of a 34 Hz oscillation in the PD state and its optimal suppression with DBS. Discrete time transfer functions were fit to post-stimulus time histograms (PSTHs) collected in open-loop, by simulating the pharmacological block of synaptic connections, to describe the behavior of the basal ganglia nuclei. These functions were then connected to create a mean-field model of the closed-loop system, which was analyzed to determine the origin of the emergent 34 Hz pathological oscillation. This analysis determined that the oscillation could emerge from the coupling between the globus pallidus external (GPe) and subthalamic nucleus (STN). When coupled, the two resonate with each other in the PD state but not in the healthy state. By characterizing how this oscillation is affected by subthreshold DBS pulses, we hypothesize that it is possible to predict stimulus frequencies capable of suppressing this oscillation. To characterize the response to the stimulus, we developed a new method for estimating phase response curves (PRCs) from population data. Using the population PRC we were able to predict frequencies that enhance and suppress the 34 Hz pathological oscillation. This provides a systematic approach to tuning DBS frequencies and could enable closed-loop tuning of stimulation parameters.},
  file = {/Users/qualia/Documents/Papers/2014 - Holt, Netoff - Origins and suppression of oscillations in a computational model of Parkinson’s disease.pdf;/Users/qualia/Documents/Papers/Holt and Netoff - 2014 - Origins and suppression of oscillations in a compu.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Holt2016,
  title = {Phasic {{Burst Stimulation}}: {{A Closed}}-{{Loop Approach}} to {{Tuning Deep Brain Stimulation Parameters}} for {{Parkinson}}'s {{Disease}}},
  shorttitle = {Phasic {{Burst Stimulation}}},
  author = {Holt, Abbey B. and Wilson, Dan and Shinn, Max and Moehlis, Jeff and Netoff, Theoden I.},
  editor = {Sporns, Olaf},
  year = {2016},
  month = jul,
  volume = {12},
  pages = {e1005011},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005011},
  file = {/Users/qualia/Documents/Papers/Holt et al. - 2016 - Phasic Burst Stimulation A Closed-Loop Approach t.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {7}
}

@article{Honegger2019,
  title = {Idiosyncratic Neural Coding and Neuromodulation of Olfactory Individuality in {{{\emph{Drosophila}}}}},
  author = {Honegger, Kyle S. and Smith, Matthew A.-Y. and Churgin, Matthew A. and Turner, Glenn C. and {de Bivort}, Benjamin L.},
  year = {2019},
  month = aug,
  pages = {201901623},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1901623116},
  abstract = {Innate behavioral biases and preferences can vary significantly among individuals of the same genotype. Though individuality is a fundamental property of behavior, it is not currently understood how individual differences in brain structure and physiology produce idiosyncratic behaviors. Here we present evidence for idiosyncrasy in olfactory behavior and neural responses in
              Drosophila
              . We show that individual female
              Drosophila
              from a highly inbred laboratory strain exhibit idiosyncratic odor preferences that persist for days. We used in vivo calcium imaging of neural responses to compare projection neuron (second-order neurons that convey odor information from the sensory periphery to the central brain) responses to the same odors across animals. We found that, while odor responses appear grossly stereotyped, upon closer inspection, many individual differences are apparent across antennal lobe (AL) glomeruli (compact microcircuits corresponding to different odor channels). Moreover, we show that neuromodulation, environmental stress in the form of altered nutrition, and activity of certain AL local interneurons affect the magnitude of interfly behavioral variability. Taken together, this work demonstrates that individual
              Drosophila
              exhibit idiosyncratic olfactory preferences and idiosyncratic neural responses to odors, and that behavioral idiosyncrasies are subject to neuromodulation and regulation by neurons in the AL.},
  file = {/Users/qualia/Documents/Papers/Honegger et al. - 2019 - Idiosyncratic neural coding and neuromodulation of.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en}
}

@article{Honey2017,
  title = {Switching between Internal and External Modes: {{A}} Multiscale Learning Principle},
  shorttitle = {Switching between Internal and External Modes},
  author = {Honey, Christopher J. and Newman, Ehren L. and Schapiro, Anna C.},
  year = {2017},
  month = dec,
  volume = {1},
  pages = {339--356},
  issn = {2472-1751},
  doi = {10.1162/NETN_a_00024},
  file = {/Users/qualia/Documents/Papers/Honey et al. - 2017 - Switching between internal and external modes A m.pdf},
  journal = {Network Neuroscience},
  language = {en},
  number = {4}
}

@article{Hong2011,
  title = {Conformists and Contrarians in a {{Kuramoto}} Model with Identical Natural Frequencies},
  author = {Hong, Hyunsuk and Strogatz, Steven H.},
  year = {2011},
  month = oct,
  volume = {84},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.84.046202},
  file = {/Users/qualia/Documents/Papers/2011 - Hong, Strogatz - Conformists and contrarians in a Kuramoto model with identical natural frequencies.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {4}
}

@article{Hong2012,
  title = {Mean-Field Behavior in Coupled Oscillators with Attractive and Repulsive Interactions},
  author = {Hong, Hyunsuk and Strogatz, Steven H.},
  year = {2012},
  month = may,
  volume = {85},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.85.056210},
  file = {/Users/qualia/Documents/Papers/2012 - Hong, Strogatz - Mean-field behavior in coupled oscillators with attractive and repulsive interactions.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Hong2016,
  title = {Phase Coherence Induced by Correlated Disorder},
  author = {Hong, Hyunsuk and O'Keeffe, Kevin P. and Strogatz, Steven H.},
  year = {2016},
  month = feb,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.022219},
  file = {/Users/qualia/Documents/Papers/Hong et al. - 2016 - Phase coherence induced by correlated disorder.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {2}
}

@article{Hoogland2009a,
  title = {Radially Expanding Transglial Calcium Waves in the Intact Cerebellum},
  author = {Hoogland, T. M. and Kuhn, B. and Gobel, W. and Huang, W. and Nakai, J. and Helmchen, F. and Flint, J. and Wang, S. S.-H.},
  year = {2009},
  month = mar,
  volume = {106},
  pages = {3496--3501},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0809269106},
  file = {/Users/qualia/Documents/Papers/Hoogland et al. - 2009 - Radially expanding transglial calcium waves in the 2.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {9}
}

@article{Hopcroft,
  title = {Foundations of {{Data Science}}},
  author = {Hopcroft, John and Kannan, Ravindran},
  pages = {414},
  file = {/Users/qualia/Documents/Papers/Hopcroft and Kannan - Foundations of Data Science.pdf},
  language = {en}
}

@article{Hopkins2005,
  title = {Attainability of Boundary Points under Reinforcement Learning},
  author = {Hopkins, Ed and Posch, Martin},
  year = {2005},
  month = oct,
  volume = {53},
  pages = {110--125},
  issn = {08998256},
  doi = {10.1016/j.geb.2004.08.002},
  abstract = {This paper investigates the properties of the most common form of reinforcement learning (the ``basic model'' of Erev and Roth, American Economic Review, 88, 848-881, 1998). Stochastic approximation theory has been used to analyse the local stability of fixed points under this learning process. However, as we show, when such points are on the boundary of the state space, for example, pure strategy equilibria, standard results from the theory of stochastic approximation do not apply. We offer what we believe to be the correct treatment of boundary points, and provide a new and more general result: this model of learning converges with zero probability to fixed points which are unstable under the Maynard Smith or adjusted version of the evolutionary replicator dynamics. For two player games these are the fixed points that are linearly unstable under the standard replicator dynamics.},
  file = {/Users/qualia/Documents/Papers/2005 - Hopkins, Posch - Attainability of boundary points under reinforcement learning.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1}
}

@article{Horvath2015,
  title = {Evidence That Transcranial Direct Current Stimulation ({{tDCS}}) Generates Little-to-No Reliable Neurophysiologic Effect beyond {{MEP}} Amplitude Modulation in Healthy Human Subjects: {{A}} Systematic Review},
  shorttitle = {Evidence That Transcranial Direct Current Stimulation ({{tDCS}}) Generates Little-to-No Reliable Neurophysiologic Effect beyond {{MEP}} Amplitude Modulation in Healthy Human Subjects},
  author = {Horvath, Jared Cooney and Forte, Jason D. and Carter, Olivia},
  year = {2015},
  month = jan,
  volume = {66},
  pages = {213--236},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2014.11.021},
  abstract = {Background: Transcranial direct current stimulation (tDCS) is a form of neuromodulation that is increasingly being utilized to examine and modify a number of cognitive and behavioral measures. The theoretical mechanisms by which tDCS generates these changes are predicated upon a rather large neurophysiological literature. However, a robust systematic review of this neurophysiological data has not yet been undertaken. 28 29 Keywords: 30 Transcranial direct current stimulation (tDCS) 31 Systematic review 32 Neurophysiology 33 Transcranial magnetic stimulation (TMS) 34 Event related potential (ERP) 35 Electroencephalography (EEG) Functional magnetic resonance imaging 36 (fMRI) 37 38
Methods: tDCS data in healthy adults (18\textendash{}50) from every neurophysiological outcome measure reported by at least two different research groups in the literature was collected. When possible, data was pooled and quantitatively analyzed to assess significance. When pooling was not possible, data was qualitatively compared to assess reliability.
Results: Of the 30 neurophysiological outcome measures reported by at least two different research groups, tDCS was found to have a reliable effect on only one: MEP amplitude. Interestingly, the magnitude of this effect has been significantly decreasing over the last 14 years.
Conclusion: Our systematic review does not support the idea that tDCS has a reliable neurophysiological effect beyond MEP amplitude modulation \textendash{} though important limitations of this review (and conclusion) are discussed. This work raises questions concerning the mechanistic foundations and general efficacy of this device \textendash{} the implications of which extend to the steadily increasing tDCS psychological literature.},
  file = {/Users/qualia/Documents/Papers/Horvath et al. - 2015 - Evidence that transcranial direct current stimulat.pdf},
  journal = {Neuropsychologia},
  language = {en}
}

@article{Horvath2015a,
  title = {Quantitative {{Review Finds No Evidence}} of {{Cognitive Effects}} in {{Healthy Populations From Single}}-Session {{Transcranial Direct Current Stimulation}} ({{tDCS}})},
  author = {Horvath, Jared Cooney and Forte, Jason D. and Carter, Olivia},
  year = {2015},
  month = may,
  volume = {8},
  pages = {535--550},
  issn = {1935861X},
  doi = {10.1016/j.brs.2015.01.400},
  abstract = {Background: Over the last 15-years, transcranial direct current stimulation (tDCS), a relatively novel form of neuromodulation, has seen a surge of popularity in both clinical and academic settings. Despite numerous claims suggesting that a single session of tDCS can modulate cognition in healthy adult populations (especially working memory and language production), the paradigms utilized and results reported in the literature are extremely variable. To address this, we conduct the largest quantitative review of the cognitive data to date.
Methods: Single-session tDCS data in healthy adults (18e50) from every cognitive outcome measure reported by at least two different research groups in the literature was collected. Outcome measures were divided into 4 broad categories: executive function, language, memory, and miscellaneous. To account for the paradigmatic variability in the literature, we undertook a three-tier analysis system; each with less-stringent inclusion criteria than the prior. Standard mean difference values with 95\% CIs were generated for included studies and pooled for each analysis.
Results: Of the 59 analyses conducted, tDCS was found to not have a significant effect on any e regardless of inclusion laxity. This includes no effect on any working memory outcome or language production task.
Conclusion: Our quantitative review does not support the idea that tDCS generates a reliable effect on cognition in healthy adults. Reasons for and limitations of this finding are discussed. This work raises important questions regarding the efficacy of tDCS, state-dependency effects, and future directions for this tool in cognitive research.},
  file = {/Users/qualia/Documents/Papers/Horvath et al. - 2015 - Quantitative Review Finds No Evidence of Cognitive.pdf},
  journal = {Brain Stimulation},
  language = {en},
  number = {3}
}

@article{Houtekamer1998,
  title = {Data {{Assimilation Using}} an {{Ensemble Kalman Filter Technique}}},
  author = {Houtekamer, P. L. and Mitchell, Herschel L.},
  year = {1998},
  month = mar,
  volume = {126},
  pages = {796--811},
  issn = {0027-0644, 1520-0493},
  doi = {10.1175/1520-0493(1998)126<0796:DAUAEK>2.0.CO;2},
  abstract = {The possibility of performing data assimilation using the flow-dependent statistics calculated from an ensemble of short-range forecasts (a technique referred to as ensemble Kalman filtering) is examined in an idealized environment. Using a three-level, quasigeostrophic, T21 model and simulated observations, experiments are performed in a perfect-model context. By using forward interpolation operators from the model state to the observations, the ensemble Kalman filter is able to utilize nonconventional observations.},
  file = {/Users/qualia/Documents/Papers/1998 - Houtekamer et al. - Data Assimilation Using an Ensemble Kalman Filter Technique.pdf;/Users/qualia/Documents/Papers/1998 - Service - Data Assimilation Using an Ensemble Kalman Filter Technique.pdf},
  journal = {Monthly Weather Review},
  language = {en},
  number = {3}
}

@article{Howard2019,
  title = {Numerical Cognition in Honeybees Enables Addition and Subtraction},
  author = {Howard, Scarlett R. and {Avargu{\`e}s-Weber}, Aurore and Garcia, Jair E. and Greentree, Andrew D. and Dyer, Adrian G.},
  year = {2019},
  month = feb,
  volume = {5},
  pages = {eaav0961},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aav0961},
  file = {/Users/qualia/Documents/Papers/Howard et al. - 2019 - Numerical cognition in honeybees enables addition .pdf},
  journal = {Science Advances},
  language = {en},
  number = {2}
}

@article{Hsieh2010,
  title = {``{{Brain}}-Reading'' of Perceived Colors Reveals a Feature Mixing Mechanism Underlying Perceptual Filling-in in Cortical Area {{V1}}},
  author = {Hsieh, Po-Jang and Tse, Peter U.},
  year = {2010},
  month = sep,
  volume = {31},
  pages = {1395--1407},
  issn = {10659471},
  doi = {10.1002/hbm.20946},
  abstract = {Visual filling-in occurs when a retinally stabilized object undergoes perceptual fading. As the term ``filling-in'' implies, it is commonly believed that information about the apparently vanished object is lost and replaced solely by information arising from the surrounding background. Here we report multivoxel pattern analysis fMRI data that challenge this long-held belief. When subjects view blue disks on a red background while fixating, the stimulus and background appear to turn a uniform purple upon perceptual fading, suggesting that a feature mixing mechanism may underlie color fillingin. We find that ensemble fMRI signals in retinotopic visual areas reliably predict (i) which of three colors a subject reports seeing; (ii) whether a subject is in a perceptually filled-in state or not; and (iii) furthermore, while subjects are in the perceptual state of filling-in, the BOLD signal activation pattern in the sub-areas of V1 corresponding to the location of the blue disks behaves as if subjects are in fact viewing a perceptually mixed color (purple), rather than the color of the disks (blue) or the color of the background (red). These results imply that the mechanism of filling-in in stimuli in which figure and background surfaces are equated is a process of ``feature mixing'', not ``feature replacement''. These data indicate that feature mixing may involve cortical areas as early as V1. Hum Brain Mapp 31:1395\textendash{}1407, 2010. VC 2010 Wiley-Liss, Inc.},
  file = {/Users/qualia/Documents/Papers/2010 - Hsieh, Tse - Brain-reading of perceived colors reveals a feature mixing mechanism underlying perceptual filling-in in cortical ar.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {9}
}

@article{Hsu2004,
  title = {Quantifying Variability in Neural Responses and Its Application for the Validation of Model Predictions},
  author = {Hsu, Anne and Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric},
  year = {2004},
  month = may,
  volume = {15},
  pages = {91--109},
  issn = {0954-898X, 1361-6536},
  doi = {10.1088/0954-898X/15/2/002},
  abstract = {A rate code assumes that a neuron's response is completely characterized by its time-varying mean firing rate. This assumption has successfully described neural responses in many systems. The noise in rate coding neurons can be quantified by the coherence function or the correlation coefficient between the neuron's deterministic time-varying mean rate and noise corrupted single spike trains. Because of the finite data size, the mean rate cannot be known exactly and must be approximated. We introduce novel unbiased estimators for the measures of coherence and correlation which are based on the extrapolation of the signal to noise ratio in the neural response to infinite data size. We then describe the application of these estimates to the validation of the class of stimulus\textendash{}response models that assume that the mean firing rate captures all the information embedded in the neural response. We explain how these quantifiers can be used to separate response prediction errors that are due to inaccurate model assumptions from errors due to noise inherent in neuronal spike trains.},
  file = {/Users/qualia/Documents/Papers/2004 - Hsu, Borst, Theunissen - Quantifying variability in neural responses and its application for the validation of model predictions.pdf},
  journal = {Network: Computation in Neural Systems},
  language = {en},
  number = {2}
}

@article{Hu2016,
  title = {Painful {{Issues}} in {{Pain Prediction}}},
  author = {Hu, Li and Iannetti, Gian Domenico},
  year = {2016},
  month = apr,
  volume = {39},
  pages = {212--220},
  issn = {01662236},
  doi = {10.1016/j.tins.2016.01.004},
  file = {/Users/qualia/Documents/Papers/Hu and Iannetti - 2016 - Painful Issues in Pain Prediction.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {4}
}

@article{Huang1998,
  title = {The Empirical Mode Decomposition and the {{Hilbert}} Spectrum for Nonlinear and Non-Stationary Time Series Analysis},
  author = {Huang, Norden E. and Shen, Zheng and Long, Steven R. and Wu, Manli C. and Shih, Hsing H. and Zheng, Quanan and Yen, Nai-Chyuan and Tung, Chi Chao and Liu, Henry H.},
  year = {1998},
  month = mar,
  volume = {454},
  pages = {903--995},
  issn = {1471-2946},
  doi = {10.1098/rspa.1998.0193},
  file = {/Users/qualia/Documents/Papers/1998 - Branch et al. - (1998) N. E. Huang, et.al. (1998) The Empirical Mode Decomposition and the Hilbert Spectrum for Nonlinear and Non.pdf},
  journal = {Proceedings of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
  language = {en},
  number = {1971}
}

@inproceedings{Huang2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  month = jul,
  pages = {2261--2269},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.243},
  file = {/Users/qualia/Documents/Papers/Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@article{Huber2014,
  title = {Investigation of the Neurovascular Coupling in Positive and Negative {{BOLD}} Responses in Human Brain at {{7T}}},
  author = {Huber, Laurentius and Goense, Jozien and Kennerley, Aneurin J. and Ivanov, Dimo and Krieger, Steffen N. and Lepsien, J{\"o}ran and Trampel, Robert and Turner, Robert and M{\"o}ller, Harald E.},
  year = {2014},
  month = aug,
  volume = {97},
  pages = {349--362},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2014.04.022},
  file = {/Users/qualia/Documents/Papers/2014 - Huber et al. - Investigation of the neurovascular coupling in positive and negative BOLD responses in human brain at 7T.pdf;/Users/qualia/Documents/Papers/Huber et al. - 2014 - Investigation of the neurovascular coupling in pos.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Hughes1997,
  title = {Intrinsic Exploration in Animals: Motives and Measurement},
  shorttitle = {Intrinsic Exploration in Animals},
  author = {Hughes, Robert N},
  year = {1997},
  month = dec,
  volume = {41},
  pages = {213--226},
  issn = {03766357},
  doi = {10.1016/S0376-6357(97)00055-7},
  abstract = {Intrinsic exploration involves exploratory acts that are not instrumental in achieving any particular goal other than performance of the acts themselves. Of the theories proposed to account for the motivation of intrinsic exploration in animals, concepts of exploratory drive, optimal arousal and fear have featured prominently. But since no single approach has adequate explanatory or predictive power, it is probably sufficient to go no further than accept that organisms may have some type of `need' for sensory change which can be satisfied mainly by intrinsic exploration. Attempts to measure the phenomenon in the laboratory can be divided into forced tests in which locomotion and other motor responses are recorded in animals placed into a totally novel environments, and free tests involving measurements of active choices of differing degrees of novelty. Because of the difficulty of distinguishing between extrinsic and intrinsic exploration with activity indices, tests of free exploration are always preferable. These include novelty-related location preferences (including spontaneous alternation and responses to brightness change), object exploration and learning for exploratory rewards all of which can be viewed as reasonably valid measures of intrinsic exploration to a greater or lesser extent. \textcopyright{} 1997 Elsevier Science B.V.},
  file = {/Users/qualia/Documents/Papers/Hughes - 1997 - Intrinsic exploration in animals motives and meas.pdf},
  journal = {Behavioural Processes},
  language = {en},
  number = {3}
}

@article{Hughes2019,
  title = {Wave {{Physics}} as an {{Analog Recurrent Neural Network}}},
  author = {Hughes, Tyler W. and Williamson, Ian A. D. and Minkov, Momchil and Fan, Shanhui},
  year = {2019},
  month = apr,
  abstract = {Analog machine learning hardware platforms promise to be faster and more energy-efficient than their digital counterparts. Wave physics, as found in acoustics and optics, is a natural candidate for building analog processors for time-varying signals. Here we identify a mapping between the dynamics of wave physics, and the computation in recurrent neural networks. This mapping indicates that physical wave systems can be trained to learn complex features in temporal data, using standard training techniques for neural networks. As a demonstration, we show that an inversely-designed inhomogeneous medium can perform vowel classification on raw audio data by simple propagation of waves through such a medium, achieving performance that is comparable to a standard digital implementation of a recurrent neural network. These findings pave the way for a new class of analog machine learning platforms, capable of fast and efficient processing of information in its native domain.},
  archivePrefix = {arXiv},
  eprint = {1904.12831},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Hughes et al. - 2019 - Wave Physics as an Analog Recurrent Neural Network.pdf},
  journal = {arXiv:1904.12831 [physics]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Physics - Computational Physics,Physics - Optics},
  language = {en},
  primaryClass = {physics}
}

@article{Huh2017,
  title = {Gradient {{Descent}} for {{Spiking Neural Networks}}},
  author = {Huh, Dongsung and Sejnowski, Terrence J.},
  year = {2017},
  month = jun,
  abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast ({$\approx$} millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory XOR task over extended duration ({$\approx$} second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as the behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
  archivePrefix = {arXiv},
  eprint = {1706.04698},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Huh and Sejnowski - 2017 - Gradient Descent for Spiking Neural Networks.pdf},
  journal = {arXiv:1706.04698 [cs, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{Humphries2006,
  title = {A {{Physiologically Plausible Model}} of {{Action Selection}} and {{Oscillatory Activity}} in the {{Basal Ganglia}}},
  author = {Humphries, M. D. and Stewart, R. D. and Gurney, K. N.},
  year = {2006},
  month = dec,
  volume = {26},
  pages = {12921--12942},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3486-06.2006},
  file = {/Users/qualia/Documents/Papers/2006 - Humphries, Stewart, Gurney - A Physiologically Plausible Model of Action Selection and Oscillatory Activity in the Basal Ganglia.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {50}
}

@article{Humphries2009,
  title = {Dopamine-Modulated Dynamic Cell Assemblies Generated by the {{GABAergic}} Striatal Microcircuit},
  author = {Humphries, Mark D. and Wood, Ric and Gurney, Kevin},
  year = {2009},
  month = oct,
  volume = {22},
  pages = {1174--1188},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.07.018},
  abstract = {The striatum, the principal input structure of the basal ganglia, is crucial to both motor control and learning. It receives convergent input from all over the neocortex, hippocampal formation, amygdala and thalamus, and is the primary recipient of dopamine in the brain. Within the striatum is a GABAergic microcircuit that acts upon these inputs, formed by the dominant medium-spiny projection neurons (MSNs) and fast-spiking interneurons (FSIs). There has been little progress in understanding the computations it performs, hampered by the non-laminar structure that prevents identification of a repeating canonical microcircuit. We here begin the identification of potential dynamically-defined computational elements within the striatum. We construct a new three-dimensional model of the striatal microcircuit's connectivity, and instantiate this with our dopamine-modulated neuron models of the MSNs and FSIs. A new model of gap junctions between the FSIs is introduced and tuned to experimental data. We introduce a novel multiple spike-train analysis method, and apply this to the outputs of the model to find groups of synchronised neurons at multiple time-scales. We find that, with realistic in vivo background input, small assemblies of synchronised MSNs spontaneously appear, consistent with experimental observations, and that the number of assemblies and the time-scale of synchronisation is strongly dependent on the simulated concentration of dopamine. We also show that feed-forward inhibition from the FSIs counter-intuitively increases the firing rate of the MSNs. Such small cell assemblies forming spontaneously only in the absence of dopamine may contribute to motor control problems seen in humans and animals following a loss of dopamine cells.},
  file = {/Users/qualia/Documents/Papers/2009 - Humphries, Wood, Gurney - Dopamine-modulated dynamic cell assemblies generated by the GABAergic striatal microcircuit.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {8}
}

@article{Humphries2010,
  title = {Reconstructing the {{Three}}-{{Dimensional GABAergic Microcircuit}} of the {{Striatum}}},
  author = {Humphries, Mark D. and Wood, Ric and Gurney, Kevin},
  editor = {Friston, Karl J.},
  year = {2010},
  month = nov,
  volume = {6},
  pages = {e1001011},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1001011},
  abstract = {A system's wiring constrains its dynamics, yet modelling of neural structures often overlooks the specific networks formed by their neurons. We developed an approach for constructing anatomically realistic networks and reconstructed the GABAergic microcircuit formed by the medium spiny neurons (MSNs) and fast-spiking interneurons (FSIs) of the adult rat striatum. We grew dendrite and axon models for these neurons and extracted probabilities for the presence of these neurites as a function of distance from the soma. From these, we found the probabilities of intersection between the neurites of two neurons given their inter-somatic distance, and used these to construct three-dimensional striatal networks. The MSN dendrite models predicted that half of all dendritic spines are within 100mm of the soma. The constructed networks predict distributions of gap junctions between FSI dendrites, synaptic contacts between MSNs, and synaptic inputs from FSIs to MSNs that are consistent with current estimates. The models predict that to achieve this, FSIs should be at most 1\% of the striatal population. They also show that the striatum is sparsely connected: FSI-MSN and MSN-MSN contacts respectively form 7\% and 1.7\% of all possible connections. The models predict two striking network properties: the dominant GABAergic input to a MSN arises from neurons with somas at the edge of its dendritic field; and FSIs are interconnected on two different spatial scales: locally by gap junctions and distally by synapses. We show that both properties influence striatal dynamics: the most potent inhibition of a MSN arises from a region of striatum at the edge of its dendritic field; and the combination of local gap junction and distal synaptic networks between FSIs sets a robust input-output regime for the MSN population. Our models thus intimately link striatal micro-anatomy to its dynamics, providing a biologically grounded platform for further study.},
  file = {/Users/qualia/Documents/Papers/2010 - Humphries, Wood, Gurney - Reconstructing the Three-Dimensional GABAergic Microcircuit of the Striatum.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {11}
}

@article{Humphries2012,
  title = {Network Effects of Subthalamic Deep Brain Stimulation Drive a Unique Mixture of Responses in Basal Ganglia Output: {{STN DBS}} Causes Mix of Output Responses},
  shorttitle = {Network Effects of Subthalamic Deep Brain Stimulation Drive a Unique Mixture of Responses in Basal Ganglia Output},
  author = {Humphries, Mark D. and Gurney, Kevin},
  year = {2012},
  month = jul,
  volume = {36},
  pages = {2240--2251},
  issn = {0953816X},
  doi = {10.1111/j.1460-9568.2012.08085.x},
  abstract = {Deep brain stimulation (DBS) is a remarkably successful treatment for the motor symptoms of Parkinson's disease. High-frequency stimulation of the subthalamic nucleus (STN) within the basal ganglia is a main clinical target, but the physiological mechanisms of therapeutic STN DBS at the cellular and network level are unclear. We set out to begin to address the hypothesis that a mixture of responses in the basal ganglia output nuclei, combining regularized firing and inhibition, is a key contributor to the effectiveness of STN DBS. We used our computational model of the complete basal ganglia circuit to show how such a mixture of responses in basal ganglia output naturally arises from the network effects of STN DBS. We replicated the diversification of responses recorded in a primate STN DBS study to show that the model's predicted mixture of responses is consistent with therapeutic STN DBS. We then showed how this `mixture of response' perspective suggests new ideas for DBS mechanisms: first, that the therapeutic frequency of STN DBS is above 100 Hz because the diversification of responses exhibits a step change above this frequency; and second, that optogenetic models of direct STN stimulation during DBS have proven therapeutically ineffective because they do not replicate the mixture of basal ganglia output responses evoked by electrical DBS.},
  file = {/Users/qualia/Documents/Papers/2012 - Humphries, Gurney - Network effects of subthalamic deep brain stimulation drive a unique mixture of responses in basal ganglia ou.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{Hung2019,
  title = {Optimizing Agent Behavior over Long Time Scales by Transporting Value},
  author = {Hung, Chia-Chun and Lillicrap, Timothy and Abramson, Josh and Wu, Yan and Mirza, Mehdi and Carnevale, Federico and Ahuja, Arun and Wayne, Greg},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {5223},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13073-w},
  file = {/Users/qualia/Documents/Papers/Hung et al. - 2019 - Optimizing agent behavior over long time scales by.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Hunt2014,
  title = {Hierarchical Competitions Subserving Multi-Attribute Choice},
  author = {Hunt, Laurence T and Dolan, Raymond J and Behrens, Timothy E J},
  year = {2014},
  month = nov,
  volume = {17},
  pages = {1613--1622},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3836},
  file = {/Users/qualia/Documents/Papers/Hunt et al. - 2014 - Hierarchical competitions subserving multi-attribu.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Hurzook,
  title = {Visual Motion Processing and Perceptual Decision Making},
  author = {Hurzook, Aziz and Trujillo, Oliver and Eliasmith, Chris},
  pages = {1},
  abstract = {We present a biologically plausible spiking network model of visual motion processing and perceptual decision making, independent of the number of choice alternatives. As an application we simulate the two-alternative forced choice (2AFC) task.},
  file = {/Users/qualia/Documents/Papers/2010 - Hurzook, Trujillo, Eliasmith - Visual motion processing and perceptual decision making.pdf;/Users/qualia/Documents/Papers/2010 - Hurzook, Trujillo, Eliasmith - Visual motion processing and perceptual decision making(2).pdf},
  language = {en}
}

@incollection{Hutchison2010,
  title = {Indirectly {{Encoding Neural Plasticity}} as a {{Pattern}} of {{Local Rules}}},
  booktitle = {From {{Animals}} to {{Animats}} 11},
  author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Risi, Sebastian and Stanley, Kenneth O.},
  editor = {Doncieux, St{\'e}phane and Girard, Beno{\^i}t and Guillot, Agn{\`e}s and Hallam, John and Meyer, Jean-Arcady and Mouret, Jean-Baptiste},
  year = {2010},
  volume = {6226},
  pages = {533--543},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-15193-4_50},
  abstract = {Biological brains can adapt and learn from past experience. In neuroevolution, i.e. evolving artificial neural networks (ANNs), one way that agents controlled by ANNs can evolve the ability to adapt is by encoding local learning rules. However, a significant problem with most such approaches is that local learning rules for every connection in the network must be discovered separately. This paper aims to show that learning rules can be effectively indirectly encoded by extending the Hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) method. Adaptive HyperNEAT is introduced to allow not only patterns of weights across the connectivity of an ANN to be generated by a function of its geometry, but also patterns of arbitrary learning rules. Several such adaptive models with different levels of generality are explored and compared. The long-term promise of the new approach is to evolve large-scale adaptive ANNs, which is a major goal for neuroevolution.},
  file = {/Users/qualia/Documents/Papers/Hutchison et al. - 2010 - Indirectly Encoding Neural Plasticity as a Pattern.pdf},
  isbn = {978-3-642-15192-7 978-3-642-15193-4},
  language = {en}
}

@article{Huth2012,
  title = {A {{Continuous Semantic Space Describes}} the {{Representation}} of {{Thousands}} of {{Object}} and {{Action Categories}} across the {{Human Brain}}},
  author = {Huth, Alexander G. and Nishimoto, Shinji and Vu, An T. and Gallant, Jack L.},
  year = {2012},
  month = dec,
  volume = {76},
  pages = {1210--1224},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.10.014},
  abstract = {Humans can see and name thousands of distinct object and action categories, so it is unlikely that each category is represented in a distinct brain area. A more efficient scheme would be to represent categories as locations in a continuous semantic space mapped smoothly across the cortical surface. To search for such a space, we used functional magnetic resonance imaging (fMRI) to measure human brain activity evoked by natural movies. We then used voxel-wise models to examine the cortical representation of 1705 object and action categories. The first few dimensions of the underlying semantic space were recovered from the fit models by principal components analysis. Projection of the recovered semantic space onto cortical flat maps shows that semantic selectivity is organized into smooth gradients that cover much of visual and nonvisual cortex. Furthermore, both the recovered semantic space and the cortical organization of the space are shared across different individuals.},
  file = {/Users/qualia/Documents/Papers/2012 - Huth et al. - A continuous semantic space describes the representation of thousands of object and action categories across the hu.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Huth2016,
  title = {Natural Speech Reveals the Semantic Maps That Tile Human Cerebral Cortex},
  author = {Huth, Alexander G. and {de Heer}, Wendy A. and Griffiths, Thomas L. and Theunissen, Fr{\'e}d{\'e}ric E. and Gallant, Jack L.},
  year = {2016},
  month = apr,
  volume = {532},
  pages = {453--458},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17637},
  file = {/Users/qualia/Documents/Papers/Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf},
  journal = {Nature},
  language = {en},
  number = {7600}
}

@article{Hutt2010,
  title = {Activity Spread and Breathers Induced by Finite Transmission Speeds in Two-Dimensional Neural Fields},
  author = {Hutt, Axel and Rougier, Nicolas},
  year = {2010},
  month = nov,
  volume = {82},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.82.055701},
  file = {/Users/qualia/Documents/Papers/2010 - Hutt, Rougier - Activity spread and breathers induced by finite transmission speeds in two-dimensional neural fields.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Huys2009,
  title = {Smoothing of, and {{Parameter Estimation}} from, {{Noisy Biophysical Recordings}}},
  author = {Huys, Quentin J. M. and Paninski, Liam},
  editor = {Friston, Karl J.},
  year = {2009},
  month = may,
  volume = {5},
  pages = {e1000379},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000379},
  abstract = {Biophysically detailed models of single cells are difficult to fit to real data. Recent advances in imaging techniques allow simultaneous access to various intracellular variables, and these data can be used to significantly facilitate the modelling task. These data, however, are noisy, and current approaches to building biophysically detailed models are not designed to deal with this. We extend previous techniques to take the noisy nature of the measurements into account. Sequential Monte Carlo (``particle filtering'') methods, in combination with a detailed biophysical description of a cell, are used for principled, model-based smoothing of noisy recording data. We also provide an alternative formulation of smoothing where the neural nonlinearities are estimated in a non-parametric manner. Biophysically important parameters of detailed models (such as channel densities, intercompartmental conductances, input resistances, and observation noise) are inferred automatically from noisy data via expectation-maximisation. Overall, we find that model-based smoothing is a powerful, robust technique for smoothing of noisy biophysical data and for inference of biophysical parameters in the face of recording noise.},
  file = {/Users/qualia/Documents/Papers/2009 - Huys, Paninski - Smoothing of, and parameter estimation from, noisy biophysical recordings.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {5}
}

@article{Hyafil2015,
  title = {Speech Encoding by Coupled Cortical Theta and Gamma Oscillations},
  author = {Hyafil, Alexandre and Fontolan, Lorenzo and Kabdebon, Claire and Gutkin, Boris and Giraud, Anne-Lise},
  year = {2015},
  month = may,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.06213},
  file = {/Users/qualia/Documents/Papers/Hyafil et al. - 2015 - Speech encoding by coupled cortical theta and gamm.pdf},
  journal = {eLife},
  language = {en}
}

@article{Ibata2008,
  title = {Rapid {{Synaptic Scaling Induced}} by {{Changes}} in {{Postsynaptic Firing}}},
  author = {Ibata, Keiji and Sun, Qian and Turrigiano, Gina G.},
  year = {2008},
  month = mar,
  volume = {57},
  pages = {819--826},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.02.031},
  abstract = {Homeostatic synaptic scaling adjusts a neuron's excitatory synaptic strengths up or down to compensate for perturbations in activity. Little is known about the molecular pathway(s) involved, nor is it clear which aspect of ``activity''\textemdash{}local synaptic signaling, postsynaptic firing, or large-scale changes in network activity\textemdash{}is required to induce synaptic scaling. Here, we selectively block either postsynaptic firing in individual neurons or a fraction of presynaptic inputs, while optically monitoring changes in synaptic strength. We find that synaptic scaling is rapidly induced by block of postsynaptic firing, but not by local synaptic blockade, and is mediated through a drop in somatic calcium influx, reduced activation of CaMKIV, and an increase in transcription. Cortical neurons thus homeostatically adjust synaptic strengths in response to changes in their own firing rate, a mechanism with the computational advantage of efficiently normalizing synaptic strengths without interfering with synapse-specific mechanisms of information storage.},
  file = {/Users/qualia/Documents/Papers/2008 - Ibata, Sun, Turrigiano - Rapid Synaptic Scaling Induced by Changes in Postsynaptic Firing.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Iemi2017,
  title = {Spontaneous {{Neural Oscillations Bias Perception}} by {{Modulating Baseline Excitability}}},
  author = {Iemi, Luca and Chaumon, Maximilien and Crouzet, S{\'e}bastien M. and Busch, Niko A.},
  year = {2017},
  month = jan,
  volume = {37},
  pages = {807--819},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1432-16.2017},
  file = {/Users/qualia/Documents/Papers/Iemi et al. - 2017 - Spontaneous Neural Oscillations Bias Perception by.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {4}
}

@article{Inagaki2017,
  title = {Discrete Attractor Dynamics Underlying Selective Persistent Activity in Frontal Cortex},
  author = {Inagaki, Hidehiko K. and Fontolan, Lorenzo and Romani, Sandro and Svoboda, Karel},
  year = {2017},
  month = oct,
  doi = {10.1101/203448},
  abstract = {Short-term memories link events separated in time, such as past sensation and future actions. Short-term memories are correlated with selective persistent activity, which can be maintained over seconds. In a delayed response task that requires short-term memory, neurons in mouse anterior lateral motor cortex (ALM) show persistent activity that instructs future actions. To elucidate the mechanisms underlying this persistent activity we combined intracellular and extracellular electrophysiology with optogenetic perturbations and network modeling. During the delay epoch, both membrane potential and population activity of ALM neurons funneled towards discrete endpoints related to specific movement directions. These endpoints were robust to transient shifts in ALM activity caused by optogenetic perturbations. Perturbations occasionally switched the population dynamics to the other endpoint, followed by incorrect actions. Our results are consistent with discrete attractor dynamics underlying short-term memory related to motor planning.},
  file = {/Users/qualia/Documents/Papers/Inagaki et al. - 2017 - Discrete attractor dynamics underlying selective p.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Inglis2001,
  title = {An Information Primacy Model of Exploratory and Foraging Behaviour},
  author = {Inglis, Ian R. and Langton, Steve and Forkman, Bj{\"o}rn and Lazarus, John},
  year = {2001},
  month = sep,
  volume = {62},
  pages = {543--557},
  issn = {00033472},
  doi = {10.1006/anbe.2001.1780},
  file = {/Users/qualia/Documents/Papers/Inglis et al. - 2001 - An information primacy model of exploratory and fo.pdf},
  journal = {Animal Behaviour},
  language = {en},
  number = {3}
}

@article{Insanally2019,
  title = {Spike-Timing-Dependent Ensemble Encoding by Non-Classically Responsive Cortical Neurons},
  author = {Insanally, Michele N and Carcea, Ioana and Field, Rachel E and Rodgers, Chris C and DePasquale, Brian and Rajan, Kanaka and DeWeese, Michael R and Albanna, Badr F and Froemke, Robert C},
  year = {2019},
  month = jan,
  volume = {8},
  issn = {2050-084X},
  doi = {10.7554/eLife.42409},
  file = {/Users/qualia/Documents/Papers/Insanally et al. - 2019 - Spike-timing-dependent ensemble encoding by non-cl.pdf;/Users/qualia/Documents/Papers/Insanally et al. - 2019 - Spike-timing-dependent ensemble encoding by non-cl.pdf},
  journal = {eLife},
  language = {en}
}

@article{Insanally2019a,
  title = {Spike-Timing-Dependent Ensemble Encoding by Non-Classically Responsive Cortical Neurons},
  author = {Insanally, Michele N and Carcea, Ioana and Field, Rachel E and Rodgers, Chris C and DePasquale, Brian and Rajan, Kanaka and DeWeese, Michael R and Albanna, Badr F and Froemke, Robert C},
  year = {2019},
  month = jan,
  volume = {8},
  pages = {e42409},
  issn = {2050-084X},
  doi = {10.7554/eLife.42409},
  abstract = {Neurons recorded in behaving animals often do not discernibly respond to sensory input and are not overtly task-modulated. These non-classically responsive neurons are difficult to interpret and are typically neglected from analysis, confounding attempts to connect neural activity to perception and behavior. Here, we describe a trial-by-trial, spike-timing-based algorithm to reveal the coding capacities of these neurons in auditory and frontal cortex of behaving rats. Classically responsive and non-classically responsive cells contained significant information about sensory stimuli and behavioral decisions. Stimulus category was more accurately represented in frontal cortex than auditory cortex, via ensembles of non-classically responsive cells coordinating the behavioral meaning of spike timings on correct but not error trials. This unbiased approach allows the contribution of all recorded neurons \textendash{} particularly those without obvious task-related, trial-averaged firing rate modulation \textendash{} to be assessed for behavioral relevance on single trials.
          , 
            Neurons encode information in the form of electrical signals called spikes. Certain neurons increase the rate at which they produce spikes under specific circumstances, e.g., whenever an animal hears a particular sound. These neurons are said to be 'classically responsive'. But not all neurons behave in this way. Others produce spikes at a variable rate that does not obviously relate to the animal's behavior. These neurons are said to be 'non-classically responsive'. They are often omitted from analyses, despite typically outnumbering their classically responsive counterparts. So, what are these neurons doing?
            To find out, Insanally et al. trained rats to respond to sounds. The animals learned to poke their nose into a window whenever they heard a specific tone, and to avoid responding whenever they heard any other tone. As the rats performed the task, Insanally et al. recorded from neurons in two areas of the brain, the frontal cortex and the auditory cortex. A computer then analyzed the activity of individual neurons during each trial.
            As expected, the firing rate of non-classically responsive cells did not relate to the animals' behavior. But the timing of this firing did. The interval between spikes contained information about which tone the animals had heard and/or how they had responded. The cells worked together in groups to encode this information. Over the course of each trial, every neuron in the group varied the interval between its spikes. Eventually, the group reached a consensus, with all neurons using the same interval to represent information relevant to the task. Groups of neurons in the frontal cortex encoded more information about the category of the tone than those in the auditory cortex.
            By including all neurons \textendash{} both classically and non-classically responsive \textendash{} this model offers a more comprehensive view of how neural activity relates to behavior. This may in turn help us understand the variable and complex neural activity seen in people with sensory and cognitive disorders.},
  file = {/Users/qualia/Documents/Papers/Insanally et al. - 2019 - Spike-timing-dependent ensemble encoding by non-cl 2.pdf},
  journal = {eLife},
  language = {en}
}

@article{Ioffe2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = feb,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archivePrefix = {arXiv},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Ioffe, Szedegy - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf;/Users/qualia/Documents/Papers/Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf},
  journal = {arXiv:1502.03167 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Ishida,
  title = {Do {{We Need Zero Training Loss A}} Er {{Achieving Zero Training Error}}?},
  author = {Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and Sugiyama, Masashi},
  pages = {27},
  abstract = {Overparameterized deep networks have the capacity to memorize training data with zero training error. Even a er memorization, the training loss continues to approach zero, making the model overcon dent and the test performance degraded. Since existing regularizers do not directly aim to avoid zero training loss, they o en fail to maintain a moderate level of training loss, ending up with a too small or too large loss. We propose a direct solution called ooding that intentionally prevents further reduction of the training loss when it reaches a reasonably small value, which we call the ooding level. Our approach makes the loss oat around the ooding level by doing mini-batched gradient descent as usual but gradient ascent if the training loss is below the ooding level. is can be implemented with one line of code, and is compatible with any stochastic optimizer and other regularizers. With ooding, the model will continue to ``random walk'' with the same non-zero training loss, and we expect it to dri into an area with a at loss landscape that leads to be er generalization. We experimentally show that ooding improves performance and as a byproduct, induces a double descent curve of the test loss.},
  file = {/Users/qualia/Documents/Papers/Ishida et al. - Do We Need Zero Training Loss A er Achieving Zero .pdf},
  language = {en}
}

@article{Ishii2002,
  title = {Control of Exploitation\textendash{}Exploration Meta-Parameter in Reinforcement Learning},
  author = {Ishii, Shin and Yoshida, Wako and Yoshimoto, Junichiro},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {665--687},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00056-4},
  abstract = {In reinforcement learning, the duality between exploitation and exploration has long been an important issue. This paper presents a new method that controls the balance between exploitation and exploration. Our learning scheme is based on model-based reinforcement learning, in which the Bayes inference with forgetting effect estimates the state-transition probability of the environment. The balance parameter, which corresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change. When applied to maze tasks, our method successfully obtains good controls by adapting to environmental changes. Recently, Usher et al. [60] has suggested that noradrenergic neurons in the locus coeruleus may control the exploitation-exploration balance in a real brain and that the balance may correspond to the level of animal's selective attention. According to this scenario, we also discuss a possible implementation in the brain.},
  file = {/Users/qualia/Documents/Papers/Ishii et al. - 2002 - Control of exploitation–exploration meta-parameter.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Ishii2002a,
  title = {Control of Exploitation\textendash{}Exploration Meta-Parameter in Reinforcement Learning},
  author = {Ishii, Shin and Yoshida, Wako and Yoshimoto, Junichiro},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {665--687},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00056-4},
  abstract = {In reinforcement learning, the duality between exploitation and exploration has long been an important issue. This paper presents a new method that controls the balance between exploitation and exploration. Our learning scheme is based on model-based reinforcement learning, in which the Bayes inference with forgetting effect estimates the state-transition probability of the environment. The balance parameter, which corresponds to the randomness in action selection, is controlled based on variation of action results and perception of environmental change. When applied to maze tasks, our method successfully obtains good controls by adapting to environmental changes. Recently, Usher et al. [60] has suggested that noradrenergic neurons in the locus coeruleus may control the exploitation-exploration balance in a real brain and that the balance may correspond to the level of animal's selective attention. According to this scenario, we also discuss a possible implementation in the brain.},
  file = {/Users/qualia/Documents/Papers/Ishii et al. - 2002 - Control of exploitation–exploration meta-parameter 2.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Ispolatov2015,
  title = {Chaos in High-Dimensional Dynamical Systems},
  author = {Ispolatov, Iaroslav and Doebeli, Michael and Allende, Sebastian and Madhok, Vaibhav},
  year = {2015},
  month = dec,
  volume = {5},
  issn = {2045-2322},
  doi = {10.1038/srep12506},
  abstract = {For general dissipative dynamical systems we study what fraction of solutions exhibit chaotic behavior depending on the dimensionality \$d\$ of the phase space. We find that a system of \$d\$ globally coupled ODE's with quadratic and cubic non-linearities with random coefficients and initial conditions, the probability of a trajectory to be chaotic increases universally from \$\textbackslash{}sim 10\^\{-5\} - 10\^\{-4\}\$ for \$d=3\$ to essentially one for \$d\textbackslash{}sim 50\$. In the limit of large \$d\$, the invariant measure of the dynamical systems exhibits universal scaling that depends on the degree of non-linearity but does not depend on the choice of coefficients, and the largest Lyapunov exponent converges to a universal scaling limit. Using statistical arguments, we provide analytical explanations for the observed scaling and for the probability of chaos.},
  archivePrefix = {arXiv},
  eprint = {1410.6403},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2014 - Ispolatov et al. - Chaos in high-dimensional dynamical systems.pdf;/Users/qualia/Documents/Papers/2015 - Ispolatov et al. - Chaos in high-dimensional dissipative dynamical systems.pdf;/Users/qualia/Documents/Papers/Ispolatov et al. - 2015 - Chaos in high-dimensional dissipative dynamical sy.pdf;/Users/qualia/Documents/Papers/Ispolatov et al. - 2015 - Chaos in high-dimensional dynamical systems.pdf},
  journal = {Scientific Reports},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Chaotic Dynamics},
  language = {en},
  number = {1}
}

@article{Itti2009,
  title = {Bayesian Surprise Attracts Human Attention},
  author = {Itti, Laurent and Baldi, Pierre},
  year = {2009},
  month = jun,
  volume = {49},
  pages = {1295--1306},
  issn = {00426989},
  doi = {10.1016/j.visres.2008.09.007},
  abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatiotemporal scales, modalities, and levels of abstraction.},
  file = {/Users/qualia/Documents/Papers/2009 - Itti, Baldi - Bayesian surprise attracts human attention.pdf},
  journal = {Vision Research},
  language = {en},
  number = {10}
}

@article{Ivanov,
  title = {Axonal {{Conduction Velocity Impacts Neuronal Network Oscillations}}},
  author = {Ivanov, Vladimir A and Polykretis, Ioannis E and Michmizos, Konstantinos P},
  pages = {4},
  abstract = {Increasing experimental evidence suggests that axonal action potential conduction velocity is a highly adaptive parameter in the adult central nervous system. Yet, the effects of this newfound plasticity on global brain dynamics is poorly understood. In this work, we analyzed oscillations in biologically plausible neuronal networks with different conduction velocity distributions. Changes of \dbend\dbend\dbend\dbend\dbend\dbend{} - \dbend\dbend\dbend\dbend\dbend\dbend{} (ms) in network mean signal transmission time resulted in substantial network oscillation frequency changes ranging in \dbend\dbend\dbend\dbend\dbend\dbend{} - \dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend\dbend{} (Hz). Our results suggest that changes in axonal conduction velocity may significantly affect both the frequency and synchrony of brain rhythms, which have well established connections to learning, memory, and other cognitive processes.},
  file = {/Users/qualia/Documents/Papers/Ivanov et al. - Axonal Conduction Velocity Impacts Neuronal Networ.pdf;/Users/qualia/Downloads/lee2011.pdf},
  language = {en}
}

@article{Iyer2013,
  title = {The {{Influence}} of {{Synaptic Weight Distribution}} on {{Neuronal Population Dynamics}}},
  author = {Iyer, Ramakrishnan and Menon, Vilas and Buice, Michael and Koch, Christof and Mihalas, Stefan},
  editor = {Sporns, Olaf},
  year = {2013},
  month = oct,
  volume = {9},
  pages = {e1003248},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003248},
  abstract = {The manner in which different distributions of synaptic weights onto cortical neurons shape their spiking activity remains open. To characterize a homogeneous neuronal population, we use the master equation for generalized leaky integrateand-fire neurons with shot-noise synapses. We develop fast semi-analytic numerical methods to solve this equation for either current or conductance synapses, with and without synaptic depression. We show that its solutions match simulations of equivalent neuronal networks better than those of the Fokker-Planck equation and we compute bounds on the network response to non-instantaneous synapses. We apply these methods to study different synaptic weight distributions in feed-forward networks. We characterize the synaptic amplitude distributions using a set of measures, called tail weight numbers, designed to quantify the preponderance of very strong synapses. Even if synaptic amplitude distributions are equated for both the total current and average synaptic weight, distributions with sparse but strong synapses produce higher responses for small inputs, leading to a larger operating range. Furthermore, despite their small number, such synapses enable the network to respond faster and with more stability in the face of external fluctuations.},
  file = {/Users/qualia/Documents/Papers/2013 - Iyer et al. - The Influence of Synaptic Weight Distribution on Neuronal Population Dynamics.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Izhikevich2003,
  title = {Relating {{STDP}} to {{BCM}}},
  author = {Izhikevich, Eugene M. and Desai, Niraj S.},
  year = {2003},
  month = jul,
  volume = {15},
  pages = {1511--1523},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603321891783},
  file = {/Users/qualia/Documents/Papers/2003 - Izhikevich, Desai - Relating STDP to BCM.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Izhikevich2003a,
  title = {Simple Model of Spiking Neurons},
  author = {Izhikevich, E.M.},
  year = {2003},
  month = nov,
  volume = {14},
  pages = {1569--1572},
  issn = {1045-9227},
  doi = {10.1109/TNN.2003.820440},
  abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin\textendash{}Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
  file = {/Users/qualia/Documents/Papers/2003 - Izhikevich - Simple model of spiking neurons.pdf},
  journal = {IEEE Transactions on Neural Networks},
  language = {en},
  number = {6}
}

@article{Izhikevich2004,
  title = {Which {{Model}} to {{Use}} for {{Cortical Spiking Neurons}}?},
  author = {Izhikevich, E.M.},
  year = {2004},
  month = sep,
  volume = {15},
  pages = {1063--1070},
  issn = {1045-9227},
  doi = {10.1109/TNN.2004.832719},
  abstract = {We discuss the biological plausibility and computational efficiency of some of the most useful models of spiking and bursting neurons. We compare their applicability to large-scale simulations of cortical neural networks.},
  file = {/Users/qualia/Documents/Papers/2004 - Izhikevich - Which model to use for cortical spiking neurons.pdf},
  journal = {IEEE Transactions on Neural Networks},
  language = {en},
  number = {5}
}

@book{Izhikevich2006,
  title = {Dynamical {{Systems}} in {{Neuroscience}}: {{The Geometry}} of {{Excitability}} and {{Bursting}}},
  shorttitle = {Dynamical {{Systems}} in {{Neuroscience}}},
  author = {Izhikevich, Eugene M.},
  year = {2006},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/2526.001.0001},
  file = {/Users/qualia/Documents/Papers/2004 - Izhikevich - Dynamical Systems in Neuroscience.pdf},
  isbn = {978-0-262-27607-8},
  language = {en}
}

@article{Izhikevich2006a,
  title = {Polychronization: {{Computation}} with {{Spikes}}},
  shorttitle = {Polychronization},
  author = {Izhikevich, Eugene M.},
  year = {2006},
  month = feb,
  volume = {18},
  pages = {245--282},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976606775093882},
  file = {/Users/qualia/Documents/Papers/2006 - Izhikevich - Polychronization computation with spikes.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@techreport{Izhikevich2018,
  title = {Measuring the Average Power of Neural Oscillations},
  author = {Izhikevich, Liz and Gao, Richard and Peterson, Erik and Voytek, Bradley},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/441626},
  abstract = {Background: Neural oscillations are often quantified as average power relative to a cognitive, perceptual, and/or behavioral task. This is commonly done using Fourier-based techniques, such as Welch's method for estimating the power spectral density, and/or by estimating narrowband oscillatory power across trials, conditions, and/or groups. The core assumption underlying these approaches is that the mean is an appropriate measure of central tendency. Despite the importance of this assumption, it has not been rigorously tested. New method: We introduce extensions of common approaches that are better suited for the physiological reality of how neural oscillations often manifest: as nonstationary, high-power bursts, rather than sustained rhythms. Log-transforming, or taking the median power, significantly reduces erroneously inflated power estimates.
Results: Analyzing 101 participants' worth of human electrophysiology, totaling 3,560 channels and over 40 hours data, we show that, in all cases examined, spectral power is not Gaussian distributed. This is true even when oscillations are prominent and sustained, such as visual cortical alpha. Power across time, at every frequency, is characterized by a substantial long tail, which implies that estimates of average power are skewed toward large, infrequent high-power oscillatory bursts.
Comparison with existing methods: In a simulated event-related experiment we show how introducing just a few high-power oscillatory bursts, as seen in real data, can, perhaps erroneously, cause significant differences between conditions using traditional methods. These erroneous effects are substantially reduced with our new methods.
Conclusions: These results call into question the validity of common statistical practices in neural oscillation research.},
  file = {/Users/qualia/Documents/Papers/Izhikevich et al. - 2018 - Measuring the average power of neural oscillations.pdf},
  language = {en},
  type = {Preprint}
}

@article{Jackson2014,
  title = {Reversal of Theta Rhythm Flow through Intact Hippocampal Circuits},
  author = {Jackson, Jesse and Amilhon, B{\'e}n{\'e}dicte and Goutagny, Romain and Bott, Jean-Bastien and Manseau, Fr{\'e}d{\'e}ric and Kortleven, Christian and Bressler, Steven L and Williams, Sylvain},
  year = {2014},
  month = oct,
  volume = {17},
  pages = {1362--1370},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3803},
  file = {/Users/qualia/Documents/Papers/Jackson et al. - 2014 - Reversal of theta rhythm flow through intact hippo.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {10}
}

@article{Jacobs2014,
  title = {Hippocampal Theta Oscillations Are Slower in Humans than in Rodents: Implications for Models of Spatial Navigation and Memory},
  shorttitle = {Hippocampal Theta Oscillations Are Slower in Humans than in Rodents},
  author = {Jacobs, Joshua},
  year = {2014},
  month = feb,
  volume = {369},
  pages = {20130304},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2013.0304},
  file = {/Users/qualia/Documents/Papers/Jacobs - 2014 - Hippocampal theta oscillations are slower in human.pdf},
  journal = {Phil. Trans. R. Soc. B},
  language = {en},
  number = {1635}
}

@article{Jaderberg,
  title = {Decoupled {{Neural Interfaces}} Using {{Synthetic Gradients}}},
  author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  pages = {16},
  abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass \textendash{} amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
  file = {/Users/qualia/Documents/Papers/Jaderberg et al. - Decoupled Neural Interfaces using Synthetic Gradie.pdf},
  language = {en}
}

@article{Jaderberg2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present Population Based Training (PBT), a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archivePrefix = {arXiv},
  eprint = {1711.09846},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf},
  journal = {arXiv:1711.09846 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Jadi2012,
  title = {Location-{{Dependent Effects}} of {{Inhibition}} on {{Local Spiking}} in {{Pyramidal Neuron Dendrites}}},
  author = {Jadi, Monika and Polsky, Alon and Schiller, Jackie and Mel, Bartlett W.},
  editor = {Gutkin, Boris S.},
  year = {2012},
  month = jun,
  volume = {8},
  pages = {e1002550},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002550},
  abstract = {Cortical computations are critically dependent on interactions between pyramidal neurons (PNs) and a menagerie of inhibitory interneuron types. A key feature distinguishing interneuron types is the spatial distribution of their synaptic contacts onto PNs, but the location-dependent effects of inhibition are mostly unknown, especially under conditions involving active dendritic responses. We studied the effect of somatic vs. dendritic inhibition on local spike generation in basal dendrites of layer 5 PNs both in neocortical slices and in simple and detailed compartmental models, with equivalent results: somatic inhibition divisively suppressed the amplitude of dendritic spikes recorded at the soma while minimally affecting dendritic spike thresholds. In contrast, distal dendritic inhibition raised dendritic spike thresholds while minimally affecting their amplitudes. On-the-path dendritic inhibition modulated both the gain and threshold of dendritic spikes depending on its distance from the spike initiation zone. Our findings suggest that cortical circuits could assign different mixtures of gain vs. threshold inhibition to different neural pathways, and thus tailor their local computations, by managing their relative activation of soma- vs. dendrite-targeting interneurons.},
  file = {/Users/qualia/Documents/Papers/2012 - Jadi et al. - Location-dependent effects of inhibition on local spiking in pyramidal neuron dendrites.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {6}
}

@article{Jadi2014,
  title = {Cortical Oscillations Arise from Contextual Interactions That Regulate Sparse Coding},
  author = {Jadi, M. P. and Sejnowski, T. J.},
  year = {2014},
  month = may,
  volume = {111},
  pages = {6780--6785},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1405300111},
  file = {/Users/qualia/Documents/Papers/2015 - Jadi, Sejnowski - Correction for Jadi and Sejnowski, Cortical oscillations arise from contextual interactions that regulate spars.pdf;/Users/qualia/Documents/Papers/Jadi and Sejnowski - 2014 - Cortical oscillations arise from contextual intera 2.pdf;/Users/qualia/Documents/Papers/Jadi and Sejnowski - 2014 - Cortical oscillations arise from contextual intera 3.pdf;/Users/qualia/Documents/Papers/Jadi and Sejnowski - 2014 - Cortical oscillations arise from contextual intera.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {18}
}

@article{Jadi2014a,
  title = {Regulating {{Cortical Oscillations}} in an {{Inhibition}}-{{Stabilized Network}}},
  author = {Jadi, Monika P. and Sejnowski, Terrence J.},
  year = {2014},
  month = may,
  volume = {102},
  pages = {830--842},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2014.2313113},
  abstract = {Understanding the anatomical and functional brain. Empirically testing such predictions is still challenging, architecture of the brain is essential for designing neurally and implementing the proposed coding and communication inspired intelligent systems. Theoretical and empirical studies strategies in neuromorphic systems could assist in our suggest a role for narrowband oscillations in shaping the understanding of the biological system.},
  file = {/Users/qualia/Documents/Papers/2014 - Jadi, Sejnowski - Regulating Cortical Oscillation in an Inhibition-Stabilized Network.pdf;/Users/qualia/Documents/Papers/Jadi and Sejnowski - 2014 - Regulating Cortical Oscillations in an Inhibition-.pdf},
  journal = {Proceedings of the IEEE},
  language = {en},
  number = {5}
}

@article{Jaegle2019,
  title = {Visual Novelty, Curiosity, and Intrinsic Reward in Machine Learning and the Brain},
  author = {Jaegle, Andrew and Mehrpour, Vahid and Rust, Nicole},
  year = {2019},
  volume = {1901.02478},
  pages = {13},
  abstract = {A strong preference for novelty emerges in infancy and is prevalent across the animal kingdom. When incorporated into reinforcement-based machine learning algorithms, visual novelty can act as an intrinsic reward signal that vastly increases the efficiency of exploration and expedites learning, particularly in situations where external rewards are difficult to obtain. Here we review parallels between recent developments in novelty-driven machine learning algorithms and our understanding of how visual novelty is computed and signaled in the primate brain. We propose that in the visual system, novelty representations are not configured with the principal goal of detecting novel objects, but rather with the broader goal of flexibly generalizing novelty information across different states in the service of driving novelty-based learning.},
  file = {/Users/qualia/Documents/Papers/Jaegle et al. - Visual novelty, curiosity, and intrinsic reward in.pdf},
  journal = {Arxiv},
  language = {en}
}

@article{Jakel2009,
  title = {Does {{Cognitive Science Need Kernels}}?},
  author = {J{\"a}kel, Frank and Sch{\"o}lkopf, Bernhard and Wichmann, Felix A.},
  year = {2009},
  month = sep,
  volume = {13},
  pages = {381--388},
  issn = {13646613},
  doi = {10.1016/j.tics.2009.06.002},
  file = {/Users/qualia/Documents/Papers/2009 - Jäkel, Schölkopf, Wichmann - Does cognitive science need kernels.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {9}
}

@article{Jang2017,
  title = {Task-Specific Feature Extraction and Classification of {{fMRI}} Volumes Using a Deep Neural Network Initialized with a Deep Belief Network: {{Evaluation}} Using Sensorimotor Tasks},
  shorttitle = {Task-Specific Feature Extraction and Classification of {{fMRI}} Volumes Using a Deep Neural Network Initialized with a Deep Belief Network},
  author = {Jang, Hojin and Plis, Sergey M. and Calhoun, Vince D. and Lee, Jong-Hwan},
  year = {2017},
  month = jan,
  volume = {145},
  pages = {314--328},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2016.04.003},
  abstract = {Feedforward deep neural networks (DNN), artificial neural networks with multiple hidden layers, have recently demonstrated a record-breaking performance in multiple areas of applications in computer vision and speech processing. Following the success, DNNs have been applied to neuroimaging modalities including functional/structural magnetic resonance imaging (MRI) and positron-emission tomography data. However, no study has explicitly applied DNNs to 3D wholebrain fMRI volumes and thereby extracted hidden volumetric representations of fMRI that are discriminative for a task performed as the fMRI volume was acquired. Our study applied fully connected feedforward DNN to fMRI volumes collected in four sensorimotor tasks (i.e., left-hand clenching, right-hand clenching, auditory attention, and visual stimulus) undertaken by 12 healthy participants. Using a leave-one-subject-out cross-validation scheme, a restricted Boltzmann machinebased deep belief network was pretrained and used to initialize weights of the DNN. The pretrained DNN was fine-tuned while systematically controlling weight-sparsity levels across hidden layers. Optimal weight-sparsity levels were determined from a minimum validation error rate of fMRI volume classification. Minimum error rates (mean  standard deviation; \%) of 6.9 ( 3.8) were obtained from the three-layer DNN with the sparsest condition of weights across the three hidden layers. These error rates were even lower than the error rates from the single-layer network (9.4 {$\pm$} 4.6) and the two-layer network (7.4 {$\pm$} 4.1). The estimated DNN weights showed spatial patterns that are remarkably task-specific, particularly in the higher layers. The output values of the third hidden layer represented distinct patterns/codes of the 3D whole-brain fMRI volume and encoded the information of the tasks as evaluated from representational similarity analysis. Our reported findings show the ability of the DNN to classify a single fMRI volume based on the extraction of hidden representations of fMRI volumes associated with tasks across multiple hidden layers. Our study may be beneficial to the automatic classification/diagnosis of neuropsychiatric and neurological diseases and prediction of disease severity and recovery in (pre-) clinical settings using fMRI volumes without requiring an estimation of activation patterns or ad hoc statistical evaluation.},
  file = {/Users/qualia/Documents/Papers/Jang et al. - 2017 - Task-specific feature extraction and classificatio.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Japyassu2017,
  title = {Extended Spider Cognition},
  author = {Japyass{\'u}, Hilton F. and Laland, Kevin N.},
  year = {2017},
  month = may,
  volume = {20},
  pages = {375--395},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-017-1069-7},
  abstract = {There is a tension between the conception of cognition as a central nervous system (CNS) process and a view of cognition as extending towards the body or the contiguous environment. The centralised conception requires large or complex nervous systems to cope with complex environments. Conversely, the extended conception involves the outsourcing of information processing to the body or environment, thus making fewer demands on the processing power of the CNS. The evolution of extended cognition should be particularly favoured among small, generalist predators such as spiders, and here, we review the literature to evaluate the fit of empirical data with these contrasting models of cognition. Spiders do not seem to be cognitively limited, displaying a large diversity of learning processes, from habituation to contextual learning, including a sense of numerosity. To tease apart the central from the extended cognition, we apply the mutual manipulability criterion, testing the existence of reciprocal causal links between the putative elements of the system. We conclude that the web threads and configurations are integral parts of the cognitive systems. The extension of cognition to the web helps to explain some puzzling features of spider behaviour and seems to promote evolvability within the group, enhancing innovation through cognitive connectivity to variable habitat features.},
  file = {/Users/qualia/Documents/Papers/Japyassú and Laland - 2017 - Extended spider cognition.pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {3}
}

@article{Jastorff2009,
  title = {Visual {{Learning Shapes}} the {{Processing}} of {{Complex Movement Stimuli}} in the {{Human Brain}}},
  author = {Jastorff, J. and Kourtzi, Z. and Giese, M. A.},
  year = {2009},
  month = nov,
  volume = {29},
  pages = {14026--14038},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3070-09.2009},
  file = {/Users/qualia/Documents/Papers/2009 - Jastorff, Kourtzi, Giese - Visual learning shapes the processing of complex movement stimuli in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {44}
}

@article{Jefferys,
  title = {Sharpening {{Ockham}}'s {{Razor}} on a {{Bayesian Strop}} ({{Key}} Terms: {{Bayes}}' Theorem {{Ockham}}'s Razor)},
  author = {Jefferys, William H and Berger, James O},
  pages = {14},
  file = {/Users/qualia/Documents/Papers/1991 - Je, Berger - Sharpening Ockham ' s Razor on a Bayesian Strop ( Key terms Bayes ' theorem Ockham ' s razor ).pdf},
  language = {en}
}

@article{Jefferys1992,
  title = {Ockham's {{Razor}} and {{Bayesian Analysis}}},
  author = {Jefferys, William H. and {work(s):}, James O. Berger Reviewed},
  year = {1992},
  volume = {80},
  pages = {64--72},
  file = {/Users/qualia/Documents/Papers/1992 - Jefferys, Berger - Ockham's Razor and Bayesian Analysis.pdf},
  journal = {American Scientist},
  language = {en},
  number = {1}
}

@article{Jehiel,
  title = {Analogy Based Expectation Equilibrium},
  author = {Jehiel, P},
  pages = {38},
  abstract = {It is assumed that players bundle nodes in which other players must move into analogy classes, and players only have expectations about the average behavior in every class. A solution concept is proposed for multi-stage games with perfect information: at every node players choose best-responses to their analogy-based expectations, and expectations are correct on average over those various nodes pooled together into the same analogy classes. The approach is applied to a variety of games. It is shown that a player may bene{\TH}t from having a coarse analogy partitioning. And for simple analogy partitioning, (1) initial cooperation followed by an end opportunistic behavior may emerge in the {\TH}nitely repeated prisoner's dilemma (or in the centipede game), (2) an agreement need not be reached immediately in bargaining games with complete information.},
  file = {/Users/qualia/Documents/Papers/2005 - Jehiel - Analogy-based expectation equilibrium.pdf},
  language = {en}
}

@article{Jensen1998,
  title = {An {{Oscillatory Short}}-{{Term Memory Buffer Model Can Account}} for {{Data}} on the {{Sternberg Task}}},
  author = {Jensen, Ole and Lisman, John E.},
  year = {1998},
  month = dec,
  volume = {18},
  pages = {10688--10699},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.18-24-10688.1998},
  file = {/Users/qualia/Documents/Papers/1998 - Jensen, Lisman - An oscillatory short-term memory buffer model can account for data on the Sternberg task.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {24}
}

@article{Jensen2002,
  title = {Oscillations in the {{Alpha Band}} (9-12 {{Hz}}) {{Increase}} with {{Memory Load}} during {{Retention}} in a {{Short}}-Term {{Memory Task}}},
  author = {Jensen, O.},
  year = {2002},
  month = aug,
  volume = {12},
  pages = {877--882},
  issn = {14602199},
  doi = {10.1093/cercor/12.8.877},
  file = {/Users/qualia/Documents/Papers/2002 - Jensen et al. - Oscillations in the alpha band (9-12 Hz) increase with memory load during retention in a short-term memory task.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {8}
}

@article{Jensen2016,
  title = {Discriminating {{Valid}} from {{Spurious Indices}} of {{Phase}}-{{Amplitude Coupling}}},
  author = {Jensen, Ole and Spaak, Eelke and Park, Hyojin},
  year = {2016},
  volume = {3},
  pages = {ENEURO.0334-16.2016},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0334-16.2016},
  abstract = {Recently there has been a strong interest in cross-frequency coupling, the interaction between neuronal oscillations in different frequency bands. In particular, measures quantifying the coupling between the phase of slow oscillations and the amplitude of fast oscillations have been applied to a wide range of data recorded from animals and humans. Some of the measures applied to detect phase-amplitude coupling have been criticized for being sensitive to nonsinusoidal properties of the oscillations and thus spuriously indicate the presence of coupling. While such instances of spurious identification of coupling have been observed, in this commentary we give concrete examples illustrating cases when the identification of cross-frequency coupling can be trusted. These examples are based on control analyses and empirical observations rather than signal-processing tools. Finally, we provide concrete advice on how to determine when measures of phase-amplitude coupling can be considered trustworthy.},
  file = {/Users/qualia/Documents/Papers/Jensen et al. - 2016 - Discriminating Valid from Spurious Indices of Phas.pdf},
  journal = {eneuro},
  language = {en},
  number = {6}
}

@article{Jiang2019,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  month = dec,
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  archivePrefix = {arXiv},
  eprint = {1912.02178},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Jiang et al. - 2019 - Fantastic Generalization Measures and Where to Fin.pdf},
  journal = {arXiv:1912.02178 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Jimura2012,
  title = {Analyses of Regional-Average Activation and Multivoxel Pattern Information Tell Complementary Stories},
  author = {Jimura, Koji and Poldrack, Russell A.},
  year = {2012},
  month = mar,
  volume = {50},
  pages = {544--552},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2011.11.007},
  abstract = {Multivariate pattern analysis (MVPA) has recently received increasing attention in functional neuroimaging due to its ability to decode mental states from fMRI signals. However, questions remain regarding both the empirical and conceptual relationships between results from MVPA and standard univariate analyses. In the current study, whole-brain univariate and searchlight MVPAs of parametric manipulations of monetary gain and loss in a decision making task (Tom et al., 2007) were compared to identify the differences in the results across these methods and the implications for understanding the underlying mental processes. The MVPA and univariate results did identify some overlapping regions in whole brain analyses. However, an analysis of consistency revealed that in many regions the effect size estimates obtained from MVPA and univariate analysis were uncorrelated. Moreover, comparison of sensitivity showed a general trend towards greater sensitivity to task manipulations by MVPA compared to univariate analysis. These results demonstrate that MVPA methods may provide a different view of the functional organization of mental processing compared to univariate analysis, wherein MVPA is more sensitive to distributed coding of information whereas univariate analysis is more sensitive to global engagement in ongoing tasks. The results also highlight the need for better ways to integrate these methods.},
  file = {/Users/qualia/Documents/Papers/2012 - Jimura, Poldrack - Analyses of regional-average activation and multivoxel pattern information tell complementary stories.pdf},
  journal = {Neuropsychologia},
  language = {en},
  number = {4}
}

@article{Jin2010,
  title = {Start/Stop Signals Emerge in Nigrostriatal Circuits during Sequence Learning},
  author = {Jin, Xin and Costa, Rui M.},
  year = {2010},
  month = jul,
  volume = {466},
  pages = {457--462},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature09263},
  file = {/Users/qualia/Documents/Papers/2010 - Jin, Costa - Startstop signals emerge in nigrostriatal circuits during sequence learning.pdf},
  journal = {Nature},
  language = {en},
  number = {7305}
}

@article{Jin2014,
  title = {Basal Ganglia Subcircuits Distinctively Encode the Parsing and Concatenation of Action Sequences},
  author = {Jin, Xin and Tecuapetla, Fatuel and Costa, Rui M},
  year = {2014},
  month = mar,
  volume = {17},
  pages = {423--430},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3632},
  file = {/Users/qualia/Documents/Papers/2014 - Jin, Tecuapetla, Costa - Basal ganglia subcircuits distinctively encode the parsing and concatenation of action sequences.pdf;/Users/qualia/Documents/Papers/Jin et al. - 2014 - Basal ganglia subcircuits distinctively encode the.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@techreport{Jin2018,
  title = {Classical-Contextual Interactions in {{V1}} May Rely on Dendritic Computations},
  author = {Jin, Lei and Behabadi, Bardia F. and Jadi, Monica P. and Ramachandra, Chaithanya A. and Mel, Bartlett W.},
  year = {2018},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/436956},
  abstract = {A signature feature of the neocortex is the dense network of horizontal connections (HCs) through which pyramidal neurons (PNs) exchange "contextual" information. In primary visual cortex (V1), HCs are thought to facilitate boundary detection, a crucial operation for object recognition, but how HCs modulate PN responses to boundary cues within their classical receptive fields (CRF) remains unknown. We began by ``asking'' natural images, through a structured data collection and ground truth labeling process, what function a V1 cell should use to compute boundary probability from aligned edge cues within and outside its CRF. The ``answer'' was an asymmetric 2-D sigmoidal function, whose nonlinear form provides the first normative account for the ``multiplicative'' center-flanker interactions previously reported in V1 neurons (Kapadia et al. 1995, 2000; Polat et al. 1998). Using a detailed compartmental model, we then show that this boundary-detecting classical-contextual interaction function can be computed with near perfect accuracy by NMDAR-dependent spatial synaptic interactions within PN dendrites \textendash{} the site where classical and contextual inputs first converge in the cortex. In additional simulations, we show that local interneuron circuitry activated by HCs can powerfully leverage the nonlinear spatial computing capabilities of PN dendrites, providing the cortex with a highly flexible substrate for integration of classical and contextual information.},
  file = {/Users/qualia/Documents/Papers/Jin et al. - 2018 - Classical-contextual interactions in V1 may rely o.pdf},
  language = {en},
  type = {Preprint}
}

@article{Jirsa2011,
  title = {Neural {{Population Modes Capture Biologically Realistic Large Scale Network Dynamics}}},
  author = {Jirsa, Viktor K. and Stefanescu, Roxana A.},
  year = {2011},
  month = feb,
  volume = {73},
  pages = {325--343},
  issn = {0092-8240, 1522-9602},
  doi = {10.1007/s11538-010-9573-9},
  abstract = {Large scale brain networks are understood nowadays to underlie the emergence of cognitive functions, though the detailed mechanisms are hitherto unknown. The challenges in the study of large scale brain networks are amongst others their high dimensionality requiring significant computational efforts, the complex connectivity across brain areas and the associated transmission delays, as well as the stochastic nature of neuronal processes. To decrease the computational effort, neurons are clustered into neural masses, which then are approximated by reduced descriptions of population dynamics. Here, we implement a neural population mode approach (Assisi et al. in Phys. Rev. Lett. 94(1):018106, 2005; Stefanescu and Jirsa in PLoS Comput. Biol. 4(11):e1000219, 2008), which parsimoniously captures various types of population behavior. We numerically demonstrate that the reduced population mode system favorably captures the high-dimensional dynamics of neuron networks with an architecture involving homogeneous local connectivity and a large-scale, fiber-like connection with time delay.},
  file = {/Users/qualia/Documents/Papers/2011 - Jirsa, Stefanescu - Neural Population Modes Capture Biologically Realistic Large Scale Network Dynamics.pdf},
  journal = {Bulletin of Mathematical Biology},
  language = {en},
  number = {2}
}

@article{Joampx000E3,
  title = {Internal State Dynamics Shape Brainwide Activity and Foraging Behaviour},
  author = {Joamp\#x000E3, o C Marques},
  pages = {27},
  file = {/Users/qualia/Documents/Papers/Joamp#x000E3 - Internal state dynamics shape brainwide activity a.pdf},
  language = {en}
}

@article{Jolivet2004,
  title = {Generalized {{Integrate}}-and-{{Fire Models}} of {{Neuronal Activity Approximate Spike Trains}} of a {{Detailed Model}} to a {{High Degree}} of {{Accuracy}}},
  author = {Jolivet, Renaud and Lewis, Timothy J. and Gerstner, Wulfram},
  year = {2004},
  month = aug,
  volume = {92},
  pages = {959--976},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00190.2004},
  file = {/Users/qualia/Documents/Papers/2004 - Jolivet, Lewis, Gerstner - Generalized integrate-and-fire models of neuronal activity approximate spike trains of a detailed mode.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Jonas2016,
  title = {Could a Neuroscientist Understand a Microprocessor?},
  author = {Jonas, Eric and Kording, Konrad},
  year = {2016},
  month = nov,
  doi = {10.1101/055624},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, that producing large, multimodal, and complex datasets will, enabled by data analysis algorithms, lead to fundamental insights into the way the brain processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. Here we take a simulated classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the processor. This suggests that current approaches in neuroscience may fall short of producing meaningful models of the brain.},
  file = {/Users/qualia/Documents/Papers/Jonas and Kording - 2016 - Could a neuroscientist understand a microprocessor.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Jones2014,
  title = {Analyzability, Ad Hoc Restrictions, and Excessive Flexibility of Evidence-Accumulation Models: {{Reply}} to Two Critical Commentaries.},
  shorttitle = {Analyzability, Ad Hoc Restrictions, and Excessive Flexibility of Evidence-Accumulation Models},
  author = {Jones, Matt and Dzhafarov, Ehtibar N.},
  year = {2014},
  volume = {121},
  pages = {689--695},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0037701},
  file = {/Users/qualia/Documents/Papers/2014 - Jones, Dzhafarov - Analyzability, ad hoc restrictions, and excessive flexibility of evidence-accumulation models Reply to two cri.pdf;/Users/qualia/Documents/Papers/Jones and Dzhafarov - 2014 - Analyzability, ad hoc restrictions, and excessive .pdf},
  journal = {Psychological Review},
  language = {en},
  number = {4}
}

@article{Jones2014a,
  title = {Unfalsifiability and Mutual Translatability of Major Modeling Schemes for Choice Reaction Time.},
  author = {Jones, Matt and Dzhafarov, Ehtibar N.},
  year = {2014},
  volume = {121},
  pages = {1--32},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0034190},
  abstract = {Much current research on speeded choice utilizes models in which the response is triggered by a stochastic process crossing a deterministic threshold. This article focuses on two such model classes, one based on continuous-time diffusion and the other on linear ballistic accumulation (LBA). Both models assume random variability in growth rates and in other model components across trials. We show that if the form of this variability is unconstrained, the models can exactly match any possible pattern of response probabilities and response time distributions. Thus, the explanatory or predictive content of these models is determined not by their structural assumptions, but rather by distributional assumptions (e.g., Gaussian distributions) that are traditionally regarded as implementation details. Selective influence assumptions (i.e., which experimental manipulations affect which model parameters) are shown to have no restrictive effect, except for the theoretically questionable assumption that speed-accuracy instructions do not affect growth rates. The second contribution of this article concerns translation of falsifiable models between universal modeling languages. Specifically, we translate the predictions of the diffusion and LBA models (with their parametric and selective influence assumptions intact) into the Grice modeling framework, in which accumulation processes are deterministic and thresholds are random variables. The Grice framework is also known to reproduce any possible pattern of response probabilities and times, and hence it can be used as a common language for comparing models. It is found that only a few simple properties of empirical data are necessary predictions of the diffusion and LBA models.},
  file = {/Users/qualia/Documents/Papers/2014 - Jones, Dzhafarov - Unfalsifiability and mutual translatability of major modelingschemes for choice reaction time.pdf;/Users/qualia/Documents/Papers/Jones and Dzhafarov - 2014 - Unfalsifiability and mutual translatability of maj.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {1}
}

@article{Jones2016,
  title = {When Brain Rhythms Aren't `Rhythmic': Implication for Their Mechanisms and Meaning},
  shorttitle = {When Brain Rhythms Aren't `Rhythmic'},
  author = {Jones, Stephanie R},
  year = {2016},
  volume = {40},
  pages = {72--80},
  issn = {09594388},
  doi = {10.1016/j.conb.2016.06.010},
  abstract = {Rhythms are a prominent signature of brain activity. Their expression is correlated with numerous examples of healthy information processing and their fluctuations are a marker of disease states. Yet, their causal or epiphenomenal role in brain function is still highly debated. We review recent studies showing brain rhythms are not always ``rhythmic'', by which we mean representative of repeated cycles of activity. Rather, high power and continuous rhythms in averaged signals can represent brief transient events on single trials whose density accumulates in the average. We also review evidence showing time-domain signals with vastly different waveforms can exhibit identical spectral-domain frequency and power. Further, non-oscillatory waveform feature can create spurious high spectral power. Knowledge of these possibilities is essential when interpreting rhythms and is easily missed without considering pre-processed data. Lastly, we discuss how these finding suggest new directions to pursue in our quest to discover the mechanism and meaning of brain rhythms.},
  file = {/Users/qualia/Documents/Papers/Jones - 2016 - When brain rhythms aren't ‘rhythmic’ implication  2.pdf;/Users/qualia/Documents/Papers/Jones - 2016 - When brain rhythms aren't ‘rhythmic’ implication .pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Jorgensen1987,
  title = {Exponential {{Dispersion Models}}},
  author = {Jorgensen, Bent},
  year = {1987},
  volume = {49},
  pages = {127--162},
  abstract = {We studygeneralpropertiesof the class of exponentialdispersionmodels,whichis the multivariategeneralizationof the errordistributionof Nelder and Wedderburn's(1972) generalizedlinear models. Since any given momentgeneratingfunctiongeneratesan exponentialdispersionmodel,thereexistsa multitudeof exponentialdispersionmodels, and some new examplesare introducedG. eneral resultson convolutionand asymptotic normalityofexponentialdispersionmodelsare presentedA. symptotitcheoryis discussed, includinga new small-dispersionasymptoticframeworkw, hichextendsthe domain of applicationoflarge-sampletheoryP. roceduresforconstructinngewexponentiadl ispersion models forcorrelateddata are introduced,includingmodels forlongitudinaldata and variancecomponentsT. he resultsof the paper unifyand generalizestandardresultsfor distributionsuch as the Poisson, the binomial,the negativebinomial,the normal,the gamma,and theinverseGaussian distributions.},
  file = {/Users/qualia/Documents/Papers/Jorgensen - 1987 - Exponential Dispersion Models.pdf},
  journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
  language = {en},
  number = {2}
}

@article{Jorgenson2015,
  title = {The {{BRAIN Initiative}}: Developing Technology to Catalyse Neuroscience Discovery},
  shorttitle = {The {{BRAIN Initiative}}},
  author = {Jorgenson, L. A. and Newsome, W. T. and Anderson, D. J. and Bargmann, C. I. and Brown, E. N. and Deisseroth, K. and Donoghue, J. P. and Hudson, K. L. and Ling, G. S. F. and MacLeish, P. R. and Marder, E. and Normann, R. A. and Sanes, J. R. and Schnitzer, M. J. and Sejnowski, T. J. and Tank, D. W. and Tsien, R. Y. and Ugurbil, K. and Wingfield, J. C.},
  year = {2015},
  month = mar,
  volume = {370},
  pages = {20140164--20140164},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2014.0164},
  file = {/Users/qualia/Documents/Papers/Jorgenson et al. - 2015 - The BRAIN Initiative developing technology to cat.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1668}
}

@article{Joseph2017,
  title = {All for {{One But Not One}} for {{All}}: {{Excitatory Synaptic Scaling}} and {{Intrinsic Excitability Are Coregulated}} by {{CaMKIV}}, {{Whereas Inhibitory Synaptic Scaling Is Under Independent Control}}},
  shorttitle = {All for {{One But Not One}} for {{All}}},
  author = {Joseph, Annelise and Turrigiano, Gina G.},
  year = {2017},
  month = jul,
  volume = {37},
  pages = {6778--6785},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0618-17.2017},
  file = {/Users/qualia/Documents/Papers/Joseph and Turrigiano - 2017 - All for One But Not One for All Excitatory Synapt.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {28}
}

@article{Jozefowicz,
  title = {An {{Empirical Exploration}} of {{Recurrent Network Architectures}}},
  author = {Jozefowicz, Rafal and Zaremba, Wojciech and Sutskever, Ilya},
  pages = {9},
  abstract = {The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear.},
  file = {/Users/qualia/Documents/Papers/2015 - Dosovitskiy, Springenberg, Brox - Learning to generate chairs with convolutional neural networks.pdf;/Users/qualia/Documents/Papers/Jozefowicz et al. - An Empirical Exploration of Recurrent Network Arch.pdf},
  language = {en}
}

@techreport{Juechems2019,
  title = {Where Does Value Come From?},
  author = {Juechems, Keno and Summerfield, Christopher},
  year = {2019},
  month = apr,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/rxf7e},
  abstract = {The computational framework of reinforcement learning (RL) has allowed us to both understand biological brains and build successful artificial agents. However, in this article we highlight open challenges for RL as a model of animal behaviour in natural environments. We ask how the external reward function is designed for biological systems, and how we can account for the context sensitivity of valuation. We argue that rather than optimizing receipt of external reward signals, animals track current and desired internal states and seek to minimise the distance to goal across multiple value dimensions. Our framework can readily account for canonical phenomena observed in the fields of psychology, behavioural ecology, and economics, and recent findings from brain imaging studies of value-guided decision-making.},
  file = {/Users/qualia/Documents/Papers/Juechems and Summerfield - 2019 - Where does value come from.pdf},
  language = {en},
  type = {Preprint}
}

@inproceedings{Juefei-Xu2018,
  title = {Perturbative {{Neural Networks}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Juefei-Xu}, Felix and Boddeti, Vishnu Naresh and Savvides, Marios},
  year = {2018},
  month = jun,
  pages = {3310--3318},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00349},
  abstract = {Convolutional neural networks are witnessing wide adoption in computer vision systems with numerous applications across a range of visual recognition tasks. Much of this progress is fueled through advances in convolutional neural network architectures and learning algorithms even as the basic premise of a convolutional layer has remained unchanged. In this paper, we seek to revisit the convolutional layer that has been the workhorse of state-of-the-art visual recognition models. We introduce a very simple, yet effective, module called a perturbation layer as an alternative to a convolutional layer. The perturbation layer does away with convolution in the traditional sense and instead computes its response as a weighted linear combination of non-linearly activated additive noise perturbed inputs. We demonstrate both analytically and empirically that this perturbation layer can be an effective replacement for a standard convolutional layer. Empirically, deep neural networks with perturbation layers, called Perturbative Neural Networks (PNNs), in lieu of convolutional layers perform comparably with standard CNNs on a range of visual datasets (MNIST, CIFAR-10, PASCAL VOC, and ImageNet) with fewer parameters.},
  file = {/Users/qualia/Documents/Papers/Juefei-Xu et al. - 2018 - Perturbative Neural Networks.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{Kadmon2015,
  title = {Transition to Chaos in Random Neuronal Networks},
  author = {Kadmon, Jonathan and Sompolinsky, Haim},
  year = {2015},
  month = nov,
  volume = {5},
  issn = {2160-3308},
  doi = {10.1103/PhysRevX.5.041030},
  abstract = {Firing patterns in the central nervous system often exhibit strong temporal irregularity and heterogeneity in their time averaged response properties. Previous studies suggested that these properties are outcome of an intrinsic chaotic dynamics. Indeed, simplified rate-based large neuronal networks with random synaptic connections are known to exhibit sharp transition from fixed point to chaotic dynamics when the synaptic gain is increased. However, the existence of a similar transition in neuronal circuit models with more realistic architectures and firing dynamics has not been established. In this work we investigate rate based dynamics of neuronal circuits composed of several subpopulations and random connectivity. Nonzero connections are either positive-for excitatory neurons, or negative for inhibitory ones, while single neuron output is strictly positive; in line with known constraints in many biological systems. Using Dynamic Mean Field Theory, we find the phase diagram depicting the regimes of stable fixed point, unstable dynamic and chaotic rate fluctuations. We characterize the properties of systems near the chaotic transition and show that dilute excitatory-inhibitory architectures exhibit the same onset to chaos as a network with Gaussian connectivity. Interestingly, the critical properties near transition depend on the shape of the single- neuron input-output transfer function near firing threshold. Finally, we investigate network models with spiking dynamics. When synaptic time constants are slow relative to the mean inverse firing rates, the network undergoes a sharp transition from fast spiking fluctuations and static firing rates to a state with slow chaotic rate fluctuations. When the synaptic time constants are finite, the transition becomes smooth and obeys scaling properties, similar to crossover phenomena in statistical mechanics},
  archivePrefix = {arXiv},
  eprint = {1508.06486},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Kadmon, Sompolinsky - Transition to chaos in random neuronal networks.pdf;/Users/qualia/Documents/Papers/Kadmon and Sompolinsky - 2015 - Transition to chaos in random neuronal networks.pdf},
  journal = {Physical Review X},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Chaotic Dynamics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {4}
}

@article{Kahnt2011,
  title = {Decoding Different Roles for {{vmPFC}} and {{dlPFC}} in Multi-Attribute Decision Making},
  author = {Kahnt, Thorsten and Heinzle, Jakob and Park, Soyoung Q. and Haynes, John-Dylan},
  year = {2011},
  month = may,
  volume = {56},
  pages = {709--715},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.058},
  abstract = {In everyday life, successful decision making requires precise representations of expected values. However, for most behavioral options more than one attribute can be relevant in order to predict the expected reward. Thus, to make good or even optimal choices the reward predictions of multiple attributes need to be integrated into a combined expected value. Importantly, the individual attributes of such multi-attribute objects can agree or disagree in their reward prediction. Here we address where the brain encodes the combined reward prediction (averaged across attributes) and where it encodes the variability of the value predictions of the individual attributes. We acquired fMRI data while subjects performed a task in which they had to integrate reward predictions from multiple attributes into a combined value. Using time-resolved pattern recognition techniques (support vector regression) we find that (1) the combined value is encoded in distributed fMRI patterns in the ventromedial prefrontal cortex (vmPFC) and that (2) the variability of value predictions of the individual attributes is encoded in the dorsolateral prefrontal cortex (dlPFC). The combined value could be used to guide choices, whereas the variability of the value predictions of individual attributes indicates an ambiguity that results in an increased difficulty of the value-integration. These results demonstrate that the different features defining multi-attribute objects are encoded in non-overlapping brain regions and therefore suggest different roles for vmPFC and dlPFC in multi-attribute decision making. \textcopyright{} 2010 Elsevier Inc. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2011 - Kahnt et al. - Decoding different roles for vmPFC and dlPFC in multi-attribute decision making.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Kaiser2017,
  title = {One {{Model To Learn Them All}}},
  author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  year = {2017},
  month = jun,
  abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
  archivePrefix = {arXiv},
  eprint = {1706.05137},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Kaiser et al. - 2017 - One Model To Learn Them All.pdf},
  journal = {arXiv:1706.05137 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Kakade2002,
  title = {Dopamine: Generalization and Bonuses},
  shorttitle = {Dopamine},
  author = {Kakade, Sham and Dayan, Peter},
  year = {2002},
  month = jun,
  volume = {15},
  pages = {549--559},
  issn = {08936080},
  doi = {10.1016/S0893-6080(02)00048-5},
  abstract = {In the temporal difference model of primate dopamine neurons, their phasic activity reports a prediction error for future reward. This model is supported by a wealth of experimental data. However, in certain circumstances, the activity of the dopamine cells seems anomalous under the model, as they respond in particular ways to stimuli that are not obviously related to predictions of reward. In this paper, we address two important sets of anomalies, those having to do with generalization and novelty. Generalization responses are treated as the natural consequence of partial information; novelty responses are treated by the suggestion that dopamine cells multiplex information about reward bonuses, including exploration bonuses and shaping bonuses. We interpret this additional role for dopamine in terms of the mechanistic attentional and psychomotor effects of dopamine, having the computational role of guiding exploration. q 2002 Published by Elsevier Science Ltd.},
  file = {/Users/qualia/Documents/Papers/2002 - Kakade, Dayan - Dopamine Generalization and bonuses.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-6}
}

@article{Kalyanakrishnan,
  title = {{{PAC Subset Selection}} in {{Stochastic Multi}}-Armed {{Bandits}}},
  author = {Kalyanakrishnan, Shivaram and Tewari, Ambuj and Auer, Peter and Stone, Peter},
  pages = {8},
  abstract = {We consider the problem of selecting, from among the arms of a stochastic n-armed bandit, a subset of size m of those arms with the highest expected rewards, based on efficiently sampling the arms. This ``subset selection'' problem finds application in a variety of areas. In the authors' previous work (Kalyanakrishnan \& Stone, 2010), this problem is framed under a PAC setting (denoted ``Explore-m''), and corresponding sampling algorithms are analyzed. Whereas the formal analysis therein is restricted to the worst case sample complexity of algorithms, in this paper, we design and analyze an algorithm (``LUCB'') with improved expected sample complexity. Interestingly LUCB bears a close resemblance to the wellknown UCB algorithm for regret minimization. The expected sample complexity bound we show for LUCB is novel even for singlearm selection (Explore-1). We also give a lower bound on the worst case sample complexity of PAC algorithms for Explore-m.},
  file = {/Users/qualia/Documents/Papers/Kalyanakrishnan et al. - PAC Subset Selection in Stochastic Multi-armed Ban.pdf},
  language = {en}
}

@article{Kamitani2005,
  title = {Decoding the Visual and Subjective Contents of the Human Brain},
  author = {Kamitani, Yukiyasu and Tong, Frank},
  year = {2005},
  month = may,
  volume = {8},
  pages = {679--685},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1444},
  file = {/Users/qualia/Documents/Papers/2005 - Kamitani, Tong - Decoding the visual and subjective contents of the human brain.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Kamps2003,
  title = {A {{Simple}} and {{Stable Numerical Solution}} for the {{Population Density Equation}}},
  author = {de Kamps, M.},
  year = {2003},
  month = sep,
  volume = {15},
  pages = {2129--2146},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976603322297322},
  file = {/Users/qualia/Documents/Papers/2003 - de Kamps - A simple and stable numerical solution for the population density equation.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {9}
}

@article{Kann2014,
  title = {Highly {{Energized Inhibitory Interneurons}} Are a {{Central Element}} for {{Information Processing}} in {{Cortical Networks}}},
  author = {Kann, Oliver and Papageorgiou, Ismini E and Draguhn, Andreas},
  year = {2014},
  month = aug,
  volume = {34},
  pages = {1270--1282},
  issn = {0271-678X, 1559-7016},
  doi = {10.1038/jcbfm.2014.104},
  file = {/Users/qualia/Documents/Papers/Kann et al. - 2014 - Highly Energized Inhibitory Interneurons are a Cen.pdf},
  journal = {Journal of Cerebral Blood Flow \& Metabolism},
  language = {en},
  number = {8}
}

@article{Kao,
  title = {Considerations in Using Recurrent Neural Networks to Probe Neural Dynamics},
  author = {Kao, Jonathan C},
  pages = {39},
  file = {/Users/qualia/Documents/Papers/Kao - Considerations in using recurrent neural networks .pdf},
  language = {en}
}

@article{Kao2009,
  title = {Multi-{{Objective Optimal Experimental Designs}} for {{ER}}-{{fMRI Using MATLAB}}},
  author = {Kao, Ming-Hung},
  year = {2009},
  volume = {30},
  issn = {1548-7660},
  doi = {10.18637/jss.v030.i11},
  abstract = {Designs for event-related functional magnetic resonance imaging (ER-fMRI) that help to efficiently achieve the statistical goals while taking into account the psychological constraints and customized requirements are in great demand. This is not only because of the popularity of ER-fMRI but also because of the high cost of ER-fMRI experiments; being able to collect highly informative data is crucial. In this paper, we develop a MATLAB program which can accommodate many user-specified experimental conditions to efficiently find ER-fMRI optimal designs.},
  file = {/Users/qualia/Documents/Papers/2009 - Kao - Multi-Objective Optimal Experimental Designs for.pdf},
  journal = {Journal of Statistical Software},
  language = {en},
  number = {11}
}

@article{Kaplan2015,
  title = {Multivariate Cross-Classification: Applying Machine Learning Techniques to Characterize Abstraction in Neural Representations},
  shorttitle = {Multivariate Cross-Classification},
  author = {Kaplan, Jonas T. and Man, Kingson and Greening, Steven G.},
  year = {2015},
  month = mar,
  volume = {9},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2015.00151},
  abstract = {Here we highlight an emerging trend in the use of machine learning classifiers to test for abstraction across patterns of neural activity. When a classifier algorithm is trained on data from one cognitive context, and tested on data from another, conclusions can be drawn about the role of a given brain region in representing information that abstracts across those cognitive contexts. We call this kind of analysis Multivariate CrossClassification (MVCC), and review several domains where it has recently made an impact. MVCC has been important in establishing correspondences among neural patterns across cognitive domains, including motor-perception matching and cross-sensory matching. It has been used to test for similarity between neural patterns evoked by perception and those generated from memory. Other work has used MVCC to investigate the similarity of representations for semantic categories across different kinds of stimulus presentation, and in the presence of different cognitive demands. We use these examples to demonstrate the power of MVCC as a tool for investigating neural abstraction and discuss some important methodological issues related to its application.},
  file = {/Users/qualia/Documents/Papers/2015 - Kaplan, Man, Greening - Multivariate cross-classification applying machine learning techniques to characterize abstraction in neu.pdf;/Users/qualia/Documents/Papers/Kaplan et al. - 2015 - Multivariate cross-classification applying machin.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Kass2011,
  title = {Assessment of Synchrony in Multiple Neural Spike Trains Using Loglinear Point Process Models},
  author = {Kass, Robert E. and Kelly, Ryan C. and Loh, Wei-Liem},
  year = {2011},
  month = jun,
  volume = {5},
  pages = {1262--1292},
  issn = {1932-6157},
  doi = {10.1214/10-AOAS429},
  file = {/Users/qualia/Documents/Papers/2011 - Kass, Kelly, Loh - Assessment of synchrony in multiple neural spike trains using loglinear point process models.pdf},
  journal = {The Annals of Applied Statistics},
  language = {en},
  number = {2B}
}

@article{Katz,
  title = {Embodying Probabilistic Inference in Biochemical Circuits},
  author = {Katz, Yarden and Springer, Michael and Fontana, Walter},
  pages = {23},
  abstract = {Probabilistic inference provides a language for describing how organisms may learn from and adapt to their environment. The computations needed to implement probabilistic inference often require specific representations, akin to having the suitable data structures for implementing certain algorithms in computer programming. Yet it is unclear how such representations can be instantiated in the stochastic, parallel-running biochemical machinery found in cells (such as single-celled organisms). Here, we show how representations for supporting inference in Markov models can be embodied in cellular circuits, by combining a concentration-dependent scheme for encoding probabilities with a mechanism for directional counting. We show how the logic of protein production and degradation constrains the computation we set out to implement. We argue that this process by which an abstract computation is shaped by its biochemical realization strikes a compromise between ``rationalistic'' information-processing perspectives and alternative approaches that emphasize embodiment.},
  file = {/Users/qualia/Documents/Papers/Katz et al. - Embodying probabilistic inference in biochemical c.pdf},
  language = {en}
}

@article{Katz1995,
  title = {Ideas of {{Calculus}} in {{Islam}} and {{India}}},
  author = {Katz, Victor J.},
  year = {1995},
  month = jun,
  volume = {68},
  pages = {163},
  issn = {0025570X},
  doi = {10.2307/2691411},
  file = {/Users/qualia/Documents/Papers/1995 - Katz - Ideas of Calculus in Islam and India.pdf},
  journal = {Mathematics Magazine},
  language = {en},
  number = {3}
}

@article{Kaufman2013,
  title = {The Roles of Monkey {{M1}} Neuron Classes in Movement Preparation and Execution},
  author = {Kaufman, Matthew T. and Churchland, Mark M. and Shenoy, Krishna V.},
  year = {2013},
  month = aug,
  volume = {110},
  pages = {817--825},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00892.2011},
  file = {/Users/qualia/Documents/Papers/2013 - Kaufman, Churchland, Shenoy - The roles of monkey M1 neuron classes in movement preparation and execution.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{Kaufman2014,
  title = {Cortical Activity in the Null Space: Permitting Preparation without Movement},
  shorttitle = {Cortical Activity in the Null Space},
  author = {Kaufman, Matthew T and Churchland, Mark M and Ryu, Stephen I and Shenoy, Krishna V},
  year = {2014},
  month = mar,
  volume = {17},
  pages = {440--448},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3643},
  file = {/Users/qualia/Documents/Papers/2014 - Kaufman et al. - Cortical activity in the null space permitting preparation without movement.pdf;/Users/qualia/Documents/Papers/Kaufman et al. - 2014 - Cortical activity in the null space permitting pr.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Kay2008,
  title = {Identifying Natural Images from Human Brain Activity},
  author = {Kay, Kendrick N. and Naselaris, Thomas and Prenger, Ryan J. and Gallant, Jack L.},
  year = {2008},
  month = mar,
  volume = {452},
  pages = {352--355},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature06713},
  file = {/Users/qualia/Documents/Papers/2008 - Kay et al. - Identifying natural images from human brain activity.pdf},
  journal = {Nature},
  language = {en},
  number = {7185}
}

@article{Kayser2010,
  title = {Neural {{Representations}} of {{Relevant}} and {{Irrelevant Features}} in {{Perceptual Decision Making}}},
  author = {Kayser, A. S. and Erickson, D. T. and Buchsbaum, B. R. and D'Esposito, M.},
  year = {2010},
  month = nov,
  volume = {30},
  pages = {15778--15789},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3163-10.2010},
  file = {/Users/qualia/Documents/Papers/2010 - Kayser et al. - Neural representations of relevant and irrelevant features in perceptual decision making.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {47}
}

@article{Kearns2002,
  title = {Near-{{Optimal Reinforcement Learning}} in {{Polynomial Time}}},
  author = {Kearns, Michael and Singh, Satinder},
  year = {2002},
  pages = {24},
  abstract = {We present new algorithms for reinforcement learning and prove that they have polynomial bounds on the resources required to achieve near-optimal return in general Markov decision processes. After observing that the number of actions required to approach the optimal return is lower bounded by the mixing time T of the optimal policy (in the undiscounted case) or by the horizon time T (in the discounted case), we then give algorithms requiring a number of actions and total computation time that are only polynomial in T and the number of states and actions, for both the undiscounted and discounted cases. An interesting aspect of our algorithms is their explicit handling of the Exploration-Exploitation trade-off.},
  file = {/Users/qualia/Documents/Papers/Kearns - Near-Optimal Reinforcement Learning in Polynomial .pdf},
  journal = {Machine Learning},
  language = {en}
}

@article{Kelly1956,
  title = {A {{New Interpretation}} of {{Information Rate}}},
  author = {Kelly, J L},
  year = {1956},
  pages = {10},
  file = {/Users/qualia/Documents/Papers/Kelly - 1956 - A New Interpretation of Information Rate.pdf},
  journal = {the bell system technical journal},
  language = {en}
}

@article{Kelly2012,
  title = {A {{Framework}} for {{Evaluating Pairwise}} and {{Multiway Synchrony Among Stimulus}}-{{Driven Neurons}}},
  author = {Kelly, Ryan C. and Kass, Robert E.},
  year = {2012},
  month = aug,
  volume = {24},
  pages = {2007--2032},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00307},
  file = {/Users/qualia/Documents/Papers/2012 - Kelly, Kass - A Framework for Evaluating Pairwise and Multiway Synchrony Among Stimulus-Driven Neurons.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Kelly2014,
  title = {The {{Role}} of {{Thalamic Population Synchrony}} in the {{Emergence}} of {{Cortical Feature Selectivity}}},
  author = {Kelly, Sean T. and Kremkow, Jens and Jin, Jianzhong and Wang, Yushi and Wang, Qi and Alonso, Jose-Manuel and Stanley, Garrett B.},
  editor = {Graham, Lyle J.},
  year = {2014},
  month = jan,
  volume = {10},
  pages = {e1003418},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003418},
  abstract = {In a wide range of studies, the emergence of orientation selectivity in primary visual cortex has been attributed to a complex interaction between feed-forward thalamic input and inhibitory mechanisms at the level of cortex. Although it is well known that layer 4 cortical neurons are highly sensitive to the timing of thalamic inputs, the role of the stimulus-driven timing of thalamic inputs in cortical orientation selectivity is not well understood. Here we show that the synchronization of thalamic firing contributes directly to the orientation tuned responses of primary visual cortex in a way that optimizes the stimulus information per cortical spike. From the recorded responses of geniculate X-cells in the anesthetized cat, we synthesized thalamic sub-populations that would likely serve as the synaptic input to a common layer 4 cortical neuron based on anatomical constraints. We used this synchronized input as the driving input to an integrate-and-fire model of cortical responses and demonstrated that the tuning properties match closely to those measured in primary visual cortex. By modulating the overall level of synchronization at the preferred orientation, we show that efficiency of information transmission in the cortex is maximized for levels of synchronization which match those reported in thalamic recordings in response to naturalistic stimuli, a property which is relatively invariant to the orientation tuning width. These findings indicate evidence for a more prominent role of the feed-forward thalamic input in cortical feature selectivity based on thalamic synchronization.},
  file = {/Users/qualia/Documents/Papers/Kelly et al. - 2014 - The Role of Thalamic Population Synchrony in the E.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {1}
}

@article{Kempster2007,
  title = {Patterns of Levodopa Response in {{Parkinson}}'s Disease: A Clinico-Pathological Study},
  shorttitle = {Patterns of Levodopa Response in {{Parkinson}}'s Disease},
  author = {Kempster, P. A. and Williams, D. R. and Selikhova, M. and Holton, J. and Revesz, T. and Lees, A. J.},
  year = {2007},
  month = aug,
  volume = {130},
  pages = {2123--2128},
  issn = {0006-8950, 1460-2156},
  doi = {10.1093/brain/awm142},
  file = {/Users/qualia/Documents/Papers/2007 - Kempster et al. - Patterns of levodopa response in Parkinson's disease A clinico-pathological study.pdf},
  journal = {Brain},
  language = {en},
  number = {8}
}

@article{Kendal2015,
  title = {Self-Organized Criticality Attributed to a Central Limit-like Convergence Effect},
  author = {Kendal, Wayne S.},
  year = {2015},
  month = mar,
  volume = {421},
  pages = {141--150},
  issn = {03784371},
  doi = {10.1016/j.physa.2014.11.035},
  abstract = {Self-organized criticality is a hypothesis used to explain the origin of 1/f noise and other scaling behaviors. Despite being proposed nearly 30 years ago, no consensus exists as to its exact definition or mathematical mechanism(s). Recently, a model for 1/f noise was proposed based on a family of statistical distributions known as the Tweedie exponential dispersion models. These distributions are characterized by an inherent scale invariance that manifests as a variance to mean power law, called fluctuation scaling; they also serve as foci of convergence in a limit theorem on independent and identically distributed distributions. Fluctuation scaling can be modeled by self-similar stochastic processes that relate the variance to mean power law to 1/f noise through their correlation structure. A hypothesis is proposed whereby the effects of self-organized criticality are mathematically modeled by the Tweedie distributions and their convergence behavior as applied to selfsimilar stochastic processes. Sandpile model fluctuations are shown to manifest 1/f noise, fluctuation scaling, and to conform to the Tweedie compound Poisson distribution. The Tweedie models and their convergence theorem allow for a mechanistic explanation of 1/f noise and fluctuation scaling in phenomena conventionally attributed to self-organized criticality, thus providing a paradigm shift in our understanding of these phenomena.},
  file = {/Users/qualia/Documents/Papers/Kendal - 2015 - Self-organized criticality attributed to a central.pdf},
  journal = {Physica A: Statistical Mechanics and its Applications},
  language = {en}
}

@article{Keramati2014,
  title = {Homeostatic Reinforcement Learning for Integrating Reward Collection and Physiological Stability},
  author = {Keramati, Mehdi and Gutkin, Boris},
  year = {2014},
  month = dec,
  volume = {3},
  pages = {e04811},
  issn = {2050-084X},
  doi = {10.7554/eLife.04811},
  abstract = {Efficient regulation of internal homeostasis and defending it against perturbations requires adaptive behavioral strategies. However, the computational principles mediating the interaction between homeostatic and associative learning processes remain undefined. Here we use a definition of primary rewards, as outcomes fulfilling physiological needs, to build a normative theory showing how learning motivated behaviors may be modulated by internal states. Within this framework, we mathematically prove that seeking rewards is equivalent to the fundamental objective of physiological stability, defining the notion of physiological rationality of behavior. We further suggest a formal basis for temporal discounting of rewards by showing that discounting motivates animals to follow the shortest path in the space of physiological variables toward the desired setpoint. We also explain how animals learn to act predictively to preclude prospective homeostatic challenges, and several other behavioral patterns. Finally, we suggest a computational role for interaction between hypothalamus and the brain reward system.
          , 
            Our survival depends on our ability to maintain internal states, such as body temperature and blood sugar levels, within narrowly defined ranges, despite being subject to constantly changing external forces. This process, which is known as homeostasis, requires humans and other animals to carry out specific behaviors\textemdash{}such as seeking out warmth or food\textemdash{}to compensate for changes in their environment. Animals must also learn to prevent the potential impact of changes that can be anticipated.
            A network that includes different regions of the brain allows animals to perform the behaviors that are needed to maintain homeostasis. However, this network is distinct from the network that supports the learning of new behaviors in general. These two systems must, therefore, interact so that animals can learn novel strategies to support their physiological stability, but it is not clear how animals do this.
            Keramati and Gutkin have now devised a mathematical model that explains the nature of this interaction, and that can account for many behaviors seen among animals, even those that might otherwise appear irrational. There are two assumptions at the heart of the model. First, it is assumed that animals are capable of guessing the impact of the outcome of their behaviors on their internal state. Second, it is assumed that animals find a behavior rewarding if they believe that the predicted impact of its outcome will reduce the difference between a particular internal state and its ideal value. For example, a form of behavior for a human might be going to the kitchen, and an outcome might be eating chocolate.
            Based on these two assumptions, the model shows that animals stabilize their internal state around its ideal value by simply learning to perform behaviors that lead to rewarding outcomes (such as going into the kitchen and eating chocolate). Their theory also explains the physiological importance of a type of behavior known as `delay discounting'. Animals displaying this form of behavior regard a positive outcome as less rewarding the longer they have to wait for it. The model proves mathematically that delay discounting is a logical way to optimize homeostasis.
            In addition to making a number of predictions that could be tested in experiments, Keramati and Gutkin argue that their model can account for the failure of homeostasis to limit food consumption whenever foods loaded with salt, sugar or fat are freely available.},
  file = {/Users/qualia/Documents/Papers/Keramati and Gutkin - 2014 - Homeostatic reinforcement learning for integrating.pdf},
  journal = {eLife},
  language = {en}
}

@article{Kharkwal2016,
  title = {Parkinsonism {{Driven}} by {{Antipsychotics Originates}} from {{Dopaminergic Control}} of {{Striatal Cholinergic Interneurons}}},
  author = {Kharkwal, Geetika and {Brami-Cherrier}, Karen and {Lizardi-Ortiz}, Jos{\'e} E. and Nelson, Alexandra B. and Ramos, Maria and Del Barrio, Daniel and Sulzer, David and Kreitzer, Anatol C. and Borrelli, Emiliana},
  year = {2016},
  month = jul,
  volume = {91},
  pages = {67--78},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.06.014},
  abstract = {Typical antipsychotics can cause disabling side effects. Specifically, antagonism of D2R signaling by the typical antipsychotic haloperidol induces parkinsonism in humans and catalepsy in rodents. Striatal dopamine D2 receptors (D2R) are major regulators of motor activity through their signaling on striatal projection neurons and interneurons. We show that D2R signaling on cholinergic interneurons contributes to an in vitro pause in firing of these otherwise tonically active neurons and to the striatal dopamine/acetylcholine balance. The selective ablation of D2R from cholinergic neurons allows discrimination between the motor-reducing and cataleptic effects of antipsychotics. The cataleptic effect of antipsychotics is triggered by blockade of D2R on cholinergic interneurons and the consequent increase of acetylcholine signaling on striatal projection neurons. These studies illuminate the critical role of D2R-mediated signaling in regulating the activity of striatal cholinergic interneurons and the mechanisms of typical antipsychotic side effects.},
  file = {/Users/qualia/Documents/Papers/Kharkwal et al. - 2016 - Parkinsonism Driven by Antipsychotics Originates f.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Kheradpisheh2018,
  title = {{{STDP}}-Based Spiking Deep Convolutional Neural Networks for Object Recognition},
  author = {Kheradpisheh, Saeed Reza and Ganjtabesh, Mohammad and Thorpe, Simon J. and Masquelier, Timoth{\'e}e},
  year = {2018},
  month = mar,
  volume = {99},
  pages = {56--67},
  issn = {08936080},
  doi = {10.1016/j.neunet.2017.12.005},
  file = {/Users/qualia/Documents/Papers/Kheradpisheh et al. - 2018 - STDP-based spiking deep convolutional neural netwo.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Khodagholy2013,
  title = {In Vivo Recordings of Brain Activity Using Organic Transistors},
  author = {Khodagholy, Dion and Doublet, Thomas and Quilichini, Pascale and Gurfinkel, Moshe and Leleux, Pierre and Ghestem, Antoine and Ismailova, Esma and Herv{\'e}, Thierry and Sanaur, S{\'e}bastien and Bernard, Christophe and Malliaras, George G.},
  year = {2013},
  month = dec,
  volume = {4},
  issn = {2041-1723},
  doi = {10.1038/ncomms2573},
  file = {/Users/qualia/Documents/Papers/2013 - Khodagholy et al. - In vivo recordings of brain activity using organic transistors.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Khodagholy2015,
  title = {{{NeuroGrid}}: Recording Action Potentials from the Surface of the Brain},
  shorttitle = {{{NeuroGrid}}},
  author = {Khodagholy, Dion and Gelinas, Jennifer N and Thesen, Thomas and Doyle, Werner and Devinsky, Orrin and Malliaras, George G and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2015},
  month = feb,
  volume = {18},
  pages = {310--315},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3905},
  file = {/Users/qualia/Documents/Papers/2015 - Khodagholy et al. - NeuroGrid recording action potentials from the surface of the brain.pdf;/Users/qualia/Documents/Papers/Khodagholy et al. - 2015 - NeuroGrid recording action potentials from the su.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {2}
}

@article{Kidd2015,
  title = {The {{Psychology}} and {{Neuroscience}} of {{Curiosity}}},
  author = {Kidd, Celeste and Hayden, Benjamin Y.},
  year = {2015},
  month = nov,
  volume = {88},
  pages = {449--460},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.09.010},
  file = {/Users/qualia/Documents/Papers/Kidd and Hayden - 2015 - The Psychology and Neuroscience of Curiosity.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Kim1994,
  title = {Glutamate-Induced Calcium Signaling in Astrocytes},
  author = {Kim, Warren T. and Rioult, Marc G. and {Cornell-Bell}, Ann H.},
  year = {1994},
  month = jun,
  volume = {11},
  pages = {173--184},
  issn = {0894-1491, 1098-1136},
  doi = {10.1002/glia.440110211},
  abstract = {Astrocytes respond to the excitatory neurotransmitter glutamate with dynamic spatio-temporal changes in intracellular calcium [Ca"],. Although they share a common wave-likeappearance, the different lCaz+ji changes-an initial spike, sustained elevation, oscillatory intracellular waves, and regenerative intercellular waves-are actually separate and distinct phenomena. These separate components of the astrocytic Ca2+response appear to be generated by two different signal transduction pathways. The metabotropic response evokes an initial spatial Ca"' spike that can propagate rapidly from cell to cell and appears to involve IP,. The metabotropic response can also produce oscillatory intracellular waves of various amplitudes and frequencies that propagate within cells and are sustained only in the presence of external Ca2+.The ionotropic response, however, evokes a sustained elevation in [Ca2+Jiassociated with receptormediated Na+ and Ca2+influx, depolarization, and voltage-dependent Ca2+influx. In addition, the ionotropicresponse can lead to regenerative intercellular waves that propagate smoothly and nondecrementally from cell to cell, possibly involving Na+/Ca2+ exchange. All these astrocytic [Ca2+Iichanges tend to appear wave-like, traveling from region t o region as a transient rise in [Ca" Ii. Nevertheless, as our understanding of the cellular events that underlie these [Ca2+Iichanges grows, it becomes increasingly clear that glutamate-induced Ca2+signaling is a composite of separate and distinct phenomena, which may be distinguished not based on appearance alone, but rather on their underlying mechanisms. o 1994 Wiley-Liss, Inc.},
  file = {/Users/qualia/Documents/Papers/Kim et al. - 1994 - Glutamate-induced calcium signaling in astrocytes.pdf},
  journal = {Glia},
  language = {en},
  number = {2}
}

@article{Kim2014,
  title = {Layer 6 {{Corticothalamic Neurons Activate}} a {{Cortical Output Layer}}, {{Layer}} 5a},
  author = {Kim, J. and Matney, C. J. and Blankenship, A. and Hestrin, S. and Brown, S. P.},
  year = {2014},
  month = jul,
  volume = {34},
  pages = {9656--9664},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1325-14.2014},
  file = {/Users/qualia/Documents/Papers/2014 - Kim et al. - Layer 6 Corticothalamic Neurons Activate a Cortical Output Layer, Layer 5a.pdf;/Users/qualia/Documents/Papers/Kim et al. - 2014 - Layer 6 Corticothalamic Neurons Activate a Cortica.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {29}
}

@article{Kim2016,
  title = {Low-Dielectric-Constant Polyimide Aerogel Composite Films with Low Water Uptake},
  author = {Kim, Jinyoung and Kwon, Jinuk and Kim, Myeongsoo and Do, Jeonguk and Lee, Daero and Han, Haksoo},
  year = {2016},
  month = jul,
  volume = {48},
  pages = {829--834},
  issn = {0032-3896, 1349-0540},
  doi = {10.1038/pj.2016.37},
  file = {/Users/qualia/Documents/Papers/Kim et al. - 2016 - Low-dielectric-constant polyimide aerogel composit.pdf},
  journal = {Polymer Journal},
  language = {en},
  number = {7}
}

@article{King2013,
  title = {Inhibitory {{Interneurons Decorrelate Excitatory Cells}} to {{Drive Sparse Code Formation}} in a {{Spiking Model}} of {{V1}}},
  author = {King, P. D. and Zylberberg, J. and DeWeese, M. R.},
  year = {2013},
  month = mar,
  volume = {33},
  pages = {5475--5485},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4188-12.2013},
  file = {/Users/qualia/Documents/Papers/2013 - King, Zylberberg, DeWeese - Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking mode.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {13}
}

@article{Kingma2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2014 - Kingman, Welling - Auto-Encoding Variational Bayes.pdf;/Users/qualia/Documents/Papers/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Kinkhabwala2011,
  title = {A Structural and Functional Ground Plan for Neurons in the Hindbrain of Zebrafish},
  author = {Kinkhabwala, A. and Riley, M. and Koyama, M. and Monen, J. and Satou, C. and Kimura, Y. and Higashijima, S.-i. and Fetcho, J.},
  year = {2011},
  month = jan,
  volume = {108},
  pages = {1164--1169},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1012185108},
  file = {/Users/qualia/Documents/Papers/Kinkhabwala et al. - 2011 - A structural and functional ground plan for neuron.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {3}
}

@article{Kira2015,
  title = {A {{Neural Implementation}} of {{Wald}}'s {{Sequential Probability Ratio Test}}},
  author = {Kira, Shinichiro and Yang, Tianming and Shadlen, Michael N.},
  year = {2015},
  month = feb,
  volume = {85},
  pages = {861--873},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.01.007},
  abstract = {Difficult decisions often require evaluation of samples of evidence acquired sequentially. A sensible strategy is to accumulate evidence, weighted by its reliability, until sufficient support is attained. An optimal statistical approach would accumulate evidence in units of logarithms of likelihood ratios (logLR) to a desired level. Studies of perceptual decisions suggest that the brain approximates an analogous procedure, but a direct test of accumulation, in units of logLR, to a threshold in units of cumulative logLR is lacking. We trained rhesus monkeys to make decisions based on a sequence of evanescent, visual cues assigned different logLR, hence different reliability. Firing rates of neurons in the lateral intraparietal area (LIP) reflected the accumulation of logLR and reached a stereotyped level before the monkeys committed to a decision. The monkeys' choices and reaction times, including their variability, were explained by LIP activity in the context of accumulation of logLR to a threshold.},
  file = {/Users/qualia/Documents/Papers/Kira et al. - 2015 - A Neural Implementation of Wald’s Sequential Proba.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Kirkpatrick2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = mar,
  volume = {114},
  pages = {3521--3526},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1611835114},
  file = {/Users/qualia/Documents/Papers/Kirkpatrick et al. - 2017 - Overcoming catastrophic forgetting in neural netwo.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {13}
}

@article{Kister,
  title = {Online {{Supplement}}: {{About}} the {{Model}}},
  author = {Kister, Robert},
  pages = {20},
  file = {/Users/qualia/Documents/Papers/2003 - Poirazi, Brannon, Mel - Pyramidal Neuron as Two-Layered Neural Network(2).pdf},
  language = {en}
}

@article{Kita1983,
  title = {The Morphology of Intracellularly Labeled Rat Subthalamic Neurons: {{A}} Light Microscopic Analysis},
  shorttitle = {The Morphology of Intracellularly Labeled Rat Subthalamic Neurons},
  author = {Kita, H. and Chang, H. T. and Kitai, S. T.},
  year = {1983},
  month = apr,
  volume = {215},
  pages = {245--257},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.902150302},
  abstract = {Light microscopic analysis of rat subthalamic (STH) neurons which were intracellularly labeled with horseradish peroxidase, following the acquisition of electrophysiologicaldata, revealed the following: (1)The somata of STH neurons were polygonal or oval with occasionally a few somatic spines. Uusally three or four primary dendrites arose from the soma. Dendritic trunks tapered slightly and divided into long, thin, sparsely spined branches. Dendrites of some STH neurons extended into the cerebral peduncle. (2) Reconstruction of the dendritic field was made in three different planes. In either sagittal or frontal planes, the dendritic field was usually oval and the long axis was parallel to the main axis of STH. In the horizontal plane, the dendritic field of all neurons was polygonal. (3)The axons of all the neurons analyzed originated from the soma and were traced beyond the borders of STH, thus indicating that they were projection neurons. All the parent axons bifurcated at least once. After bifurcation, one axon branch coursed dorsolaterally within the cerebral peduncle and terminated in the globus pallidus. The other branch coursed caudally or mediocaudally and arborized in the substantia nigra. Frequently, the axon branches projecting toward the globus pallidus emitted fine axon collaterals within the entopeduncular nucleus. (4)About one-half of the analyzed STH neurons had intranuclear axon collaterals. The neurons with intranuclear collaterals had a higher dendritic tipslstems ratio than neurons without intranuclear collatera l \textasciitilde{}T. his observation indicated that STH neurons could be divided into two groups according to their axonal morphology. (5)The axonal terminal arborization observed in all the target sites (i.e., globus pallidus, entopeduncular nucleus, STH, and substantia nigra) were formed with varicose collateral branches which also gave rise to short filaments with beaded endings. Some of these projection neurons could therefore communicate with the target neurons in the globus pallidus, substantia nigra, entopeduncular nucleus, as well as STH through their collateral system.},
  file = {/Users/qualia/Documents/Papers/1983 - Kita, Chang, Kitai - The morphology of intracellularly labelled rat subthalamic nucleus a light microscopic analysis.pdf;/Users/qualia/Documents/Papers/Kita et al. - 1983 - The morphology of intracellularly labeled rat subt.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {3}
}

@article{Kivinen2004,
  title = {Online {{Learning}} with {{Kernels}}},
  author = {Kivinen, J. and Smola, A.J. and Williamson, R.C.},
  year = {2004},
  month = aug,
  volume = {52},
  pages = {2165--2176},
  issn = {1053-587X},
  doi = {10.1109/TSP.2004.830991},
  abstract = {Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection.},
  file = {/Users/qualia/Documents/Papers/2004 - Kivinen, Smola, Williamson - Online Learning with Kernels.pdf},
  journal = {IEEE Transactions on Signal Processing},
  language = {en},
  number = {8}
}

@article{Klimesch2012,
  title = {Alpha-Band Oscillations, Attention, and Controlled Access to Stored Information},
  author = {Klimesch, Wolfgang},
  year = {2012},
  month = dec,
  volume = {16},
  pages = {606--617},
  issn = {13646613},
  doi = {10.1016/j.tics.2012.10.007},
  file = {/Users/qualia/Documents/Papers/2012 - Klimesch - Alpha-band oscillations, attention, and controlled access to stored information.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {12}
}

@article{Knight1972,
  title = {Dynamics of {{Encoding}} in a {{Population}} of {{Neurons}}},
  author = {Knight, B. W.},
  year = {1972},
  month = jun,
  volume = {59},
  pages = {734--766},
  issn = {0022-1295, 1540-7748},
  doi = {10.1085/jgp.59.6.734},
  abstract = {A simple encoder model, which is a reasonable idealization from known electrophysiological properties, yields a population in which the variation of the firing rate with time is a perfect replica of the shape of the input stimulus. A population of noise-free encoders which depart even slightly from the simple model yield a very much degraded copy of the input stimulus. The presence of noise improves the performance of such a population. The firing rate of a population of neurons is related to the firing rate of a single member in a subtle way.},
  file = {/Users/qualia/Documents/Papers/1972 - Knight - Dynamics of Encoding in a Population of Neurons.pdf},
  journal = {The Journal of General Physiology},
  language = {en},
  number = {6}
}

@article{Knowlton2014,
  title = {Dynamical Estimation of Neuron and Network Properties {{III}}: Network Analysis Using Neuron Spike Times},
  shorttitle = {Dynamical Estimation of Neuron and Network Properties {{III}}},
  author = {Knowlton, Chris and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2014},
  month = jun,
  volume = {108},
  pages = {261--273},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-014-0601-y},
  abstract = {Estimating the behavior of a network of neurons requires accurate models of the individual neurons along with accurate characterizations of the connections among them. Whereas for a single cell, measurements of the intracellular voltage are technically feasible and sufficient to characterize a useful model of its behavior, making sufficient numbers of simultaneous intracellular measurements to characterize even small networks is infeasible. This paper builds on prior work on single neurons to explore whether knowledge of the time of spiking of neurons in a network, once the nodes (neurons) have been characterized biophysically, can provide enough information to usefully constrain the functional architecture of the network: the existence of synaptic links among neurons and their strength. Using standardized voltage and synaptic gating variable waveforms associated with a spike, we demonstrate that the functional architecture of a small network of model neurons can be established.},
  file = {/Users/qualia/Documents/Papers/2014 - Knowlton et al. - Dynamical estimation of neuron and network properties III Network analysis using neuron spike times.pdf;/Users/qualia/Documents/Papers/Knowlton et al. - 2014 - Dynamical estimation of neuron and network propert.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3}
}

@article{Kobayashi2019,
  title = {Common Neural Code for Reward and Information Value},
  author = {Kobayashi, Kenji and Hsu, Ming},
  year = {2019},
  month = jun,
  volume = {116},
  pages = {13061--13066},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1820145116},
  abstract = {Adaptive information seeking is critical for goal-directed behavior. Growing evidence suggests the importance of intrinsic motives such as curiosity or need for novelty, mediated through dopaminergic valuation systems, in driving information-seeking behavior. However, valuing information for its own sake can be highly suboptimal when agents need to evaluate instrumental benefit of information in a forward-looking manner. Here we show that information-seeking behavior in humans is driven by subjective value that is shaped by both instrumental and noninstrumental motives, and that this subjective value of information (SVOI) shares a common neural code with more basic reward value. Specifically, using a task where subjects could purchase information to reduce uncertainty about outcomes of a monetary lottery, we found information purchase decisions could be captured by a computational model of SVOI incorporating utility of anticipation, a form of noninstrumental motive for information seeking, in addition to instrumental benefits. Neurally, trial-by-trial variation in SVOI was correlated with activity in striatum and ventromedial prefrontal cortex. Furthermore, cross-categorical decoding revealed that, within these regions, SVOI and expected utility of lotteries were represented using a common code. These findings provide support for the common currency hypothesis and shed insight on neurocognitive mechanisms underlying information-seeking behavior.},
  file = {/Users/qualia/Documents/Papers/Kobayashi and Hsu - 2019 - Common neural code for reward and information valu.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {26}
}

@incollection{Kocsis2006,
  title = {Bandit {{Based Monte}}-{{Carlo Planning}}},
  booktitle = {Machine {{Learning}}: {{ECML}} 2006},
  author = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and F{\"u}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra},
  year = {2006},
  volume = {4212},
  pages = {282--293},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11871842_29},
  abstract = {For large state-space Markovian Decision Problems MonteCarlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.},
  file = {/Users/qualia/Documents/Papers/Kocsis and Szepesvári - 2006 - Bandit Based Monte-Carlo Planning.pdf},
  isbn = {978-3-540-45375-8 978-3-540-46056-5},
  language = {en}
}

@article{Kolchinsky2018,
  title = {Semantic Information, Autonomous Agency and Non-Equilibrium Statistical Physics},
  author = {Kolchinsky, Artemy and Wolpert, David H.},
  year = {2018},
  month = dec,
  volume = {8},
  pages = {20180041},
  issn = {2042-8898, 2042-8901},
  doi = {10.1098/rsfs.2018.0041},
  file = {/Users/qualia/Documents/Papers/Kolchinsky and Wolpert - 2018 - Semantic information, autonomous agency and non-eq.pdf},
  journal = {Interface Focus},
  language = {en},
  number = {6}
}

@article{Kopell2014,
  title = {Beyond the {{Connectome}}: {{The Dynome}}},
  shorttitle = {Beyond the {{Connectome}}},
  author = {Kopell, Nancy J. and Gritton, Howard J. and Whittington, Miles A. and Kramer, Mark A.},
  year = {2014},
  month = sep,
  volume = {83},
  pages = {1319--1328},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.08.016},
  file = {/Users/qualia/Documents/Papers/2014 - Kopell et al. - Perspective Beyond the Connectome The Dynome.pdf;/Users/qualia/Documents/Papers/Kopell et al. - 2014 - Beyond the Connectome The Dynome.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Kostrikov2020,
  title = {Image {{Augmentation Is All You Need}}: {{Regularizing Deep Reinforcement Learning}} from {{Pixels}}},
  shorttitle = {Image {{Augmentation Is All You Need}}},
  author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
  year = {2020},
  month = apr,
  abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC) [20], are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based [21, 31, 22] methods and recently proposed contrastive learning [42]. Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
  archivePrefix = {arXiv},
  eprint = {2004.13649},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Kostrikov et al. - 2020 - Image Augmentation Is All You Need Regularizing D.pdf},
  journal = {arXiv:2004.13649 [cs, eess, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{Kostuk2012,
  title = {Dynamical Estimation of Neuron and Network Properties {{II}}: Path Integral {{Monte Carlo}} Methods},
  shorttitle = {Dynamical Estimation of Neuron and Network Properties {{II}}},
  author = {Kostuk, Mark and Toth, Bryan A. and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2012},
  month = mar,
  volume = {106},
  pages = {155--167},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-012-0487-5},
  file = {/Users/qualia/Documents/Papers/2012 - Kostuk et al. - Dynamical estimation of neuron and network properties II Path integral Monte Carlo methods.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3}
}

@article{Koyama2008,
  title = {Spike {{Train Probability Models}} for {{Stimulus}}-{{Driven Leaky Integrate}}-and-{{Fire Neurons}}},
  author = {Koyama, Shinsuke and Kass, Robert E.},
  year = {2008},
  month = jul,
  volume = {20},
  pages = {1776--1795},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2008.06-07-540},
  file = {/Users/qualia/Documents/Papers/2008 - Koyama, Kass - Spike train probability models for stimulus-driven leaky integrate-and-fire neurons.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {7}
}

@article{Kreiss1997,
  title = {The {{Response}} of {{Subthalamic Nucleus Neurons}} to {{Dopamine Receptor Stimulation}} in a {{Rodent Model}} of {{Parkinson}}'s {{Disease}}},
  author = {Kreiss, Deborah S. and Mastropietro, Christopher W. and Rawji, Saima S. and Walters, Judith R.},
  year = {1997},
  month = sep,
  volume = {17},
  pages = {6807--6819},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.17-17-06807.1997},
  file = {/Users/qualia/Documents/Papers/1997 - Kreiss et al. - The response of subthalamic nucleus neurons to dopamine receptor stimulation in a rodent model of Parkinson's dis.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {17}
}

@article{Kreitz2015,
  title = {Working-Memory Performance Is Related to Spatial Breadth of Attention},
  author = {Kreitz, Carina and Furley, Philip and Memmert, Daniel and Simons, Daniel J.},
  year = {2015},
  month = nov,
  volume = {79},
  pages = {1034--1041},
  issn = {0340-0727, 1430-2772},
  doi = {10.1007/s00426-014-0633-x},
  abstract = {Working memory and attention are closely related constructs. Models of working memory often incorporate an attention component, and some even equate working memory and attentional control. Although some attention-related processes, including inhibitory control of response conflict and interference resolution, are strongly associated with working memory, for other aspects of attention the link is less clear. We examined the association between working-memory performance and attentional breadth, the ability to spread attention spatially. If the link between attention and working memory is broader than inhibitory and interference resolution processes, then working-memory performance might also be associated with other attentional abilities, including attentional breadth. We tested 123 participants on a variety of working-memory and attentional-breadth measures, finding a strong correlation between performances on these two types of tasks. This finding demonstrates that the link between working memory and attention extends beyond inhibitory processes.},
  file = {/Users/qualia/Documents/Papers/2014 - Kreitz et al. - Working-memory performance is related to spatial breadth of attention.pdf;/Users/qualia/Documents/Papers/Kreitz et al. - 2015 - Working-memory performance is related to spatial b.pdf},
  journal = {Psychological Research},
  language = {en},
  number = {6}
}

@article{Kriegeskorte2007,
  title = {Analyzing for Information, Not Activation, to Exploit High-Resolution {{fMRI}}},
  author = {Kriegeskorte, Nikolaus and Bandettini, Peter},
  year = {2007},
  month = dec,
  volume = {38},
  pages = {649--662},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.02.022},
  file = {/Users/qualia/Documents/Papers/2007 - Kriegeskorte, Bandettini - Analyzing for information, not activation, to exploit high-resolution fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Kriegeskorte2007a,
  title = {Combining the Tools: {{Activation}}- and Information-Based {{fMRI}} Analysis},
  shorttitle = {Combining the Tools},
  author = {Kriegeskorte, Nikolaus and Bandettini, Peter},
  year = {2007},
  month = dec,
  volume = {38},
  pages = {666--668},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.06.030},
  file = {/Users/qualia/Documents/Papers/2007 - Kriegeskorte, Bandettini - Combining the tools activation- and information-based fMRI analysis.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Kriegeskorte2008,
  title = {Representational Similarity Analysis \textendash{} Connecting the Branches of Systems Neuroscience},
  author = {Kriegeskorte, Nikolaus},
  year = {2008},
  issn = {16625137},
  doi = {10.3389/neuro.06.004.2008},
  abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities.The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
  file = {/Users/qualia/Documents/Papers/2008 - Kriegeskorte, Mur, Bandettini - Representational similarity analysis - connecting the branches of systems neuroscience.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Kriegeskorte2011,
  title = {Pattern-Information Analysis: {{From}} Stimulus Decoding to Computational-Model Testing},
  shorttitle = {Pattern-Information Analysis},
  author = {Kriegeskorte, Nikolaus},
  year = {2011},
  month = may,
  volume = {56},
  pages = {411--421},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.01.061},
  abstract = {Pattern-information analysis has become an important new paradigm in functional imaging. Here I review and compare existing approaches with a focus on the question of what we can learn from them in terms of brain theory. The most popular and widespread method is stimulus decoding by response-pattern classification. This approach addresses the question whether activity patterns in a given region carry information about the stimulus category. Pattern classification uses generic models of the stimulus\textendash{}response relationship that do not mimic brain information processing and treats the stimulus space as categorical\textemdash{}a simplification that is often helpful, but also limiting in terms of the questions that can be addressed. We can address the question whether representations are consistent across different stimulus sets or tasks by crossdecoding, where the classifier is trained with one set of stimuli (or task) and tested with another. Beyond pattern classification, a major new direction is the integration of computational models of brain information processing into pattern-information analysis. This approach enables us to address the question to what extent competing computational models are consistent with the stimulus representations in a brain region. Two methods that test computational models are voxel receptive-field modeling and representational similarity analysis. These methods sample the stimulus (or mental-state) space more richly, estimate a separate response pattern for each stimulus, and can generalize from the stimulus sample to a stimulus population. Computational models that mimic brain information processing predict responses from stimuli. The reverse transform can be modeled to reconstruct stimuli from responses. Stimulus reconstruction is a challenging feat of engineering, but the implications of the results for brain theory are not always clear. Exploratory pattern analyses complement the confirmatory approaches mentioned so far and can reveal strong, unexpected effects that might be missed when testing only a restricted set of predefined hypotheses.},
  file = {/Users/qualia/Documents/Papers/2011 - Kriegeskorte - Pattern-information analysis from stimulus decoding to computational-model testing.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Kriegeskorte2016,
  title = {Inferring Brain-Computational Mechanisms with Models of Activity Measurements},
  author = {Kriegeskorte, Nikolaus and Diedrichsen, J{\"o}rn},
  year = {2016},
  month = oct,
  volume = {371},
  pages = {20160278},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2016.0278},
  file = {/Users/qualia/Documents/Papers/Kriegeskorte and Diedrichsen - 2016 - Inferring brain-computational mechanisms with mode.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1705}
}

@article{Kriegeskorte2018,
  title = {Cognitive Computational Neuroscience},
  author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
  year = {2018},
  month = sep,
  volume = {21},
  pages = {1148--1160},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-018-0210-5},
  file = {/Users/qualia/Documents/Papers/Kriegeskorte and Douglas - 2018 - Cognitive computational neuroscience.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Kriegman2020,
  title = {A Scalable Pipeline for Designing Reconfigurable Organisms},
  author = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
  year = {2020},
  month = jan,
  volume = {117},
  pages = {1853--1859},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1910837117},
  abstract = {Living systems are more robust, diverse, complex, and supportive of human life than any technology yet created. However, our ability to create novel lifeforms is currently limited to varying existing organisms or bioengineering organoids in vitro. Here we show a scalable pipeline for creating functional novel lifeforms: AI methods automatically design diverse candidate lifeforms in silico to perform some desired function, and transferable designs are then created using a cell-based construction toolkit to realize living systems with the predicted behaviors. Although some steps in this pipeline still require manual intervention, complete automation in future would pave the way to designing and deploying unique, bespoke living systems for a wide range of functions.},
  file = {/Users/qualia/Documents/Papers/Kriegman et al. - 2020 - A scalable pipeline for designing reconfigurable o.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {4}
}

@article{Krnjevic1971,
  title = {The Mechanism of Excitation by Acetylcholine in the Cerebral Cortex},
  author = {Krnjevi{\'c}, K. and Pumain, R. and Renaud, L.},
  year = {1971},
  month = may,
  volume = {215},
  pages = {247--268},
  issn = {00223751},
  doi = {10.1113/jphysiol.1971.sp009467},
  file = {/Users/qualia/Documents/Papers/1971 - Krnjevj, Pumain, Renaudt - THE MECHANISM OF EXCITATION BY ACETYLCHOLINE IN THE CEREBRAL CORTEX BY.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {1}
}

@article{Krueger,
  title = {Active {{Reinforcement Learning}}: {{Observing Rewards}} at a {{Cost}}},
  author = {Krueger, David and Leike, Jan and Evans, Owain and Salvatier, John},
  pages = {9},
  abstract = {Active reinforcement learning (ARL) is a variant on reinforcement learning where the agent does not observe the reward unless it chooses to pay a query cost c {$>$} 0. The central question of ARL is how to quantify the long-term value of reward information. Even in multi-armed bandits, computing the value of this information is intractable and we have to rely on heuristics. We propose and evaluate several heuristic approaches for ARL in multi-armed bandits and (tabular) Markov decision processes, and discuss and illustrate some challenging aspects of the ARL problem.},
  file = {/Users/qualia/Documents/Papers/Krueger et al. - Active Reinforcement Learning Observing Rewards a.pdf},
  language = {en}
}

@article{Ku2008,
  title = {Comparison of Pattern Recognition Methods in Classifying High-Resolution {{BOLD}} Signals Obtained at High Magnetic Field in Monkeys},
  author = {Ku, Shih-pi and Gretton, Arthur and Macke, Jakob and Logothetis, Nikos K.},
  year = {2008},
  month = sep,
  volume = {26},
  pages = {1007--1014},
  issn = {0730725X},
  doi = {10.1016/j.mri.2008.02.016},
  abstract = {Pattern recognition methods have shown that functional magnetic resonance imaging (fMRI) data can reveal significant information about brain activity. For example, in the debate of how object categories are represented in the brain, multivariate analysis has been used to provide evidence of a distributed encoding scheme [Science 293:5539 (2001) 2425\textendash{}2430]. Many follow-up studies have employed different methods to analyze human fMRI data with varying degrees of success [Nature reviews 7:7 (2006) 523\textendash{}534]. In this study, we compare four popular pattern recognition methods: correlation analysis, support-vector machines (SVM), linear discriminant analysis (LDA) and Gaussian na{\"i}ve Bayes (GNB), using data collected at high field (7 Tesla) with higher resolution than usual fMRI studies. We investigate prediction performance on single trials and for averages across varying numbers of stimulus presentations. The performance of the various algorithms depends on the nature of the brain activity being categorized: for several tasks, many of the methods work well, whereas for others, no method performs above chance level. An important factor in overall classification performance is careful preprocessing of the data, including dimensionality reduction, voxel selection and outlier elimination.},
  file = {/Users/qualia/Documents/Papers/2008 - Ku et al. - Comparison of pattern recognition methods in classifying high-resolution BOLD signals obtained at high magnetic field.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {7}
}

@article{Kuchibhotla2017,
  title = {Parallel Processing by Cortical Inhibition Enables Context-Dependent Behavior},
  author = {Kuchibhotla, Kishore V and Gill, Jonathan V and Lindsay, Grace W and Papadoyannis, Eleni S and Field, Rachel E and Sten, Tom A Hindmarsh and Miller, Kenneth D and Froemke, Robert C},
  year = {2017},
  month = jan,
  volume = {20},
  pages = {62--71},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4436},
  file = {/Users/qualia/Documents/Papers/Kuchibhotla et al. - 2017 - Parallel processing by cortical inhibition enables.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {1}
}

@article{Kuczala2016,
  title = {Eigenvalue Spectra of Large Correlated Random Matrices},
  author = {Kuczala, Alexander and Sharpee, Tatyana O.},
  year = {2016},
  month = nov,
  volume = {94},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.050101},
  file = {/Users/qualia/Documents/Papers/Kuczala and Sharpee - 2016 - Eigenvalue spectra of large correlated random matr.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Kuga2011,
  title = {Large-{{Scale Calcium Waves Traveling}} through {{Astrocytic Networks In Vivo}}},
  author = {Kuga, N. and Sasaki, T. and Takahara, Y. and Matsuki, N. and Ikegaya, Y.},
  year = {2011},
  month = feb,
  volume = {31},
  pages = {2607--2614},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5319-10.2011},
  file = {/Users/qualia/Documents/Papers/Kuga et al. - 2011 - Large-Scale Calcium Waves Traveling through Astroc.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Kuhn2006,
  title = {Modulation of Beta Oscillations in the Subthalamic Area during Motor Imagery in {{Parkinson}}'s Disease},
  author = {K{\"u}hn, Andrea A. and Doyle, Louise and Pogosyan, Alek and Yarrow, Kielan and Kupsch, Andreas and Schneider, Gerd-Helge and Hariz, Marwan I. and Trottenberg, Thomas and Brown, Peter},
  year = {2006},
  month = mar,
  volume = {129},
  pages = {695--706},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awh715},
  file = {/Users/qualia/Documents/Papers/2006 - Kühn et al. - Modulation of beta oscillations in the subthalamic area during motor imagery in Parkinson's disease.pdf},
  journal = {Brain},
  language = {en},
  number = {3}
}

@article{Kulkarni2016,
  title = {Hierarchical {{Deep Reinforcement Learning}}: {{Integrating Temporal Abstraction}} and {{Intrinsic Motivation}}},
  shorttitle = {Hierarchical {{Deep Reinforcement Learning}}},
  author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
  year = {2016},
  month = apr,
  abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.},
  archivePrefix = {arXiv},
  eprint = {1604.06057},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Kulkarni et al. - 2016 - Hierarchical Deep Reinforcement Learning Integrat.pdf},
  journal = {arXiv:1604.06057 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Kulkarni2016a,
  title = {Deep {{Successor Reinforcement Learning}}},
  author = {Kulkarni, Tejas D and Saeedi, Ardavan and Gautam, Simanta and Gershman, Samuel J},
  year = {2016},
  volume = {1606.02396v1},
  pages = {1--10},
  file = {/Users/qualia/Documents/Papers/Kulkarni et al. - Deep Successor Reinforcement Learning.pdf},
  journal = {Arxiv},
  language = {en}
}

@article{Kulkarni2019,
  title = {Unsupervised {{Learning}} of {{Object Keypoints}} for {{Perception}} and {{Control}}},
  author = {Kulkarni, Tejas and Gupta, Ankush and Ionescu, Catalin and Borgeaud, Sebastian and Reynolds, Malcolm and Zisserman, Andrew and Mnih, Volodymyr},
  year = {2019},
  month = jun,
  abstract = {The study of object representations in computer vision has primarily focused on developing representations that are useful for image classification, object detection, or semantic segmentation as downstream tasks. In this work we aim to learn object representations that are useful for control and reinforcement learning (RL). To this end, we introduce Transporter, a neural network architecture for discovering concise geometric object representations in terms of keypoints or image-space coordinates. Our method learns from raw video frames in a fully unsupervised manner, by transporting learnt image features between video frames using a keypoint bottleneck. The discovered keypoints track objects and object parts across long time-horizons more accurately than recent similar methods. Furthermore, consistent long-term tracking enables two notable results in control domains \textendash{} (1) using the keypoint co-ordinates and corresponding image features as inputs enables highly sample-efficient reinforcement learning; (2) learning to explore by controlling keypoint locations drastically reduces the search space, enabling deep exploration (leading to states unreachable through random action exploration) without any extrinsic rewards.},
  archivePrefix = {arXiv},
  eprint = {1906.11883},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Kulkarni et al. - 2019 - Unsupervised Learning of Object Keypoints for Perc.pdf},
  journal = {arXiv:1906.11883 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Kumar2011,
  title = {The {{Role}} of {{Inhibition}} in {{Generating}} and {{Controlling Parkinson}}?S {{Disease Oscillations}} in the {{Basal Ganglia}}},
  shorttitle = {The {{Role}} of {{Inhibition}} in {{Generating}} and {{Controlling Parkinson}}?},
  author = {Kumar, Arvind and Cardanobile, Stefano and Rotter, Stefan and Aertsen, Ad},
  year = {2011},
  volume = {5},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2011.00086},
  abstract = {Movement disorders in Parkinson's disease (PD) are commonly associated with slow oscillations and increased synchrony of neuronal activity in the basal ganglia. The neural mechanisms underlying this dynamic network dysfunction, however, are only poorly understood. Here, we show that the strength of inhibitory inputs from striatum to globus pallidus external (GPe) is a key parameter controlling oscillations in the basal ganglia. Specifically, the increase in striatal activity observed in PD is sufficient to unleash the oscillations in the basal ganglia. This finding allows us to propose a unified explanation for different phenomena: absence of oscillation in the healthy state of the basal ganglia, oscillations in dopamine-depleted state and quenching of oscillations under deep-brain-stimulation (DBS). These novel insights help us to better understand and optimize the function of DBS protocols. Furthermore, studying the model behavior under transient increase of activity of the striatal neurons projecting to the indirect pathway, we are able to account for both motor impairment in PD patients and for reduced response inhibition in DBS implanted patients.},
  file = {/Users/qualia/Documents/Papers/2011 - Kumar et al. - The Role of Inhibition in Generating and Controlling Parkinsons Disease Oscillations in the Basal Ganglia.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Kumar2015,
  title = {Ask {{Me Anything}}: {{Dynamic Memory Networks}} for {{Natural Language Processing}}},
  shorttitle = {Ask {{Me Anything}}},
  author = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  year = {2015},
  month = jun,
  abstract = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
  archivePrefix = {arXiv},
  eprint = {1506.07285},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Kumar et al. - Ask Me Anything Dynamic Memory Networks for Natural Language Processing.pdf;/Users/qualia/Documents/Papers/Kumar et al. - 2015 - Ask Me Anything Dynamic Memory Networks for Natur.pdf},
  journal = {arXiv:1506.07285 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Kunori2014,
  title = {Voltage-{{Sensitive Dye Imaging}} of {{Primary Motor Cortex Activity Produced}} by {{Ventral Tegmental Area Stimulation}}},
  author = {Kunori, N. and Kajiwara, R. and Takashima, I.},
  year = {2014},
  month = jun,
  volume = {34},
  pages = {8894--8903},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5286-13.2014},
  file = {/Users/qualia/Documents/Papers/Kunori et al. - 2014 - Voltage-Sensitive Dye Imaging of Primary Motor Cor.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {26}
}

@article{Kushnir2017,
  title = {Neural Classifiers with Limited Connectivity and Recurrent Readouts},
  author = {Kushnir, Lyudmila and Fusi, Stefano},
  year = {2017},
  month = dec,
  doi = {10.1101/157289},
  abstract = {For many neural network models that are based on perceptrons, the number of activity patterns that can be classified is limited by the number of plastic connections that each neuron receives, even when the total number of neurons is much larger. This poses the problem of how the biological brain can take advantage of its huge number of neurons given that the connectivity is extremely sparse, especially when long range connections are considered. One possible way to overcome this limitation in the case of feed-forward networks is to combine multiple perceptrons together, as in committee machines. The number of classifiable random patterns would then grow linearly with the number of perceptrons, even when each perceptron has limited connectivity. However, the problem is moved to the downstream readout neurons, which would need a number of connections that is as large as the number of perceptrons. Here we propose a different approach in which the readout is implemented by connecting multiple perceptrons in a recurrent attractor neural network. We show with analytical calculations that the number of random classifiable patterns can grow unboundedly with the number of perceptrons, even when the connectivity of each perceptron remains finite. Most importantly both the recurrent connectivity and the connectivity of a downstream readout are also finite. Our study shows that feed-forward neural classifiers with numerous long range connections connecting different layers can be replaced by networks with sparse long range connectivity and local recurrent connectivity without sacrificing the classification performance. Our strategy could be used in the future to design more general scalable network architectures with limited connectivity, which resemble more closely brain neural circuits dominated by recurrent connectivity.},
  file = {/Users/qualia/Documents/Papers/Kushnir and Fusi - 2017 - Neural classifiers with limited connectivity and r.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Kuznetsov,
  title = {Elements of {{Applied Bifurcation Theory}}, {{Second Edition}}},
  author = {Kuznetsov, Yuri A},
  pages = {614},
  file = {/Users/qualia/Documents/Papers/1998 - Kuznetsov - Elements of Applied Bifurcation Theory.pdf}
}

@article{Kvam2015,
  title = {Interference Effects of Choice on Confidence: {{Quantum}} Characteristics of Evidence Accumulation},
  shorttitle = {Interference Effects of Choice on Confidence},
  author = {Kvam, Peter D. and Pleskac, Timothy J. and Yu, Shuli and Busemeyer, Jerome R.},
  year = {2015},
  month = aug,
  volume = {112},
  pages = {10645--10650},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1500688112},
  file = {/Users/qualia/Documents/Papers/Kvam et al. - 2015 - Interference effects of choice on confidence Quan.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {34}
}

@article{Laan2017,
  title = {Echo State Networks with Multiple Read-out Modules},
  author = {Laan, Andres and Vicente, Raul},
  year = {2017},
  month = mar,
  doi = {10.1101/017558},
  abstract = {We propose a new readout architecture for echo state networks where multiple linear readout modules are activated at distinct time points to varying degrees by a separate controller module. The controller module, like the reservoir of the echo state network, can be initialized randomly. All linear readout modules are trained through simple linear regression, which is the only adaptive step in the modified algorithm. The resulting architecture provides modest improvements on a variety of time series processing tasks (between 5 to 50\% in performance metric depending on the task studied). The novel architecture is guaranteed to perform at least as accurately as a conventional linear readout. It can be utilized as a general purpose readout method when augmentations to performance relative to the standard method is needed.},
  file = {/Users/qualia/Documents/Papers/2015 - Laan - Echo state networks with multiple read-out modules.pdf;/Users/qualia/Documents/Papers/Laan and Vicente - 2017 - Echo state networks with multiple read-out modules.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Laconte2005,
  title = {Support Vector Machines for Temporal Classification of Block Design {{fMRI}} Data},
  author = {Laconte, S and Strother, S and Cherkassky, V and Anderson, J and Hu, X},
  year = {2005},
  month = jun,
  volume = {26},
  pages = {317--329},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2005.01.048},
  file = {/Users/qualia/Documents/Papers/2005 - LaConte et al. - Support vector machines for temporal classification of block design fMRI data.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{LaConte2007,
  title = {Real-Time {{fMRI}} Using Brain-State Classification},
  author = {LaConte, Stephen M. and Peltier, Scott J. and Hu, Xiaoping P.},
  year = {2007},
  month = oct,
  volume = {28},
  pages = {1033--1044},
  issn = {10659471, 10970193},
  doi = {10.1002/hbm.20326},
  abstract = {We have implemented a real-time functional magnetic resonance imaging system based on multivariate classification. This approach is distinctly different from spatially localized real-time implementations, since it does not require prior assumptions about functional localization and individual performance strategies, and has the ability to provide feedback based on intuitive translations of brain state rather than localized fluctuations. Thus this approach provides the capability for a new class of experimental designs in which real-time feedback control of the stimulus is possible\textemdash{}rather than using a fixed paradigm, experiments can adaptively evolve as subjects receive brain-state feedback. In this report, we describe our implementation and characterize its performance capabilities. We observed \$80\% classification accuracy using whole brain, block-design, motor data. Within both left and right motor task conditions, important differences exist between the initial transient period produced by task switching (changing between rapid left or right index finger button presses) and the subsequent stable period during sustained activity. Further analysis revealed that very high accuracy is achievable during stable task periods, and that the responsiveness of the classifier to changes in task condition can be much faster than signal time-to-peak rates. Finally, we demonstrate the versatility of this implementation with respect to behavioral task, suggesting that our results are applicable across a spectrum of cognitive domains. Beyond basic research, this technology can complement electroencephalography-based brain computer interface research, and has potential applications in the areas of biofeedback rehabilitation, lie detection, learning studies, virtual reality-based training, and enhanced conscious awareness. Hum Brain Mapp 28:1033\textendash{}1044, 2007. VC 2006 Wiley-Liss, Inc.},
  file = {/Users/qualia/Documents/Papers/2007 - LaConte, Peltier, Hu - Real-time fMRI using brain-state classification.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {10}
}

@article{Lagorce2015,
  title = {{{STICK}}: {{Spike Time Interval Computational Kernel}}, a {{Framework}} for {{General Purpose Computation Using Neurons}}, {{Precise Timing}}, {{Delays}}, and {{Synchrony}}},
  shorttitle = {{{STICK}}},
  author = {Lagorce, Xavier and Benosman, Ryad},
  year = {2015},
  month = nov,
  volume = {27},
  pages = {2261--2317},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00783},
  file = {/Users/qualia/Documents/Papers/2015 - Lagorce, Benosman - STICK Spike Time Interval Computational Kernel, a Framework for General Purpose Computation Using Neurons, Pr.pdf;/Users/qualia/Documents/Papers/Lagorce and Benosman - 2015 - STICK Spike Time Interval Computational Kernel, a.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@article{Lahiri,
  title = {A Universal Tradeoff between Power, Precision and Speed in Physical Communication},
  author = {Lahiri, Subhaneil and {Sohl-Dickstein}, Jascha and Ganguli, Surya},
  pages = {6},
  file = {/Users/qualia/Documents/Papers/Lahiri et al. - A universal tradeoﬀ between power, precision and s.pdf},
  language = {en}
}

@article{Laing2006,
  title = {On the Application of ``Equation-Free Modelling'' to Neural Systems},
  author = {Laing, Carlo R.},
  year = {2006},
  month = feb,
  volume = {20},
  pages = {5--23},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-006-3843-z},
  abstract = {Equation-free modelling'' is a recentlydeveloped technique for bridging the gap between detailed, microscopic descriptions of systems and macroscopic descriptions of their collective behaviour. It uses short, repeated bursts of simulation of the microscopic dynamics to analyse the effective macroscopic equations, even though such equations are not directly available for evaluation. This paper demonstrates these techniques on a variety of networks of model neurons, and discusses the advantages and limitations of such an approach. New results include an understanding of the effects of including gap junctions in a model capable of sustaining spatially localised ``bumps'' of activity, and an investigation of a network of coupled bursting neurons.},
  file = {/Users/qualia/Documents/Papers/2006 - Laing - On the application of equation-free modelling to neural systems.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Laing2014,
  title = {Numerical {{Bifurcation Theory}} for {{High}}-{{Dimensional Neural Models}}},
  author = {Laing, Carlo R},
  year = {2014},
  volume = {4},
  pages = {13},
  issn = {2190-8567},
  doi = {10.1186/2190-8567-4-13},
  abstract = {Numerical bifurcation theory involves finding and then following certain types of solutions of differential equations as parameters are varied, and determining whether they undergo any bifurcations (qualitative changes in behaviour). The primary technique for doing this is numerical continuation, where the solution of interest satisfies a parametrised set of algebraic equations, and branches of solutions are followed as the parameter is varied. An effective way to do this is with pseudoarclength continuation. We give an introduction to pseudo-arclength continuation and then demonstrate its use in investigating the behaviour of a number of models from the field of computational neuroscience. The models we consider are high dimensional, as they result from the discretisation of neural field models\textemdash{}nonlocal differential equations used to model macroscopic pattern formation in the cortex. We consider both stationary and moving patterns in one spatial dimension, and then translating patterns in two spatial dimensions. A variety of results from the literature are discussed, and a number of extensions of the technique are given.},
  file = {/Users/qualia/Documents/Papers/2014 - Laing - Numerical Bifurcation Theory for High-Dimensional Neural Models.pdf;/Users/qualia/Documents/Papers/2014 - Laing - Numerical Bifurcation Theory for High-Dimensional Neural Models(2).pdf;/Users/qualia/Documents/Papers/Laing - 2014 - Numerical Bifurcation Theory for High-Dimensional  2.pdf;/Users/qualia/Documents/Papers/Laing - 2014 - Numerical Bifurcation Theory for High-Dimensional .pdf},
  journal = {The Journal of Mathematical Neuroscience},
  language = {en},
  number = {1}
}

@article{Lainscsek2015,
  title = {Delay {{Differential Analysis}} of {{Time Series}}},
  author = {Lainscsek, Claudia and Sejnowski, Terrence J.},
  year = {2015},
  month = mar,
  volume = {27},
  pages = {594--614},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00706},
  file = {/Users/qualia/Documents/Papers/2014 - Lehky et al. - Dimensionality of object representations in monkey inferotemporal cortex(2).pdf;/Users/qualia/Documents/Papers/Lainscsek and Sejnowski - 2015 - Delay Differential Analysis of Time Series.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {3}
}

@article{Laje2013,
  title = {Robust Timing and Motor Patterns by Taming Chaos in Recurrent Neural Networks},
  author = {Laje, Rodrigo and Buonomano, Dean V},
  year = {2013},
  month = jul,
  volume = {16},
  pages = {925--933},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3405},
  file = {/Users/qualia/Documents/Papers/2013 - Laje, Buonomano - Robust timing and motor patterns by taming chaos in recurrent neural networks.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {7}
}

@article{Lajoie2014,
  title = {Structured Chaos Shapes Spike-Response Noise Entropy in Balanced Neural Networks},
  author = {Lajoie, Guillaume and Thivierge, Jean-Philippe and {Shea-Brown}, Eric},
  year = {2014},
  month = oct,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00123},
  abstract = {Large networks of sparsely coupled, excitatory and inhibitory cells occur throughout the brain. For many models of these networks, a striking feature is that their dynamics are chaotic and thus, are sensitive to small perturbations. How does this chaos manifest in the neural code? Specifically, how variable are the spike patterns that such a network produces in response to an input signal? To answer this, we derive a bound for a general measure of variability\textemdash{}spike-train entropy. This leads to important insights on the variability of multi-cell spike pattern distributions in large recurrent networks of spiking neurons responding to fluctuating inputs. The analysis is based on results from random dynamical systems theory and is complemented by detailed numerical simulations. We find that the spike pattern entropy is an order of magnitude lower than what would be extrapolated from single cells. This holds despite the fact that network coupling becomes vanishingly sparse as network size grows\textemdash{}a phenomenon that depends on ``extensive chaos,'' as previously discovered for balanced networks without stimulus drive. Moreover, we show how spike pattern entropy is controlled by temporal features of the inputs. Our findings provide insight into how neural networks may encode stimuli in the presence of inherently chaotic dynamics.},
  file = {/Users/qualia/Documents/Papers/Lajoie et al. - 2014 - Structured chaos shapes spike-response noise entro.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Lambert2012,
  title = {Confirmation of Functional Zones within the Human Subthalamic Nucleus: {{Patterns}} of Connectivity and Sub-Parcellation Using Diffusion Weighted Imaging},
  shorttitle = {Confirmation of Functional Zones within the Human Subthalamic Nucleus},
  author = {Lambert, Christian and Zrinzo, Ludvic and Nagy, Zoltan and Lutti, Antoine and Hariz, Marwan and Foltynie, Thomas and Draganski, Bogdan and Ashburner, John and Frackowiak, Richard},
  year = {2012},
  month = mar,
  volume = {60},
  pages = {83--94},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.11.082},
  abstract = {The subthalamic nucleus (STN) is a small, glutamatergic nucleus situated in the diencephalon. A critical component of normal motor function, it has become a key target for deep brain stimulation in the treatment of Parkinson's disease. Animal studies have demonstrated the existence of three functional sub-zones but these have never been shown conclusively in humans. In this work, a data driven method with diffusion weighted imaging demonstrated that three distinct clusters exist within the human STN based on brain connectivity profiles. The STN was successfully sub-parcellated into these regions, demonstrating good correspondence with that described in the animal literature. The local connectivity of each sub-region supported the hypothesis of bilateral limbic, associative and motor regions occupying the anterior, mid and posterior portions of the nucleus respectively. This study is the first to achieve in-vivo, non-invasive anatomical parcellation of the human STN into three anatomical zones within normal diagnostic scan times, which has important future implications for deep brain stimulation surgery.},
  file = {/Users/qualia/Documents/Papers/2012 - Lambert et al. - Confirmation of functional zones within the human subthalamic nucleus Patterns of connectivity and sub-parcellat.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Lange2013,
  title = {Reduced {{Occipital Alpha Power Indexes Enhanced Excitability Rather}} than {{Improved Visual Perception}}},
  author = {Lange, J. and Oostenveld, R. and Fries, P.},
  year = {2013},
  month = feb,
  volume = {33},
  pages = {3212--3220},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3755-12.2013},
  file = {/Users/qualia/Documents/Papers/2013 - Lange, Oostenveld, Fries - Reduced Occipital Alpha Power Indexes Enhanced Excitability Rather than Improved Visual Perception.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Lansdell2019,
  title = {Spiking Allows Neurons to Estimate Their Causal Effect},
  author = {Lansdell, Benjamin James and Kording, Konrad Paul},
  year = {2019},
  month = feb,
  doi = {10.1101/253351},
  abstract = {Neural plasticity can be seen as ultimately aiming at the maximization of reward. However, the world is complicated and nonlinear and so are neurons' firing properties. A neuron learning to make changes that lead to the maximization of reward is an estimation problem: would there be more reward if the neural activity had been different? Statistically, this is a causal inference problem. Here we show how the spiking discontinuity of neurons can be a tool to estimate the causal influence of a neuron's activity on reward. We show how it can be used to derive a novel learning rule that can operate in the presence of non-linearities and the confounding influence of other neurons. We establish a link between simple learning rules and an existing causal inference method from econometrics.},
  file = {/Users/qualia/Documents/Papers/Lansdell and Kording - 2019 - Spiking allows neurons to estimate their causal ef.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Latimer2015,
  title = {Single-Trial Spike Trains in Parietal Cortex Reveal Discrete Steps during Decision-Making},
  author = {Latimer, K. W. and Yates, J. L. and Meister, M. L. R. and Huk, A. C. and Pillow, J. W.},
  year = {2015},
  month = jul,
  volume = {349},
  pages = {184--187},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaa4056},
  abstract = {Neurons in the macaque lateral intraparietal (LIP) area exhibit firing rates that appear to ramp upwards or downwards during decision-making. These ramps are commonly assumed to reflect the gradual accumulation of evidence towards a decision threshold. However, the ramping in trialaveraged responses could instead arise from instantaneous jumps at different times on different trials. We examined single-trial responses in LIP using statistical methods for fitting and comparing latent dynamical spike train models. We compared models with latent spike rates governed by either continuous diffusion-to-bound dynamics or discrete ``stepping'' dynamics. Roughly three-quarters of the choice-selective neurons we recorded were better described by the stepping model. Moreover, the inferred steps carried more information about the animal's choice than spike counts.},
  file = {/Users/qualia/Documents/Papers/2015 - Biphenyls - HHS Public Access.pdf;/Users/qualia/Documents/Papers/2015 - Latimer et al. - Single-trial spike trains in parietal cortex reveal discrete steps during decision-making.pdf;/Users/qualia/Documents/Papers/Latimer et al. - 2015 - Single-trial spike trains in parietal cortex revea 2.pdf;/Users/qualia/Documents/Papers/Latimer et al. - 2015 - Single-trial spike trains in parietal cortex revea.pdf},
  journal = {Science},
  language = {en},
  number = {6244}
}

@article{Law2015,
  title = {Data {{Assimilation}}: {{A Mathematical Introduction}}},
  shorttitle = {Data {{Assimilation}}},
  author = {Law, K. J. H. and Stuart, A. M. and Zygalakis, K. C.},
  year = {2015},
  month = jun,
  abstract = {These notes provide a systematic mathematical treatment of the subject of data assimilation.},
  archivePrefix = {arXiv},
  eprint = {1506.07825},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - June - Data Assimilation A Mathematical Introduction.pdf;/Users/qualia/Documents/Papers/Law et al. - 2015 - Data Assimilation A Mathematical Introduction.pdf},
  journal = {arXiv:1506.07825 [math, stat]},
  keywords = {Mathematics - Dynamical Systems,Mathematics - Optimization and Control,Statistics - Methodology},
  language = {en},
  primaryClass = {math, stat}
}

@article{Leblois2006,
  title = {Competition between {{Feedback Loops Underlies Normal}} and {{Pathological Dynamics}} in the {{Basal Ganglia}}},
  author = {Leblois, A.},
  year = {2006},
  month = mar,
  volume = {26},
  pages = {3567--3583},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5050-05.2006},
  file = {/Users/qualia/Documents/Papers/2006 - Leblois - Competition between Feedback Loops Underlies Normal and Pathological Dynamics in the Basal Ganglia.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {13}
}

@article{Lebovich2019,
  title = {Idiosyncratic Choice Bias Naturally Emerges from Intrinsic Stochasticity in Neuronal Dynamics},
  author = {Lebovich, Lior and Darshan, Ran and Lavi, Yoni and Hansel, David and Loewenstein, Yonatan},
  year = {2019},
  month = sep,
  issn = {2397-3374},
  doi = {10.1038/s41562-019-0682-7},
  file = {/Users/qualia/Documents/Papers/Lebovich et al. - 2019 - Idiosyncratic choice bias naturally emerges from i.pdf},
  journal = {Nat Hum Behav},
  language = {en}
}

@article{Lee2003,
  title = {"{{Gamma}} (40 {{Hz}}) Phase Synchronicity" and Symptom Dimensions in Schizophrenia},
  author = {Lee, Kwang-Hyuk and Williams, Leanne and Haig, Albert and Gordon, Evian},
  year = {2003},
  month = jan,
  volume = {8},
  pages = {57--71},
  issn = {1354-6805, 1464-0619},
  doi = {10.1080/713752240},
  file = {/Users/qualia/Documents/Papers/2010 - Lee et al. - Gamma ( 40 Hz ) phase synchronicity and symptom dimensions in schizophrenia.pdf},
  journal = {Cognitive Neuropsychiatry},
  language = {en},
  number = {1}
}

@article{Lee2005,
  title = {Learning and Decision Making in Monkeys during a Rock\textendash{}Paper\textendash{}Scissors Game},
  author = {Lee, Daeyeol and McGreevy, Benjamin P. and Barraclough, Dominic J.},
  year = {2005},
  month = oct,
  volume = {25},
  pages = {416--430},
  issn = {09266410},
  doi = {10.1016/j.cogbrainres.2005.07.003},
  abstract = {Game theory provides a solution to the problem of finding a set of optimal decision-making strategies in a group. However, people seldom play such optimal strategies and adjust their strategies based on their experience. Accordingly, many theories postulate a set of variables related to the probabilities of choosing various strategies and describe how such variables are dynamically updated. In reinforcement learning, these value functions are updated based on the outcome of the player's choice, whereas belief learning allows the value functions of all available choices to be updated according to the choices of other players. We investigated the nature of learning process in monkeys playing a competitive game with ternary choices, using a rock \textendash{} paper \textendash{} scissors game. During the baseline condition in which the computer selected its targets randomly, each animal displayed biases towards some targets. When the computer exploited the pattern of animal's choice sequence but not its reward history, the animal's choice was still systematically biased by the previous choice of the computer. This bias was reduced when the computer exploited both the choice and reward histories of the animal. Compared to simple models of reinforcement learning or belief learning, these adaptive processes were better described by a model that incorporated the features of both models. These results suggest that stochastic decision-making strategies in primates during social interactions might be adjusted according to both actual and hypothetical payoffs.},
  file = {/Users/qualia/Documents/Papers/2005 - Lee, McGreevy, Barraclough - Learning and decision making in monkeys during a rock-paper-scissors game.pdf},
  journal = {Cognitive Brain Research},
  language = {en},
  number = {2}
}

@article{Lee2011,
  title = {Investigation of Melodic Contour Processing in the Brain Using Multivariate Pattern-Based {{fMRI}}},
  author = {Lee, Yune-Sang and Janata, Petr and Frost, Carlton and Hanke, Michael and Granger, Richard},
  year = {2011},
  month = jul,
  volume = {57},
  pages = {293--300},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.02.006},
  abstract = {Music perception generally involves processing the frequency relationships between successive pitches and 34 extraction of the melodic contour. Previous evidence has suggested that the `ups' and `downs' of melodic 35 contour are categorically and automatically processed, but knowledge of the brain regions that discriminate 36 different types of contour is limited. Here, we examined melodic contour discrimination using multivariate 37 pattern analysis (MVPA) of fMRI data. Twelve non-musicians were presented with various ascending and 38 descending melodic sequences while being scanned. Whole-brain MVPA was used to identify regions in 39 which the local pattern of activity accurately discriminated between contour categories. We identified three 40 distinct cortical loci: the right superior temporal sulcus (rSTS), the left inferior parietal lobule (lIPL), and the 41 anterior cingulate cortex (ACC). These results complement previous findings of melodic processing within the 42 rSTS, and extend our understanding of the way in which abstract auditory sequences are categorized by the 43 human brain.},
  file = {/Users/qualia/Documents/Papers/2011 - Lee et al. - Investigation of melodic contour processing in the brain using multivariate pattern-based fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Lee2011a,
  title = {Psychological Models of Human and Optimal Performance in Bandit Problems},
  author = {Lee, Michael D. and Zhang, Shunan and Munro, Miles and Steyvers, Mark},
  year = {2011},
  month = jun,
  volume = {12},
  pages = {164--174},
  issn = {13890417},
  doi = {10.1016/j.cogsys.2010.07.007},
  abstract = {In bandit problems, a decision-maker must choose between a set of alternatives, each of which has a fixed but unknown rate of reward, to maximize their total number of rewards over a sequence of trials. Performing well in these problems requires balancing the need to search for highly-rewarding alternatives, with the need to capitalize on those alternatives already known to be reasonably good. Consistent with this motivation, we develop a new psychological model that relies on switching between latent exploration and exploitation states. We test the model over a range of two-alternative bandit problems, against both human and optimal decision-making data, comparing it to benchmark models from the reinforcement learning literature. By making inferences about the latent states from optimal decision-making behavior, we characterize how people should switch between exploration and exploitation. By making inferences from human data, we begin to characterize how people actually do switch. We discuss the implications of these findings for understanding and measuring the competing demands of exploration and exploitation in sequential decision-making.},
  file = {/Users/qualia/Documents/Papers/2011 - Lee et al. - Psychological models of human and optimal performance in bandit problems.pdf;/Users/qualia/Downloads/Lee et al. - 2011 - Psychological models of human and optimal performa.pdf},
  journal = {Cognitive Systems Research},
  language = {en},
  number = {2}
}

@article{Lee2014,
  title = {Interneuron Subtypes and Orientation Tuning},
  author = {Lee, Seung-Hee and Kwan, Alex C. and Dan, Yang},
  year = {2014},
  month = apr,
  volume = {508},
  pages = {E1-E2},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature13128},
  file = {/Users/qualia/Documents/Papers/Lee et al. - 2014 - Interneuron subtypes and orientation tuning.pdf},
  journal = {Nature},
  language = {en},
  number = {7494}
}

@article{Lee2014a,
  title = {Two {{Functionally Distinct Networks}} of {{Gap Junction}}-{{Coupled Inhibitory Neurons}} in the {{Thalamic Reticular Nucleus}}},
  author = {Lee, S.-C. and Patrick, S. L. and Richardson, K. A. and Connors, B. W.},
  year = {2014},
  month = sep,
  volume = {34},
  pages = {13170--13182},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0562-14.2014},
  file = {/Users/qualia/Documents/Papers/Lee et al. - 2014 - Two Functionally Distinct Networks of Gap Junction.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {39}
}

@article{Legon2014,
  title = {Transcranial Focused Ultrasound Modulates the Activity of Primary Somatosensory Cortex in Humans},
  author = {Legon, Wynn and Sato, Tomokazu F and Opitz, Alexander and Mueller, Jerel and Barbour, Aaron and Williams, Amanda and Tyler, William J},
  year = {2014},
  month = feb,
  volume = {17},
  pages = {322--329},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3620},
  file = {/Users/qualia/Documents/Papers/2014 - Legon et al. - Transcranial focused ultrasound modulates the activity of primary somatosensory cortex in humans.pdf;/Users/qualia/Documents/Papers/Legon et al. - 2014 - Transcranial focused ultrasound modulates the acti.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {2}
}

@article{Legon2016,
  title = {Altered {{Prefrontal Excitation}}/{{Inhibition Balance}} and {{Prefrontal Output}}: {{Markers}} of {{Aging}} in {{Human Memory Networks}}},
  shorttitle = {Altered {{Prefrontal Excitation}}/{{Inhibition Balance}} and {{Prefrontal Output}}},
  author = {Legon, Wynn and Punzell, Steven and Dowlati, Ehsan and Adams, Sarah E. and Stiles, Alexandra B. and Moran, Rosalyn J.},
  year = {2016},
  month = oct,
  volume = {26},
  pages = {4315--4326},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhv200},
  abstract = {Memory impairments and heightened prefrontal cortical (PFC) activity are hallmarks of cognitive and neurobiological human aging. While structural integrity of PFC gray matter and interregional white matter tracts are thought to impact memory processing, the balance of neurotransmitters within the PFC itself is less well understood. We used fMRI to establish wholebrain networks involved in a memory encoding task and dynamic causal models (DCMs) for fMRI to determine the causal relationships between these areas. These data revealed enhanced connectivity from PFC to medial temporal cortex that negatively correlated with recall ability. To better understand the intrinsic activity within the PFC, DCM for EEG was employed after continuous theta burst transcranial magnetic stimulation (TMS) to the PFC to assess the effect on excitatory/inhibitory (E/I) synaptic ratios and behavior. These data revealed that the young cohort had a stable E/I ratio that was unaffected by the TMS intervention, while the aged cohort exhibited lower E/I ratios driven by a greater intrinsic inhibitory tone. TMS to the aged cohort resulted in decreased intrinsic inhibition and a decrement in memory performance. These results demonstrate increased topdown influence of PFC upon medial temporal lobe in healthy aging that is associated with decreased memory and may be due to unstable local inhibitory tone within the PFC.},
  file = {/Users/qualia/Documents/Papers/2015 - Legon et al. - Altered Prefrontal ExcitationInhibition Balance and Prefrontal Output Markers of Aging in Human Memory Networks.pdf;/Users/qualia/Documents/Papers/Legon et al. - 2016 - Altered Prefrontal ExcitationInhibition Balance a.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {11}
}

@article{Lehman2011,
  title = {Abandoning {{Objectives}}: {{Evolution Through}} the {{Search}} for {{Novelty Alone}}},
  shorttitle = {Abandoning {{Objectives}}},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  year = {2011},
  month = jun,
  volume = {19},
  pages = {189--223},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/EVCO_a_00025},
  abstract = {In evolutionary computation, the fitness function normally measures progress towards an objective in the search space, effectively acting as an objective function. Through deception, such objective functions may actually prevent the objective from being reached. While methods exist to mitigate deception, they leave the underlying pathology untreated: Objective functions themselves may actively misdirect search towards dead ends. This paper proposes an approach to circumventing deception that also yields a new perspective on open-ended evolution: Instead of either explicitly seeking an objective or modeling natural evolution to capture open-endedness, the idea is to simply search for behavioral novelty. Even in an objective-based problem, such novelty search ignores the objective. Because many points in the search space collapse to a single behavior, the search for novelty is often feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. By decoupling open-ended search from artificial life worlds, the search for novelty is applicable to real world problems. Counterintuitively, in the maze navigation and biped walking tasks in this paper, novelty search significantly outperforms objective-based search, suggesting the strange conclusion that some problems are best solved by methods that ignore the objective. The main lesson is the inherent limitation of the objective-based paradigm and the unexploited opportunity to guide search through other means.},
  file = {/Users/qualia/Documents/Papers/Lehman and Stanley - 2011 - Abandoning Objectives Evolution Through the Searc.pdf},
  journal = {Evolutionary Computation},
  language = {en},
  number = {2}
}

@inproceedings{Lehman2015,
  title = {Enhancing {{Divergent Search}} through {{Extinction Events}}},
  booktitle = {Proceedings of the 2015 on {{Genetic}} and {{Evolutionary Computation Conference}} - {{GECCO}} '15},
  author = {Lehman, Joel and Miikkulainen, Risto},
  year = {2015},
  pages = {951--958},
  publisher = {{ACM Press}},
  address = {{Madrid, Spain}},
  doi = {10.1145/2739480.2754668},
  abstract = {A challenge in evolutionary computation is to create representations as evolvable as those in natural evolution. This paper hypothesizes that extinction events, i.e. mass extinctions, can significantly increase evolvability, but only when combined with a divergent search algorithm, i.e. a search driven towards diversity (instead of optimality). Extinctions amplify diversity-generation by creating unpredictable evolutionary bottlenecks. Persisting through multiple such bottlenecks is more likely for lineages that diversify across many niches, resulting in indirect selection pressure for the capacity to evolve. This hypothesis is tested through experiments in two evolutionary robotics domains. The results show that combining extinction events with divergent search increases evolvability, while combining them with convergent search offers no similar benefit. The conclusion is that extinction events may provide a simple and effective mechanism to enhance performance of divergent search algorithms.},
  file = {/Users/qualia/Documents/Papers/Lehman and Miikkulainen - 2015 - Enhancing Divergent Search through Extinction Even.pdf},
  isbn = {978-1-4503-3472-3},
  language = {en}
}

@inproceedings{Lehman2018,
  title = {Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '18},
  author = {Lehman, Joel and Chen, Jay and Clune, Jeff and Stanley, Kenneth O.},
  year = {2018},
  pages = {117--124},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  doi = {10.1145/3205455.3205473},
  abstract = {While neuroevolution (evolving neural networks) has a successful track record across a variety of domains from reinforcement learning to artificial life, it is rarely applied to large, deep neural networks. A central reason is that while random mutation generally works in low dimensions, a random perturbation of thousands or millions of weights is likely to break existing functionality, providing no learning signal even if some individual weight changes were beneficial. This paper proposes a solution by introducing a family of safe mutation (SM) operators that aim within the mutation operator itself to find a degree of change that does not alter network behavior too much, but still facilitates exploration. Importantly, these SM operators do not require any additional interactions with the environment. The most effective SM variant capitalizes on the intriguing opportunity to scale the degree of mutation of each individual weight according to the sensitivity of the network's outputs to that weight, which requires computing the gradient of outputs with respect to the weights (instead of the gradient of error, as in conventional deep learning). This safe mutation through gradients (SM-G) operator dramatically increases the ability of a simple genetic algorithm-based neuroevolution method to find solutions in high-dimensional domains that require deep and/or recurrent neural networks (which tend to be particularly brittle to mutation), including domains that require processing raw pixels. By improving our ability to evolve deep neural networks, this new safer approach to mutation expands the scope of domains amenable to neuroevolution.},
  file = {/Users/qualia/Documents/Papers/Lehman et al. - 2018 - Safe mutations for deep and recurrent neural netwo.pdf},
  isbn = {978-1-4503-5618-3},
  language = {en}
}

@article{Lehman2018a,
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  shorttitle = {The {{Surprising Creativity}} of {{Digital Evolution}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Altenberg, Lee and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'e}noy, Antoine and Gagn{\'e}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c c}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinski, Jason},
  year = {2018},
  month = mar,
  abstract = {Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
  archivePrefix = {arXiv},
  eprint = {1803.03453},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Lehman et al. - 2018 - The Surprising Creativity of Digital Evolution A .pdf},
  journal = {arXiv:1803.03453 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Lehnertz2009,
  title = {Synchronization Phenomena in Human Epileptic Brain Networks},
  author = {Lehnertz, Klaus and Bialonski, Stephan and Horstmann, Marie-Therese and Krug, Dieter and Rothkegel, Alexander and Staniek, Matth{\"a}us and Wagner, Tobias},
  year = {2009},
  month = sep,
  volume = {183},
  pages = {42--48},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2009.05.015},
  abstract = {Epilepsy is a malfunction of the brain that affects over 50 million people worldwide. Epileptic seizures are usually characterized by an abnormal synchronized firing of neurons involved in the epileptic process. In human epilepsy the exact mechanisms underlying seizure generation are still uncertain as are mechanisms underlying seizure spreading and termination. There is now growing evidence that an improved understanding of the epileptic process can be achieved through the analysis of properties of epileptic brain networks and through the analysis of interactions in such networks. In this overview, we summarize recent methodological developments to assess synchronization phenomena in human epileptic brain networks and present findings obtained from analyses of brain electromagnetic signals recorded in epilepsy patients.},
  file = {/Users/qualia/Documents/Papers/2009 - Lehnertz et al. - Synchronization phenomena in human epileptic brain networks.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en},
  number = {1}
}

@article{Leibo,
  title = {Psychlab: {{A Psychology Laboratory}} for {{Deep Reinforcement Learning Agents}}},
  author = {Leibo, Joel Z and Beattie, Charles and Anderson, Keith and Casta{\~n}eda, Antonio Garc{\'i}a and Sanchez, Manuel and Green, Simon and Gruslys, Audrunas and Legg, Shane and Hassabis, Demis and Botvinick, Matthew M},
  pages = {28},
  abstract = {Psychlab is a simulated psychology laboratory inside the first-person 3D game world of DeepMind Lab (Beattie et al., 2016). Psychlab enables implementations of classical laboratory psychological experiments so that they work with both human and artificial agents. Psychlab has a simple and flexible API that enables users to easily create their own tasks. As examples, we are releasing Psychlab implementations of several classical experimental paradigms including visual search, change detection, random dot motion discrimination, and multiple object tracking. We also contribute a study of the visual psychophysics of a specific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg et al., 2016). This study leads to the surprising conclusion that UNREAL learns more quickly about larger target stimuli than it does about smaller stimuli. In turn, this insight motivates a specific improvement in the form of a simple model of foveal vision that turns out to significantly boost UNREAL's performance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By open-sourcing Psychlab we hope to facilitate a range of future such studies that simultaneously advance deep reinforcement learning and improve its links with cognitive science.},
  file = {/Users/qualia/Documents/Papers/Leibo et al. - Psychlab A Psychology Laboratory for Deep Reinfor.pdf},
  language = {en}
}

@article{Leibold2008,
  title = {Temporal Compression Mediated by Short-Term Synaptic Plasticity},
  author = {Leibold, C. and Gundlfinger, A. and Schmidt, R. and Thurley, K. and Schmitz, D. and Kempter, R.},
  year = {2008},
  month = mar,
  volume = {105},
  pages = {4417--4422},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0708711105},
  file = {/Users/qualia/Documents/Papers/2008 - Leibold et al. - Temporal compression mediated by short-term synaptic plasticity.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {11}
}

@article{Leike2017,
  title = {{{AI Safety Gridworlds}}},
  author = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A. and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
  year = {2017},
  month = nov,
  abstract = {We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modification, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and specification problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.},
  archivePrefix = {arXiv},
  eprint = {1711.09883},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Leike et al. - 2017 - AI Safety Gridworlds.pdf},
  journal = {arXiv:1711.09883 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{LeMasson1993,
  title = {Activity-Dependent Regulation of Conductances in Model Neurons},
  author = {LeMasson, G and Marder, E and Abbott, L.},
  year = {1993},
  month = mar,
  volume = {259},
  pages = {1915--1917},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.8456317},
  file = {/Users/qualia/Documents/Papers/1993 - LeMasson, Marder, Abbott - Activity-dependent regulation of conductances in model neurons.pdf},
  journal = {Science},
  language = {en},
  number = {5103}
}

@article{Lempitsky,
  title = {Andrea {{Vedaldi University}} of {{Oxford}}},
  author = {Lempitsky, Victor},
  pages = {10},
  abstract = {Deep convolutional networks have become a popular tool for image generation and restoration. Generally, their excellent performance is imputed to their ability to learn realistic image priors from a large number of example images. In this paper, we show that, on the contrary, the structure of a generator network is sufficient to capture a great deal of low-level image statistics prior to any learning. In order to do so, we show that a randomly-initialized neural network can be used as a handcrafted prior with excellent results in standard inverse problems such as denoising, superresolution, and inpainting. Furthermore, the same prior can be used to invert deep neural representations to diagnose them, and to restore images based on flash-no flash input pairs.},
  file = {/Users/qualia/Documents/Papers/Lempitsky - Andrea Vedaldi University of Oxford.pdf},
  language = {en}
}

@article{Leshno1993,
  title = {Multilayer Feedforward Networks with a Nonpolynomial Activation Function Can Approximate Any Function},
  author = {Leshno, Moshe and Lin, Vladimir Ya. and Pinkus, Allan and Schocken, Shimon},
  year = {1993},
  month = jan,
  volume = {6},
  pages = {861--867},
  issn = {08936080},
  doi = {10.1016/S0893-6080(05)80131-5},
  abstract = {Several researchers characterized the activation fimction under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation fimction can approximate an3, continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role o f the threshold, asserting that without it the last theorem does not hold.},
  file = {/Users/qualia/Documents/Papers/Leshno et al. - 1993 - Multilayer feedforward networks with a nonpolynomi.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {6}
}

@article{Levesque2005,
  title = {{{GABAergic}} Interneurons in Human Subthalamic Nucleus},
  author = {L{\'e}vesque, Julie-Christine and Parent, Andr{\'e}},
  year = {2005},
  month = may,
  volume = {20},
  pages = {574--584},
  issn = {0885-3185, 1531-8257},
  doi = {10.1002/mds.20374},
  file = {/Users/qualia/Documents/Papers/2005 - Lévesque, André - GABAergic interneurons in human subthalamic nucleus.pdf},
  journal = {Movement Disorders},
  language = {en},
  number = {5}
}

@techreport{Levi-Aharoni2019,
  title = {Surprise Response as a Probe for Compressed Memory States},
  author = {{Levi-Aharoni}, Hadar and Shriki, Oren and Tishby, Naftali},
  year = {2019},
  month = may,
  institution = {{Neuroscience}},
  doi = {10.1101/627133},
  abstract = {The limited capacity of recent memory inevitably leads to partial memory of past stimuli. There is also evidence that behavioral and neural responses to novel or rare stimuli are dependent on one's memory of past stimuli. Thus, these responses may serve as a probe of different individuals' remembering and forgetting characteristics. Here, we utilize two lossy compression models of stimulus sequences that inherently involve forgetting, which in addition to being a necessity under many conditions, also has theoretical and behavioral advantages. One model is based on a simple stimulus counter and the other on the Information Bottleneck (IB) framework. These models are applied to analyze a novelty-detection event-related potential commonly known as the P300.},
  file = {/Users/qualia/Documents/Papers/Levi-Aharoni et al. - 2019 - Surprise response as a probe for compressed memory.pdf},
  language = {en},
  type = {Preprint}
}

@article{Levy,
  title = {Galton's {{Two Papers}} on {{Voting}} as {{Robust Estimation}}},
  author = {Levy, David M and Peart, Sandra},
  pages = {10},
  abstract = {The relationship between voting and robust estimation was discussed by Francis Galton in 1907. His two papers in Nature are discussed and reprinted.},
  file = {/Users/qualia/Documents/Papers/2002 - Levy, Peart - Galton ’ s two papers on voting as robust estimation ∗.pdf},
  language = {en}
}

@article{Levya,
  title = {The {{Tale}} of {{Galton}}'s {{Mean}}: {{The Influence}} of {{Experts}}},
  author = {Levy, David M and Peart, Sandra J},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/2007 - Levy, Peart - The Influence of Experts.pdf},
  language = {en}
}

@article{Lewis-Peacock2008,
  title = {Temporary {{Activation}} of {{Long}}-{{Term Memory Supports Working Memory}}},
  author = {{Lewis-Peacock}, J. A. and Postle, B. R.},
  year = {2008},
  month = aug,
  volume = {28},
  pages = {8765--8771},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1953-08.2008},
  file = {/Users/qualia/Documents/Papers/2008 - Lewis-Peacock, Postle - Temporary activation of long-term memory supports working memory.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {35}
}

@article{Li2007,
  title = {Flexible {{Coding}} for {{Categorical Decisions}} in the {{Human Brain}}},
  author = {Li, S. and Ostwald, D. and Giese, M. and Kourtzi, Z.},
  year = {2007},
  month = nov,
  volume = {27},
  pages = {12321--12330},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3795-07.2007},
  file = {/Users/qualia/Documents/Papers/2007 - Li et al. - Flexible coding for categorical decisions in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {45}
}

@article{Li2009,
  title = {Learning {{Shapes}} the {{Representation}} of {{Behavioral Choice}} in the {{Human Brain}}},
  author = {Li, Sheng and Mayhew, Stephen D. and Kourtzi, Zoe},
  year = {2009},
  month = may,
  volume = {62},
  pages = {441--452},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.03.016},
  abstract = {Making successful decisions under uncertainty due to noisy sensory signals is thought to benefit from previous experience. However, the human brain mechanisms that mediate flexible decisions through learning remain largely unknown. Comparing behavioral choices of human observers with those of a pattern classifier based on multivoxel single-trial fMRI signals, we show that category learning shapes processes related to decision variables in frontal and higher occipitotemporal regions rather than signal detection or response execution in primary visual or motor areas. In particular, fMRI signals in prefrontal regions reflect the observers' behavioral choice according to the learned decision criterion only in the context of the categorization task. In contrast, higher occipitotemporal areas show learning-dependent changes in the representation of perceived categories that are sustained after training independent of the task. These findings demonstrate that learning shapes selective representations of sensory readout signals in accordance with the decision criterion to support flexible decisions.},
  file = {/Users/qualia/Documents/Papers/2009 - Li, Mayhew, Kourtzi - Learning shapes the representation of behavioral choice in the human brain.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Li2010,
  title = {Estimating Coupling Direction between Neuronal Populations with Permutation Conditional Mutual Information},
  author = {Li, Xiaoli and Ouyang, Gaoxiang},
  year = {2010},
  month = aug,
  volume = {52},
  pages = {497--507},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.003},
  abstract = {To further understand functional connectivity in the brain, we need to identify the coupling direction between neuronal signals recorded from different brain areas. In this paper, we present a novel methodology based on permutation analysis and conditional mutual information for estimation of a directionality index between two neuronal populations. First, the reliability of this method is numerically assessed with a coupled mass neural model; the simulations show that this method is superior to the conditional mutual information method and the Granger causality method for identifying the coupling direction between unidirectional or bidirectional neuronal populations that are generated by the mass neuronal model. The method is also applied to investigate the coupling direction between neuronal populations in CA1 and CA3 in the rat hippocampal tetanus toxin model of focal epilepsy; the propagation direction of the seizure events could be elucidated through this coupling direction estimation method. All together, these results suggest that the permutation conditional mutual information method is a promising technique for estimating directional coupling between mutually interconnected neuronal populations.},
  file = {/Users/qualia/Documents/Papers/2010 - Li, Ouyang - Estimating coupling direction between neuronal populations with permutation conditional mutual information.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Li2013,
  title = {Using a Million Cell Simulation of the Cerebellum: {{Network}} Scaling and Task Generality},
  shorttitle = {Using a Million Cell Simulation of the Cerebellum},
  author = {Li, Wen-Ke and Hausknecht, Matthew J. and Stone, Peter and Mauk, Michael D.},
  year = {2013},
  month = nov,
  volume = {47},
  pages = {95--102},
  issn = {08936080},
  doi = {10.1016/j.neunet.2012.11.005},
  abstract = {Several factors combine to make it feasible to build computer simulations of the cerebellum and to test them in biologically realistic ways. These simulations can be used to help understand the computational contributions of various cerebellar components, including the relevance of the enormous number of neurons in the granule cell layer. In previous work we have used a simulation containing 12000 granule cells to develop new predictions and to account for various aspects of eyelid conditioning, a form of motor learning mediated by the cerebellum. Here we demonstrate the feasibility of scaling up this simulation to over one million granule cells using parallel graphics processing unit (GPU) technology. We observe that this increase in number of granule cells requires only twice the execution time of the smaller simulation on the GPU. We demonstrate that this simulation, like its smaller predecessor, can emulate certain basic features of conditioned eyelid responses, with a slight improvement in performance in one measure. We also use this simulation to examine the generality of the computation properties that we have derived from studying eyelid conditioning. We demonstrate that this scaled up simulation can learn a high level of performance in a classic machine learning task, the cart\textendash{}pole balancing task. These results suggest that this parallel GPU technology can be used to build very large-scale simulations whose connectivity ratios match those of the real cerebellum and that these simulations can be used guide future studies on cerebellar mediated tasks and on machine learning problems.},
  file = {/Users/qualia/Documents/Papers/2012 - Li et al. - Using a million cell simulation of the cerebellum Network scaling and task generality.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Li2016,
  title = {Robust Neuronal Dynamics in Premotor Cortex during Motor Planning},
  author = {Li, Nuo and Daie, Kayvon and Svoboda, Karel and Druckmann, Shaul},
  year = {2016},
  month = apr,
  volume = {532},
  pages = {459--464},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature17643},
  file = {/Users/qualia/Documents/Papers/Li et al. - 2016 - Robust neuronal dynamics in premotor cortex during.pdf},
  journal = {Nature},
  language = {en},
  number = {7600}
}

@article{Li2016a,
  title = {Hyperband: {{A Novel Bandit}}-{{Based Approach}} to {{Hyperparameter Optimization}}},
  shorttitle = {Hyperband},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2016},
  month = mar,
  abstract = {Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration nonstochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.},
  archivePrefix = {arXiv},
  eprint = {1603.06560},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Li et al. - 2016 - Hyperband A Novel Bandit-Based Approach to Hyperp.pdf},
  journal = {arXiv:1603.06560 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Li2017,
  title = {Lifting the Veil on the Dynamics of Neuronal Activities Evoked by Transcranial Magnetic Stimulation},
  author = {Li, Bingshuo and Virtanen, Juha P and Oeltermann, Axel and Schwarz, Cornelius and Giese, Martin A and Ziemann, Ulf and Benali, Alia},
  year = {2017},
  month = nov,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.30552},
  abstract = {Transcranial magnetic stimulation (TMS) is a widely used non-invasive tool to study and modulate human brain functions. However, TMS-evoked activity of individual neurons has remained largely inaccessible due to the large TMS-induced electromagnetic fields. Here, we present a general method providing direct in vivo electrophysiological access to TMS-evoked neuronal activity 0.8\textendash{}1 ms after TMS onset. We translated human single-pulse TMS to rodents and unveiled time-grained evoked activities of motor cortex layer V neurons that show high-frequency spiking within the first 6 ms depending on TMS-induced current orientation and a multiphasic spike-rhythm alternating between excitation and inhibition in the 6\textendash{}300 ms epoch, all of which can be linked to various human TMS responses recorded at the level of spinal cord and muscles. The advance here facilitates a new level of insight into the TMS-brain interaction that is vital for developing this noninvasive tool to purposefully explore and effectively treat the human brain.},
  file = {/Users/qualia/Documents/Papers/Li et al. - 2017 - Lifting the veil on the dynamics of neuronal activ.pdf},
  journal = {eLife},
  language = {en}
}

@article{Li2017a,
  title = {Single-Impulse Panoramic Photoacoustic Computed Tomography of Small-Animal Whole-Body Dynamics at High Spatiotemporal Resolution},
  author = {Li, Lei and Zhu, Liren and Ma, Cheng and Lin, Li and Yao, Junjie and Wang, Lidai and Maslov, Konstantin and Zhang, Ruiying and Chen, Wanyi and Shi, Junhui and Wang, Lihong V.},
  year = {2017},
  month = may,
  volume = {1},
  issn = {2157-846X},
  doi = {10.1038/s41551-017-0071},
  file = {/Users/qualia/Documents/Papers/Li et al. - 2017 - Single-impulse panoramic photoacoustic computed to.pdf},
  journal = {Nature Biomedical Engineering},
  language = {en},
  number = {5}
}

@article{Liao,
  title = {Theory of {{Deep Learning II}}: {{Landscape}} of the {{Empirical Risk}} in {{Deep Learning}}},
  author = {Liao, Qianli and Poggio, Tomaso},
  pages = {45},
  abstract = {Previous theoretical work on deep learning and neural network optimization tend to focus on avoiding saddle points and local minima. However, the practical observation is that, at least in the case of the most successful Deep Convolutional Neural Networks (DCNNs), practitioners can always increase the network size to fit the training data (an extreme example would be [1]). The most successful DCNNs such as VGG and ResNets are best used with a degree of ``overparametrization''. In this work, we characterize with a mix of theory and experiments, the landscape of the empirical risk of overparametrized DCNNs. We first prove in the regression framework the existence of a large number of degenerate global minimizers with zero empirical error (modulo inconsistent equations). The argument that relies on the use of Bezout theorem is rigorous when the RELUs are replaced by a polynomial nonlinearity (which empirically works as well). As described in our Theory III [2] paper, the same minimizers are degenerate and thus very likely to be found by SGD that will furthermore select with higher probability the most robust zero-minimizer. We further experimentally explored and visualized the landscape of empirical risk of a DCNN on CIFAR-10 during the entire training process and especially the global minima. Finally, based on our theoretical and experimental results, we propose an intuitive model of the landscape of DCNN's empirical loss surface, which might not be as complicated as people commonly believe.},
  file = {/Users/qualia/Documents/Papers/Liao and Poggio - Theory of Deep Learning II Landscape of the Empir.pdf},
  language = {en}
}

@article{Liapis,
  title = {Transforming {{Exploratory Creativity}} with {{DeLeNoX}}},
  author = {Liapis, Antonios},
  pages = {8},
  abstract = {We introduce DeLeNoX (Deep Learning Novelty Explorer), a system that autonomously creates artifacts in constrained spaces according to its own evolving interestingness criterion. DeLeNoX proceeds in alternating phases of exploration and transformation. In the exploration phases, a version of novelty search augmented with constraint handling searches for maximally diverse artifacts using a given distance function. In the transformation phases, a deep learning autoencoder learns to compress the variation between the found artifacts into a lower-dimensional space. The newly trained encoder is then used as the basis for a new distance function, transforming the criteria for the next exploration phase. In the current paper, we apply DeLeNoX to the creation of spaceships suitable for use in two-dimensional arcade-style computer games, a representative problem in procedural content generation in games. We also situate DeLeNoX in relation to the distinction between exploratory and transformational creativity, and in relation to Schmidhuber's theory of creativity through the drive for compression progress.},
  file = {/Users/qualia/Documents/Papers/Liapis - Transforming Exploratory Creativity with DeLeNoX.pdf},
  language = {en}
}

@article{Lieder,
  title = {When to Use Which Heuristic? {{A}} Rational Solution to the Strategy Selection Problem},
  author = {Lieder, Falk and Griffiths, Tom},
  pages = {1},
  file = {/Users/qualia/Documents/Papers/2015 - Lieder, Griffiths - When to use which heuristic A rational solution to the strategy selection problem.pdf;/Users/qualia/Documents/Papers/Lieder and Grifﬁths - When to use which heuristic A rational solution t.pdf},
  language = {en}
}

@article{Lien2013,
  title = {Tuned Thalamic Excitation Is Amplified by Visual Cortical Circuits},
  author = {Lien, Anthony D and Scanziani, Massimo},
  year = {2013},
  month = sep,
  volume = {16},
  pages = {1315--1323},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3488},
  file = {/Users/qualia/Documents/Papers/2013 - Lien, Scanziani - Tuned thalamic excitation is amplified by visual cortical circuits.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Lienard2014,
  title = {A Biologically Constrained Model of the Whole Basal Ganglia Addressing the Paradoxes of Connections and Selection},
  author = {Li{\'e}nard, Jean and Girard, Beno{\^i}t},
  year = {2014},
  month = jun,
  volume = {36},
  pages = {445--468},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-013-0476-2},
  file = {/Users/qualia/Documents/Papers/2014 - Liénard, Girard - A biologically constrained model of the whole basal ganglia addressing the paradoxes of connections and select.pdf;/Users/qualia/Documents/Papers/Liénard and Girard - 2014 - A biologically constrained model of the whole basa.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Lienard2017,
  title = {Beta-{{Band Oscillations}} without {{Pathways}}: The Opposing {{Roles}} of {{D2}} and {{D5 Receptors}}},
  shorttitle = {Beta-{{Band Oscillations}} without {{Pathways}}},
  author = {Lienard, Jean F. and Cos, Ignasi and Girard, Benoit},
  year = {2017},
  month = jul,
  doi = {10.1101/161661},
  abstract = {Parkinson's disease is characterized by the death of dopaminergic neurons and the emergence of strong {$\beta$}-band oscillations throughout the basal ganglia nuclei. According to the mainstream theory, this synchrony is mediated by a dopamine deficit within the striatum creating a functional imbalance between the D1-expressing medium spiny neurons, which project to the internal segment of the globus pallidus, and D2-expressing one, which target its external segment, and ultimately leads to oscillatory activity. However, anatomical evidence gathered in rodents and primates has shown that striatal neurons are for the most part not organized into independent populations differentially targeting the two segments of the globus pallidus, nor alternatively expressing D1 or D2 receptors, thus calling for an alternative mechanism through which the lack of dopamine may cause oscillations. Here we adopt a computational approach in which we investigate a model whose parameters are fit to an extensive set of anatomical and physiological constraints from non-human primates, including axonal transmission delays gathered from eight experimental studies. Investigating the lack of dopamine in this model revealed that in the absence of segregated pathways, {$\beta$}-band oscillations emerge as a consequence of the extra-striate dopaminergic receptors reduced activity. These oscillations are caused by synchronous activity within the external globus pallidus-subthalamic nucleus loop, and their frequency are modulated by the transmission delays between these nuclei. Our model delivers a parsimonious explanation of oscillations that does not require any external driving influence from cortex, nor specific medium spiny neuron properties.},
  file = {/Users/qualia/Documents/Papers/Lienard et al. - 2017 - Beta-Band Oscillations without Pathways the oppos 2.pdf;/Users/qualia/Documents/Papers/Lienard et al. - 2017 - Beta-Band Oscillations without Pathways the oppos.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Lin,
  title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
  author = {Lin, Long-Ji},
  pages = {29},
  abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus twofold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.},
  file = {/Users/qualia/Documents/Papers/1992 - Lin - Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching.pdf},
  language = {en}
}

@article{Lin2002,
  title = {Modulation of Synaptic Delay during Synaptic Plasticity},
  author = {Lin, Jen-Wei and Faber, Donald S.},
  year = {2002},
  month = sep,
  volume = {25},
  pages = {449--455},
  issn = {01662236},
  doi = {10.1016/S0166-2236(02)02212-9},
  file = {/Users/qualia/Documents/Papers/2002 - Lin, Faber - Modulation of synaptic delay during synaptic plasticity.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {9}
}

@article{Lin2015,
  title = {The {{Nature}} of {{Shared Cortical Variability}}},
  author = {Lin, I-Chun and Okun, Michael and Carandini, Matteo and Harris, Kenneth D.},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {644--656},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.06.035},
  abstract = {Neuronal responses of sensory cortex are highly variable, and this variability is correlated across neurons. To assess how variability reflects factors shared across a neuronal population, we analyzed the activity of many simultaneously recorded neurons in visual cortex. We developed a simple model that comprises two sources of shared variability: a multiplicative gain, which uniformly scales each neuron's sensory drive, and an additive offset, which affects different neurons to different degrees. This model captured the variability of spike counts and reproduced the dependence of pairwise correlations on neuronal tuning and stimulus orientation. The relative contributions of the additive and multiplicative fluctuations could vary over time and had marked impact on population coding. These observations indicate that shared variability of neuronal populations in sensory cortex can be largely explained by two factors that modulate the whole population.},
  file = {/Users/qualia/Documents/Papers/Lin et al. - 2015 - The Nature of Shared Cortical Variability.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Lin2018,
  title = {All-Optical Machine Learning Using Diffractive Deep Neural Networks},
  author = {Lin, Xing and Rivenson, Yair and Yardimci, Nezih T. and Veli, Muhammed and Luo, Yi and Jarrahi, Mona and Ozcan, Aydogan},
  year = {2018},
  month = sep,
  volume = {361},
  pages = {1004--1008},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aat8084},
  file = {/Users/qualia/Documents/Papers/Lin et al. - 2018 - All-optical machine learning using diffractive dee.pdf},
  journal = {Science},
  language = {en},
  number = {6406}
}

@article{Lin2018a,
  title = {Deep {{Gradient Compression}}: {{Reducing}} the {{Communication Bandwidth}} for {{Distributed Training}}},
  shorttitle = {Deep {{Gradient Compression}}},
  author = {Lin, Yujun and Han, Song and Mao, Huizi and Wang, Yu and Dally, William J.},
  year = {2018},
  month = feb,
  abstract = {Large-scale distributed training requires significant communication bandwidth for gradient exchange that limits the scalability of multi-node training, and requires expensive high-bandwidth network infrastructure. The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections. In this paper, we find 99.9\% of the gradient exchange in distributed SGD are redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth. To preserve accuracy during this compression, DGC employs four methods: momentum correction, local gradient clipping, momentum factor masking, and warm-up training. We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. On these scenarios, Deep Gradient Compression achieves a gradient compression ratio from 270\texttimes{} to 600\texttimes{} without losing accuracy, cutting the gradient size of ResNet-50 from 97MB to 0.35MB, and for DeepSpeech from 488MB to 0.74MB. Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.},
  archivePrefix = {arXiv},
  eprint = {1712.01887},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Lin et al. - 2018 - Deep Gradient Compression Reducing the Communicat.pdf},
  journal = {arXiv:1712.01887 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Lina,
  title = {{{ResNet}} with One-Neuron Hidden Layers Is a {{Universal Approximator}}},
  author = {Lin, Hongzhou and Jegelka, Stefanie},
  pages = {10},
  abstract = {We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. {$\mathscr{l}$}1(Rd). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21, 11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.},
  file = {/Users/qualia/Documents/Papers/Lin and Jegelka - ResNet with one-neuron hidden layers is a Universa.pdf},
  language = {en}
}

@article{Lindahl2016,
  title = {Untangling {{Basal Ganglia Network Dynamics}} and {{Function}}: {{Role}} of {{Dopamine Depletion}} and {{Inhibition Investigated}} in a {{Spiking Network Model}}},
  shorttitle = {Untangling {{Basal Ganglia Network Dynamics}} and {{Function}}},
  author = {Lindahl, Mikael and Hellgren Kotaleski, Jeanette},
  year = {2016},
  volume = {3},
  pages = {ENEURO.0156-16.2016},
  issn = {2373-2822},
  doi = {10.1523/ENEURO.0156-16.2016},
  file = {/Users/qualia/Documents/Papers/Lindahl and Hellgren Kotaleski - 2016 - Untangling Basal Ganglia Network Dynamics and Func.pdf},
  journal = {eneuro},
  language = {en},
  number = {6}
}

@article{Linderman2016,
  title = {Recurrent Switching Linear Dynamical Systems},
  author = {Linderman, Scott W. and Miller, Andrew C. and Adams, Ryan P. and Blei, David M. and Paninski, Liam and Johnson, Matthew J.},
  year = {2016},
  month = oct,
  abstract = {Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These ``recurrent'' switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable.},
  archivePrefix = {arXiv},
  eprint = {1610.08466},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Linderman et al. - 2016 - Recurrent switching linear dynamical systems.pdf},
  journal = {arXiv:1610.08466 [stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@article{Lindsley1939,
  title = {A {{Longitudinal Study}} of the {{Occipital Alpha Rhythm}} in {{Normal Children}}: {{Frequency}} and {{Amplitude Standards}}},
  shorttitle = {A {{Longitudinal Study}} of the {{Occipital Alpha Rhythm}} in {{Normal Children}}},
  author = {Lindsley, Donald B.},
  year = {1939},
  month = sep,
  volume = {55},
  pages = {197--213},
  issn = {0885-6559},
  doi = {10.1080/08856559.1939.10533190},
  file = {/Users/qualia/Documents/Papers/Lindsley - 1939 - A Longitudinal Study of the Occipital Alpha Rhythm.pdf},
  journal = {The Pedagogical Seminary and Journal of Genetic Psychology},
  language = {en},
  number = {1}
}

@article{Lintas2012,
  title = {Dopamine Deficiency Increases Synchronized Activity in the Rat Subthalamic Nucleus},
  author = {Lintas, Alessandra and Silkis, Isabella G. and Alb{\'e}ri, Lavinia and Villa, Alessandro E.P.},
  year = {2012},
  month = jan,
  volume = {1434},
  pages = {142--151},
  issn = {00068993},
  doi = {10.1016/j.brainres.2011.09.005},
  abstract = {Abnormal neuronal activity in the subthalamic nucleus (STN) plays a crucial role in the pathophysiology of Parkinson's disease (PD). In this study we investigated changes in rat STN neuronal activity after 28 days following the injection of 6-OHDA in the substantia nigra pars compacta (SNc). This drug provoked a lesion of SNc that induced a dopamine (DA) depletion assessed by changes in rotating capacity in response to apomorphine injection and by histological analysis. By means of extracellular recordings and waveshape spike sorting it was possible to analyze simultaneous spike trains and compute the crosscorrelations. Based on the analysis of the autocorrelograms we classified four types of firing patterns: regular (Poissonian-like), oscillatory (in the range 4\textendash{}12 Hz), bursty and cells characterized by a long refractoriness. The distribution of unit types in the control (n = 61) and lesioned (n = 83) groups was similar, as well as the firing rate. In 6-OHDA treated rats we observed a significant increase (from 26\% to 48\%) in the number of pairs with synchronous firing. These data suggest that the synchronous activity of STN cells, provoked by loss of DA cells in SNc, is likely to be among the most significant dysfunctions in the basal ganglia of Parkinsonian patients. We raise the hypothesis that in normal conditions, DA maintains a balance between funneling information via the hyperdirect cortico-subthalamic pathway and parallel processing through the parallel cortico-basal ganglia-subthalamic pathways, both of which are necessary for selected motor behaviors.},
  file = {/Users/qualia/Documents/Papers/2012 - Lintas et al. - Dopamine deficiency increases synchronized activity in the rat subthalamic nucleus.pdf},
  journal = {Brain Research},
  language = {en}
}

@article{Lipshutz2015,
  title = {Existence, {{Uniqueness}}, and {{Stability}} of {{Slowly Oscillating Periodic Solutions}} for {{Delay Differential Equations}} with {{Nonnegativity Constraints}}},
  author = {Lipshutz, David and Williams, Ruth J.},
  year = {2015},
  month = jan,
  volume = {47},
  pages = {4467--4535},
  issn = {0036-1410, 1095-7154},
  doi = {10.1137/140980806},
  abstract = {Deterministic dynamical system models with delayed feedback and nonnegativity constraints arise in a variety of applications in science and engineering. Under certain conditions oscillatory behavior has been observed and it is of interest to know when this behavior is periodic. Here we consider one-dimensional delay differential equations with nonnegativity constraints as prototypes for such models. We obtain sufficient conditions for the existence of slowly oscillating periodic solutions (SOPS) of such equations when the delay/lag interval is long and the dynamics depend only on the current and delayed state. Under further assumptions, including possibly longer delay intervals and restricting the dynamics to depend only on the delayed state, we prove uniqueness and exponential stability for such solutions. To prove these results, we develop a theory for studying perturbations of these constrained SOPS. We illustrate our results with simple examples of biochemical reaction network models and an Internet rate control model.},
  file = {/Users/qualia/Documents/Papers/2015 - Lipshutz, Williams - EXISTENCE, UNIQUENESS, AND STABILITY OF SLOWLY OSCILLATING PERIODIC SOLUTIONS FOR DELAY DIFFERENTIAL EQUATIO.pdf;/Users/qualia/Documents/Papers/Lipshutz and Williams - 2015 - Existence, Uniqueness, and Stability of Slowly Osc 2.pdf;/Users/qualia/Documents/Papers/Lipshutz and Williams - 2015 - Existence, Uniqueness, and Stability of Slowly Osc.pdf},
  journal = {SIAM Journal on Mathematical Analysis},
  language = {en},
  number = {6}
}

@article{Lipton,
  title = {Combating {{Reinforcement Learning}}'s {{Sisyphean Curse}} with {{Intrinsic Fear}}},
  author = {Lipton, Zachary C and Azizzadenesheli, Kamyar and Kumar, Abhishek and Li, Lihong and Gao, Jianfeng and Deng, Li},
  pages = {14},
  abstract = {Many practical environments contain catastrophic states that an optimal agent would visit infrequently or never. Even on toy problems, Deep Reinforcement Learning (DRL) agents tend to periodically revisit these states upon forgetting their existence under a new policy. We introduce intrinsic fear (IF), a learned reward shaping that guards DRL agents against periodic catastrophes. IF agents possess a fear model trained to predict the probability of imminent catastrophe. This score is then used to penalize the Qlearning objective. Our theoretical analysis bounds the reduction in average return due to learning on the perturbed objective. We also prove robustness to classi cation errors. As a bonus, IF models tend to learn faster, owing to reward shaping. Experiments demonstrate that intrinsic-fear DQNs solve otherwise pathological environments and improve on several Atari games.},
  file = {/Users/qualia/Documents/Papers/Lipton et al. - Combating Reinforcement Learning’s Sisyphean Curse.pdf},
  language = {en}
}

@article{Lisman2013,
  title = {The {{Theta}}-{{Gamma Neural Code}}},
  author = {Lisman, John E. and Jensen, Ole},
  year = {2013},
  month = mar,
  volume = {77},
  pages = {1002--1016},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.03.007},
  file = {/Users/qualia/Documents/Papers/2013 - Lisman, Jensen - The θ-γ neural code.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Litwin-Kumar2014,
  title = {Formation and Maintenance of Neuronal Assemblies through Synaptic Plasticity},
  author = {{Litwin-Kumar}, Ashok and Doiron, Brent},
  year = {2014},
  month = dec,
  volume = {5},
  issn = {2041-1723},
  doi = {10.1038/ncomms6319},
  file = {/Users/qualia/Documents/Papers/Litwin-Kumar and Doiron - 2014 - Formation and maintenance of neuronal assemblies t.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Litwin-Kumar2019,
  title = {Constraining Computational Models Using Electron Microscopy Wiring Diagrams},
  author = {{Litwin-Kumar}, Ashok and Turaga, Srinivas C},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {94--100},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.07.007},
  file = {/Users/qualia/Documents/Papers/Litwin-Kumar and Turaga - 2019 - Constraining computational models using electron m.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Litwin-Kumar2019a,
  title = {Constraining Computational Models Using Electron Microscopy Wiring Diagrams},
  author = {{Litwin-Kumar}, Ashok and Turaga, Srinivas C},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {94--100},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.07.007},
  file = {/Users/qualia/Documents/Papers/Litwin-Kumar and Turaga - 2019 - Constraining computational models using electron m 2.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Litwin-Kumar2019b,
  title = {Constraining Computational Models Using Electron Microscopy Wiring Diagrams},
  author = {{Litwin-Kumar}, Ashok and Turaga, Srinivas C},
  year = {2019},
  month = oct,
  volume = {58},
  pages = {94--100},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.07.007},
  file = {/Users/qualia/Documents/Papers/Litwin-Kumar and Turaga - 2019 - Constraining computational models using electron m 3.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Liu,
  title = {{{HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITECTURE SEARCH}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  pages = {13},
  abstract = {We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6\% on CIFAR-10 and 20.3\% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3\% less top-1 accuracy on CIFAR-10 and 0.1\% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.},
  file = {/Users/qualia/Documents/Papers/Liu et al. - HIERARCHICAL REPRESENTATIONS FOR EFFICIENT ARCHITE.pdf},
  language = {en}
}

@article{Liu1998,
  title = {A {{Model Neuron}} with {{Activity}}-{{Dependent Conductances Regulated}} by {{Multiple Calcium Sensors}}},
  author = {Liu, Zheng and Golowasch, Jorge and Marder, Eve and Abbott, L. F.},
  year = {1998},
  month = apr,
  volume = {18},
  pages = {2309--2320},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.18-07-02309.1998},
  file = {/Users/qualia/Documents/Papers/1998 - Liu et al. - A model neuron with activity-dependent conductances regulated by multiple calcium sensors.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Liu2009,
  title = {Embedding {{Multiple Trajectories}} in {{Simulated Recurrent Neural Networks}} in a {{Self}}-{{Organizing Manner}}},
  author = {Liu, J. K. and Buonomano, D. V.},
  year = {2009},
  month = oct,
  volume = {29},
  pages = {13172--13181},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2358-09.2009},
  file = {/Users/qualia/Documents/Papers/2009 - Liu, Buonomano - Embedding Multiple Trajectories in Simulated Recurrent Neural Networks in a Self-Organizing Manner.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {42}
}

@article{Liu2019,
  title = {Mouse Navigation Strategies for Odor Source Localization},
  author = {Liu, Annie and Papale, Andrew and Hengenius, James and Patel, Khusbu and Ermentrout, Bard and Urban, Nathaniel},
  year = {2019},
  month = feb,
  doi = {10.1101/558643},
  abstract = {Navigating an odor landscape is a critical behavior for the survival of many species, including mice. One ethologically relevant mouse behavior is locating food using odor concentration gradients. To model this behavior, we use a naturalistic open field odor-based spot-finding task, examining navigation strategies as mice search for and approach an odor source. Mice were trained to navigate to odor sources paired with food reward. We detected behavioral changes consistent with localization of the odor source when mice were 10cm away from the source. These behaviors included both orientation towards the source and increased exploration time. We found that the amplitude of "casting," lateral back and forth head movement, increased exponentially with proximity to the source. We then created concentration-dependent models to simulate mouse behavior, which provided evidence for a serial-sniffing strategy (sampling concentration, moving in space, then sampling again) and a stereo-sniffing strategy (inter-nostril comparison of concentration in a single sniff). Together, these results elucidate key components of behavioral strategies for odor-based navigation.},
  file = {/Users/qualia/Documents/Papers/Liu et al. - 2019 - Mouse navigation strategies for odor source locali.pdf;/Users/qualia/Documents/Papers/Liu et al. - 2019 - Mouse navigation strategies for odor source locali.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Lizbinski2018,
  title = {Intrinsic and {{Extrinsic Neuromodulation}} of {{Olfactory Processing}}},
  author = {Lizbinski, Kristyn M. and Dacks, Andrew M.},
  year = {2018},
  month = jan,
  volume = {11},
  issn = {1662-5102},
  doi = {10.3389/fncel.2017.00424},
  abstract = {Neuromodulation is a ubiquitous feature of neural systems, allowing flexible, context specific control over network dynamics. Neuromodulation was first described in invertebrate motor systems and early work established a basic dichotomy for neuromodulation as having either an intrinsic origin (i.e., neurons that participate in network coding) or an extrinsic origin (i.e., neurons from independent networks). In this conceptual dichotomy, intrinsic sources of neuromodulation provide a ``memory'' by adjusting network dynamics based upon previous and ongoing activation of the network itself, while extrinsic neuromodulators provide the context of ongoing activity of other neural networks. Although this dichotomy has been thoroughly considered in motor systems, it has received far less attention in sensory systems. In this review, we discuss intrinsic and extrinsic modulation in the context of olfactory processing in invertebrate and vertebrate model systems. We begin by discussing presynaptic modulation of olfactory sensory neurons by local interneurons (LNs) as a mechanism for gain control based on ongoing network activation. We then discuss the cell-class specific effects of serotonergic centrifugal neurons on olfactory processing. Finally, we briefly discuss the integration of intrinsic and extrinsic neuromodulation (metamodulation) as an effective mechanism for exerting global control over olfactory network dynamics. The heterogeneous nature of neuromodulation is a recurring theme throughout this review as the effects of both intrinsic and extrinsic modulation are generally non-uniform.},
  file = {/Users/qualia/Documents/Papers/Lizbinski and Dacks - 2018 - Intrinsic and Extrinsic Neuromodulation of Olfacto.pdf},
  journal = {Frontiers in Cellular Neuroscience},
  language = {en}
}

@article{Logan2002,
  title = {An Instance Theory of Attention and Memory.},
  author = {Logan, Gordon D.},
  year = {2002},
  volume = {109},
  pages = {376--400},
  issn = {0033-295X},
  doi = {10.1037//0033-295X.109.2.376},
  file = {/Users/qualia/Documents/Papers/2002 - Logan - An instance theory of attention and memory.pdf},
  journal = {Psychological Review},
  language = {en},
  number = {2}
}

@article{London2010,
  title = {Sensitivity to Perturbations in Vivo Implies High Noise and Suggests Rate Coding in Cortex},
  author = {London, Michael and Roth, Arnd and Beeren, Lisa and H{\"a}usser, Michael and Latham, Peter E.},
  year = {2010},
  month = jul,
  volume = {466},
  pages = {123--127},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature09086},
  abstract = {It is well known that neural activity exhibits variability, in the sense that identical sensory stimuli produce different responses, but it has been difficult to determine what this variability means. Is it noise, or does it carry important information \textendash{} about, for example, the internal state of the organism? We address this issue from the bottom up, by asking whether small perturbations to activity in cortical networks are amplified. Based on in vivo whole-cell recordings in rat barrel cortex, we find that a perturbation consisting of a single extra spike in one neuron produces \textasciitilde{}28 additional spikes in its postsynaptic targets, and we show, using simultaneous intra- and extracellular recordings, that a single spike produces a detectable increase in firing rate in the local network. Theoretical analysis indicates that this amplification leads to intrinsic, stimulusindependent variations in membrane potential on the order of {$\pm$}2.2 - 4.5 mV \textendash{} variations that are pure noise, and so carry no information at all. Therefore, for the brain to perform reliable computations, it must either use a rate code, or generate very large, fast depolarizing events, such as those proposed by the theory of synfire chains \textendash{} yet in our in vivo recordings, we found that such events were very rare. Our findings are consistent with the idea that cortex is likely to use primarily a rate code.},
  file = {/Users/qualia/Documents/Papers/2011 - Manuscript - NIH Public Access.pdf},
  journal = {Nature},
  language = {en},
  number = {7302}
}

@article{Lopes2012,
  title = {Exploration in {{Model}}-Based {{Reinforcement Learning}} by {{Empirically Estimating Learning Progress}}},
  author = {Lopes, Manuel and Lang, Tobias and Toussaint, Marc and Oudeyer, Pierre-yves},
  year = {2012},
  volume = {25},
  pages = {1--9},
  abstract = {Formal exploration approaches in model-based reinforcement learning estimate the accuracy of the currently learned model without consideration of the empirical prediction error. For example, PAC-MDP approaches such as R-MAX base their model certainty on the amount of collected data, while Bayesian approaches assume a prior over the transition dynamics. We propose extensions to such approaches which drive exploration solely based on empirical estimates of the learner's accuracy and learning progress. We provide a ``sanity check'' theoretical analysis, discussing the behavior of our extensions in the standard stationary finite state-action case. We then provide experimental studies demonstrating the robustness of these exploration measures in cases of non-stationary environments or where original approaches are misled by wrong domain assumptions.},
  file = {/Users/qualia/Documents/Papers/Lopes et al. - Exploration in Model-based Reinforcement Learning .pdf},
  journal = {NIPS},
  language = {en}
}

@article{LopesdaSilva1973,
  title = {Organization of Thalamic and Cortical Alpha Rhythms: {{Spectra}} and Coherences},
  shorttitle = {Organization of Thalamic and Cortical Alpha Rhythms},
  author = {{Lopes da Silva}, F.H and {van Lierop}, T.H.M.T and Schrijer, C.F and {Storm van Leeuwen}, W},
  year = {1973},
  month = dec,
  volume = {35},
  pages = {627--639},
  issn = {00134694},
  doi = {10.1016/0013-4694(73)90216-2},
  file = {/Users/qualia/Documents/Papers/1973 - Lierop, Schrijer, Leeuwen - 1973b). Notwithstanding recent advances on physio- logical data on thalamic rhythmic activity in anim.pdf;/Users/qualia/Documents/Papers/Lopes da Silva et al. - 1973 - Organization of thalamic and cortical alpha rhythm.pdf},
  journal = {Electroencephalography and Clinical Neurophysiology},
  language = {en},
  number = {6}
}

@article{LopesdaSilva1974,
  title = {Model of Brain Rhythmic Activity: {{The}} Alpha-Rhythm of the Thalamus},
  shorttitle = {Model of Brain Rhythmic Activity},
  author = {{Lopes da Silva}, F. H. and Hoeks, A. and Smits, H. and Zetterberg, L. H.},
  year = {1974},
  volume = {15},
  pages = {27--37},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00270757},
  abstract = {I. A model of a neuronal network has been set up in a digital computer based on histological and biophysical data experimentally obtained from the thalamus; the model includes two populations of neurons interconnected by means of negative feedback; in the model allowance is also made for other sort of interactions.},
  file = {/Users/qualia/Documents/Papers/1974 - Lopes da Silva, Smits, Health - The Alpha-Rhythm of the Thalamus.pdf},
  journal = {Kybernetik},
  language = {en},
  number = {1}
}

@incollection{LopesdaSilva1976,
  title = {Models of {{Neuronal Populations}}: {{The Basic Mechanisms}} of {{Rhythmicity}}},
  shorttitle = {Models of {{Neuronal Populations}}},
  booktitle = {Progress in {{Brain Research}}},
  author = {{Lopes da Silva}, F.H. and {van Rotterdam}, A. and Barts, P. and {van Heusden}, E. and Burr, W.},
  year = {1976},
  volume = {45},
  pages = {281--308},
  publisher = {{Elsevier}},
  doi = {10.1016/S0079-6123(08)60995-4},
  file = {/Users/qualia/Documents/Papers/1976 - Lopes da Silva et al. - Models of neuronal populations the basic mechanisms of rhythmicity.pdf},
  isbn = {978-0-444-41457-1},
  language = {en}
}

@article{Lopez-Azcarate2010,
  title = {Coupling between {{Beta}} and {{High}}-{{Frequency Activity}} in the {{Human Subthalamic Nucleus May Be}} a {{Pathophysiological Mechanism}} in {{Parkinson}}'s {{Disease}}},
  author = {{Lopez-Azcarate}, J. and Tainta, M. and {Rodriguez-Oroz}, M. C. and Valencia, M. and Gonzalez, R. and Guridi, J. and Iriarte, J. and Obeso, J. A. and Artieda, J. and Alegre, M.},
  year = {2010},
  month = may,
  volume = {30},
  pages = {6667--6677},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5459-09.2010},
  file = {/Users/qualia/Documents/Papers/2010 - Lopez-Azcarate et al. - Coupling between Beta and High-Frequency Activity in the Human Subthalamic Nucleus May Be a Pathophysiolo.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {19}
}

@article{Lopez-Fidalgo2007,
  title = {An Optimal Experimental Design Criterion for Discriminating between Non-Normal Models},
  author = {{L{\'o}pez-Fidalgo}, J. and Tommasi, C. and Trandafir, P. C.},
  year = {2007},
  month = apr,
  volume = {69},
  pages = {231--242},
  issn = {1369-7412, 1467-9868},
  doi = {10.1111/j.1467-9868.2007.00586.x},
  abstract = {Typically T -optimality is used to obtain optimal designs to discriminate between homoscedastic models with normally distributed observations. Some extensions of this criterion have been made for the heteroscedastic case and binary response models in the literature. In this paper, a new criterion based on the Kullback\textendash{}Leibler distance is proposed to discriminate between rival models with non-normally distributed observations. The criterion is coherent with the approaches mentioned above. An equivalence theorem is provided for this criterion and an algorithm to compute optimal designs is developed. The criterion is applied to discriminate between the popular Michaelis\textendash{}Menten model and a typical extension of it under the log-normal and the gamma distributions.},
  file = {/Users/qualia/Documents/Papers/2007 - López-Fidalgo, Tommasi, Trandafir - An optimal experimental design criterion for discriminating between non-normal models.pdf},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  language = {en},
  number = {2}
}

@incollection{Loreto2016,
  title = {Dynamics on {{Expanding Spaces}}: {{Modeling}} the {{Emergence}} of {{Novelties}}},
  shorttitle = {Dynamics on {{Expanding Spaces}}},
  booktitle = {Creativity and {{Universality}} in {{Language}}},
  author = {Loreto, Vittorio and Servedio, Vito D. P. and Strogatz, Steven H. and Tria, Francesca},
  editor = {Degli Esposti, Mirko and Altmann, Eduardo G. and Pachet, Fran{\c c}ois},
  year = {2016},
  pages = {59--83},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-24403-7_5},
  abstract = {Novelties are part of our daily lives. We constantly adopt new technologies, conceive new ideas, meet new people, and experiment with new situations. Occasionally, we as individual, in a complicated cognitive and sometimes fortuitous process, come up with something that is not only new to us, but to our entire society so that what is a personal novelty can turn into an innovation at a global level. Innovations occur throughout social, biological, and technological systems and, though we perceive them as a very natural ingredient of our human experience, little is known about the processes determining their emergence. Still the statistical occurrence of innovations shows striking regularities that represent a starting point to get a deeper insight in the whole phenomenology. This paper represents a small step in that direction, focusing on reviewing the scientific attempts to effectively model the emergence of the new and its regularities, with an emphasis on more recent contributions: from the plain Simon's model tracing back to the 1950s, to the newest model of Polya's urn with triggering of one novelty by another. What seems to be key in the successful modeling schemes proposed so far is the idea of looking at evolution as a path in a complex space, physical, conceptual, biological, and technological, whose structure and topology get continuously reshaped and expanded by the occurrence of the new. Mathematically, it is very interesting to look at the consequences of the interplay between the ``actual'' and the ``possible'' and this is the aim of this short review.},
  file = {/Users/qualia/Documents/Papers/Loreto et al. - 2016 - Dynamics on Expanding Spaces Modeling the Emergen.pdf},
  isbn = {978-3-319-24401-3 978-3-319-24403-7},
  language = {en}
}

@article{Lotfi2014,
  title = {A {{Novel Single Neuron Perceptron}} with {{Universal Approximation}} and {{XOR Computation Properties}}},
  author = {Lotfi, Ehsan and {Akbarzadeh-T}, M.-R.},
  year = {2014},
  volume = {2014},
  pages = {1--6},
  issn = {1687-5265, 1687-5273},
  doi = {10.1155/2014/746376},
  abstract = {We propose a biologically motivated brain-inspired single neuron perceptron (SNP) with universal approximation and XOR computation properties. This computational model extends the input pattern and is based on the excitatory and inhibitory learning rules inspired from neural connections in the human brain's nervous system. The resulting architecture of SNP can be trained by supervised excitatory and inhibitory online learning rules. The main features of proposed single layer perceptron are universal approximation property and low computational complexity. The method is tested on 6 UCI (University of California, Irvine) pattern recognition and classification datasets. Various comparisons with multilayer perceptron (MLP) with gradient decent backpropagation (GDBP) learning algorithm indicate the superiority of the approach in terms of higher accuracy, lower time, and spatial complexity, as well as faster training. Hence, we believe the proposed approach can be generally applicable to various problems such as in pattern recognition and classification.},
  file = {/Users/qualia/Documents/Papers/Lotfi and Akbarzadeh-T - 2014 - A Novel Single Neuron Perceptron with Universal Ap.pdf},
  journal = {Computational Intelligence and Neuroscience},
  language = {en}
}

@article{Lourens2015,
  title = {Exploiting Pallidal Plasticity for Stimulation in {{Parkinson}}'s Disease},
  author = {Lourens, Marcel A J and Schwab, Bettina C and Nirody, Jasmine A and Meijer, Hil G E and {van Gils}, Stephan A},
  year = {2015},
  month = apr,
  volume = {12},
  pages = {026005},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/12/2/026005},
  abstract = {Objective. Continuous application of high-frequency deep brain stimulation (DBS) often effectively reduces motor symptoms of Parkinson's disease patients. While there is a growing need for more effective and less traumatic stimulation, the exact mechanism of DBS is still unknown. Here, we present a methodology to exploit the plasticity of GABAergic synapses inside the external globus pallidus (GPe) for the optimization of DBS. Approach. Assuming the existence of spike-timing-dependent plasticity (STDP) at GABAergic GPe\textendash{}GPe synapses, we simulate neural activity in a network model of the subthalamic nucleus and GPe. In particular, we test different DBS protocols in our model and quantify their influence on neural synchrony. Main results. In an exemplary set of biologically plausible model parameters, we show that STDP in the GPe has a direct influence on neural activity and especially the stability of firing patterns. STDP stabilizes both uncorrelated firing in the healthy state and correlated firing in the parkinsonian state. Alternative stimulation protocols such as coordinated reset stimulation can clearly profit from the stabilizing effect of STDP. These results are widely independent of the STDP learning rule. Significance. Once the model settings, e.g., connection architectures, have been described experimentally, our model can be adjusted and directly applied in the development of novel stimulation protocols. More efficient stimulation leads to both minimization of side effects and savings in battery power.},
  file = {/Users/qualia/Documents/Papers/2015 - Lourens et al. - Exploiting pallidal plasticity for stimulation in Parkinson's disease.pdf;/Users/qualia/Documents/Papers/Lourens et al. - 2015 - Exploiting pallidal plasticity for stimulation in .pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {2}
}

@article{Lowet2015,
  title = {Input-{{Dependent Frequency Modulation}} of {{Cortical Gamma Oscillations Shapes Spatial Synchronization}} and {{Enables Phase Coding}}},
  author = {Lowet, Eric and Roberts, Mark and Hadjipapas, Avgis and Peter, Alina and {van der Eerden}, Jan and De Weerd, Peter},
  editor = {Ermentrout, Bard},
  year = {2015},
  month = feb,
  volume = {11},
  pages = {e1004072},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004072},
  file = {/Users/qualia/Documents/Papers/Lowet et al. - 2015 - Input-Dependent Frequency Modulation of Cortical G.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {2}
}

@article{Lowet2016,
  title = {Neuronal Gamma-Band Synchronization Regulated by Instantaneous Modulations of the Oscillation Frequency},
  author = {Lowet, Eric and Roberts, Mark and Peter, Alina and Gips, Bart and De Weerd, Peter},
  year = {2016},
  month = sep,
  doi = {10.1101/070672},
  abstract = {Neuronal gamma-band synchronization shapes information flow during sensory and cognitive processing. A common view is that a stable and shared frequency over time is required for robust and functional synchronization. To the contrary, we found that non-stationary instantaneous frequency modulations were essential for synchronization. First, we recorded gamma rhythms in monkey visual area V1, and found that they synchronized by continuously modulating their frequency difference in a phase-dependent manner. The frequency modulation properties regulated both the phase-locking and the preferred phase-relation between gamma rhythms. Second, our experimental observations were in agreement with a biophysical model of gamma rhythms and were accurately predicted by the theory of weakly coupled oscillators revealing the underlying theoretical principles that govern gamma synchronization. Thus, synchronization through instantaneous frequency modulations represents a fundamental principle of gamma-band neural coordination that is likely generalizable to other brain rhythms.},
  file = {/Users/qualia/Documents/Papers/Lowet et al. - 2016 - Neuronal gamma-band synchronization regulated by i.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Lu2017,
  title = {The {{Expressive Power}} of {{Neural Networks}}: {{A View}} from the {{Width}}},
  author = {Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  year = {2017},
  pages = {9},
  abstract = {The expressive power of neural networks is important for understanding deep learning. Most existing works consider this problem from the view of the depth of a network. In this paper, we study how width affects the expressiveness of neural networks. Classical results state that depth-bounded (e.g. depth-2) networks with suitable activation functions are universal approximators. We show a universal approximation theorem for width-bounded ReLU networks: width-(n + 4) ReLU networks, where n is the input dimension, are universal approximators. Moreover, except for a measure zero set, all functions cannot be approximated by width-n ReLU networks, which exhibits a phase transition. Several recent works demonstrate the benefits of depth by proving the depth-efficiency of neural networks. That is, there are classes of deep networks which cannot be realized by any shallow network whose size is no more than an exponential bound. Here we pose the dual question on the width-efficiency of ReLU networks: Are there wide networks that cannot be realized by narrow networks whose size is not substantially larger? We show that there exist classes of wide networks which cannot be realized by any narrow network whose depth is no more than a polynomial bound. On the other hand, we demonstrate by extensive experiments that narrow networks whose size exceed the polynomial bound by a constant factor can approximate wide and shallow network with high accuracy. Our results provide more comprehensive evidence that depth may be more effective than width for the expressiveness of ReLU networks.},
  file = {/Users/qualia/Documents/Papers/Lu et al. - The Expressive Power of Neural Networks A View fr.pdf},
  journal = {NIPS},
  language = {en}
}

@article{Lucken2013,
  title = {Desynchronization Boost by Non-Uniform Coordinated Reset Stimulation in Ensembles of Pulse-Coupled Neurons},
  author = {L{\"u}cken, Leonhard and Yanchuk, Serhiy and Popovych, Oleksandr V. and Tass, Peter A.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00063},
  abstract = {Several brain diseases are characterized by abnormal neuronal synchronization. Desynchronization of abnormal neural synchrony is theoretically compelling because of the complex dynamical mechanisms involved. We here present a novel type of coordinated reset (CR) stimulation. CR means to deliver phase resetting stimuli at different neuronal sub-populations sequentially, i.e., at times equidistantly distributed in a stimulation cycle. This uniform timing pattern seems to be intuitive and actually applies to the neural network models used for the study of CR so far. CR resets the population to an unstable cluster state from where it passes through a desynchronized transient, eventually resynchronizing if left unperturbed. In contrast, we show that the optimal stimulation times are non-uniform. Using the model of weakly pulse-coupled neurons with phase response curves, we provide an approach that enables to determine optimal stimulation timing patterns that substantially maximize the desynchronized transient time following the application of CR stimulation. This approach includes an optimization search for clusters in a low-dimensional pulse coupled map. As a consequence, model-specific non-uniformly spaced cluster states cause considerably longer desynchronization transients. Intriguingly, such a desynchronization boost with non-uniform CR stimulation can already be achieved by only slight modifications of the uniform CR timing pattern. Our results suggest that the non-uniformness of the stimulation times can be a medically valuable parameter in the calibration procedure for CR stimulation, where the latter has successfully been used in clinical and pre-clinical studies for the treatment of Parkinson's disease and tinnitus.},
  file = {/Users/qualia/Documents/Papers/2013 - Lücken et al. - Desynchronization boost by non-uniform coordinated reset stimulation in ensembles of pulse-coupled neurons.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Luczak2009,
  title = {Spontaneous {{Events Outline}} the {{Realm}} of {{Possible Sensory Responses}} in {{Neocortical Populations}}},
  author = {Luczak, Artur and Barth{\'o}, Peter and Harris, Kenneth D.},
  year = {2009},
  month = may,
  volume = {62},
  pages = {413--425},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.03.014},
  abstract = {Neocortical assemblies produce complex activity patterns both in response to sensory stimuli and spontaneously without sensory input. To investigate the structure of these patterns, we recorded from populations of 40\textendash{}100 neurons in auditory and somatosensory cortices of anesthetized and awake rats using silicon microelectrodes. Population spike time patterns were broadly conserved across multiple sensory stimuli and spontaneous events. Although individual neurons showed timing variations between stimuli, these were not sufficient to disturb a generally conserved sequential organization observed at the population level, lasting for approximately 100 ms with spiking reliability decaying progressively after event onset. Preserved constraints were also seen in population firing rate vectors, with vectors evoked by individual stimuli occupying subspaces of a larger but still constrained space outlined by the set of spontaneous events. These results suggest that population spike patterns are drawn from a limited ``vocabulary,'' sampled widely by spontaneous events but more narrowly by sensory responses.},
  file = {/Users/qualia/Documents/Papers/2009 - Luczak, Barth, Harris - Spontaneous Events Outline the Realm of Possible Sensory Responses in Neocortical Populations.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Lundqvist2010,
  title = {Bistable, {{Irregular Firing}} and {{Population Oscillations}} in a {{Modular Attractor Memory Network}}},
  author = {Lundqvist, Mikael and Compte, Albert and Lansner, Anders},
  editor = {Morrison, Abigail},
  year = {2010},
  month = jun,
  volume = {6},
  pages = {e1000803},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000803},
  abstract = {Attractor neural networks are thought to underlie working memory functions in the cerebral cortex. Several such models have been proposed that successfully reproduce firing properties of neurons recorded from monkeys performing working memory tasks. However, the regular temporal structure of spike trains in these models is often incompatible with experimental data. Here, we show that the in vivo observations of bistable activity with irregular firing at the single cell level can be achieved in a large-scale network model with a modular structure in terms of several connected hypercolumns. Despite high irregularity of individual spike trains, the model shows population oscillations in the beta and gamma band in ground and active states, respectively. Irregular firing typically emerges in a high-conductance regime of balanced excitation and inhibition. Population oscillations can produce such a regime, but in previous models only a non-coding ground state was oscillatory. Due to the modular structure of our network, the oscillatory and irregular firing was maintained also in the active state without fine-tuning. Our model provides a novel mechanistic view of how irregular firing emerges in cortical populations as they go from beta to gamma oscillations during memory retrieval.},
  file = {/Users/qualia/Documents/Papers/2010 - Lundqvist, Compte, Lansner - Bistable, irregular firing and population oscillations in a modular attractor memory network.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {6}
}

@article{Lundqvist2016,
  title = {Gamma and {{Beta Bursts Underlie Working Memory}}},
  author = {Lundqvist, Mikael and Rose, Jonas and Herman, Pawel and Brincat, Scott L. and Buschman, Timothy J. and Miller, Earl K.},
  year = {2016},
  month = apr,
  volume = {90},
  pages = {152--164},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.02.028},
  abstract = {Working memory is thought to result from sustained neuron spiking. However, computational models suggest complex dynamics with discrete oscillatory bursts. We analyzed local field potential (LFP) and spiking from the prefrontal cortex (PFC) of monkeys performing a working memory task. There were brief bursts of narrow-band gamma oscillations (45\textendash{}100 Hz), varied in time and frequency, accompanying encoding and re-activation of sensory information. They appeared at a minority of recording sites associated with spiking reflecting the to-beremembered items. Beta oscillations (20\textendash{}35 Hz) also occurred in brief, variable bursts but reflected a default state interrupted by encoding and decoding. Only activity of neurons reflecting encoding/ decoding correlated with changes in gamma burst rate. Thus, gamma bursts could gate access to, and prevent sensory interference with, working memory. This supports the hypothesis that working memory is manifested by discrete oscillatory dynamics and spiking, not sustained activity.},
  file = {/Users/qualia/Documents/Papers/Lundqvist et al. - 2016 - Gamma and Beta Bursts Underlie Working Memory.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Lupyan2013,
  title = {The Difficulties of Executing Simple Algorithms: {{Why}} Brains Make Mistakes Computers Don't},
  shorttitle = {The Difficulties of Executing Simple Algorithms},
  author = {Lupyan, Gary},
  year = {2013},
  month = dec,
  volume = {129},
  pages = {615--636},
  issn = {00100277},
  doi = {10.1016/j.cognition.2013.08.015},
  abstract = {It is shown that educated adults routinely make errors in placing stimuli into familiar, welldefined categories such as TRIANGLE and ODD NUMBER. Scalene triangles are often rejected as instances of triangles and 798 is categorized by some as an odd number. These patterns are observed both in timed and untimed tasks, hold for people who can fully express the necessary and sufficient conditions for category membership, and for individuals with varying levels of education. A sizeable minority of people believe that 400 is more even than 798 and that an equilateral triangle is the most ``trianglest'' of triangles. Such beliefs predict how people instantiate other categories with necessary and sufficient conditions, e.g., GRANDMOTHER. I argue that the distributed and graded nature of mental representations means that human algorithms, unlike conventional computer algorithms, only approximate rule-based classification and never fully abstract from the specifics of the input. This input-sensitivity is critical to obtaining the kind of cognitive flexibility at which humans excel, but comes at the cost of generally poor abilities to perform context-free computations. If human algorithms cannot be trusted to produce unfuzzy representations of odd numbers, triangles, and grandmothers, the idea that they can be trusted to do the heavy lifting of moment-to-moment cognition that is inherent in the metaphor of mind as digital computer still common in cognitive science, needs to be seriously reconsidered.},
  file = {/Users/qualia/Documents/Papers/2013 - Lupyan - The difficulties of executing simple algorithms Why brains make mistakes computers don't.pdf},
  journal = {Cognition},
  language = {en},
  number = {3}
}

@article{Luzzana,
  title = {The Regulation of Oxygen Affinity of Human Haemoglobin},
  author = {Luzzana, M},
  pages = {2},
  file = {/Users/qualia/Documents/Papers/1972 - Brindley, Cgraggs - The electrical activity in the motor cortex that accompanices volentary movement.pdf},
  language = {en}
}

@article{Ly2017,
  title = {A {{Tutorial}} on {{Fisher Information}}},
  author = {Ly, Alexander and Marsman, Maarten and Verhagen, Josine and Grasman, Raoul and Wagenmakers, Eric-Jan},
  year = {2017},
  month = may,
  abstract = {In many statistical applications that concern mathematical psychologists, the concept of Fisher information plays an important role. In this tutorial we clarify the concept of Fisher information as it manifests itself across three different statistical paradigms. First, in the frequentist paradigm, Fisher information is used to construct hypothesis tests and confidence intervals using maximum likelihood estimators; second, in the Bayesian paradigm, Fisher information is used to define a default prior; finally, in the minimum description length paradigm, Fisher information is used to measure model complexity.},
  archivePrefix = {arXiv},
  eprint = {1705.01064},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2014 - Ly et al. - A Tutorial on Fisher Information.pdf;/Users/qualia/Documents/Papers/Ly et al. - 2017 - A Tutorial on Fisher Information.pdf},
  journal = {arXiv:1705.01064 [math, stat]},
  keywords = {62-01; 62B10 (Primary); 62F03; 62F12; 62F15; 62B10 (Secondary),Mathematics - Statistics Theory},
  language = {en},
  primaryClass = {math, stat}
}

@article{Lynch,
  title = {Computational {{Tradeoffs}} in {{Biological Neural Networks}}: {{Self}}-{{Stabilizing Winner}}-{{Take}}-{{All Networks}}},
  author = {Lynch, Nancy and Musco, Cameron and Parter, Merav},
  pages = {43},
  abstract = {We initiate a line of investigation into biological neural networks from an algorithmic perspective. We develop a simplified but biologically plausible model for distributed computation in stochastic spiking neural networks and study tradeoffs between computation time and network complexity in this model. Our aim is to abstract real neural networks in a way that, while not capturing all interesting features, preserves high-level behavior and allows us to make biologically relevant conclusions.},
  file = {/Users/qualia/Documents/Papers/Lynch et al. - Computational Tradeoﬀs in Biological Neural Networ.pdf},
  language = {en}
}

@techreport{Lyu2019,
  title = {Scenes That Produce More Consistent Fixation Maps Are More Memorable},
  author = {Lyu, Muxuan and Choe, Kyoung Whan and Kardan, Omid and Kotabe, Hiroki and Henderson, John M. and Berman, Marc},
  year = {2019},
  month = jul,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/3e8qm},
  abstract = {Studying factors that contribute to scene memorability is important for understanding human vision and memory. Here we demonstrated in two different eye-tracking datasets that the higher the fixation map consistency (also called inter-observer congruency of fixation maps) of a scene, the higher its memorability is. To provide a mechanistic explanation for how a scene can produce more or less consistent fixation maps across viewers, we created a simple computational model by assuming some high signal regions in a scene that will attract more fixations than other regions (ambient noise). We then varied the amplitude of the signal relative to noise (SNR) to examine the relationship between SNR and fixation map consistency. Our model showed that the higher a scene's SNR, the higher its fixation map consistency, suggesting that fixation map consistency reflects the SNR of a scene, an intrinsic scene property that can affect human vision and memory.},
  file = {/Users/qualia/Documents/Papers/Lyu et al. - 2019 - Scenes that produce more consistent fixation maps .pdf},
  language = {en},
  type = {Preprint}
}

@incollection{Maass1994,
  title = {A {{Comparison}} of the {{Computational Power}} of {{Sigmoid}} and {{Boolean Threshold Circuits}}},
  booktitle = {Theoretical {{Advances}} in {{Neural Computation}} and {{Learning}}},
  author = {Maass, W. and Schnitger, G. and Sontag, E. D.},
  editor = {Roychowdhury, Vwani and Siu, Kai-Yeung and Orlitsky, Alon},
  year = {1994},
  pages = {127--151},
  publisher = {{Springer US}},
  address = {{Boston, MA}},
  doi = {10.1007/978-1-4615-2696-4_4},
  abstract = {We examine the power of constant depth circuits with sigmoid (i.e. smooth) threshold gates for computing boolean functions. It is shown that, for depth 2, constant size circuits of this type are strictly more powerful than constant size boolean threshold circuits (i.e. circuits with linear threshold gates). On the other hand it turns out that, for any constant depth d, polynomial size sigmoid threshold circuits with polynomially bounded weights compute exactly the same boolean functions as the corresponding circuits with linear threshold gates.},
  file = {/Users/qualia/Documents/Papers/1994 - Maass, Schnitger, Sontag - A comparison of the computational power of sigmoid and Boolean threshold circuits.pdf},
  isbn = {978-1-4613-6160-2 978-1-4615-2696-4},
  language = {en}
}

@article{Maass2002,
  title = {Real-{{Time Computing Without Stable States}}: {{A New Framework}} for {{Neural Computation Based}} on {{Perturbations}}},
  shorttitle = {Real-{{Time Computing Without Stable States}}},
  author = {Maass, Wolfgang and Natschl{\"a}ger, Thomas and Markram, Henry},
  year = {2002},
  month = nov,
  volume = {14},
  pages = {2531--2560},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976602760407955},
  file = {/Users/qualia/Documents/Papers/2002 - Maass, Natschläger, Markram - Real-Time Computing Without Stable States A New Framework for Neural Computation Based on Perturba.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@incollection{Maass2011,
  title = {Liquid {{State Machines}}: {{Motivation}}, {{Theory}}, and {{Applications}}},
  shorttitle = {Liquid {{State Machines}}},
  booktitle = {Computability in {{Context}}},
  author = {Maass, Wolfgang},
  year = {2011},
  month = feb,
  pages = {275--296},
  publisher = {{IMPERIAL COLLEGE PRESS}},
  doi = {10.1142/9781848162778_0008},
  collaborator = {Cooper, S Barry and Sorbi, Andrea},
  file = {/Users/qualia/Documents/Papers/2010 - Maass - Liquid State Machines Motivation, Theory, and Applications.pdf},
  isbn = {978-1-84816-245-7 978-1-84816-277-8},
  language = {en}
}

@article{Mace2011,
  title = {Functional Ultrasound Imaging of the Brain},
  author = {Mac{\'e}, Emilie and Montaldo, Gabriel and Cohen, Ivan and Baulac, Michel and Fink, Mathias and Tanter, Mickael},
  year = {2011},
  month = aug,
  volume = {8},
  pages = {662--664},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.1641},
  file = {/Users/qualia/Documents/Papers/2011 - Macé et al. - Functional ultrasound imaging of the brain.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {8}
}

@article{Machta2013,
  title = {Parameter {{Space Compression Underlies Emergent Theories}} and {{Predictive Models}}},
  author = {Machta, Benjamin B. and Chachra, Ricky and Transtrum, Mark K. and Sethna, James P.},
  year = {2013},
  month = nov,
  volume = {342},
  pages = {604--607},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1238723},
  abstract = {We report a similarity between the microscopic parameter dependance of emergent theories in physics and that of multiparameter models common in other areas of science. In both cases, predictions are possible despite large uncertainties in the microscopic parameters because these details are compressed into just a few governing parameters that are sufficient to describe relevant observables. We make this commonality explicit by examining parameter sensitivity in a hopping model of diffusion and a generalized Ising model of ferromagnetism. We trace the emergence of a smaller effective model to the development of a hierarchy of parameter importance quantified by the eigenvalues of the Fisher Information Matrix. Strikingly, the same hierarchy appears ubiquitously in models taken from diverse areas of science. We conclude that the emergence of effective continuum and universal theories in physics is due to the same parameter space hierarchy that underlies predictive modeling in other areas of science.},
  archivePrefix = {arXiv},
  eprint = {1303.6738},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2013 - Machta et al. - Parameter space compression underlies emergent theories and predictive models.pdf},
  journal = {Science},
  keywords = {Condensed Matter - Statistical Mechanics,Mathematical Physics,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Other Quantitative Biology},
  language = {en},
  number = {6158}
}

@book{MacKay2003,
  title = {Information {{Theory}}, {{Inference}}, and {{Learning Algorithms}}},
  author = {MacKay, David J C},
  year = {2003},
  edition = {Second},
  file = {/Users/qualia/Documents/Papers/2003 - Mackay - Information Theory , Inference , and Learning Algorithms.pdf},
  language = {en}
}

@article{Macy,
  title = {Learning Dynamics in Social Dilemmas},
  author = {Macy, Michael W and Flache, Andreas},
  pages = {8},
  file = {/Users/qualia/Documents/Papers/2002 - Macy, Flache - Learning dynamics in social dilemmas.pdf},
  language = {en}
}

@article{Mahon2010,
  title = {Judging Semantic Similarity: An Event-Related {{fMRI}} Study with Auditory Word Stimuli},
  shorttitle = {Judging Semantic Similarity},
  author = {Mahon, B.Z. and Caramazza, A.},
  year = {2010},
  month = aug,
  volume = {169},
  pages = {279--286},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2010.04.029},
  abstract = {Much of mental life consists in thinking about object concepts that are not currently within the scope of perception. The general system that enables multiple representations to be maintained and compared is referred to as ``working memory'' [Repov{\v s} G, Baddeley A (2006) Neuroscience 139:5\textendash{}21], and involves regions in medial and lateral parietal and frontal cortex [e.g., Smith EE, Jonides J (1999) Science 283:1657\textendash{}1661]. It has been assumed that the contents of working memory index information in regions of the brain that are critical for processing and storing object knowledge. To study the processes involved in thinking about common object concepts, we used event related fMRI to study BOLD activity while participants made judgments of conceptual similarity over pairs of sequentially presented auditory words. Through a combination of conventional fMRI analysis approaches and multi-voxel pattern analysis (MVPA), we show that the brain responses associated with the second word in a pair carry information about the conceptual similarity between the two members of the pair. This was the case in frontal and parietal regions involved in the working memory and decision components of the task for both analysis approaches. However, in other regions of the brain, including early visual regions, MVPA permitted classification of semantic distance relationships where conventional averaging approaches failed to show a difference. These findings suggest that diffuse and statistically sub-threshold ``scattering'' of BOLD activity in some regions may carry substantial information about the contents of mental representations. \textcopyright{} 2010 IBRO. Published by Elsevier Ltd. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2010 - Mahon, Caramazza - Judging semantic similarity an event-related fMRI study with auditory word stimuli.pdf},
  journal = {Neuroscience},
  language = {en},
  number = {1}
}

@article{Maia2017,
  title = {An {{Integrative Perspective}} on the {{Role}} of {{Dopamine}} in {{Schizophrenia}}},
  author = {Maia, Tiago V. and Frank, Michael J.},
  year = {2017},
  month = jan,
  volume = {81},
  pages = {52--66},
  issn = {00063223},
  doi = {10.1016/j.biopsych.2016.05.021},
  abstract = {We propose that schizophrenia involves a combination of decreased phasic dopamine responses for relevant stimuli and increased spontaneous phasic dopamine release. Using insights from computational reinforcement-learning models and basic-science studies of the dopamine system, we show that each of these two disturbances contributes to a specific symptom domain and explains a large set of experimental findings associated with that domain. Reduced phasic responses for relevant stimuli help to explain negative symptoms and provide a unified explanation for the following experimental findings in schizophrenia, most of which have been shown to correlate with negative symptoms: reduced learning from rewards; blunted activation of the ventral striatum, midbrain, and other limbic regions for rewards and positive prediction errors; blunted activation of the ventral striatum during reward anticipation; blunted autonomic responding for relevant stimuli; blunted neural activation for aversive outcomes and aversive prediction errors; reduced willingness to expend effort for rewards; and psychomotor slowing. Increased spontaneous phasic dopamine release helps to explain positive symptoms and provides a unified explanation for the following experimental findings in schizophrenia, most of which have been shown to correlate with positive symptoms: aberrant learning for neutral cues (assessed with behavioral and autonomic responses), and aberrant, increased activation of the ventral striatum, midbrain, and other limbic regions for neutral cues, neutral outcomes, and neutral prediction errors. Taken together, then, these two disturbances explain many findings in schizophrenia. We review evidence supporting their co-occurrence and consider their differential implications for the treatment of positive and negative symptoms.},
  file = {/Users/qualia/Documents/Papers/Maia and Frank - 2017 - An Integrative Perspective on the Role of Dopamine.pdf},
  journal = {Biological Psychiatry},
  language = {en},
  number = {1}
}

@article{Maimon2009,
  title = {Beyond {{Poisson}}: {{Increased Spike}}-{{Time Regularity}} across {{Primate Parietal Cortex}}},
  shorttitle = {Beyond {{Poisson}}},
  author = {Maimon, Gaby and Assad, John A.},
  year = {2009},
  month = may,
  volume = {62},
  pages = {426--440},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.03.021},
  abstract = {Cortical areas differ in their patterns of connectivity, cellular composition, and functional architecture. Spike trains, on the other hand, are commonly assumed to follow similarly irregular dynamics across neocortex. We examined spike-time statistics in four parietal areas using a method that accounts for nonstationarities in firing rate. We found that, whereas neurons in visual areas fire irregularly, many cells in association and motor-like parietal regions show increasingly regular spike trains by comparison. Regularity was evident both in the shape of interspike interval distributions and in spike-count variability across trials. Thus, Poisson-like randomness is not a universal feature of neocortex. Rather, many parietal cells have reduced trial-to-trial variability in spike counts that could provide for more reliable firing-rate signals. These results suggest that spiking dynamics may play different roles in different cortical areas and should not be assumed to arise from fundamentally irreducible noise sources.},
  file = {/Users/qualia/Documents/Papers/2009 - Maimon, Assad - Beyond Poisson Increased Spike-Time Regularity across Primate Parietal Cortex.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Mainen1995,
  title = {Reliability of {{Spike Timing}} in {{Neocortical Neurons}}},
  author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
  year = {1995},
  volume = {268},
  pages = {1503--1506},
  file = {/Users/qualia/Documents/Papers/2009 - Mainen, Sejnowski - Reliability of Spike Timing in Neocortical Neurons.pdf},
  journal = {Science, New Series},
  number = {5216}
}

@article{Mainen1996,
  title = {Influence of Dendritic Structure on Firing Pattern in Model Neocortical Neurons},
  author = {Mainen, Zachary F. and Sejnowski, Terrence J.},
  year = {1996},
  month = jul,
  volume = {382},
  pages = {363--366},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/382363a0},
  file = {/Users/qualia/Documents/Papers/1996 - Mainen, Sejnowski - Influence of dendritic structure on firing pattern in model neocortical neurons(2).pdf},
  journal = {Nature},
  language = {en},
  number = {6589}
}

@article{Majumder2005,
  title = {Enhanced Flow in Carbon Nanotubes},
  author = {Majumder, Mainak and Chopra, Nitin and Andrews, Rodney and Hinds, Bruce J.},
  year = {2005},
  month = nov,
  volume = {438},
  pages = {44--44},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/438044a},
  file = {/Users/qualia/Documents/Papers/2005 - Strogatz et al. - Crowd synchrony on the Millennium Bridge.pdf},
  journal = {Nature},
  language = {en},
  number = {7064}
}

@article{Mallet2007,
  title = {Stimulation of Subterritories of the Subthalamic Nucleus Reveals Its Role in the Integration of the Emotional and Motor Aspects of Behavior},
  author = {Mallet, L. and Schupbach, M. and N'Diaye, K. and Remy, P. and Bardinet, E. and Czernecki, V. and Welter, M.-L. and Pelissolo, A. and Ruberg, M. and Agid, Y. and Yelnik, J.},
  year = {2007},
  month = jun,
  volume = {104},
  pages = {10661--10666},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0610849104},
  file = {/Users/qualia/Documents/Papers/2007 - Mallet et al. - Stimulation of subterritories of the subthalamic nucleus reveals its role in the integration of the emotional and.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {25}
}

@article{Mallet2012,
  title = {Dichotomous {{Organization}} of the {{External Globus Pallidus}}},
  author = {Mallet, Nicolas and Micklem, Benjamin R. and Henny, Pablo and Brown, Matthew T. and Williams, Claire and Bolam, J. Paul and Nakamura, Kouichi C. and Magill, Peter J.},
  year = {2012},
  month = jun,
  volume = {74},
  pages = {1075--1086},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.04.027},
  abstract = {Different striatal projection neurons are the origin of a dual organization essential for basal ganglia function. We have defined an analogous division of labor in the external globus pallidus (GPe) of Parkinsonian rats, showing that the distinct temporal activities of two populations of GPe neuron in vivo are underpinned by distinct molecular profiles and axonal connectivities. A first population of prototypic GABAergic GPe neurons fire antiphase to subthalamic nucleus (STN) neurons, often express parvalbumin, and target downstream basal ganglia nuclei, including STN. In contrast, a second population (arkypallidal neurons) fire in-phase with STN neurons, express preproenkephalin, and only innervate the striatum. This novel cell type provides the largest extrinsic GABAergic innervation of striatum, targeting both projection neurons and interneurons. We conclude that GPe exhibits several core components of a dichotomous organization as fundamental as that in striatum. Thus, two populations of GPe neuron together orchestrate activities across all basal ganglia nuclei in a cell-type-specific manner.},
  file = {/Users/qualia/Documents/Papers/2012 - Mallet et al. - Dichotomous Organization of the External Globus Pallidus.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Mandelbrot1968,
  title = {Fractional {{Brownian Motions}}, {{Fractional Noises}} and {{Applications}}},
  author = {Mandelbrot, Benoit B. and Ness, John W. Van},
  year = {1968},
  volume = {10},
  pages = {422--437},
  file = {/Users/qualia/Documents/Papers/1968 - Mandelbrot, Van Ness - Fractional Brownian Motions, Fractional Noises and Applications.pdf},
  journal = {SIAM Review},
  language = {en},
  number = {4}
}

@article{Mania,
  title = {Simple Random Search Provides a Competitive Approach to Reinforcement Learning},
  author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  pages = {22},
  abstract = {A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-ofthe-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.},
  file = {/Users/qualia/Documents/Papers/Mania et al. - Simple random search provides a competitive approa.pdf},
  language = {en}
}

@article{Mante2013,
  title = {Context-Dependent Computation by Recurrent Dynamics in Prefrontal Cortex},
  author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V. and Newsome, William T.},
  year = {2013},
  month = nov,
  volume = {503},
  pages = {78--84},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature12742},
  file = {/Users/qualia/Documents/Papers/2013 - Mante et al. - Context-dependent computation by recurrent dynamics in prefrontal cortex.pdf},
  journal = {Nature},
  language = {en},
  number = {7474}
}

@article{Manwani1999,
  title = {Detecting and {{Estimating Signals}} in {{Noisy Cable Structures}}, {{II}}: {{Information Theoretical Analysis}}},
  shorttitle = {Detecting and {{Estimating Signals}} in {{Noisy Cable Structures}}, {{II}}},
  author = {Manwani, Amit and Koch, Christof},
  year = {1999},
  month = nov,
  volume = {11},
  pages = {1831--1873},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976699300015981},
  file = {/Users/qualia/Documents/Papers/1999 - Manwani, Koch - Detecting and estimating signals in noisy cable structures, II information theoretical analysis.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Mao,
  title = {Modeling the {{Role}} of the {{Basal Ganglia}} in {{Motor Control}} and {{Motor Programming}}},
  author = {Mao, Zhi-Hong},
  pages = {166},
  abstract = {The basal ganglia (BG) are a group of highly interconnected nuclei buried deep in the brain. They are involved in an important range of brain functions, including both lower-level movement control and higher-level cognitive decision making. Dysfunction of the BG has been linked to the human neurological disorders such as Parkinson's disease, Huntington's disease, and schizophrenia.},
  file = {/Users/qualia/Documents/Papers/2005 - Mao - Modeling the Role of the Basal Ganglia in Motor Control and Motor Programming.pdf},
  language = {en}
}

@article{Marblestone2016,
  title = {Towards an Integration of Deep Learning and Neuroscience},
  author = {Marblestone, Adam Henry and Wayne, Greg and Kording, Konrad P},
  year = {2016},
  month = aug,
  doi = {10.1101/058545},
  abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) these cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
  file = {/Users/qualia/Documents/Papers/Marblestone et al. - 2016 - Towards an integration of deep learning and neuros.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Marchiori2008,
  title = {Predicting {{Human Interactive Learning}} by {{Regret}}-{{Driven Neural Networks}}},
  author = {Marchiori, D. and Warglien, M.},
  year = {2008},
  month = feb,
  volume = {319},
  pages = {1111--1113},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1151185},
  file = {/Users/qualia/Documents/Papers/2008 - Marchiori, Warglien - Predicting human interactive learning by regret-driven neural networks.pdf},
  journal = {Science},
  language = {en},
  number = {5866}
}

@article{Marcus2014,
  title = {The Atoms of Neural Computation},
  author = {Marcus, G. and Marblestone, A. and Dean, T.},
  year = {2014},
  month = oct,
  volume = {346},
  pages = {551--552},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1261661},
  file = {/Users/qualia/Documents/Papers/Marcus et al. - 2014 - The atoms of neural computation.pdf},
  journal = {Science},
  language = {en},
  number = {6209}
}

@article{Marcus2014a,
  title = {The Atoms of Neural Computation},
  author = {Marcus, G. and Marblestone, A. and Dean, T.},
  year = {2014},
  month = oct,
  volume = {346},
  pages = {551--552},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1261661},
  file = {/Users/qualia/Documents/Papers/Marcus et al. - 2014 - The atoms of neural computation 2.pdf},
  journal = {Science},
  language = {en},
  number = {6209}
}

@article{Marder2007,
  title = {Understanding {{Circuit Dynamics Using}} the {{Stomatogastric Nervous System}} of {{Lobsters}} and {{Crabs}}},
  author = {Marder, Eve and Bucher, Dirk},
  year = {2007},
  month = mar,
  volume = {69},
  pages = {291--316},
  issn = {0066-4278, 1545-1585},
  doi = {10.1146/annurev.physiol.69.031905.161516},
  abstract = {Studies of the stomatogastric nervous systems of lobsters and crabs have led to numerous insights into the cellular and circuit mechanisms that generate rhythmic motor patterns. The small number of easily identifiable neurons allowed the establishment of connectivity diagrams among the neurons of the stomatogastric ganglion. We now know that (a) neuromodulatory substances reconfigure circuit dynamics by altering synaptic strength and voltage-dependent conductances and (b) individual neurons can switch among different functional circuits. Computational and experimental studies of single-neuron and network homeostatic regulation have provided insight into compensatory mechanisms that can underlie stable network performance. Many of the observations first made using the stomatogastric nervous system can be generalized to other invertebrate and vertebrate circuits.},
  file = {/Users/qualia/Documents/Papers/Marder and Bucher - 2007 - Understanding Circuit Dynamics Using the Stomatoga.pdf},
  journal = {Annual Review of Physiology},
  language = {en},
  number = {1}
}

@article{Marder2014,
  title = {Neuromodulation of {{Circuits}} with {{Variable Parameters}}: {{Single Neurons}} and {{Small Circuits Reveal Principles}} of {{State}}-{{Dependent}} and {{Robust Neuromodulation}}},
  shorttitle = {Neuromodulation of {{Circuits}} with {{Variable Parameters}}},
  author = {Marder, Eve and O'Leary, Timothy and Shruti, Sonal},
  year = {2014},
  month = jul,
  volume = {37},
  pages = {329--346},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-071013-013958},
  file = {/Users/qualia/Documents/Papers/2014 - Marder, O'Leary, Shruti - Neuromodulation of Circuits with Variable Parameters Single Neurons and Small Circuits Reveal Principle.pdf;/Users/qualia/Documents/Papers/Marder et al. - 2014 - Neuromodulation of Circuits with Variable Paramete.pdf},
  journal = {Annual Review of Neuroscience},
  language = {en},
  number = {1}
}

@article{Marder2015,
  title = {Robust Circuit Rhythms in Small Circuits Arise from Variable Circuit Components and Mechanisms},
  author = {Marder, Eve and Goeritz, Marie L and Otopalik, Adriane G},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {156--163},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.10.012},
  file = {/Users/qualia/Documents/Papers/Marder et al. - 2015 - Robust circuit rhythms in small circuits arise fro 2.pdf;/Users/qualia/Documents/Papers/Marder et al. - 2015 - Robust circuit rhythms in small circuits arise fro.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Marinelli2014,
  title = {Heterogeneity of Dopamine Neuron Activity across Traits and States},
  author = {Marinelli, M. and McCutcheon, J.E.},
  year = {2014},
  month = dec,
  volume = {282},
  pages = {176--197},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2014.07.034},
  abstract = {Midbrain dopamine neurons fire irregularly, with interspersed clusters of high-frequency spikes, commonly called `bursts'. In this review we examine such heterogeneity in activity, and provide insight into how it can participate in psychiatric conditions such as drug addiction. We first describe several techniques used to evaluate dopamine neuron activity, and comment on the different measures that each provides. We next describe the activity of dopamine neurons in `basal' conditions. Specifically, we discuss how the use of anesthesia and reduced preparations may alter aspects of dopamine cell activity, and how there is heterogeneity across species and regions. We also describe how dopamine cell firing changes throughout the peri-adolescent period and how dopamine neuron activity differs across the population. In the final section, we discuss how dopamine neuron activity changes in response to life events. First, we focus attention on drugs of abuse. Drugs themselves change firing activity through a variety of mechanisms, with effects on firing while drug is present differing from those seen after drug discontinuation. We then review how stimuli that are rewarding, aversive, or salient can evoke changes in firing rate and discharge pattern of dopamine neurons, and provide behavioral relevance of dopamine signaling. Finally, we discuss how stress can modulate dopamine neuron firing and how this may contribute to the role that stressful experiences play in psychiatric disorders such as addiction and depression.},
  file = {/Users/qualia/Documents/Papers/Marinelli and McCutcheon - 2014 - Heterogeneity of dopamine neuron activity across t.pdf},
  journal = {Neuroscience},
  language = {en}
}

@article{Markowitz2008,
  title = {Rate-Specific Synchrony: {{Using}} Noisy Oscillations to Detect Equally Active Neurons},
  shorttitle = {Rate-Specific Synchrony},
  author = {Markowitz, D. A. and Collman, F. and Brody, C. D. and Hopfield, J. J. and Tank, D. W.},
  year = {2008},
  month = jun,
  volume = {105},
  pages = {8422--8427},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0803183105},
  file = {/Users/qualia/Documents/Papers/2008 - Markowitz et al. - Rate-specific synchrony using noisy oscillations to detect equally active neurons.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {24}
}

@article{Markram1997,
  title = {Regulation of {{Synaptic Efficacy}} by {{Coincidence}} of {{Postsynaptic APs}} and {{EPSPs}}},
  author = {Markram, H.},
  year = {1997},
  month = jan,
  volume = {275},
  pages = {213--215},
  issn = {00368075, 10959203},
  doi = {10.1126/science.275.5297.213},
  file = {/Users/qualia/Documents/Papers/1997 - Markram et al. - Regulation of synaptic efficacy by coincidence of postsynaptic APs and EPSPs.pdf},
  journal = {Science},
  language = {en},
  number = {5297}
}

@article{Markram2015,
  title = {Reconstruction and {{Simulation}} of {{Neocortical Microcircuitry}}},
  author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and {Alonso-Nanclares}, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, S{\'e}bastien and Le B{\'e}, Jean-Vincent and Magalh{\~a}es, Bruno R.C. and {Merch{\'a}n-P{\'e}rez}, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and {Mu{\~n}oz-C{\'e}spedes}, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodr{\'i}guez, Jos{\'e}-Rodrigo and Riquelme, Juan Luis and R{\"o}ssert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and {Toledo-Rodriguez}, Maria and Tr{\"a}nkler, Thomas and Van Geit, Werner and D{\'i}az, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Sch{\"u}rmann, Felix},
  year = {2015},
  month = oct,
  volume = {163},
  pages = {456--492},
  issn = {00928674},
  doi = {10.1016/j.cell.2015.09.029},
  file = {/Users/qualia/Documents/Papers/2015 - Markram - Reconstruction and Simulation of Neocortical Microcircuitry.pdf;/Users/qualia/Documents/Papers/Markram et al. - 2015 - Reconstruction and Simulation of Neocortical Micro.pdf},
  journal = {Cell},
  language = {en},
  number = {2}
}

@article{Marquand2011,
  title = {Pattern {{Classification}} of {{Working Memory Networks Reveals Differential Effects}} of {{Methylphenidate}}, {{Atomoxetine}} and {{Placebo}} in {{Healthy Volunteers}}},
  author = {Marquand, Andre F and De Simoni, Sara and O'Daly, Owen G and Williams, Steven CR and {Mour{\~a}o-Miranda}, Janaina and Mehta, Mitul A},
  year = {2011},
  month = may,
  volume = {36},
  pages = {1237--1247},
  issn = {0893-133X, 1740-634X},
  doi = {10.1038/npp.2011.9},
  file = {/Users/qualia/Documents/Papers/2011 - Marquand et al. - Pattern classification of working memory networks reveals differential effects of methylphenidate, atomoxetine,.pdf},
  journal = {Neuropsychopharmacology},
  language = {en},
  number = {6}
}

@article{Marques2019,
  title = {Internal State Dynamics Shape Brainwide Activity and Foraging Behaviour},
  author = {Marques, Joao and Meng, Li and Schaak, Diane and Robson, Drew and Li, Jennifer},
  year = {2019},
  pages = {27},
  file = {/Users/qualia/Documents/Papers/Joao et al - 2019 - Internal state dynamics shape brainwide activity a.pdf},
  journal = {Nature},
  language = {en}
}

@article{Marreiros2009,
  title = {Population Dynamics under the {{Laplace}} Assumption},
  author = {Marreiros, Andr{\'e} C. and Kiebel, Stefan J. and Daunizeau, Jean and Harrison, Lee M. and Friston, Karl J.},
  year = {2009},
  month = feb,
  volume = {44},
  pages = {701--714},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.10.008},
  abstract = {In this paper, we describe a generic approach to modelling dynamics in neuronal populations. This approach models a full density on the states of neuronal populations but finesses this high-dimensional problem by reformulating density dynamics in terms of ordinary differential equations on the sufficient statistics of the densities considered (c.f., the method of moments). The particular form for the population density we adopt is a Gaussian density (c.f., the Laplace assumption). This means population dynamics are described by equations governing the evolution of the population's mean and covariance. We derive these equations from the Fokker-Planck formalism and illustrate their application to a conductance-based model of neuronal exchanges. One interesting aspect of this formulation is that we can uncouple the mean and covariance to furnish a neural-mass model, which rests only on the populations mean. This enables us to compare equivalent mean-field and neural-mass models of the same populations and evaluate, quantitatively, the contribution of population variance to the expected dynamics. The mean-field model presented here will form the basis of a dynamic causal model of observed electromagnetic signals in future work.},
  file = {/Users/qualia/Documents/Papers/2009 - Marreiros et al. - Population dynamics under the Laplace assumption.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Martens2009,
  title = {Exact Results for the {{Kuramoto}} Model with a Bimodal Frequency Distribution},
  author = {Martens, E. A. and Barreto, E. and Strogatz, S. H. and Ott, E. and So, P. and Antonsen, T. M.},
  year = {2009},
  month = feb,
  volume = {79},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.79.026204},
  file = {/Users/qualia/Documents/Papers/2009 - Martens et al. - Exact results for the Kuramoto model with a bimodal frequency distribution.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {2}
}

@article{Marti2018,
  title = {Correlations between Synapses in Pairs of Neurons Slow down Dynamics in Randomly Connected Neural Networks},
  author = {Mart{\'i}, Daniel and Brunel, Nicolas and Ostojic, Srdjan},
  year = {2018},
  month = jun,
  volume = {97},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.97.062314},
  file = {/Users/qualia/Documents/Papers/Martí et al. - 2018 - Correlations between synapses in pairs of neurons .pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Martin2018,
  title = {Differential Contributions of Subthalamic Beta Rhythms and 1/f Broadband Activity to Motor Symptoms in {{Parkinson}}'s Disease},
  author = {Martin, Stephanie and Iturrate, I{\~n}aki and Chavarriaga, Ricardo and Leeb, Robert and Sobolewski, Aleksander and Li, Andrew M. and Zaldivar, Julien and {Peciu-Florianu}, Iulia and Pralong, Etienne and {Castro-Jim{\'e}nez}, Mayte and Benninger, David and Vingerhoets, Fran{\c c}ois and Knight, Robert T. and Bloch, Jocelyne and Mill{\'a}n, Jos{\'e} del R.},
  year = {2018},
  month = dec,
  volume = {4},
  issn = {2373-8057},
  doi = {10.1038/s41531-018-0068-y},
  file = {/Users/qualia/Documents/Papers/Martin et al. - 2018 - Differential contributions of subthalamic beta rhy.pdf},
  journal = {npj Parkinson's Disease},
  language = {en},
  number = {1}
}

@article{Martinez-Ramon2006,
  title = {{{fMRI}} Pattern Classification Using Neuroanatomically Constrained Boosting},
  author = {{Mart{\'i}nez-Ram{\'o}n}, Manel and Koltchinskii, Vladimir and Heileman, Gregory L. and Posse, Stefan},
  year = {2006},
  month = jul,
  volume = {31},
  pages = {1129--1141},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.01.022},
  file = {/Users/qualia/Documents/Papers/2006 - Martínez-Ramón et al. - fMRI pattern classification using neuroanatomically constrained boosting.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Marvel2009,
  title = {Energy {{Landscape}} of {{Social Balance}}},
  author = {Marvel, Seth A. and Strogatz, Steven H. and Kleinberg, Jon M.},
  year = {2009},
  month = nov,
  volume = {103},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.103.198701},
  file = {/Users/qualia/Documents/Papers/2009 - Marvel, Strogatz, Kleinberg - Energy landscape of social balance.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {19}
}

@article{Marvel2011,
  title = {Continuous-Time Model of Structural Balance},
  author = {Marvel, Seth A. and Kleinberg, Jon and Kleinberg, Robert D. and Strogatz, Steven H.},
  year = {2011},
  month = feb,
  volume = {108},
  pages = {1771--1776},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1013213108},
  file = {/Users/qualia/Documents/Papers/2011 - Marvel et al. - Continuous-time model of structural balance.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {5}
}

@article{Marvel2012,
  title = {Encouraging {{Moderation}}: {{Clues}} from a {{Simple Model}} of {{Ideological Conflict}}},
  shorttitle = {Encouraging {{Moderation}}},
  author = {Marvel, Seth A. and Hong, Hyunsuk and Papush, Anna and Strogatz, Steven H.},
  year = {2012},
  month = sep,
  volume = {109},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.109.118702},
  file = {/Users/qualia/Documents/Papers/2012 - Marvel et al. - Encouraging moderation Clues from a simple model of ideological conflict.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {11}
}

@article{Marzen2015,
  title = {Time {{Resolution Dependence}} of {{Information Measures}} for {{Spiking Neurons}}: {{Atoms}}, {{Scaling}}, and {{Universality}}},
  shorttitle = {Time {{Resolution Dependence}} of {{Information Measures}} for {{Spiking Neurons}}},
  author = {Marzen, Sarah E. and DeWeese, Michael R. and Crutchfield, James P.},
  year = {2015},
  month = apr,
  abstract = {The mutual information between stimulus and spike-train response is commonly used to monitor neural coding efficiency, but neuronal computation broadly conceived requires more refined and targeted information measures of input-output joint processes. A first step towards that larger goal is to develop information measures for individual output processes, including information generation (entropy rate), stored information (statistical complexity), predictable information (excess entropy), and active information accumulation (bound information rate). We calculate these for spike trains generated by a variety of noise-driven integrate-and-fire neurons as a function of time resolution and for alternating renewal processes. We show that their time-resolution dependence reveals coarsegrained structural properties of interspike interval statistics; e.g., {$\tau$} -entropy rates that diverge less quickly than the firing rate indicate interspike interval correlations. We also find evidence that the excess entropy and regularized statistical complexity of different types of integrate-and-fire neurons are universal in the continuous-time limit in the sense that they do not depend on mechanism details. This suggests a surprising simplicity in the spike trains generated by these model neurons. Interestingly, neurons with gamma-distributed ISIs and neurons whose spike trains are alternating renewal processes do not fall into the same universality class. These results lead to two conclusions. First, the dependence of information measures on time resolution reveals mechanistic details about spike train generation. Second, information measures can be used as model selection tools for analyzing spike train processes.},
  archivePrefix = {arXiv},
  eprint = {1504.04756},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Marzen, DeWeese, Crutchfield - Time resolution dependence of information measures for spiking neurons scaling and universality.pdf;/Users/qualia/Documents/Papers/Marzen et al. - 2015 - Time Resolution Dependence of Information Measures.pdf},
  journal = {arXiv:1504.04756 [cond-mat, physics:nlin, q-bio]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Mathematics - Probability,Nonlinear Sciences - Chaotic Dynamics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {cond-mat, physics:nlin, q-bio}
}

@article{Marzen2016,
  title = {Weak Universality in Sensory Tradeoffs},
  author = {Marzen, Sarah and DeDeo, Simon},
  year = {2016},
  month = dec,
  volume = {94},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.060101},
  file = {/Users/qualia/Documents/Papers/Marzen and DeDeo - 2016 - Weak universality in sensory tradeoffs.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Massey2012,
  title = {High Resolution {{MR}} Anatomy of the Subthalamic Nucleus: {{Imaging}} at 9.{{4T}} with Histological Validation},
  shorttitle = {High Resolution {{MR}} Anatomy of the Subthalamic Nucleus},
  author = {Massey, L.A. and Miranda, M.A. and Zrinzo, L. and {Al-Helli}, O. and Parkes, H.G. and Thornton, J.S. and So, P.-W. and White, M.J. and Mancini, L. and Strand, C. and Holton, J.L. and Hariz, M.I. and Lees, A.J. and Revesz, T. and Yousry, T.A.},
  year = {2012},
  month = feb,
  volume = {59},
  pages = {2035--2044},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2011.10.016},
  abstract = {Using conventional MRI the subthalamic nucleus (STN) is not clearly defined. Our objective was to define the anatomy of the STN using 9.4 T MRI of post mortem tissue with histological validation. Spin-echo (SE) and 3D gradient-echo (GE) images were obtained at 9.4 T in 8 post mortem tissue blocks and compared directly with corresponding histological slides prepared with Luxol Fast Blue/Cresyl Violet (LFB/CV) in 4 cases and Perl stain in 3. The variability of the STN anatomy was studied using internal reference points. The anatomy of the STN and surrounding structures was demonstrated in all three anatomical planes using 9.4 T MR images in concordance with LFB/CV stained histological sections. Signal hypointensity was seen in 6/8 cases in the anterior and medial STN that corresponded with regions of more intense Perl staining. There was significant variability in the volume, shape and location of the borders of the STN. Using 9.4 T MRI, the internal signal characteristics and borders of the STN are clearly defined and significant anatomical variability is apparent. Direct visualisation of the STN is possible using high field MRI and this is particularly relevant, given its anatomical variability, for planning deep brain stimulation.},
  file = {/Users/qualia/Documents/Papers/2012 - Massey et al. - High resolution MR anatomy of the subthalamic nucleus Imaging at 9.4T with histological validation.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Mastro2017,
  title = {Cell-Specific Pallidal Intervention Induces Long-Lasting Motor Recovery in Dopamine-Depleted Mice},
  author = {Mastro, Kevin J and Zitelli, Kevin T and Willard, Amanda M and Leblanc, Kimberly H and Kravitz, Alexxai V and Gittis, Aryn H},
  year = {2017},
  month = jun,
  volume = {20},
  pages = {815--823},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4559},
  file = {/Users/qualia/Documents/Papers/Mastro et al. - 2017 - Cell-specific pallidal intervention induces long-l.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Mather,
  title = {Exploration, {{Play}}, and {{Habituation}} in {{Octopuses}} ({{Octopus}} Dofleini)},
  author = {Mather, Jennifer A and Anderson, Roland C},
  pages = {6},
  file = {/Users/qualia/Documents/Papers/Mather and Anderson - Exploration, Play, and Habituation in Octopuses (O.pdf},
  language = {en}
}

@article{Mathera,
  title = {What Is in an Octopus's Mind?},
  author = {Mather, Jennifer},
  pages = {29},
  abstract = {It is difficult to imagine what an animal as different from us as the octopus `thinks', but we can make some progress. In the Umwelt or perceptual world of an octopus, what the lateralized monocular eyes perceive is not color but the plane of polarization of light. Information is processed by a bilateral brain but manipulation is done by a radially symmetrical set of eight arms. Octopuses do not self-monitor by vision. Their skin pattern system, used for excellent camouflage, is open loop. The output of the motor system of the eight arms is organized at several levels \textemdash{} brain, intrabrachial commissure and local brachial ganglia. Octopuses may be motivated by a combination of fear and exploration. Several actions \textemdash{} a head bob for motion parallax, a `Passing Cloud' skin display to startle prey, and particularly exploration by their arms \textemdash{} demonstrate the presence of a controlling mind, motivated to gather information. Yet most octopuses are solitary and many are cannibalistic, so they must always be on guard, even against conspecifics. The actions of octopuses can be domain general, with flexible problem-solving strategies, enabling them to survive ``by their wits'' in a challenging and variable environment.},
  file = {/Users/qualia/Documents/Papers/Mather - What is in an octopus’s mind.pdf},
  language = {en}
}

@article{Mathewson2009,
  title = {To {{See}} or {{Not}} to {{See}}: {{Prestimulus Phase Predicts Visual Awareness}}},
  shorttitle = {To {{See}} or {{Not}} to {{See}}},
  author = {Mathewson, K. E. and Gratton, G. and Fabiani, M. and Beck, D. M. and Ro, T.},
  year = {2009},
  month = mar,
  volume = {29},
  pages = {2725--2732},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3963-08.2009},
  file = {/Users/qualia/Documents/Papers/2009 - Mathewson et al. - To See or Not to See Prestimulus α Phase Predicts Visual Awareness.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {9}
}

@article{Mathewson2017,
  title = {High and Dry? {{Comparing}} Active Dry {{EEG}} Electrodes to Active and Passive Wet Electrodes: {{Active}} Dry vs. Active \& Passive Wet {{EEG}} Electrodes},
  shorttitle = {High and Dry?},
  author = {Mathewson, Kyle E. and Harrison, Tyler J. L. and Kizuk, Sayeed A. D.},
  year = {2017},
  month = jan,
  volume = {54},
  pages = {74--82},
  issn = {00485772},
  doi = {10.1111/psyp.12536},
  abstract = {Dry electrodes are becoming popular for both lab-based and consumer-level electrophysiological-recording technologies because they better afford the ability to move traditional lab-based research into the real world. It is unclear, however, how dry electrodes compare in data quality to traditional electrodes. The current study compared three EEG electrode types: (a) passive-wet electrodes with no onboard amplification, (b) actively amplified, wet electrodes with moderate impedance levels, and low impedance levels, and (c) active-dry electrodes with very high impedance. Participants completed a classic P3 auditory oddball task to elicit characteristic EEG signatures and eventrelated potentials (ERPs). Across the three electrode types, we compared single-trial noise, average ERPs, scalp topographies, ERP noise, and ERP statistical power as a function of number of trials. We extended past work showing active electrodes' insensitivity to moderate levels of interelectrode impedance when compared to passive electrodes in the same amplifier. Importantly, the new dry electrode system could reliably measure EEG spectra and ERP components comparable to traditional electrode types. As expected, however, dry active electrodes with very high interelectrode impedance exhibited marked increases in single-trial and average noise levels, which decreased statistical power, requiring more trials to detect significant effects. This power decrease must be considered as a tradeoff with the ease of application and long-term use. The current results help set constraints on experimental design with novel dry electrodes, and provide important evidence needed to measure brain activity in novel settings and situations.},
  file = {/Users/qualia/Documents/Papers/Mathewson et al. - 2017 - High and dry Comparing active dry EEG electrodes .pdf},
  journal = {Psychophysiology},
  language = {en},
  number = {1}
}

@article{Mazzoni2015,
  title = {Computing the {{Local Field Potential}} ({{LFP}}) from {{Integrate}}-and-{{Fire Network Models}}},
  author = {Mazzoni, Alberto and Lind{\'e}n, Henrik and Cuntz, Hermann and Lansner, Anders and Panzeri, Stefano and Einevoll, Gaute T.},
  editor = {Roth, Arnd},
  year = {2015},
  month = dec,
  volume = {11},
  pages = {e1004584},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004584},
  abstract = {Leaky integrate-and-fire (LIF) network models are commonly used to study how the spiking dynamics of neural networks changes with stimuli, tasks or dynamic network states. However, neurophysiological studies in vivo often rather measure the mass activity of neuronal microcircuits with the local field potential (LFP). Given that LFPs are generated by spatially separated currents across the neuronal membrane, they cannot be computed directly from quantities defined in models of point-like LIF neurons. Here, we explore the best approximation for predicting the LFP based on standard output from point-neuron LIF networks. To search for this best ``LFP proxy'', we compared LFP predictions from candidate proxies based on LIF network output (e.g, firing rates, membrane potentials, synaptic currents) with ``ground-truth'' LFP obtained when the LIF network synaptic input currents were injected into an analogous three-dimensional (3D) network model of multi-compartmental neurons with realistic morphology, spatial distributions of somata and synapses. We found that a specific fixed linear combination of the LIF synaptic currents provided an accurate LFP proxy, accounting for most of the variance of the LFP time course observed in the 3D network for all recording locations. This proxy performed well over a broad set of conditions, including substantial variations of the neuronal morphologies. Our results provide a simple formula for estimating the time course of the LFP from LIF network simulations in cases where a single pyramidal population dominates the LFP generation, and thereby facilitate quantitative comparison between computational models and experimental LFP recordings in vivo.},
  file = {/Users/qualia/Documents/Papers/2015 - Mazzoni et al. - Computing the Local Field Potential (LFP) from Integrate-and-Fire Network Models.pdf;/Users/qualia/Documents/Papers/Mazzoni et al. - 2015 - Computing the Local Field Potential (LFP) from Int.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{McCarthy2011,
  title = {Striatal Origin of the Pathologic Beta Oscillations in {{Parkinson}}'s Disease},
  author = {McCarthy, M. M. and {Moore-Kochlacs}, C. and Gu, X. and Boyden, E. S. and Han, X. and Kopell, N.},
  year = {2011},
  month = jul,
  volume = {108},
  pages = {11620--11625},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1107748108},
  file = {/Users/qualia/Documents/Papers/2011 - McCarthy et al. - Striatal origin of the pathologic beta oscillations in Parkinson's disease.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {28}
}

@article{Mcculloch,
  title = {A {{LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY}}},
  author = {Mcculloch, Warren S and Pitts, Walter},
  pages = {17},
  file = {/Users/qualia/Documents/Papers/1990 - Mcculloch, Pitts - A logical calculus nervous activity.pdf},
  language = {en}
}

@article{McDuff2009,
  title = {Multivoxel {{Pattern Analysis Reveals Increased Memory Targeting}} and {{Reduced Use}} of {{Retrieved Details}} during {{Single}}-{{Agenda Source Monitoring}}},
  author = {McDuff, S. G. R. and Frankel, H. C. and Norman, K. A.},
  year = {2009},
  month = jan,
  volume = {29},
  pages = {508--516},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3587-08.2009},
  file = {/Users/qualia/Documents/Papers/2009 - McDuff, Frankel, Norman - Multivoxel pattern analysis reveals increased memory targeting and reduced use of retrieved details dur.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{McGill1978,
  title = {Variations of {{Box Plots}}},
  author = {McGill, Robert and Tukey, John W. and Larsen, Wayne A.},
  year = {1978},
  month = feb,
  volume = {32},
  pages = {12},
  issn = {00031305},
  doi = {10.2307/2683468},
  file = {/Users/qualia/Documents/Papers/1978 - McGill, Tukey, Larsen - Variations of box plots.pdf;/Users/qualia/Documents/Papers/McGill et al. - 1978 - Variations of Box Plots.pdf},
  journal = {The American Statistician},
  language = {en},
  number = {1}
}

@article{McIntosh2017,
  title = {Deep {{Learning Models}} of the {{Retinal Response}} to {{Natural Scenes}}},
  author = {McIntosh, Lane T. and Maheswaranathan, Niru and Nayebi, Aran and Ganguli, Surya and Baccus, Stephen A.},
  year = {2017},
  month = feb,
  abstract = {A central challenge in sensory neuroscience is to understand neural computations and circuit mechanisms that underlie the encoding of ethologically relevant, natural stimuli. In multilayered neural circuits, nonlinear processes such as synaptic transmission and spiking dynamics present a significant obstacle to the creation of accurate computational models of responses to natural stimuli. Here we demonstrate that deep convolutional neural networks (CNNs) capture retinal responses to natural scenes nearly to within the variability of a cell's response, and are markedly more accurate than linear-nonlinear (LN) models and Generalized Linear Models (GLMs). Moreover, we find two additional surprising properties of CNNs: they are less susceptible to overfitting than their LN counterparts when trained on small amounts of data, and generalize better when tested on stimuli drawn from a different distribution (e.g. between natural scenes and white noise). An examination of the learned CNNs reveals several properties. First, a richer set of feature maps is necessary for predicting the responses to natural scenes compared to white noise. Second, temporally precise responses to slowly varying inputs originate from feedforward inhibition, similar to known retinal mechanisms. Third, the injection of latent noise sources in intermediate layers enables our model to capture the sub-Poisson spiking variability observed in retinal ganglion cells. Fourth, augmenting our CNNs with recurrent lateral connections enables them to capture contrast adaptation as an emergent property of accurately describing retinal responses to natural scenes. These methods can be readily generalized to other sensory modalities and stimulus ensembles. Overall, this work demonstrates that CNNs not only accurately capture sensory circuit responses to natural scenes, but also can yield information about the circuit's internal structure and function.},
  archivePrefix = {arXiv},
  eprint = {1702.01825},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/McIntosh et al. - 2017 - Deep Learning Models of the Retinal Response to Na.pdf},
  journal = {arXiv:1702.01825 [q-bio, stat]},
  keywords = {Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryClass = {q-bio, stat}
}

@article{McIntyre2002,
  title = {Extracellular {{Stimulation}} of {{Central Neurons}}: {{Influence}} of {{Stimulus Waveform}} and {{Frequency}} on {{Neuronal Output}}},
  shorttitle = {Extracellular {{Stimulation}} of {{Central Neurons}}},
  author = {McIntyre, Cameron C. and Grill, Warren M.},
  year = {2002},
  month = oct,
  volume = {88},
  pages = {1592--1604},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.2002.88.4.1592},
  file = {/Users/qualia/Documents/Papers/2002 - McIntyre, Grill - Extracellular Stimulation of Central Neurons Influence of Stimulus Waveform and Frequency on Neuronal Output.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {4}
}

@article{McKeown2003,
  title = {Independent Component Analysis of Functional {{MRI}}: What Is Signal and What Is Noise?},
  shorttitle = {Independent Component Analysis of Functional {{MRI}}},
  author = {McKeown, M},
  year = {2003},
  month = oct,
  volume = {13},
  pages = {620--629},
  issn = {09594388},
  doi = {10.1016/j.conb.2003.09.012},
  abstract = {Many sources of fluctuation contribute to the functional magnetic resonance imaging (fMRI) signal, complicating attempts to infer those changes that are truly related to brain activation. Unlike methods of analysis of fMRI data that test the time course of each voxel against a hypothesized waveform, data-driven methods, such as independent component analysis and clustering, attempt to find common features within the data. This exploratory approach can be revealing when the brain activation is difficult to predict beforehand, such as with complex stimuli and internal shifts of activation that are not time-locked to an easily specified sensory or motor event. These methods can be further improved by incorporating prior knowledge regarding the temporal and spatial extent of brain activation.},
  file = {/Users/qualia/Documents/Papers/2003 - McKeown, Hansen, Sejnowski - Independent component analysis of functional MRI what is signal and what is noise.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {5}
}

@article{McMahan2016,
  title = {Communication-{{Efficient Learning}} of {{Deep Networks}} from {{Decentralized Data}}},
  author = {McMahan, H. Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Ag{\"u}era},
  year = {2016},
  month = feb,
  abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10\textendash{}100\texttimes{} as compared to synchronized stochastic gradient descent.},
  archivePrefix = {arXiv},
  eprint = {1602.05629},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/McMahan et al. - 2016 - Communication-Efficient Learning of Deep Networks .pdf},
  journal = {arXiv:1602.05629 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{McNeal1976,
  title = {Analysis of a {{Model}} for {{Excitation}} of {{Myelinated Nerve}}},
  author = {McNeal, Donald R.},
  year = {1976},
  month = jul,
  volume = {BME-23},
  pages = {329--337},
  issn = {0018-9294},
  doi = {10.1109/TBME.1976.324593},
  abstract = {Excellent models have been presented in the literature which relate membrane potential to transverse membrane current and which describe the propagation of action potentials along the axon, for both myelinated and nonmyelinated fibers. There is not, however, an adequate model for nerve excitation which allows one to compute the threshold of a nerve fiber for pulses of finite duration using electrodes that are not in direct contact with the fiber. This paper considers this problem and presents a model of the electrical properties of myelinated nerve which describes the time course of events following stimulus application up to the initiation of the action potential. The time-varying current and potential at all nodes can be computed from the model, and the strength-duration curve can be determined for arbitrary electrode geometries, although only the case of a monopolar electrode is considered in this paper. It is shown that even when the stimulus is a constant-current pulse, the membrane current at the nodes varies considerably with time. The strength-duration curve calculated from the model is consistent with previously published experimental data, and the model provides a quantitative relationship between threshold and fiber diameter which shows there is less selectivity among fibers of large diameter than those of small diameter.},
  file = {/Users/qualia/Documents/Papers/1976 - Mcneal - Analysis of a Model for Excitation of Myelinated Nerve.pdf;/Users/qualia/Documents/Papers/McNeal - 1976 - Analysis of a Model for Excitation of Myelinated N.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {4}
}

@article{Megias2001,
  title = {Total Number and Distribution of Inhibitory and Excitatory Synapses on Hippocampal {{CA1}} Pyramidal Cells},
  author = {Meg\'{ı}as, M and Emri, Zs and Freund, T.F and Guly{\'a}s, A.I},
  year = {2001},
  month = feb,
  volume = {102},
  pages = {527--540},
  issn = {03064522},
  doi = {10.1016/S0306-4522(00)00496-6},
  abstract = {The integrative properties of neurons depend strongly on the number, proportions and distribution of excitatory and inhibitory synaptic inputs they receive. In this study the three-dimensional geometry of dendritic trees and the density of symmetrical and asymmetrical synapses on different cellular compartments of rat hippocampal CA I area pyramidal cells was measured to calculate the total number and distribution of excitatory and inhibitory inputs on a single cell.},
  file = {/Users/qualia/Documents/Papers/2001 - Megías et al. - Total number and distribution of inhibitory and excitatory synapses on hippocampal CA1 pyramidal cells.pdf},
  journal = {Neuroscience},
  language = {en},
  number = {3}
}

@article{Mehlhorn2015,
  title = {Unpacking the Exploration\textendash{}Exploitation Tradeoff: {{A}} Synthesis of Human and Animal Literatures.},
  shorttitle = {Unpacking the Exploration\textendash{}Exploitation Tradeoff},
  author = {Mehlhorn, Katja and Newell, Ben R. and Todd, Peter M. and Lee, Michael D. and Morgan, Kate and Braithwaite, Victoria A. and Hausmann, Daniel and Fiedler, Klaus and Gonzalez, Cleotilde},
  year = {2015},
  month = jul,
  volume = {2},
  pages = {191--215},
  issn = {2325-9973, 2325-9965},
  doi = {10.1037/dec0000033},
  file = {/Users/qualia/Documents/Papers/Mehlhorn et al. - 2015 - Unpacking the exploration–exploitation tradeoff A.pdf},
  journal = {Decision},
  language = {en},
  number = {3}
}

@article{Mejias,
  title = {Feedforward and Feedback Frequency-Dependent Interactions in a Large-Scale Laminar Network of the Primate Cortex},
  author = {Mejias, Jorge F and Murray, John D and Kennedy, Henry and Wang, Xiao-Jing},
  pages = {26},
  abstract = {Interactions between top-down and bottom-up processes in the cerebral cortex hold the key to understanding predictive coding, executive control and a gamut of other brain functions. The underlying circuit mechanism, however, remains poorly understood and represents a major challenge in neuroscience. In the present work we tackled this problem using a large-scale computational model of the primate cortex constrained by new directed and weighted connectivity data. In our model, the interplay between feedforward and feedback signaling depends on the cortical laminar structure and involves complex dynamics across multiple (intra-laminar, inter-laminar, inter-areal and whole cortex) scales. The model was tested by reproducing, and shedding insights into, a wide range of neurophysiological findings about frequency-dependent interactions between visual cortical areas: feedforward pathways are associated with enhanced gamma (30-70 Hz) oscillations, whereas feedback projections selectively modulate alpha/low beta (8-15 Hz) oscillations. We found that in order for the model to account for the experimental observations, the feedback projection needs to predominantly target infragranular layers in a target area, which leads to a proposed circuit substrate for predictive coding. The model reproduces a functional hierarchy based on frequency-dependent Granger causality analysis of inter-areal signaling, as reported in recent monkey and human experiments. Taken together, this work highlights the importance of multi-scale approaches and provides a modeling platform for studies of large-scale brain circuit dynamics and functions.},
  file = {/Users/qualia/Documents/Papers/Mejias et al. - Feedforward and feedback frequency-dependent inter.pdf},
  language = {en}
}

@article{Mejias2014,
  title = {Differential Effects of Excitatory and Inhibitory Heterogeneity on the Gain and Asynchronous State of Sparse Cortical Networks},
  author = {Mejias, Jorge F. and Longtin, Andr{\~A}\textcopyright{}},
  year = {2014},
  month = sep,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00107},
  abstract = {Recent experimental and theoretical studies have highlighted the importance of cell-to-cell differences in the dynamics and functions of neural networks, such as in different types of neural coding or synchronization. It is still not known, however, how neural heterogeneity can affect cortical computations, or impact the dynamics of typical cortical circuits constituted of sparse excitatory and inhibitory networks. In this work, we analytically and numerically study the dynamics of a typical cortical circuit with a certain level of neural heterogeneity. Our circuit includes realistic features found in real cortical populations, such as network sparseness, excitatory, and inhibitory subpopulations of neurons, and different cell-to-cell heterogeneities for each type of population in the system. We find highly differentiated roles for heterogeneity, depending on the subpopulation in which it is found. In particular, while heterogeneity among excitatory neurons non-linearly increases the mean firing rate and linearizes the f-I curves, heterogeneity among inhibitory neurons may decrease the network activity level and induces divisive gain effects in the f-I curves of the excitatory cells, providing an effective gain control mechanism to influence information flow. In addition, we compute the conditions for stability of the network activity, finding that the synchronization onset is robust to inhibitory heterogeneity, but it shifts to lower input levels for higher excitatory heterogeneity. Finally, we provide an extension of recently reported heterogeneity-induced mechanisms for signal detection under rate coding, and we explore the validity of our findings when multiple sources of heterogeneity are present. These results allow for a detailed characterization of the role of neural heterogeneity in asynchronous cortical networks.},
  file = {/Users/qualia/Documents/Papers/Mejias and Longtin - 2014 - Differential effects of excitatory and inhibitory .pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Mel2004,
  title = {On the {{Fight Between Excitation}} and {{Inhibition}}: {{Location Is Everything}}},
  shorttitle = {On the {{Fight Between Excitation}} and {{Inhibition}}},
  author = {Mel, B. W. and Schiller, J.},
  year = {2004},
  month = sep,
  volume = {2004},
  pages = {pe44-pe44},
  issn = {1945-0877, 1937-9145},
  doi = {10.1126/stke.2502004pe44},
  file = {/Users/qualia/Documents/Papers/2004 - Mel, Schiller - On the fight between excitation and inhibition location is everything.pdf},
  journal = {Science Signaling},
  language = {en},
  number = {250}
}

@article{Meng2014,
  title = {A {{Unified Approach}} to {{Linking Experimental}}, {{Statistical}} and {{Computational Analysis}} of {{Spike Train Data}}},
  author = {Meng, Liang and Kramer, Mark A. and Middleton, Steven J. and Whittington, Miles A. and Eden, Uri T.},
  editor = {Chacron, Maurice J.},
  year = {2014},
  month = jan,
  volume = {9},
  pages = {e85269},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0085269},
  abstract = {A fundamental issue in neuroscience is how to identify the multiple biophysical mechanisms through which neurons generate observed patterns of spiking activity. In previous work, we proposed a method for linking observed patterns of spiking activity to specific biophysical mechanisms based on a state space modeling framework and a sequential Monte Carlo, or particle filter, estimation algorithm. We have shown, in simulation, that this approach is able to identify a space of simple biophysical models that were consistent with observed spiking data (and included the model that generated the data), but have yet to demonstrate the application of the method to identify realistic currents from real spike train data. Here, we apply the particle filter to spiking data recorded from rat layer V cortical neurons, and correctly identify the dynamics of an slow, intrinsic current. The underlying intrinsic current is successfully identified in four distinct neurons, even though the cells exhibit two distinct classes of spiking activity: regular spiking and bursting. This approach \textendash{} linking statistical, computational, and experimental neuroscience \textendash{} provides an effective technique to constrain detailed biophysical models to specific mechanisms consistent with observed spike train data.},
  file = {/Users/qualia/Documents/Papers/2014 - Meng et al. - A unified approach to linking experimental, statistical and computational analysis of spike train data.pdf;/Users/qualia/Documents/Papers/Meng et al. - 2014 - A Unified Approach to Linking Experimental, Statis.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {1}
}

@article{Mensch,
  title = {Learning {{Neural Representations}} of {{Human Cognition}} across {{Many fMRI Studies}}},
  author = {Mensch, Arthur and Mairal, Julien and Bzdok, Danilo and Thirion, Bertrand and Varoquaux, Ga{\"e}l},
  pages = {14},
  abstract = {Cognitive neuroscience is enjoying rapid increase in extensive public brain-imaging datasets. It opens the door to large-scale statistical models. Finding a unified perspective for all available data calls for scalable and automated solutions to an old challenge: how to aggregate heterogeneous information on brain function into a universal cognitive system that relates mental operations/cognitive processes/psychological tasks to brain networks? We cast this challenge in a machine-learning approach to predict conditions from statistical brain maps across different studies. For this, we leverage multi-task learning and multi-scale dimension reduction to learn low-dimensional representations of brain images that carry cognitive information and can be robustly associated with psychological stimuli. Our multi-dataset classification model achieves the best prediction performance on several large reference datasets, compared to models without cognitive-aware low-dimension representations; it brings a substantial performance boost to the analysis of small datasets, and can be introspected to identify universal template cognitive concepts.},
  file = {/Users/qualia/Documents/Papers/Mensch et al. - Learning Neural Representations of Human Cognition.pdf},
  language = {en}
}

@article{Mensch2018,
  title = {Stochastic {{Subsampling}} for {{Factorizing Huge Matrices}}},
  author = {Mensch, Arthur and Mairal, Julien and Thirion, Bertrand and Varoquaux, Gael},
  year = {2018},
  month = jan,
  volume = {66},
  pages = {113--128},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2017.2752697},
  abstract = {We present a matrix-factorization algorithm that scales to input matrices with both huge number of rows and columns. Learned factors may be sparse or dense and/or non-negative, which makes our algorithm suitable for dictionary learning, sparse component analysis, and non-negative matrix factorization. Our algorithm streams matrix columns while subsampling them to iteratively learn the matrix factors. At each iteration, the row dimension of a new sample is reduced by subsampling, resulting in lower time complexity compared to a simple streaming algorithm. Our method comes with convergence guarantees to reach a stationary point of the matrix-factorization problem. We demonstrate its efficiency on massive functional Magnetic Resonance Imaging data (2 TB), and on patches extracted from hyperspectral images (103 GB). For both problems, which involve different penalties on rows and columns, we obtain significant speed-ups compared to state-of-the-art algorithms.},
  file = {/Users/qualia/Documents/Papers/Mensch et al. - 2018 - Stochastic Subsampling for Factorizing Huge Matric.pdf},
  journal = {IEEE Transactions on Signal Processing},
  language = {en},
  number = {1}
}

@article{Merker2016,
  title = {Cortical {{Gamma Oscillations}}: {{Details}} of {{Their Genesis Preclude}} a {{Role}} in {{Cognition}}},
  shorttitle = {Cortical {{Gamma Oscillations}}},
  author = {Merker, Bjorn H.},
  year = {2016},
  month = jul,
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00078},
  file = {/Users/qualia/Documents/Papers/Merker - 2016 - Cortical Gamma Oscillations Details of Their Gene.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Meshulam2018,
  title = {Coarse--Graining, Fixed Points, and Scaling in a Large Population of Neurons},
  author = {Meshulam, Leenoy and Gauthier, Jeffrey L. and Brody, Carlos D. and Tank, David W. and Bialek, William},
  year = {2018},
  month = sep,
  abstract = {We develop a phenomenological coarse--graining procedure for activity in a large network of neurons, and apply this to recordings from a population of 1000+ cells in the hippocampus. Distributions of coarse--grained variables seem to approach a fixed non--Gaussian form, and we see evidence of scaling in both static and dynamic quantities. These results suggest that the collective behavior of the network is described by a non--trivial fixed point.},
  archivePrefix = {arXiv},
  eprint = {1809.08461},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Meshulam et al. - 2018 - Coarse--graining, fixed points, and scaling in a l.pdf},
  journal = {arXiv:1809.08461 [physics, q-bio]},
  keywords = {Physics - Biological Physics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {physics, q-bio}
}

@article{Mesterton-Gibbons1998,
  title = {Animal {{Contests}} as {{Evolutionary Games}}: {{Paradoxical}} Behavior Can Be Understood in the Context of Evolutionary Stable Strategies. {{The}} Trick Is to Discover Which Game the Animal Is Playing},
  author = {{Mesterton-Gibbons}, Michael and {work(s):}, Eldridge S. Adams Reviewed},
  year = {1998},
  volume = {86},
  pages = {334--341},
  file = {/Users/qualia/Documents/Papers/Mesterton-Gibbons and work(s) - 1998 - Animal Contests as Evolutionary Games Paradoxical.pdf},
  journal = {American Scientist},
  language = {en},
  number = {4}
}

@article{Meyer2010,
  title = {Predicting Visual Stimuli on the Basis of Activity in Auditory Cortices},
  author = {Meyer, Kaspar and Kaplan, Jonas T and Essex, Ryan and Webber, Cecelia and Damasio, Hanna and Damasio, Antonio},
  year = {2010},
  month = jun,
  volume = {13},
  pages = {667--668},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2533},
  file = {/Users/qualia/Documents/Papers/2010 - Meyer et al. - Predicting visual stimuli on the basis of activity in auditory cortices.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Meyer2011,
  title = {Seeing {{Touch Is Correlated}} with {{Content}}-{{Specific Activity}} in {{Primary Somatosensory Cortex}}},
  author = {Meyer, Kaspar and Kaplan, Jonas T. and Essex, Ryan and Damasio, Hanna and Damasio, Antonio},
  year = {2011},
  month = sep,
  volume = {21},
  pages = {2113--2121},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhq289},
  file = {/Users/qualia/Documents/Papers/2011 - Meyer et al. - Seeing touch is correlated with content-specific activity in primary somatosensory cortex.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {9}
}

@article{Mhaskar,
  title = {Learning {{Functions}}: {{When Is Deep Better Than Shallow}}},
  author = {Mhaskar, Hrushikesh and Liao, Qianli and Poggio, Tomaso},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/Mhaskar et al. - Learning Functions When Is Deep Better Than Shall.pdf},
  language = {en}
}

@article{Miconi2017,
  title = {Biologically Plausible Learning in Recurrent Neural Networks Reproduces Neural Dynamics Observed during Cognitive Tasks},
  author = {Miconi, Thomas},
  year = {2017},
  month = feb,
  doi = {10.1101/057729},
  abstract = {Neural activity during cognitive tasks exhibits complex dynamics that flexibly encode task\-relevant variables. Recurrent neural networks operating in the near\-chaotic regime, which spontaneously generate rich dynamics, have been proposed as a model of cortical computation during cognitive tasks. However, existing methods for training these networks are either biologically implausible, and/or require a continuous, real\-time error signal to guide the learning process. The lack of a biological learning method currently restricts the plausibility of recurrent networks as models of cortical computation. Here we show that a biologically plausible learning rule can train such recurrent networks, guided solely by delayed, phasic rewards at the end of each trial, for nontrivial tasks. We use this method to learn various tasks from the experimental literature, showing that this learning rule can successfully implement flexible associations, memory maintenance, nonlinear mixed selectivities, and coordination among multiple outputs. We show that the resulting networks exhibit complex dynamics previously observed in animal cortex, such as dynamic encoding and maintenance of task features, switching from stimulus\-specific to response\-specific representations, and selective integration of relevant input streams. We conclude that recurrent neural networks offer a plausible model of cortical dynamics during both learning and performance of flexible behavior.},
  file = {/Users/qualia/Documents/Papers/Miconi - 2017 - Biologically plausible learning in recurrent neura.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Miconi2019,
  title = {{{BACKPROPAMINE}}: {{TRAINING SELF}}-{{MODIFYING NEU}}- {{RAL NETWORKS WITH DIFFERENTIABLE NEUROMODU}}- {{LATED PLASTICITY}}},
  author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
  year = {2019},
  pages = {15},
  abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
  file = {/Users/qualia/Documents/Papers/Miconi et al. - 2019 - BACKPROPAMINE TRAINING SELF-MODIFYING NEU- RAL NE.pdf},
  language = {en}
}

@article{Miller,
  title = {When {{Recurrent Models Don}}'t {{Need To Be Recurrent}}},
  author = {Miller, John and Hardt, Moritz},
  pages = {23},
  abstract = {We prove stable recurrent neural networks are well approximated by feed-forward networks for the purpose of both inference and training by gradient descent. Our result applies to a broad range of non-linear recurrent neural networks under a natural stability condition, which we observe is also necessary. Complementing our theoretical findings, we verify the conclusions of our theory on both real and synthetic tasks. Furthermore, we demonstrate recurrent models satisfying the stability assumption of our theory can have excellent performance on real sequence learning tasks.},
  file = {/Users/qualia/Documents/Papers/Miller and Hardt - When Recurrent Models Don’t Need To Be Recurrent.pdf},
  language = {en}
}

@article{Miller1956,
  title = {The {{Magical Number Seven}}, {{Plus}} or {{Minus Two}}: {{Some Limits}} on {{Our Capacity}} for {{Processing Information}}},
  author = {Miller, George},
  year = {1956},
  volume = {63},
  pages = {81--97},
  file = {/Users/qualia/Documents/Papers/httpspider.apa.orgftdocsrev1994aprilrev101.pdf},
  journal = {The Psychological Review},
  language = {en}
}

@article{Miller2001,
  title = {Processing in Layer 4 of the Neocortical Circuit: New Insights from Visual and Somatosensory Cortex},
  shorttitle = {Processing in Layer 4 of the Neocortical Circuit},
  author = {Miller, K},
  year = {2001},
  month = aug,
  volume = {11},
  pages = {488--497},
  issn = {09594388},
  doi = {10.1016/S0959-4388(00)00239-7},
  file = {/Users/qualia/Documents/Papers/2001 - Miller, Pinto, Simons - Processing in layer 4 of the neocortical circuit New insights from visual and somatosensory cortex.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {4}
}

@article{Miller2002,
  title = {Neural {{Noise Can Explain Expansive}}, {{Power}}-{{Law Nonlinearities}} in {{Neural Response Functions}}},
  author = {Miller, Kenneth D. and Troyer, Todd W.},
  year = {2002},
  month = feb,
  volume = {87},
  pages = {653--659},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00425.2001},
  file = {/Users/qualia/Documents/Papers/2002 - Miller, Troyer - Neural noise can explain expansive, power-law nonlinearities in neural response functions.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Miller2003,
  title = {Understanding {{Layer}} 4 of the {{Cortical Circuit}}: {{A Model Based}} on {{Cat V1}}},
  shorttitle = {Understanding {{Layer}} 4 of the {{Cortical Circuit}}},
  author = {Miller, K. D.},
  year = {2003},
  month = jan,
  volume = {13},
  pages = {73--82},
  issn = {14602199},
  doi = {10.1093/cercor/13.1.73},
  file = {/Users/qualia/Documents/Papers/2003 - Miller - Understanding layer 4 of the cortical circuit a model based on cat V1.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Miller2013,
  title = {Cortical Circuits for the Control of Attention},
  author = {Miller, Earl K and Buschman, Timothy J},
  year = {2013},
  month = apr,
  volume = {23},
  pages = {216--222},
  issn = {09594388},
  doi = {10.1016/j.conb.2012.11.011},
  file = {/Users/qualia/Documents/Papers/2013 - Miller, Buschman - Cortical circuits for the control of attention.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {2}
}

@article{Miocinovic2006,
  title = {Computational {{Analysis}} of {{Subthalamic Nucleus}} and {{Lenticular Fasciculus Activation During Therapeutic Deep Brain Stimulation}}},
  author = {Miocinovic, Svjetlana and Parent, Martin and Butson, Christopher R. and Hahn, Philip J. and Russo, Gary S. and Vitek, Jerrold L. and McIntyre, Cameron C.},
  year = {2006},
  month = sep,
  volume = {96},
  pages = {1569--1580},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00305.2006},
  file = {/Users/qualia/Documents/Papers/2006 - Miocinovic et al. - Computational analysis of subthalamic nucleus and lenticular fasciculus activation during therapeutic deep br.pdf;/Users/qualia/Documents/Papers/2006 - Miocinovic et al. - Computational analysis of subthalamic nucleus and lenticular fasciculus activation during therapeutic deep(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Mirollo1990,
  title = {Synchronization of {{Pulse}}-{{Coupled Biological Oscillators}}},
  author = {Mirollo, Renato E. and Strogatz, Steven H.},
  year = {1990},
  volume = {50},
  pages = {1645--1662},
  abstract = {A simplemodel forsynchronousfiringof biologicaloscillatorsbased on Peskin'smodel of thecardiacpacemaker[Mathematicaal spectsofheartphysiologyC,ourantInstitutoefMathematicalSciences, New York UniversityN, ew York, 1975,pp. 268-278] is studied.The model consistsof a populationof identicalintegrate-and-fiorsecillators.The couplingbetweenoscillatorsis pulsatile:whena givenoscillator fires,it pulls theothersup by a fixedamount,or bringsthemto thefiringthresholdw, hicheveris less. The main resultis thatforalmostall initialconditions,thepopulationevolvesto a statein whichall the oscillatorsare firingsynchronouslyT. he relationshipbetweenthe model and real communitiesof biologicaloscillatorsis discussed;examplesincludepopulationsofsynchronouslyflashingfirefliesc,rickets thatchirpinunison,electricallysynchronoups acemakercells,and groupsofwomenwhosemenstruaclycles become mutuallysynchronized.},
  file = {/Users/qualia/Documents/Papers/Mirollo and Strogatz - 1990 - Synchronization of Pulse-Coupled Biological Oscill.pdf},
  journal = {SIAM Journal on Applied Mathematics},
  language = {en},
  number = {6}
}

@article{Mischler2016,
  title = {On a Kinetic {{FitzHugh}}-{{Nagumo}} Model of Neuronal Network},
  author = {Mischler, St{\'e}phane and Qui{\~n}inao, Crist{\'o}bal and Touboul, Jonathan},
  year = {2016},
  month = mar,
  volume = {342},
  pages = {1001--1042},
  issn = {0010-3616, 1432-0916},
  doi = {10.1007/s00220-015-2556-9},
  abstract = {We investigate existence and uniqueness of solutions of a McKean-Vlasov evolution PDE representing the macroscopic behaviour of interacting Fitzhugh-Nagumo neurons. This equation is hypoelliptic, nonlocal and has unbounded coefficients. We prove existence of a solution to the evolution equation and non trivial stationary solutions. Moreover, we demonstrate uniqueness of the stationary solution in the weakly nonlinear regime. Eventually, using a semigroup factorisation method, we show exponential nonlinear stability in the small connectivity regime.},
  archivePrefix = {arXiv},
  eprint = {1503.00492},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Mischler et al. - 2016 - On a kinetic FitzHugh-Nagumo model of neuronal net.pdf},
  journal = {Communications in Mathematical Physics},
  keywords = {Mathematics - Analysis of PDEs,Mathematics - Functional Analysis,Mathematics - Spectral Theory},
  language = {en},
  number = {3}
}

@article{Mishina2014,
  title = {Exploration of Genetically Encoded Voltage Indicators Based on a Chimeric Voltage Sensing Domain},
  author = {Mishina, Yukiko and Mutoh, Hiroki and Song, Chenchen and Kn{\~A}\textparagraph{}pfel, Thomas},
  year = {2014},
  month = sep,
  volume = {7},
  issn = {1662-5099},
  doi = {10.3389/fnmol.2014.00078},
  abstract = {Deciphering how the brain generates cognitive function from patterns of electrical signals is one of the ultimate challenges in neuroscience. To this end, it would be highly desirable to monitor the activities of very large numbers of neurons while an animal engages in complex behaviors. Optical imaging of electrical activity using genetically encoded voltage indicators (GEVIs) has the potential to meet this challenge. Currently prevalent GEVIs are based on the voltage-sensitive fluorescent protein (VSFP) prototypical design or on the voltage-dependent state transitions of microbial opsins.We recently introduced a newVSFP design in which the voltage-sensing domain (VSD) is sandwiched between a fluorescence resonance energy transfer pair of fluorescent proteins (termed VSFP-Butterflies) and also demonstrated a series of chimeric VSD in which portions of the VSD of Ciona intestinalis voltage-sensitive phosphatase are substituted by homologous portions of a voltage-gated potassium channel subunit.These chimeric VSD had faster sensing kinetics than that of the native Ci-VSD. Here, we describe a new set of VSFPs that combine chimeric VSD with the Butterfly structure. We show that these chimeric VSFP-Butterflies can report membrane voltage oscillations of up to 200 Hz in cultured cells and report sensory evoked cortical population responses in living mice. This class of GEVIs may be suitable for imaging of brain rhythms in behaving mammalians.},
  file = {/Users/qualia/Documents/Papers/Mishina et al. - 2014 - Exploration of genetically encoded voltage indicat.pdf},
  journal = {Frontiers in Molecular Neuroscience},
  language = {en}
}

@article{Mishra2006,
  title = {Selective Attention through Phase Relationship of Excitatory and Inhibitory Input Synchrony in a Model Cortical Neuron},
  author = {Mishra, Jyoti and Fellous, Jean-Marc and Sejnowski, Terrence J.},
  year = {2006},
  month = nov,
  volume = {19},
  pages = {1329--1346},
  issn = {08936080},
  doi = {10.1016/j.neunet.2006.08.005},
  abstract = {Neurons in area V 2 and V 4 exhibit stimulus specific tuning to single stimuli, and respond at intermediate firing rates when presented with two differentially preferred stimuli (`pair response'). Selective attention to one of the two stimuli causes the neuron's firing rate to shift from the intermediate pair response towards the response to the attended stimulus as if it were presented alone. Attention to single stimuli reduces the response threshold of the neuron and increases spike synchronization at gamma frequencies. The intrinsic and network mechanisms underlying these phenomena were investigated in a multi-compartmental biophysical model of a reconstructed cat V 4 neuron. Differential stimulus preference was generated through a greater ratio of excitatory to inhibitory synapses projecting from one of two input V 2 populations. Feedforward inhibition and synaptic depression dynamics were critical to generating the intermediate pair response. Neuronal gain effects were simulated using gamma frequency range correlations in the feedforward excitatory and inhibitory inputs to the V 4 neuron. For single preferred stimulus presentations, correlations within the inhibitory population out of phase with correlations within the excitatory input significantly reduced the response threshold of the V 4 neuron. The pair response to simultaneously active preferred and non-preferred V 2 populations could also undergo an increase or decrease in gain via the same mechanism, where correlations in feedforward inhibition are out of phase with gamma band correlations within the excitatory input corresponding to the attended stimulus. The results of this model predict that top-down attention may bias the V 4 neuron's response using an inhibitory correlation phase shift mechanism.},
  file = {/Users/qualia/Documents/Papers/2006 - Mishra, Fellous, Sejnowski - Selective attention through phase relationship of excitatory and inhibitory input synchrony in a mod.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {9}
}

@techreport{Miskovic2019,
  title = {Control {{Theory Concepts}} for {{Modeling Uncertainty}} in {{Enzyme Kinetics}} of {{Biochemical Networks}}},
  author = {Miskovic, Ljubisa and Tokic, Milenko and Savoglidis, Georgios and Hatzimanikatis, Vassily},
  year = {2019},
  month = apr,
  institution = {{Systems Biology}},
  doi = {10.1101/618777},
  abstract = {Analysis of the dynamic and steady-state properties of biochemical networks hinge on information about the parameters of enzyme kinetics. The lack of experimental data characterizing enzyme activities and kinetics along with the associated uncertainties impede the development of kinetic models, and researchers commonly use Monte Carlo sampling to explore the parameter space. However, the sampling of parameter spaces is a computationally expensive task for larger biochemical networks. To address this issue, we exploit the fact that reaction rates of biochemical reactions and network responses can be expressed as a function of displacements from thermodynamic equilibrium of elementary reaction steps and concentrations of free enzymes and their intermediary complexes. For a set of kinetic mechanisms ubiquitously found in biochemistry, we express kinetic responses of enzymes to changes in network metabolite concentrations through these quantities both analytically and schematically. The tailor-made sampling of these quantities allows for characterizing the missing kinetic parameters and accelerating the efforts towards building genome-scale kinetic metabolic models.},
  file = {/Users/qualia/Documents/Papers/Miskovic et al. - 2019 - Control Theory Concepts for Modeling Uncertainty i.pdf},
  language = {en},
  type = {Preprint}
}

@article{Mitchell2004,
  title = {Learning to {{Decode Cognitive States}} from {{Brain Images}}},
  author = {Mitchell, Tom M. and Hutchinson, Rebecca and Niculescu, Radu S. and Pereira, Francisco and Wang, Xuerui and Just, Marcel and Newman, Sharlene},
  year = {2004},
  month = oct,
  volume = {57},
  pages = {145--175},
  issn = {0885-6125},
  doi = {10.1023/B:MACH.0000035475.85309.1b},
  abstract = {Over the past decade, functional Magnetic Resonance Imaging (fMRI) has emerged as a powerful new instrument to collect vast quantities of data about activity in the human brain. A typical fMRI experiment can produce a three-dimensional image related to the human subject's brain activity every half second, at a spatial resolution of a few millimeters. As in other modern empirical sciences, this new instrumentation has led to a flood of new data, and a corresponding need for new data analysis methods. We describe recent research applying machine learning methods to the problem of classifying the cognitive state of a human subject based on fRMI data observed over a single time interval. In particular, we present case studies in which we have successfully trained classifiers to distinguish cognitive states such as (1) whether the human subject is looking at a picture or a sentence, (2) whether the subject is reading an ambiguous or non-ambiguous sentence, and (3) whether the word the subject is viewing is a word describing food, people, buildings, etc. This learning problem provides an interesting case study of classifier learning from extremely high dimensional (105 features), extremely sparse (tens of training examples), noisy data. This paper summarizes the results obtained in these three case studies, as well as lessons learned about how to successfully apply machine learning methods to train classifiers in such settings.},
  file = {/Users/qualia/Documents/Papers/2004 - Mitchell et al. - Learning to Decode Cognitive States from Brain Images.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {1/2}
}

@article{Mizuseki2013,
  title = {Theta Oscillations Decrease Spike Synchrony in the Hippocampus and Entorhinal Cortex},
  author = {Mizuseki, K. and Buzsaki, G.},
  year = {2013},
  month = dec,
  volume = {369},
  pages = {20120530--20120530},
  issn = {0962-8436, 1471-2970},
  doi = {10.1098/rstb.2012.0530},
  file = {/Users/qualia/Documents/Papers/Mizuseki and Buzsaki - 2013 - Theta oscillations decrease spike synchrony in the.pdf},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  language = {en},
  number = {1635}
}

@article{Mnih,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Harley, Tim and Lillicrap, Timothy P and Silver, David and Kavukcuoglu, Koray},
  pages = {19},
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  file = {/Users/qualia/Documents/Papers/Mnih et al. - Asynchronous Methods for Deep Reinforcement Learni.pdf},
  language = {en}
}

@article{Mnih2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  pages = {9},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  file = {/Users/qualia/Documents/Papers/Mnih et al. - Playing Atari with Deep Reinforcement Learning.pdf},
  journal = {NIPS},
  language = {en}
}

@article{Mnih2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  volume = {518},
  pages = {529--533},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14236},
  file = {/Users/qualia/Documents/Papers/2015 - Minh - Human-level control through deep reinforcement learning.pdf;/Users/qualia/Documents/Papers/Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf},
  journal = {Nature},
  language = {en},
  number = {7540}
}

@article{Moca2014,
  title = {Membrane {{Resonance Enables Stable}} and {{Robust Gamma Oscillations}}},
  author = {Moca, Vasile V. and Nikoli{\'c}, Danko and Singer, Wolf and Mure{\c s}an, Raul C.},
  year = {2014},
  month = jan,
  volume = {24},
  pages = {119--142},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhs293},
  abstract = {Neuronal mechanisms underlying beta/gamma oscillations (20\textendash{}80 Hz) are not completely understood. Here, we show that in vivo beta/gamma oscillations in the cat visual cortex sometimes exhibit remarkably stable frequency even when inputs fluctuate dramatically. Enhanced frequency stability is associated with stronger oscillations measured in individual units and larger power in the local field potential. Simulations of neuronal circuitry demonstrate that membrane properties of inhibitory interneurons strongly determine the characteristics of emergent oscillations. Exploration of networks containing either integrator or resonator inhibitory interneurons revealed that: (i) Resonance, as opposed to integration, promotes robust oscillations with large power and stable frequency via a mechanism called RING (Resonance INduced Gamma); resonance favors synchronization by reducing phase delays between interneurons and imposes bounds on oscillation cycle duration; (ii) Stability of frequency and robustness of the oscillation also depend on the relative timing of excitatory and inhibitory volleys within the oscillation cycle; (iii) RING can reproduce characteristics of both Pyramidal INterneuron Gamma (PING) and INterneuron Gamma (ING), transcending such classifications; (iv) In RING, robust gamma oscillations are promoted by slow but are impaired by fast inputs. Results suggest that interneuronal membrane resonance can be an important ingredient for generation of robust gamma oscillations having stable frequency.},
  file = {/Users/qualia/Documents/Papers/Moca et al. - 2014 - Membrane Resonance Enables Stable and Robust Gamma 2.pdf;/Users/qualia/Documents/Papers/Moca et al. - 2014 - Membrane Resonance Enables Stable and Robust Gamma.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Mochizuki2016,
  title = {Similarity in {{Neuronal Firing Regimes}} across {{Mammalian Species}}},
  author = {Mochizuki, Y. and Onaga, T. and Shimazaki, H. and Shimokawa, T. and Tsubo, Y. and Kimura, R. and Saiki, A. and Sakai, Y. and Isomura, Y. and Fujisawa, S. and Shibata, K.-i. and Hirai, D. and Furuta, T. and Kaneko, T. and Takahashi, S. and Nakazono, T. and Ishino, S. and Sakurai, Y. and Kitsukawa, T. and Lee, J. W. and Lee, H. and Jung, M. W. and Babul, C. and Maldonado, P. E. and Takahashi, K. and {Arce-McShane}, F. I. and Ross, C. F. and Sessle, B. J. and Hatsopoulos, N. G. and Brochier, T. and Riehle, A. and Chorley, P. and Grun, S. and Nishijo, H. and {Ichihara-Takeda}, S. and Funahashi, S. and Shima, K. and Mushiake, H. and Yamane, Y. and Tamura, H. and Fujita, I. and Inaba, N. and Kawano, K. and Kurkin, S. and Fukushima, K. and Kurata, K. and Taira, M. and Tsutsui, K.-I. and Ogawa, T. and Komatsu, H. and Koida, K. and Toyama, K. and Richmond, B. J. and Shinomoto, S.},
  year = {2016},
  month = may,
  volume = {36},
  pages = {5736--5747},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0230-16.2016},
  file = {/Users/qualia/Documents/Papers/Mochizuki et al. - 2016 - Similarity in Neuronal Firing Regimes across Mamma.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {21}
}

@article{Mochol2015,
  title = {Stochastic Transitions into Silence Cause Noise Correlations in Cortical Circuits},
  author = {Mochol, Gabriela and {Hermoso-Mendizabal}, Ainhoa and Sakata, Shuzo and Harris, Kenneth D. and {de la Rocha}, Jaime},
  year = {2015},
  month = mar,
  volume = {112},
  pages = {3529--3534},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1410509112},
  file = {/Users/qualia/Documents/Papers/2015 - Mochol et al. - Stochastic transitions into silence cause noise correlations in cortical circuits.pdf;/Users/qualia/Documents/Papers/Mochol et al. - 2015 - Stochastic transitions into silence cause noise co.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {11}
}

@article{Modolo2008,
  title = {Dynamics of the {{Subthalamo}}-Pallidal {{Complex}} in {{Parkinson}}'s {{Disease During Deep Brain Stimulation}}},
  author = {Modolo, J. and Henry, J. and Beuter, A.},
  year = {2008},
  month = aug,
  volume = {34},
  pages = {251--266},
  issn = {0092-0606, 1573-0689},
  doi = {10.1007/s10867-008-9095-y},
  abstract = {The dynamics of the subthalamo-pallidal complex in Parkinson's disease during deep brain stimulation (DBS) were studied using two models, a simple firing-rate model and a population-based model. We extended the simple firing-rate model of the complex formed by the subthalamic nucleus (STN) and the external segment of the Globus Pallidus (GPe) to explore its dynamical regime during DBS. More specifically, the modulation of neuronal activity (i.e., pattern and amplitude) during DBS was studied. A similar approach was used with the population-based model. Simulation results revealed a gradual decrease in bursting activity in STN cells when the DBS frequency increased. In addition, the contribution of the stimulation current type (mono- or biphasic) to the results was also examined. A comparison of the two models indicated that the population-based model was more biologically realistic and more appropriate for exploring DBS mechanisms. Understanding the underlying mechanisms of DBS is a prerequisite for developing new stimulation protocols.},
  file = {/Users/qualia/Documents/Papers/2008 - Modolo, Henry, Beuter - Dynamics of the subthalamo-pallidal complex in Parkinson's disease during deep brain stimulation.pdf},
  journal = {Journal of Biological Physics},
  language = {en},
  number = {3-4}
}

@article{Moldakarimov2015,
  title = {Feedback Stabilizes Propagation of Synchronous Spiking in Cortical Neural Networks},
  author = {Moldakarimov, Samat and Bazhenov, Maxim and Sejnowski, Terrence J.},
  year = {2015},
  month = feb,
  volume = {112},
  pages = {2545--2550},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1500643112},
  file = {/Users/qualia/Documents/Papers/Moldakarimov et al. - 2015 - Feedback stabilizes propagation of synchronous spi.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {8}
}

@article{Molgedey1992,
  title = {Suppressing Chaos in Neural Networks by Noise},
  author = {Molgedey, L. and Schuchhardt, J. and Schuster, H. G.},
  year = {1992},
  month = dec,
  volume = {69},
  pages = {3717--3719},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.69.3717},
  file = {/Users/qualia/Documents/Papers/1992 - Molgedey, Schuchhardt, Schuster - Suppressing chaos in neural networks by noise.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {26}
}

@article{Montgomery2004,
  title = {Discrete Synaptic States Define a Major Mechanism of Synapse Plasticity},
  author = {Montgomery, Johanna M. and Madison, Daniel V.},
  year = {2004},
  month = dec,
  volume = {27},
  pages = {744--750},
  issn = {01662236},
  doi = {10.1016/j.tins.2004.10.006},
  file = {/Users/qualia/Documents/Papers/2004 - Montgomery, Madison - Discrete synaptic states define a major mechanism of synapse plasticity.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {12}
}

@article{Moore2017,
  title = {Dynamics of Cortical Dendritic Membrane Potential and Spikes in Freely Behaving Rats},
  author = {Moore, Jason J. and Ravassard, Pascal M. and Ho, David and Acharya, Lavanya and Kees, Ashley L. and Vuong, Cliff and Mehta, Mayank R.},
  year = {2017},
  month = mar,
  volume = {355},
  pages = {eaaj1497},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaj1497},
  file = {/Users/qualia/Documents/Papers/Moore et al. - 2017 - Dynamics of cortical dendritic membrane potential .pdf},
  journal = {Science},
  language = {en},
  number = {6331}
}

@article{Moran2008,
  title = {Bayesian Estimation of Synaptic Physiology from the Spectral Responses of Neural Masses},
  author = {Moran, R.J. and Stephan, K.E. and Kiebel, S.J. and Rombach, N. and O'Connor, W.T. and Murphy, K.J. and Reilly, R.B. and Friston, K.J.},
  year = {2008},
  month = aug,
  volume = {42},
  pages = {272--284},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.01.025},
  file = {/Users/qualia/Documents/Papers/2008 - Moran et al. - Bayesian estimation of synaptic physiology from the spectral responses of neural masses.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Moran2008a,
  title = {Subthalamic Nucleus Functional Organization Revealed by Parkinsonian Neuronal Oscillations and Synchrony},
  author = {Moran, A. and Bergman, H. and Israel, Z. and {Bar-Gad}, I.},
  year = {2008},
  month = dec,
  volume = {131},
  pages = {3395--3409},
  issn = {1460-2156, 0006-8950},
  doi = {10.1093/brain/awn270},
  file = {/Users/qualia/Documents/Papers/2008 - Moran et al. - Subthalamic nucleus functional organization revealed by parkinsonian neuronal oscillations and synchrony.pdf},
  journal = {Brain},
  language = {en},
  number = {12}
}

@article{Morcos2019,
  title = {One Ticket to Win Them All: Generalizing Lottery Ticket Initializations across Datasets and Optimizers},
  shorttitle = {One Ticket to Win Them All},
  author = {Morcos, Ari S. and Yu, Haonan and Paganini, Michela and Tian, Yuandong},
  year = {2019},
  month = oct,
  abstract = {The success of lottery ticket initializations [7] suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these ``winning ticket'' initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.},
  archivePrefix = {arXiv},
  eprint = {1906.02773},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Morcos et al. - 2019 - One ticket to win them all generalizing lottery t.pdf},
  journal = {arXiv:1906.02773 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Morishita2017,
  title = {Postoperative Lead Migration in Deep Brain Stimulation Surgery: {{Incidence}}, Risk Factors, and Clinical Impact},
  shorttitle = {Postoperative Lead Migration in Deep Brain Stimulation Surgery},
  author = {Morishita, Takashi and Hilliard, Justin D. and Okun, Michael S. and Neal, Dan and Nestor, Kelsey A. and Peace, David and Hozouri, Alden A. and Davidson, Mark R. and Bova, Francis J. and Sporrer, Justin M. and Oyama, Genko and Foote, Kelly D.},
  editor = {Toft, Mathias},
  year = {2017},
  month = sep,
  volume = {12},
  pages = {e0183711},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0183711},
  file = {/Users/qualia/Documents/Papers/Morishita et al. - 2017 - Postoperative lead migration in deep brain stimula.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {9}
}

@article{Morris1981,
  title = {Voltage Oscillations in the Barnacle Giant Muscle Fiber},
  author = {Morris, C. and Lecar, H.},
  year = {1981},
  month = jul,
  volume = {35},
  pages = {193--213},
  issn = {00063495},
  doi = {10.1016/S0006-3495(81)84782-0},
  abstract = {Barnacle muscle fibers subjected to constant current stimulation produce a variety of types of oscillatory behavior when the internal medium contains the Ca"+ chelator EGTA. Oscillations are abolished if Ca"+ is removed from the external medium, or if the K+ conductance is blocked. Available voltage-clamp data indicate that the cell's active conductance systems are exceptionally simple. Given the complexity of barnacle fiber voltage behavior, this seems paradoxical. This paper presents an analysis of the possible modes of behavior available to a system of two noninactivating conductance mechanisms, and indicates a good correspondence to the types of behavior exhibited by barnacle fiber. The differential equations of a simple equivalent circuit for the fiber are dealt with by means of some of the mathematical techniques of nonlinear mechanics. General features of the system are (a) a propensity to produce damped or sustained oscillations over a rather broad parameter range, and (b) considerable latitude in the shape of the oscillatory potentials. It is concluded that for cells subject to changeable parameters (either from cell to cell or with time during cellular activity), a system dominated by two noninactivating conductances can exhibit varied oscillatory and bistable behavior.},
  file = {/Users/qualia/Documents/Papers/1981 - Morris, Lecar - Voltage oscillations in the barnacle giant muscle fiber.pdf;/Users/qualia/Documents/Papers/Morris and Lecar - 1981 - Voltage oscillations in the barnacle giant muscle .pdf},
  journal = {Biophysical Journal},
  language = {en},
  number = {1}
}

@article{Morrison,
  title = {Diversity of Emergent Dynamics in Competitive Threshold-Linear Networks: A Preliminary Report},
  author = {Morrison, Katherine and Degeratu, Anda and Itskov, Vladimir and Curto, Carina},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/Morrison et al. - Diversity of emergent dynamics in competitive thre.pdf},
  language = {en}
}

@article{Mosqueiro2017,
  title = {Task Allocation and Site Fidelity Jointly Influence Foraging Regulation in Honeybee Colonies},
  author = {Mosqueiro, Thiago and Cook, Chelsea and Huerta, Ramon and Gadau, J{\"u}rgen and Smith, Brian and {Pinter-Wollman}, Noa},
  year = {2017},
  month = aug,
  volume = {4},
  pages = {170344},
  issn = {2054-5703},
  doi = {10.1098/rsos.170344},
  abstract = {Variation in behavior among group members often impacts collective outcomes. Individuals may vary both in the task that they perform and in the persistence with which they perform each task. Although both the distribution of individuals among tasks and differences among individuals in behavioral persistence can each impact collective behavior, we do not know if and how they jointly affect collective outcomes. Here we use a detailed computational model to examine the joint impact of colony-level distribution among tasks and behavioral persistence of individuals, specifically their fidelity to particular resource sites, on the collective tradeoff between exploring for new resources and exploiting familiar ones. We developed an agent-based model of foraging honey bees, parameterized by data from 5 colonies, in which we simulated scouts, who search the environment for new resources, and individuals who are recruited by the scouts to the newly found resources, i.e., recruits. We varied the persistence to return to a particular food source of both scouts and recruits and found that for each value of persistence there is a different optimal ratio of scouts to recruits that maximizes resource collection by the colony. Furthermore, changes to the persistence of scouts induced opposite effects from changes to the persistence of recruits on the collective foraging of the colony. The proportion of scouts that resulted in the most resources collected by the colony decreased as the persistence of recruits increased. However, this optimal proportion of scouts increased as the persistence of scouts increased. Thus, behavioral persistence and task participation can interact to impact a colony's collective behavior in orthogonal directions. Our work provides new insights and generates new hypotheses into how variation in behavior at both the individual and colony levels jointly impact the trade-off between exploring for new resources and exploiting familiar ones.},
  file = {/Users/qualia/Documents/Papers/Mosqueiro et al. - 2017 - Task allocation and site fidelity jointly influenc.pdf},
  journal = {Royal Society Open Science},
  language = {en},
  number = {8}
}

@article{Mostafa2015,
  title = {Rhythmic Inhibition Allows Neural Networks to Search for Maximally Consistent States},
  author = {Mostafa, Hesham and Muller, Lorenz K. and Indiveri, Giacomo},
  year = {2015},
  month = dec,
  volume = {27},
  pages = {2510--2547},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00785},
  abstract = {Gamma-band rhythmic inhibition is a ubiquitous phenomenon in neural circuits yet its computational role still remains elusive. We show that a model of Gamma-band rhythmic inhibition allows networks of coupled cortical circuit motifs to search for network configurations that best reconcile external inputs with an internal consistency model encoded in the network connectivity. We show that Hebbian plasticity allows the networks to learn the consistency model by example. The search dynamics driven by rhythmic inhibition enable the described networks to solve di cult constraint satisfaction problems without making assumptions about the form of stochastic fluctuations in the network. We show that the search dynamics are well approximated by a stochastic sampling process. We use the described networks to reproduce perceptual multi-stability phenomena with switching times that are a good match to experimental data and show that they provide a general neural framework which can be used to model other 'perceptual inference' phenomena.},
  archivePrefix = {arXiv},
  eprint = {1503.02777},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Mostafa, Müller, Indiveri - Rhythmic Inhibition Allows Neural Networks to Search for Maximally Consistent States(2).pdf;/Users/qualia/Documents/Papers/Mostafa et al. - 2015 - Rhythmic inhibition allows neural networks to sear 2.pdf;/Users/qualia/Documents/Papers/Mostafa et al. - 2015 - Rhythmic inhibition allows neural networks to sear 3.pdf;/Users/qualia/Documents/Papers/Mostafa et al. - 2015 - Rhythmic inhibition allows neural networks to sear.pdf},
  journal = {Neural Computation},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {12}
}

@article{Mourao-Miranda2007,
  title = {Dynamic Discrimination Analysis: {{A}} Spatial\textendash{}Temporal {{SVM}}},
  shorttitle = {Dynamic Discrimination Analysis},
  author = {{Mour{\~a}o-Miranda}, Janaina and Friston, Karl J. and Brammer, Michael},
  year = {2007},
  month = may,
  volume = {36},
  pages = {88--99},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.02.020},
  file = {/Users/qualia/Documents/Papers/2007 - Mourão-Miranda, Friston, Brammer - Dynamic discrimination analysis a spatial-temporal SVM.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@incollection{Mouret2011,
  title = {Novelty-{{Based Multiobjectivization}}},
  booktitle = {New {{Horizons}} in {{Evolutionary Robotics}}},
  author = {Mouret, Jean-Baptiste},
  editor = {Kacprzyk, Janusz and Doncieux, St{\'e}phane and Bred{\`e}che, Nicolas and Mouret, Jean-Baptiste},
  year = {2011},
  volume = {341},
  pages = {139--154},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-18272-3_10},
  abstract = {Novelty search is a recent and promising approach to evolve neuro-controllers, especially to drive robots. The main idea is to maximize the novelty of behaviors instead of the efficiency. However, abandoning the efficiency objective(s) may be too radical in many contexts. In this paper, a Pareto-based multi-objective evolutionary algorithm is employed to reconcile novelty search with objective-based optimization by following a multiobjectivization process. Several multiobjectivizations based on behavioral novelty and on behavioral diversity are compared on a maze navigation task. Results show that the multiobjectivizations is better at fine-tuning behaviors than basic novelty search while keeping a comparable number of iterations to converge.},
  file = {/Users/qualia/Documents/Papers/Mouret - 2011 - Novelty-Based Multiobjectivization.pdf},
  isbn = {978-3-642-18271-6 978-3-642-18272-3},
  language = {en}
}

@article{Mouret2015,
  title = {Illuminating Search Spaces by Mapping Elites},
  author = {Mouret, Jean-Baptiste and Clune, Jeff},
  year = {2015},
  month = apr,
  abstract = {Many fields use search algorithms, which automatically explore a search space to find high-performing solutions: chemists search through the space of molecules to discover new drugs; engineers search for stronger, cheaper, safer designs, scientists search for models that best explain data, etc. The goal of search algorithms has traditionally been to return the single highest-performing solution in a search space. Here we describe a new, fundamentally different type of algorithm that is more useful because it provides a holistic view of how high-performing solutions are distributed throughout a search space. It creates a map of high-performing solutions at each point in a space defined by dimensions of variation that a user gets to choose. This Multi-dimensional Archive of Phenotypic Elites (MAP-Elites) algorithm illuminates search spaces, allowing researchers to understand how interesting attributes of solutions combine to affect performance, either positively or, equally of interest, negatively. For example, a drug company may wish to understand how performance changes as the size of molecules and their cost-to-produce vary. MAP-Elites produces a large diversity of high-performing, yet qualitatively different solutions, which can be more helpful than a single, high-performing solution. Interestingly, because MAP-Elites explores more of the search space, it also tends to find a better overall solution than state-of-the-art search algorithms. We demonstrate the benefits of this new algorithm in three different problem domains ranging from producing modular neural networks to designing simulated and real soft robots. Because MAP- Elites (1) illuminates the relationship between performance and dimensions of interest in solutions, (2) returns a set of high-performing, yet diverse solutions, and (3) improves finding a single, best solution, it will advance science and engineering.},
  archivePrefix = {arXiv},
  eprint = {1504.04909},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Mouret and Clune - 2015 - Illuminating search spaces by mapping elites.pdf},
  journal = {arXiv:1504.04909 [cs, q-bio]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing,Computer Science - Robotics,Quantitative Biology - Populations and Evolution},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{Mu2019,
  title = {Glia {{Accumulate Evidence}} That {{Actions Are Futile}} and {{Suppress Unsuccessful Behavior}}},
  author = {Mu, Yu and Bennett, Davis V. and Rubinov, Mikail and Narayan, Sujatha and Yang, Chao-Tsung and Tanimoto, Masashi and Mensh, Brett D. and Looger, Loren L. and Ahrens, Misha B.},
  year = {2019},
  month = jun,
  volume = {178},
  pages = {27-43.e19},
  issn = {00928674},
  doi = {10.1016/j.cell.2019.05.050},
  abstract = {When a behavior repeatedly fails to achieve its goal, animals often give up and become passive, which can be strategic for preserving energy or regrouping between attempts. It is unknown how the brain identifies behavioral failures and mediates this behavioral-state switch. In larval zebrafish swimming in virtual reality, visual feedback can be withheld so that swim attempts fail to trigger expected visual flow. After tens of seconds of such motor futility, animals became passive for similar durations. Whole-brain calcium imaging revealed noradrenergic neurons that responded specifically to failed swim attempts and radial astrocytes whose calcium levels accumulated with increasing numbers of failed attempts. Using cell ablation and optogenetic or chemogenetic activation, we found that noradrenergic neurons progressively activated brainstem radial astrocytes, which then suppressed swimming. Thus, radial astrocytes perform a computation critical for behavior: they accumulate evidence that current actions are ineffective and consequently drive changes in behavioral states.},
  file = {/Users/qualia/Documents/Papers/Mu et al. - 2019 - Glia Accumulate Evidence that Actions Are Futile a.pdf},
  journal = {Cell},
  language = {en},
  number = {1}
}

@article{Mu2019a,
  title = {Glia {{Accumulate Evidence}} That {{Actions Are Futile}} and {{Suppress Unsuccessful Behavior}}},
  author = {Mu, Yu and Bennett, Davis V. and Rubinov, Mikail and Narayan, Sujatha and Yang, Chao-Tsung and Tanimoto, Masashi and Mensh, Brett D. and Looger, Loren L. and Ahrens, Misha B.},
  year = {2019},
  month = jun,
  volume = {178},
  pages = {27-43.e19},
  issn = {00928674},
  doi = {10.1016/j.cell.2019.05.050},
  abstract = {When a behavior repeatedly fails to achieve its goal, animals often give up and become passive, which can be strategic for preserving energy or regrouping between attempts. It is unknown how the brain identifies behavioral failures and mediates this behavioral-state switch. In larval zebrafish swimming in virtual reality, visual feedback can be withheld so that swim attempts fail to trigger expected visual flow. After tens of seconds of such motor futility, animals became passive for similar durations. Whole-brain calcium imaging revealed noradrenergic neurons that responded specifically to failed swim attempts and radial astrocytes whose calcium levels accumulated with increasing numbers of failed attempts. Using cell ablation and optogenetic or chemogenetic activation, we found that noradrenergic neurons progressively activated brainstem radial astrocytes, which then suppressed swimming. Thus, radial astrocytes perform a computation critical for behavior: they accumulate evidence that current actions are ineffective and consequently drive changes in behavioral states.},
  file = {/Users/qualia/Documents/Papers/Mu et al. - 2019 - Glia Accumulate Evidence that Actions Are Futile a 2.pdf},
  journal = {Cell},
  language = {en},
  number = {1}
}

@article{Muller2011,
  title = {Spike-{{Timing Dependent Plasticity}} and {{Feed}}-{{Forward Input Oscillations Produce Precise}} and {{Invariant Spike Phase}}-{{Locking}}},
  author = {Muller, Lyle and Brette, Romain and Gutkin, Boris},
  year = {2011},
  volume = {5},
  issn = {1662-5188},
  doi = {10.3389/fncom.2011.00045},
  file = {/Users/qualia/Documents/Papers/2011 - Muller, Brette, Gutkin - Spike-Timing Dependent Plasticity and Feed-Forward Input Oscillations Produce Precise and Invariant Spik.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Mumford2014,
  title = {The Impact of Study Design on Pattern Estimation for Single-Trial Multivariate Pattern Analysis},
  author = {Mumford, Jeanette A. and Davis, Tyler and Poldrack, Russell A.},
  year = {2014},
  month = dec,
  volume = {103},
  pages = {130--138},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2014.09.026},
  abstract = {A prerequisite for a pattern analysis using functional magnetic resonance imaging (fMRI) data is estimating the patterns from time series data, which then are input into the pattern analysis. Here we focus on how the combination of study design (order and spacing of trials) with pattern estimator impacts the Type I error rate of the subsequent pattern analysis. When Type I errors are inflated, the results are no longer valid, so this work serves as a guide for designing and analyzing MVPA studies with controlled false positive rates. The MVPA strategies examined are pattern classification and similarity, utilizing single trial activation patterns from the same functional run. Primarily focusing on the Least Squares Single and Least Square All pattern estimators, we show that collinearities in the models, along with temporal autocorrelation, can cause false positive correlations between activation pattern estimates that adversely impact the false positive rates of pattern similarity and classification analyses. It may seem intuitive that increasing the interstimulus interval (ISI) would alleviate this issue, but remaining weak correlations between activation patterns persist and have a strong influence in pattern similarity analyses. Pattern similarity analyses using only activation patterns estimated from the same functional run of data are susceptible to inflated false positives unless trials are randomly ordered, with a different randomization for each subject. In other cases, where there is any structure to trial order, valid pattern similarity analysis results can only be obtained if similarity computations are restricted to pairs of activation patterns from independent runs. Likewise, for pattern classification, false positives are minimized when the testing and training sets in cross validation do not contain patterns estimated from the same run.},
  file = {/Users/qualia/Documents/Papers/2014 - Mumford, Davis, Poldrack - The impact of study design on pattern estimation for single-trial multivariate pattern analysis.pdf;/Users/qualia/Documents/Papers/Mumford et al. - 2014 - The impact of study design on pattern estimation f.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Munoz-Moreno2013,
  title = {A {{Magnetic Resonance Image Based Atlas}} of the {{Rabbit Brain}} for {{Automatic Parcellation}}},
  author = {{Mu{\~n}oz-Moreno}, Emma and {Arbat-Plana}, Ariadna and Batalle, Dafnis and Soria, Guadalupe and Illa, Miriam and {Prats-Galino}, Alberto and Eixarch, Elisenda and Gratacos, Eduard},
  editor = {Malmierca, Manuel S.},
  year = {2013},
  month = jul,
  volume = {8},
  pages = {e67418},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0067418},
  abstract = {Rabbit brain has been used in several works for the analysis of neurodevelopment. However, there are not specific digital rabbit brain atlases that allow an automatic identification of brain regions, which is a crucial step for various neuroimage analyses, and, instead, manual delineation of areas of interest must be performed in order to evaluate a specific structure. For this reason, we propose an atlas of the rabbit brain based on magnetic resonance imaging, including both structural and diffusion weighted, that can be used for the automatic parcellation of the rabbit brain. Ten individual atlases, as well as an average template and probabilistic maps of the anatomical regions were built. In addition, an example of automatic segmentation based on this atlas is described.},
  file = {/Users/qualia/Documents/Papers/2013 - Muñoz-Moreno et al. - A Magnetic Resonance Image Based Atlas of the Rabbit Brain for Automatic Parcellation.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {7}
}

@article{Mur2009,
  title = {Revealing Representational Content with Pattern-Information {{fMRI}}\textemdash{}an Introductory Guide},
  author = {Mur, Marieke and Bandettini, Peter A. and Kriegeskorte, Nikolaus},
  year = {2009},
  month = mar,
  volume = {4},
  pages = {101--109},
  issn = {1749-5024, 1749-5016},
  doi = {10.1093/scan/nsn044},
  file = {/Users/qualia/Documents/Papers/2009 - Mur, Bandettini, Kriegeskorte - Revealing representational content with pattern-information fMRI--an introductory guide.pdf},
  journal = {Social Cognitive and Affective Neuroscience},
  language = {en},
  number = {1}
}

@article{Murphy2003,
  title = {Multiplicative {{Gain Changes Are Induced}} by {{Excitation}} or {{Inhibition Alone}}},
  author = {Murphy, Brendan K. and Miller, Kenneth D.},
  year = {2003},
  month = nov,
  volume = {23},
  pages = {10040--10051},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.23-31-10040.2003},
  file = {/Users/qualia/Documents/Papers/2003 - Murphy, Miller - Multiplicative gain changes are induced by excitation or inhibition alone.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {31}
}

@article{Musall2019,
  title = {Single-Trial Neural Dynamics Are Dominated by Richly Varied Movements},
  author = {Musall, Simon and Kaufman, Matthew T. and Juavinett, Ashley L. and Gluf, Steven and Churchland, Anne K.},
  year = {2019},
  month = oct,
  volume = {22},
  pages = {1677--1686},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0502-4},
  file = {/Users/qualia/Documents/Papers/Musall et al. - 2019 - Single-trial neural dynamics are dominated by rich.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {10}
}

@article{Muscinelli2017,
  title = {Exponentially {{Long Orbits}} in {{Hopfield Neural Networks}}},
  author = {Muscinelli, Samuel P. and Gerstner, Wulfram and Brea, Johanni},
  year = {2017},
  month = feb,
  volume = {29},
  pages = {458--484},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00919},
  file = {/Users/qualia/Documents/Papers/Muscinelli et al. - 2017 - Exponentially Long Orbits in Hopfield Neural Netwo.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Muto2013,
  title = {Real-{{Time Visualization}} of {{Neuronal Activity}} during {{Perception}}},
  author = {Muto, Akira and Ohkura, Masamichi and Abe, Gembu and Nakai, Junichi and Kawakami, Koichi},
  year = {2013},
  month = feb,
  volume = {23},
  pages = {307--311},
  issn = {09609822},
  doi = {10.1016/j.cub.2012.12.040},
  abstract = {To understand how the brain perceives the external world, it is desirable to observe neuronal activity in the brain in real time during perception. The zebrafish is a suitable model animal for fluorescence imaging studies to visualize neuronal activity because its body is transparent through the embryonic and larval stages. Imaging studies have been carried out to monitor neuronal activity in the larval spinal cord and brain using Ca2+ indicator dyes [1\textendash{}3] and DNA-encoded Ca2+ indicators, such as Cameleon [4], GFPaequorin [5], and GCaMPs [6\textendash{}12]. However, temporal and spatial resolution and sensitivity of these tools are still limited, and imaging of brain activity during perception of a natural object has not yet been demonstrated. Here we demonstrate visualization of neuronal activity in the optic tectum of larval zebrafish by genetically expressing the new version of GCaMP. First, we demonstrate Ca2+ transients in the tectum evoked by a moving spot on a display and identify direction-selective neurons. Second, we show tectal activity during perception of a natural object, a swimming paramecium, revealing a functional visuotopic map. Finally, we image the tectal responses of a free-swimming larval fish to a paramecium and thereby correlate neuronal activity in the brain with prey capture behavior.},
  file = {/Users/qualia/Documents/Papers/2013 - Muto et al. - Real-Time Visualization of Neuronal Activity during Perception.pdf},
  journal = {Current Biology},
  language = {en},
  number = {4}
}

@article{Muyesser,
  title = {Learning Model-Based Strategies in Simple Environments with Hierarchical q-Networks},
  author = {Muyesser, Necati Alp and Dunovan, Kyle and Verstynen, Timothy},
  pages = {29},
  abstract = {Recent advances in deep learning have allowed artificial agents to rival human-level performance on a wide range of complex tasks; however, the ability of these networks to learn generalizable strategies remains a pressing challenge. This critical limitation is due in part to two factors: the opaque information representation in deep neural networks and the complexity of the task environments in which they are typically deployed. Here we propose a novel Hierarchical Q-Network (HQN), motivated by theories of the hierarchical organization of the human prefrontal cortex, that attempts to identify lower dimensional patterns in the value landscape that can be exploited to construct an internal model of rules in simple environments. We draw on combinatorial games, where there exists a single optimal strategy for winning that generalizes across other features of the game, to probe the strategy generalization of the HQN and other reinforcement learning (RL) agents using variations of Wythoff's game. Traditional RL approaches failed to reach satisfactory performance on variants of Wythoff's Game; however, the HQN learned heuristic-like strategies that generalized across changes in board configuration. More importantly, the HQN allowed for transparent inspection of the agent's internal model of the game following training. Our results show how a biologically inspired hierarchical learner can facilitate learning abstract rules to promote robust and flexible action policies in simplified training environments with clearly delineated optimal strategies.},
  file = {/Users/qualia/Documents/Papers/Muyesser et al. - Learning model-based strategies in simple environm.pdf},
  language = {en}
}

@article{Myerson2003,
  title = {Effects of {{Age}}, {{Domain}}, and {{Processing Demands}} on {{Memory Span}}: {{Evidence}} for {{Differential Decline}}},
  shorttitle = {Effects of {{Age}}, {{Domain}}, and {{Processing Demands}} on {{Memory Span}}},
  author = {Myerson, Joel and Emery, Lisa and White, Desir{\'e}e A. and Hale, Sandra},
  year = {2003},
  month = mar,
  volume = {10},
  pages = {20--27},
  issn = {1382-5585, 1744-4128},
  doi = {10.1076/anec.10.1.20.13454},
  abstract = {Analysis of cross-sectional data from the normative sample of the Wechsler Memory Scale \textendash{} Third Edition (WMS-III) revealed different patterns of age-related differences in memory span measures depending on the type of memory item, processing demands, and the age of the older adult group. Regression of memory span on age revealed that the slope for Spatial Span raw scores was significantly more negative than the slope for Digit Span raw scores. There was no significant difference, however, either between the slopes for forward and backward Digit Span or between the slopes for forward and backward Spatial Span. Regression of Letter-Number Sequencing raw scores on age showed a distinctive, curvilinear pattern. Taken together, the present findings suggest that at least two mechanisms are involved in age-related differences in memory span. One mechanism, associated with a relatively linear decrease in memory span as a function of age, may differentially affect the storage of different types of information (e.g., sequences of digits vs. spatial locations). The other mechanism, evidenced by the curvilinear trend in Letter-Number Sequencing scores, may be tentatively attributed to a decline in executive aspects of working memory that becomes increasingly pronounced with age.},
  file = {/Users/qualia/Documents/Papers/2003 - Myerson et al. - Effects of age, domain, and processing demands on memory span Evidence for differential decline.pdf},
  journal = {Aging, Neuropsychology, and Cognition},
  language = {en},
  number = {1}
}

@article{Nadim2014,
  title = {Neuromodulation of Neurons and Synapses},
  author = {Nadim, Farzan and Bucher, Dirk},
  year = {2014},
  month = dec,
  volume = {29},
  pages = {48--56},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.05.003},
  file = {/Users/qualia/Documents/Papers/2014 - Nadim, Bucher - Neuromodulation of neurons and synapses.pdf;/Users/qualia/Documents/Papers/Nadim and Bucher - 2014 - Neuromodulation of neurons and synapses 2.pdf;/Users/qualia/Documents/Papers/Nadim and Bucher - 2014 - Neuromodulation of neurons and synapses.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Nagabandi2019,
  title = {Deep {{Online Learning}} via {{Meta}}-{{Learning}}: {{Continual Adaptation}} for {{Model}}-{{Based RL}}},
  shorttitle = {Deep {{Online Learning}} via {{Meta}}-{{Learning}}},
  author = {Nagabandi, Anusha and Finn, Chelsea and Levine, Sergey},
  year = {2019},
  month = jan,
  abstract = {Humans and animals can learn complex predictive models that allow them to accurately and reliably reason about real-world phenomena, and they can adapt such models extremely quickly in the face of unexpected changes. Deep neural network models allow us to represent very complex functions, but lack this capacity for rapid online adaptation. The goal in this paper is to develop a method for continual online learning from an incoming stream of data, using deep neural network models. We formulate an online learning procedure that uses stochastic gradient descent to update model parameters, and an expectation maximization algorithm with a Chinese restaurant process prior to develop and maintain a mixture of models to handle non-stationary task distributions. This allows for all models to be adapted as necessary, with new models instantiated for task changes and old models recalled when previously seen tasks are encountered again. Furthermore, we observe that meta-learning can be used to meta-train a model such that this direct online adaptation with SGD is effective, which is otherwise not the case for large function approximators. In this work, we apply our meta-learning for online learning (MOLe) approach to model-based reinforcement learning, where adapting the predictive model is critical for control; we demonstrate that MOLe outperforms alternative prior methods, and enables effective continuous adaptation in non-stationary task distributions such as varying terrains, motor failures, and unexpected disturbances.},
  archivePrefix = {arXiv},
  eprint = {1812.07671},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Nagabandi et al. - 2019 - Deep Online Learning via Meta-Learning Continual .pdf},
  journal = {arXiv:1812.07671 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Naik2019,
  title = {Discounted {{Reinforcement Learning Is Not}} an {{Optimization Problem}}},
  author = {Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Yao, Hengshuai and Sutton, Richard S.},
  year = {2019},
  month = nov,
  abstract = {Discounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks.},
  archivePrefix = {arXiv},
  eprint = {1910.02140},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Naik et al. - 2019 - Discounted Reinforcement Learning Is Not an Optimi.pdf},
  journal = {arXiv:1910.02140 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{Naik2019a,
  title = {Discounted {{Reinforcement Learning Is Not}} an {{Optimization Problem}}},
  author = {Naik, Abhishek and Shariff, Roshan and Yasui, Niko and Yao, Hengshuai and Sutton, Richard S.},
  year = {2019},
  month = nov,
  abstract = {Discounted reinforcement learning is fundamentally incompatible with function approximation for control in continuing tasks. It is not an optimization problem in its usual formulation, so when using function approximation there is no optimal policy. We substantiate these claims, then go on to address some misconceptions about discounting and its connection to the average reward formulation. We encourage researchers to adopt rigorous optimization approaches, such as maximizing average reward, for reinforcement learning in continuing tasks.},
  archivePrefix = {arXiv},
  eprint = {1910.02140},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Naik et al. - 2019 - Discounted Reinforcement Learning Is Not an Optimi 2.pdf},
  journal = {arXiv:1910.02140 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{Nandy2019,
  title = {Optogenetically Induced Low-Frequency Correlations Impair Perception},
  author = {Nandy, Anirvan and Nassi, Jonathan J and Jadi, Monika P and Reynolds, John},
  year = {2019},
  volume = {8:e35123},
  pages = {18},
  file = {/Users/qualia/Documents/Papers/Nandy et al. - Optogenetically induced low-frequency correlations.pdf},
  journal = {eLife},
  language = {en}
}

@article{Naselaris2011,
  title = {Encoding and Decoding in {{fMRI}}},
  author = {Naselaris, Thomas and Kay, Kendrick N. and Nishimoto, Shinji and Gallant, Jack L.},
  year = {2011},
  month = may,
  volume = {56},
  pages = {400--410},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.07.073},
  abstract = {Over the past decade fMRI researchers have developed increasingly sensitive techniques for analyzing the information represented in BOLD activity. The most popular of these techniques is linear classification, a simple technique for decoding information about experimental stimuli or tasks from patterns of activity across an array of voxels. A more recent development is the voxel-based encoding model, which describes the information about the stimulus or task that is represented in the activity of single voxels. Encoding and decoding are complementary operations: encoding uses stimuli to predict activity while decoding uses activity to predict information about the stimuli. However, in practice these two operations are often confused, and their respective strengths and weaknesses have not been made clear. Here we use the concept of a linearizing feature space to clarify the relationship between encoding and decoding. We show that encoding and decoding operations can both be used to investigate some of the most common questions about how information is represented in the brain. However, focusing on encoding models offers two important advantages over decoding. First, an encoding model can in principle provide a complete functional description of a region of interest, while a decoding model can provide only a partial description. Second, while it is straightforward to derive an optimal decoding model from an encoding model it is much more difficult to derive an encoding model from a decoding model. We propose a systematic modeling approach that begins by estimating an encoding model for every voxel in a scan and ends by using the estimated encoding models to perform decoding.},
  file = {/Users/qualia/Documents/Papers/2011 - Naselaris et al. - Encoding and decoding in fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Naud2008,
  title = {Firing Patterns in the Adaptive Exponential Integrate-and-Fire Model},
  author = {Naud, Richard and Marcille, Nicolas and Clopath, Claudia and Gerstner, Wulfram},
  year = {2008},
  month = nov,
  volume = {99},
  pages = {335--347},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-008-0264-7},
  abstract = {For simulations of large spiking neuron networks, an accurate, simple and versatile single-neuron modeling framework is required. Here we explore the versatility of a simple two-equation model: the adaptive exponential integrate-and-fire neuron. We show that this model generates multiple firing patterns depending on the choice of parameter values, and present a phase diagram describing the transition from one firing type to another. We give an analytical criterion to distinguish between continuous adaption, initial bursting, regular bursting and two types of tonic spiking. Also, we report that the deterministic model is capable of producing irregular spiking when stimulated with constant current, indicating low-dimensional chaos. Lastly, the simple model is fitted to real experiments of cortical neurons under step current stimulation. The results provide support for the suitability of simple models such as the adaptive exponential integrate-and-fire neuron for large network simulations.},
  file = {/Users/qualia/Documents/Papers/2008 - Naud et al. - Firing patterns in the adaptive exponential integrate-and-fire model.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {4-5}
}

@article{Naumann2016,
  title = {From {{Whole}}-{{Brain Data}} to {{Functional Circuit Models}}: {{The Zebrafish Optomotor Response}}},
  shorttitle = {From {{Whole}}-{{Brain Data}} to {{Functional Circuit Models}}},
  author = {Naumann, Eva A. and Fitzgerald, James E. and Dunn, Timothy W. and Rihel, Jason and Sompolinsky, Haim and Engert, Florian},
  year = {2016},
  month = nov,
  volume = {167},
  pages = {947-960.e20},
  issn = {00928674},
  doi = {10.1016/j.cell.2016.10.019},
  abstract = {Detailed descriptions of brain-scale sensorimotor circuits underlying vertebrate behavior remain elusive. Recent advances in zebrafish neuroscience offer new opportunities to dissect such circuits via whole-brain imaging, behavioral analysis, functional perturbations, and network modeling. Here, we harness these tools to generate a brain-scale circuit model of the optomotor response, an orienting behavior evoked by visual motion. We show that such motion is processed by diverse neural response types distributed across multiple brain regions. To transform sensory input into action, these regions sequentially integrate eye- and direction-specific sensory streams, refine representations via interhemispheric inhibition, and demix locomotor instructions to independently drive turning and forward swimming. While experiments revealed many neural response types throughout the brain, modeling identified the dimensions of functional connectivity most critical for the behavior. We thus reveal how distributed neurons collaborate to generate behavior and illustrate a paradigm for distilling functional circuit models from whole-brain data.},
  file = {/Users/qualia/Documents/Papers/Naumann et al. - 2016 - From Whole-Brain Data to Functional Circuit Models.pdf},
  journal = {Cell},
  language = {en},
  number = {4}
}

@article{Nauta1978,
  title = {Efferent Projections of the Subthalamic Nucleus: {{An}} Autoradiographic Study in Monkey and Cat},
  shorttitle = {Efferent Projections of the Subthalamic Nucleus},
  author = {Nauta, Haring J. W. and Cole, Monroe},
  year = {1978},
  month = jul,
  volume = {180},
  pages = {1--16},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.901800102},
  abstract = {The efferent projections of the subthalamic nucleus were studied with the autoradiographic tracing technique in Rhesus monkey and cat. From the data i t appears that the major efferent projections of the nucleus are to the pallidal complex and the substantia nigra. In both monkey and cat, the projection to the pallidal complex is truly massive and is directed a t both pallidal segments. The projection field includes an infracommissural part of the pallidal complex bordering on the substantia innominata. In the monkey the termination in the pallidal complex is organized in several characteristic bands oriented parallel to the medullary laminae. The subthalamo-pallidal projection in monkey further appears to be topographically organized. The projections to the substantia nigra is prominent in both cat and monkey though not as massive as that to the pallidal complex. The distribution of termination in the substantia nigra favors the more ventral strata near the cerebral peduncle. In the monkey the terminal distribution appears to avoid regions of the substantia nigra containing pigmented neurons and it is suggested that the subthalamonigral pathway may prefer non-dopaminergic neurons. In addition to the above major projections, sparse projections were noted to the thalamic nuclei ventralis lateralis and ventralis anterior, to the putamen, and to the mesencephalic nucleus tegmenti pedunculopontinus, pars compacta. The findings are discussed.},
  file = {/Users/qualia/Documents/Papers/1978 - Nauta, Cole - Efferent projections of the subthalamic nucleus An autoradiographic study in monkey and cat.pdf;/Users/qualia/Documents/Papers/Nauta and Cole - 1978 - Efferent projections of the subthalamic nucleus A.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Neftci,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  author = {Neftci, Emre O and Mostafa, Hesham and Zenke, Friedemann},
  pages = {21},
  abstract = {A growing number of neuromorphic spiking neural network processors that emulate biological neural networks create an imminent need for methods and tools to enable them to solve real-world signal processing problems. Like conventional neural networks, spiking neural networks are particularly efficient when trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This tutorial elucidates step-by-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and datadriven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  file = {/Users/qualia/Documents/Papers/Neftci et al. - Surrogate Gradient Learning in Spiking Neural Netw.pdf},
  language = {en}
}

@article{Neftci2019,
  title = {Reinforcement Learning in Artificial and Biological Systems},
  author = {Neftci, Emre O. and Averbeck, Bruno B.},
  year = {2019},
  month = mar,
  volume = {1},
  pages = {133--143},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0025-4},
  file = {/Users/qualia/Documents/Papers/Neftci and Averbeck - 2019 - Reinforcement learning in artificial and biologica.pdf},
  journal = {Nat Mach Intell},
  language = {en},
  number = {3}
}

@article{Nelson2015,
  title = {Excitatory/{{Inhibitory Balance}} and {{Circuit Homeostasis}} in {{Autism Spectrum Disorders}}},
  author = {Nelson, Sacha B. and Valakh, Vera},
  year = {2015},
  month = aug,
  volume = {87},
  pages = {684--698},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.07.033},
  file = {/Users/qualia/Documents/Papers/Nelson and Valakh - 2015 - ExcitatoryInhibitory Balance and Circuit Homeosta.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Netoff2005,
  title = {Synchronization in {{Hybrid Neuronal Networks}} of the {{Hippocampal Formation}}},
  author = {Netoff, Theoden I. and Banks, Matthew I. and Dorval, Alan D. and Acker, Corey D. and Haas, Julie S. and Kopell, Nancy and White, John A.},
  year = {2005},
  month = mar,
  volume = {93},
  pages = {1197--1208},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00982.2004},
  file = {/Users/qualia/Documents/Papers/2005 - Netoff et al. - Synchronization in hybrid neuronal networks of the hippocampal formation(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Nevin2010,
  title = {Focusing on Optic Tectum Circuitry through the Lens of Genetics},
  author = {Nevin, Linda M and Robles, Estuardo and Baier, Herwig and Scott, Ethan K},
  year = {2010},
  volume = {8},
  pages = {126},
  issn = {1741-7007},
  doi = {10.1186/1741-7007-8-126},
  abstract = {The visual pathway is tasked with processing incoming signals from the retina and converting this information into adaptive behavior. Recent studies of the larval zebrafish tectum have begun to clarify how the `microcircuitry' of this highly organized midbrain structure filters visual input, which arrives in the superficial layers and directs motor output through efferent projections from its deep layers. The new emphasis has been on the specific function of neuronal cell types, which can now be reproducibly labeled, imaged and manipulated using genetic and optical techniques. Here, we discuss recent advances and emerging experimental approaches for studying tectal circuits as models for visual processing and sensorimotor transformation by the vertebrate brain.},
  file = {/Users/qualia/Documents/Papers/2010 - Nevin et al. - Focusing on optic tectum circuitry through the lens of genetics.pdf},
  journal = {BMC Biology},
  language = {en},
  number = {1}
}

@incollection{Newell1973,
  title = {{{YOU CAN}}'{{T PLAY}} 20 {{QUESTIONS WITH NATURE AND WIN}}: {{PROJECTIVE COMMENTS ON THE PAPERS OF THIS SYMPOSIUM}}},
  shorttitle = {{{YOU CAN}}'{{T PLAY}} 20 {{QUESTIONS WITH NATURE AND WIN}}},
  booktitle = {Visual {{Information Processing}}},
  author = {Newell, Allen},
  year = {1973},
  pages = {283--308},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-0-12-170150-5.50012-3},
  file = {/Users/qualia/Documents/Papers/1973 - Newell - You can't play 20 questions with nature and win Projective comments on the papers of this symposium.pdf;/Users/qualia/Documents/Papers/Newell - 1973 - YOU CAN'T PLAY 20 QUESTIONS WITH NATURE AND WIN P.pdf},
  isbn = {978-0-12-170150-5},
  language = {en}
}

@article{Newhall2015,
  title = {Synchrony in Stochastically Driven Neuronal Networks with Complex Topologies},
  author = {Newhall, Katherine A. and Shkarayev, Maxim S. and Kramer, Peter R. and Kova{\v c}i{\v c}, Gregor and Cai, David},
  year = {2015},
  month = may,
  volume = {91},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.91.052806},
  file = {/Users/qualia/Documents/Papers/2015 - Newhall et al. - Synchrony in stochastically driven neuronal networks with complex topologies.pdf;/Users/qualia/Documents/Papers/Newhall et al. - 2015 - Synchrony in stochastically driven neuronal networ.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Newman,
  title = {Limitations of the Asymptotic Approach to Dynamics},
  author = {Newman, Julian and Lucas, Maxime and Stefanovska, Aneta},
  pages = {13},
  file = {/Users/qualia/Documents/Papers/Newman et al. - Limitations of the asymptotic approach to dynamics.pdf},
  language = {en}
}

@article{Newman1997,
  title = {Calcium {{Waves}} in {{Retinal Glial Cells}}},
  author = {Newman, E. A.},
  year = {1997},
  month = feb,
  volume = {275},
  pages = {844--847},
  issn = {00368075, 10959203},
  doi = {10.1126/science.275.5301.844},
  file = {/Users/qualia/Documents/Papers/Newman - 1997 - Calcium Waves in Retinal Glial Cells.pdf},
  journal = {Science},
  language = {en},
  number = {5301}
}

@article{Newman2002,
  title = {Random Graph Models of Social Networks},
  author = {Newman, M. E. J. and Watts, D. J. and Strogatz, S. H.},
  year = {2002},
  month = feb,
  volume = {99},
  pages = {2566--2572},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.012582999},
  file = {/Users/qualia/Documents/Papers/2002 - Newman, Watts, Strogatz - Random graph models of social networks.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {Supplement 1}
}

@article{Neyman1976,
  title = {Tests of Statistical Hypotheses and Their Use in Studies of Natural Phenomena},
  author = {Neyman, Jerzy},
  year = {1976},
  month = jan,
  volume = {5},
  pages = {737--751},
  issn = {0361-0926, 1532-415X},
  doi = {10.1080/03610927608827392},
  abstract = {Contrary t o ideas suggested by the t i t l e o f the conference a t which t h e present paper was presented, t h e a u t h o r i s n o t aware o f a conceptual difference between a " t e s t o f a s t a t i s t i c a l hypothesis" and a " t e s t o f s i g n i f i c a n c e " and uses these terms i n t e r changeably. A study o f any serious substantive problem involves a sequence o f i n c i d e n t s a t which one i s forced t o pause and cons i d e r what t o do next. I n an e f f o r t t o reduce the frequency o f misdirected a c t i v i t i e s one uses s t a t i s t i c a l tests. The procedure i s i l l u s t r a t e d on two examples: (i)Le Cam's (and a s s o c i a t e s ' ) study o f imnunotherapy o f cancer and (ii)a socio-economic experiment r e l a t i n g t o low-income homeownership problems.},
  file = {/Users/qualia/Documents/Papers/1976 - Neyman - Tests of statistical hypotheses and their use in studies of natural phenomena.pdf;/Users/qualia/Documents/Papers/Neyman - 1976 - Tests of statistical hypotheses and their use in s.pdf},
  journal = {Communications in Statistics - Theory and Methods},
  language = {en},
  number = {8}
}

@article{Neymotin2011,
  title = {Synaptic Information Transfer in Computer Models of Neocortical Columns},
  author = {Neymotin, Samuel A. and Jacobs, Kimberle M. and Fenton, Andr{\'e} A. and Lytton, William W.},
  year = {2011},
  month = feb,
  volume = {30},
  pages = {69--84},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-010-0253-4},
  file = {/Users/qualia/Documents/Papers/2011 - Neymotin et al. - Synaptic information transfer in computer models of neocortical columns(2).pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1}
}

@article{Neymotin2013,
  title = {Ih {{Tunes Theta}}/{{Gamma Oscillations}} and {{Cross}}-{{Frequency Coupling In}} an {{In Silico CA3 Model}}},
  author = {Neymotin, Samuel A. and Hilscher, Markus M. and Moulin, Thiago C. and Skolnick, Yosef and Lazarewicz, Maciej T. and Lytton, William W.},
  editor = {Cymbalyuk, Gennady},
  year = {2013},
  month = oct,
  volume = {8},
  pages = {e76285},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0076285},
  abstract = {Ih channels are uniquely positioned to act as neuromodulatory control points for tuning hippocampal theta (4\textendash{}12 Hz) and gamma (w25 Hz) oscillations, oscillations which are thought to have importance for organization of information flow. Ih contributes to neuronal membrane resonance and resting membrane potential, and is modulated by second messengers. We investigated Ih oscillatory control using a multiscale computer model of hippocampal CA3, where each cell class (pyramidal, basket, and oriens-lacunosum moleculare cells), contained type-appropriate isoforms of Ih. Our model demonstrated that modulation of pyramidal and basket Ih allows tuning theta and gamma oscillation frequency and amplitude. Pyramidal Ih also controlled cross-frequency coupling (CFC) and allowed shifting gamma generation towards particular phases of the theta cycle, effected via Ih 's ability to set pyramidal excitability. Our model predicts that in vivo neuromodulatory control of Ih allows flexibly controlling CFC and the timing of gamma discharges at particular theta phases.},
  file = {/Users/qualia/Documents/Papers/2013 - Neymotin et al. - Ih Tunes ThetaGamma Oscillations and Cross-Frequency Coupling In an In Silico CA3 Model.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {10}
}

@article{Ng1999,
  title = {Policy Invariance under Reward Transformations: {{Theory}} and Application to Reward Shaping.},
  author = {Ng, Andrew and Harada, Daishi and Russell, Stuart},
  year = {1999},
  pages = {278--287},
  file = {/Users/qualia/Documents/Papers/Ng et al. - 1999 - Policy invariance under reward transformations Th.pdf},
  journal = {In Proceedings of the Sixteenth International Conference on Machine Learning}
}

@article{Ngamga2007,
  title = {Recurrence Analysis of Strange Nonchaotic Dynamics},
  author = {Ngamga, E. J. and Nandi, A. and Ramaswamy, R. and Romano, M. C. and Thiel, M. and Kurths, J.},
  year = {2007},
  month = mar,
  volume = {75},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.75.036222},
  file = {/Users/qualia/Documents/Papers/2008 - Ngamga et al. - Recurrence analysis of strange nonchaotic dynamics in driven excitable systems.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {3}
}

@article{Nichols2002,
  title = {Nonparametric Permutation Tests for Functional Neuroimaging: {{A}} Primer with Examples},
  shorttitle = {Nonparametric Permutation Tests for Functional Neuroimaging},
  author = {Nichols, Thomas E. and Holmes, Andrew P.},
  year = {2002},
  month = jan,
  volume = {15},
  pages = {1--25},
  issn = {1065-9471, 1097-0193},
  doi = {10.1002/hbm.1058},
  abstract = {Requiring only minimal assumptions for validity, nonparametric permutation testing provides a flexible and intuitive methodology for the statistical analysis of data from functional neuroimaging experiments, at some computational expense. Introduced into the functional neuroimaging literature by Holmes et al. ([1996]: J Cereb Blood Flow Metab 16:7\textendash{}22), the permutation approach readily accounts for the multiple comparisons problem implicit in the standard voxel-by-voxel hypothesis testing framework. When the appropriate assumptions hold, the nonparametric permutation approach gives results similar to those obtained from a comparable Statistical Parametric Mapping approach using a general linear model with multiple comparisons corrections derived from random field theory. For analyses with low degrees of freedom, such as single subject PET/SPECT experiments or multi-subject PET/SPECT or fMRI designs assessed for population effects, the nonparametric approach employing a locally pooled (smoothed) variance estimate can outperform the comparable Statistical Parametric Mapping approach. Thus, these nonparametric techniques can be used to verify the validity of less computationally expensive parametric approaches. Although the theory and relative advantages of permutation approaches have been discussed by various authors, there has been no accessible explication of the method, and no freely distributed software implementing it. Consequently, there have been few practical applications of the technique. This article, and the accompanying MATLAB software, attempts to address these issues. The standard nonparametric randomization and permutation testing ideas are developed at an accessible level, using practical examples from functional neuroimaging, and the extensions for multiple comparisons described. Three worked examples from PET and fMRI are presented, with discussion, and comparisons with standard parametric approaches made where appropriate. Practical considerations are given throughout, and relevant statistical concepts are expounded in appendices. Hum. Brain Mapping 15:1\textendash{}25, 2001. \textcopyright{} 2001 Wiley-Liss, Inc.},
  file = {/Users/qualia/Documents/Papers/2002 - Nichols, Holmes - Nonparametric permutation tests for functional neuroimaging a primer with examples.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {1}
}

@article{Nicholson2019,
  title = {Is the Cell Really a Machine?},
  author = {Nicholson, Daniel J.},
  year = {2019},
  month = sep,
  volume = {477},
  pages = {108--126},
  issn = {00225193},
  doi = {10.1016/j.jtbi.2019.06.002},
  abstract = {It has become customary to conceptualize the living cell as an intricate piece of machinery, different to a man-made machine only in terms of its superior complexity. This familiar understanding grounds the conviction that a cell's organization can be explained reductionistically, as well as the idea that its molecular pathways can be construed as deterministic circuits. The machine conception of the cell owes a great deal of its success to the methods traditionally used in molecular biology. However, the recent introduction of novel experimental techniques capable of tracking individual molecules within cells in real time is leading to the rapid accumulation of data that are inconsistent with an engineering view of the cell. This paper examines four major domains of current research in which the challenges to the machine conception of the cell are particularly pronounced: cellular architecture, protein complexes, intracellular transport, and cellular behaviour. It argues that a new theoretical understanding of the cell is emerging from the study of these phenomena which emphasizes the dynamic, self-organizing nature of its constitution, the fluidity and plasticity of its components, and the stochasticity and non-linearity of its underlying processes.},
  file = {/Users/qualia/Documents/Papers/Nicholson - 2019 - Is the cell really a machine.pdf},
  journal = {Journal of Theoretical Biology},
  language = {en}
}

@article{Nir2008,
  title = {{{BOLD}} and Spiking Activity},
  author = {Nir, Yuval and Dinstein, Ilan and Malach, Rafael and Heeger, David J},
  year = {2008},
  month = may,
  volume = {11},
  pages = {523--524},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn0508-523},
  file = {/Users/qualia/Documents/Papers/2008 - Malach, Nir, Heeger - Co r r e s p on d e n c e.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Nishimoto2011,
  title = {Reconstructing {{Visual Experiences}} from {{Brain Activity Evoked}} by {{Natural Movies}}},
  author = {Nishimoto, Shinji and Vu, An T. and Naselaris, Thomas and Benjamini, Yuval and Yu, Bin and Gallant, Jack L.},
  year = {2011},
  month = oct,
  volume = {21},
  pages = {1641--1646},
  issn = {09609822},
  doi = {10.1016/j.cub.2011.08.031},
  abstract = {Quantitative modeling of human brain activity can provide crucial insights about cortical representations [1, 2] and can form the basis for brain decoding devices [3\textendash{}5]. Recent functional magnetic resonance imaging (fMRI) studies have modeled brain activity elicited by static visual patterns and have reconstructed these patterns from brain activity [6\textendash{}8]. However, blood oxygen level-dependent (BOLD) signals measured via fMRI are very slow [9], so it has been difficult to model brain activity elicited by dynamic stimuli such as natural movies. Here we present a new motion-energy [10, 11] encoding model that largely overcomes this limitation. The model describes fast visual information and slow hemodynamics by separate components. We recorded BOLD signals in occipitotemporal visual cortex of human subjects who watched natural movies and fit the model separately to individual voxels. Visualization of the fit models reveals how early visual areas represent the information in movies. To demonstrate the power of our approach, we also constructed a Bayesian decoder [8] by combining estimated encoding models with a sampled natural movie prior. The decoder provides remarkable reconstructions of the viewed movies. These results demonstrate that dynamic brain activity measured under naturalistic conditions can be decoded using current fMRI technology.},
  file = {/Users/qualia/Documents/Papers/2011 - Nishimoto et al. - Reconstructing visual experiences from brain activity evoked by natural movies.pdf},
  journal = {Current Biology},
  language = {en},
  number = {19}
}

@article{Niu2011,
  title = {{{HOGWILD}}!: {{A Lock}}-{{Free Approach}} to {{Parallelizing Stochastic Gradient Descent}}},
  shorttitle = {{{HOGWILD}}!},
  author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
  year = {2011},
  month = jun,
  abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called Hogwild! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then Hogwild! achieves a nearly optimal rate of convergence. We demonstrate experimentally that Hogwild! outperforms alternative schemes that use locking by an order of magnitude.},
  archivePrefix = {arXiv},
  eprint = {1106.5730},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2011 - Niu et al. - HOGWILD! A Lock-Free Approach to Parallelizing Stochastic Gradient Descent.pdf},
  journal = {arXiv:1106.5730 [cs, math]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  language = {en},
  primaryClass = {cs, math}
}

@article{Niv2005,
  title = {How Fast to Work: {{Response}} Vigor, Motivation and Tonic Dopamine},
  author = {Niv, Yael and Daw, Nathaniel D and Dayan, Peter},
  year = {2005},
  volume = {18},
  pages = {8},
  abstract = {Reinforcement learning models have long promised to unify computational, psychological and neural accounts of appetitively conditioned behavior. However, the bulk of data on animal conditioning comes from free-operant experiments measuring how fast animals will work for reinforcement. Existing reinforcement learning (RL) models are silent about these tasks, because they lack any notion of vigor. They thus fail to address the simple observation that hungrier animals will work harder for food, as well as stranger facts such as their sometimes greater productivity even when working for irrelevant outcomes such as water. Here, we develop an RL framework for free-operant behavior, suggesting that subjects choose how vigorously to perform selected actions by optimally balancing the costs and benefits of quick responding. Motivational states such as hunger shift these factors, skewing the tradeoff. This accounts normatively for the effects of motivation on response rates, as well as many other classic findings. Finally, we suggest that tonic levels of dopamine may be involved in the computation linking motivational state to optimal responding, thereby explaining the complex vigor-related effects of pharmacological manipulation of dopamine.},
  file = {/Users/qualia/Documents/Papers/Niv et al. - How fast to work Response vigor, motivation and t.pdf},
  language = {en}
}

@article{Nogaret2016,
  title = {Automatic {{Construction}} of {{Predictive Neuron Models}} through {{Large Scale Assimilation}} of {{Electrophysiological Data}}},
  author = {Nogaret, Alain and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2016},
  month = dec,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep32749},
  file = {/Users/qualia/Documents/Papers/Nogaret et al. - 2016 - Automatic Construction of Predictive Neuron Models 2.pdf;/Users/qualia/Documents/Papers/Nogaret et al. - 2016 - Automatic Construction of Predictive Neuron Models 3.pdf;/Users/qualia/Documents/Papers/Nogaret et al. - 2016 - Automatic Construction of Predictive Neuron Models.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Nolte2018,
  title = {Cortical Reliability amid Noise and Chaos},
  author = {Nolte, Max and Reimann, Michael W and King, James G and Markram, Henry and Muller, Eilif B},
  year = {2018},
  month = jun,
  doi = {10.1101/304121},
  abstract = {Typical responses of cortical neurons to identical sensory stimuli are highly variable. It has thus been proposed that the cortex primarily uses a rate code. However, other studies have argued for spike-time coding under certain conditions. The potential role of spike-time coding is constrained by the intrinsic variability of cortical circuits, which remains largely unexplored. Here, we quantified this intrinsic variability using a biophysical model of rat neocortical microcircuitry with biologically realistic noise sources. We found that stochastic neurotransmitter release is a critical component of this variability, which, amplified by recurrent connectivity, causes rapid chaotic divergence with a time constant on the order of 10-20 milliseconds. Surprisingly, weak thalamocortical stimuli can transiently overcome the chaos, and induce reliable spike times with millisecond precision. We show that this effect relies on recurrent cortical connectivity, and is not a simple effect of feed-forward thalamocortical input. We conclude that recurrent cortical architecture supports millisecond spike-time reliability amid noise and chaotic network dynamics, resolving a long-standing debate.},
  file = {/Users/qualia/Documents/Papers/Nolte et al. - 2018 - Cortical reliability amid noise and chaos.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Noonan2016,
  title = {Distinct {{Mechanisms}} for {{Distractor Suppression}} and {{Target Facilitation}}},
  author = {Noonan, M. P. and Adamian, N. and Pike, A. and Printzlau, F. and Crittenden, B. M. and Stokes, M. G.},
  year = {2016},
  month = feb,
  volume = {36},
  pages = {1797--1807},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2133-15.2016},
  file = {/Users/qualia/Documents/Papers/Noonan et al. - 2016 - Distinct Mechanisms for Distractor Suppression and.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {6}
}

@article{Norman2006,
  title = {Beyond Mind-Reading: Multi-Voxel Pattern Analysis of {{fMRI}} Data},
  shorttitle = {Beyond Mind-Reading},
  author = {Norman, Kenneth A. and Polyn, Sean M. and Detre, Greg J. and Haxby, James V.},
  year = {2006},
  month = sep,
  volume = {10},
  pages = {424--430},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.07.005},
  file = {/Users/qualia/Documents/Papers/2006 - Norman et al. - Beyond mind-reading multi-voxel pattern analysis of fMRI data.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {9}
}

@article{Norton2003,
  title = {Can Ultrasound Be Used to Stimulate Nerve Tissue?},
  author = {Norton, Stephen J},
  year = {2003},
  pages = {9},
  abstract = {Background: The stimulation of nerve or cortical tissue by magnetic induction is a relatively new tool for the non-invasive study of the brain and nervous system. Transcranial magnetic stimulation (TMS), for example, has been used for the functional mapping of the motor cortex and may have potential for treating a variety of brain disorders.
Methods and Results: A new method of stimulating active tissue is proposed by propagating ultrasound in the presence of a magnetic field. Since tissue is conductive, particle motion created by an ultrasonic wave will induce an electric current density generated by Lorentz forces. An analytical derivation is given for the electric field distribution induced by a collimated ultrasonic beam. An example shows that peak electric fields of up to 8 V/m appear to be achievable at the upper range of diagnostic intensities. This field strength is about an order of magnitude lower than fields typically associated with TMS; however, the electric field gradients induced by ultrasound can be quite high (about 60 kV/m2 at 4 MHz), which theoretically play a more important role in activation than the field magnitude. The latter value is comparable to TMS-induced gradients.
Conclusion: The proposed method could be used to locally stimulate active tissue by inducing an electric field in regions where the ultrasound is focused. Potential advantages of this method compared to TMS is that stimulation of cortical tissue could be highly localized as well as achieved at greater depths in the brain than is currently possible with TMS.},
  file = {/Users/qualia/Documents/Papers/2003 - Norton - Can ultrasound be used to stimulate nerve tissue.pdf},
  journal = {BioMedical Engineering OnLine},
  language = {en}
}

@article{Nosofsky1985,
  title = {Overall Similarity and the Identification of Separable-Dimension Stimuli: {{A}} Choice Model Analysis},
  shorttitle = {Overall Similarity and the Identification of Separable-Dimension Stimuli},
  author = {Nosofsky, Robert},
  year = {1985},
  month = sep,
  volume = {38},
  pages = {415--432},
  issn = {0031-5117, 1532-5962},
  doi = {10.3758/BF03207172},
  file = {/Users/qualia/Documents/Papers/1985 - Nosofsky - Overall similarity and the identification of separable-dimension stimuli a choice model analysis.pdf},
  journal = {Perception \& Psychophysics},
  language = {en},
  number = {5}
}

@article{Nunes2016,
  title = {Multi-Alternative Decision-Making with Non-Stationary Inputs},
  author = {Nunes, Luana F. and Gurney, Kevin},
  year = {2016},
  month = aug,
  volume = {3},
  pages = {160376},
  issn = {2054-5703},
  doi = {10.1098/rsos.160376},
  file = {/Users/qualia/Documents/Papers/Nunes and Gurney - 2016 - Multi-alternative decision-making with non-station.pdf},
  journal = {Royal Society Open Science},
  language = {en},
  number = {8}
}

@article{Nutt2001,
  title = {Interactions between Deep Brain Stimulation and Levodopa in {{Parkinson}}'s Disease},
  author = {Nutt, J.G. and Rufener, S.L. and Carter, J.H. and Anderson, V.C. and Pahwa, R. and Hammerstad, J.P. and Burchiel, K.J.},
  year = {2001},
  month = nov,
  volume = {57},
  pages = {1835--1842},
  issn = {0028-3878, 1526-632X},
  doi = {10.1212/WNL.57.10.1835},
  file = {/Users/qualia/Documents/Papers/2001 - Nutt et al. - Interactions between deep brain stimulation and levodopa in Parkinson ’ s disease.pdf},
  journal = {Neurology},
  language = {en},
  number = {10}
}

@article{Oakden-Rayner2019,
  title = {Hidden {{Stratification Causes Clinically Meaningful Failures}} in {{Machine Learning}} for {{Medical Imaging}}},
  author = {{Oakden-Rayner}, Luke and Dunnmon, Jared and Carneiro, Gustavo and R{\'e}, Christopher},
  year = {2019},
  month = sep,
  abstract = {Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model still consistently misses a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring and describing hidden stratification effects, and characterize these effects both on multiple medical imaging datasets and via synthetic experiments on the well-characterised CIFAR-100 benchmark dataset. We find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20\% on clinically important subsets. Finally, we explore the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.},
  archivePrefix = {arXiv},
  eprint = {1909.12475},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Oakden-Rayner et al. - 2019 - Hidden Stratification Causes Clinically Meaningful.pdf},
  journal = {arXiv:1909.12475 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Oberheim2006,
  title = {Astrocytic Complexity Distinguishes the Human Brain},
  author = {Oberheim, Nancy Ann and Wang, Xiaohai and Goldman, Steven and Nedergaard, Maiken},
  year = {2006},
  month = oct,
  volume = {29},
  pages = {547--553},
  issn = {01662236},
  doi = {10.1016/j.tins.2006.08.004},
  file = {/Users/qualia/Documents/Papers/Oberheim et al. - 2006 - Astrocytic complexity distinguishes the human brai.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {10}
}

@article{Oberheim2009,
  title = {Uniquely {{Hominid Features}} of {{Adult Human Astrocytes}}},
  author = {Oberheim, N. A. and Takano, T. and Han, X. and He, W. and Lin, J. H. C. and Wang, F. and Xu, Q. and Wyatt, J. D. and Pilcher, W. and Ojemann, J. G. and Ransom, B. R. and Goldman, S. A. and Nedergaard, M.},
  year = {2009},
  month = mar,
  volume = {29},
  pages = {3276--3287},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4707-08.2009},
  file = {/Users/qualia/Documents/Papers/Oberheim et al. - 2009 - Uniquely Hominid Features of Adult Human Astrocyte.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {10}
}

@article{Ocker2014,
  title = {Kv7 Channels Regulate Pairwise Spiking Covariability in Health and Disease},
  author = {Ocker, Gabriel Koch and Doiron, Brent},
  year = {2014},
  month = jul,
  volume = {112},
  pages = {340--352},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00084.2014},
  file = {/Users/qualia/Documents/Papers/Ocker and Doiron - 2014 - Kv7 channels regulate pairwise spiking covariabili.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Ocker2017,
  title = {Linking Structure and Activity in Nonlinear Spiking Networks},
  author = {Ocker, Gabriel Koch and Josi{\'c}, Kre{\v s}imir and {Shea-Brown}, Eric and Buice, Michael A.},
  year = {2017},
  month = mar,
  doi = {10.1101/080705},
  abstract = {Recent experimental advances are producing an avalanche of data on both neural connectivity and neural activity. To take full advantage of these two emerging datasets we need a framework that links them, revealing how collective neural activity arises from the structure of neural connectivity and intrinsic neural dynamics. This problem of structure-driven activity has drawn major interest in computational neuroscience. Existing methods for relating activity and architecture in spiking networks rely on linearizing activity around a central operating point and thus fail to capture the nonlinear responses of individual neurons that are the hallmark of neural information processing. Here, we overcome this limitation and present a new relationship between connectivity and activity in networks of nonlinear spiking neurons by developing a diagrammatic fluctuation expansion based on statistical field theory. We explicitly show how recurrent network structure produces pairwise and higher-order correlated activity, and how nonlinearities impact the networks' spiking activity. Our findings open new avenues to investigating how single-neuron nonlinearities\textemdash{}including those of different cell types\textemdash{}combine with connectivity to shape population activity and function.},
  file = {/Users/qualia/Documents/Papers/Ocker et al. - 2017 - Linking structure and activity in nonlinear spikin 2.pdf;/Users/qualia/Documents/Papers/Ocker et al. - 2017 - Linking structure and activity in nonlinear spikin.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{OConnor2016,
  title = {Deep {{Spiking Networks}}},
  author = {O'Connor, Peter and Welling, Max},
  year = {2016},
  month = feb,
  abstract = {We introduce an algorithm to do backpropagation on a spiking network. Our network is "spiking" in the sense that our neurons accumulate their activation into a potential over time, and only send out a signal (a ``spike'') when this potential crosses a threshold and the neuron is reset. Neurons only update their states when receiving signals from other neurons. Total computation of the network thus scales with the number of spikes caused by an input rather than network size. We show that the spiking Multi-Layer Perceptron behaves identically, during both prediction and training, to a conventional deep network of rectified-linear units, in the limiting case where we run the spiking network for a long time. We apply this architecture to a conventional classification problem (MNIST) and achieve performance very close to that of a conventional Multi-Layer Perceptron with the same architecture. Our network is a natural architecture for learning based on streaming event-based data, and is a stepping stone towards using spiking neural networks to learn efficiently on streaming data.},
  archivePrefix = {arXiv},
  eprint = {1602.08323},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/O'Connor and Welling - 2016 - Deep Spiking Networks.pdf},
  journal = {arXiv:1602.08323 [cs]},
  keywords = {68T01,Computer Science - Neural and Evolutionary Computing,F.1.1},
  language = {en},
  primaryClass = {cs}
}

@article{Ogawa2011,
  title = {Neural Representation of Observed Actions in the Parietal and Premotor Cortex},
  author = {Ogawa, Kenji and Inui, Toshio},
  year = {2011},
  month = may,
  volume = {56},
  pages = {728--735},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.10.043},
  abstract = {We investigated the neural representation of observed actions in the human parietal and premotor cortex, which comprise the action observation network or the mirror neuron system for action recognition. Participants observed object-directed hand actions, in which action as well as other properties were independently manipulated: action (grasp or touch), object (cup or bottle), perspective (1st or 3rd person), hand (right or left), and image size (large or small). We then used multi-voxel pattern analysis to determine whether each feature could be correctly decoded from regional activities. The early visual area showed significant above-chance classification accuracy, particularly high in perspective, hand, and size, consistent with pixel-wise dissimilarity of stimuli. In contrast, the highest decoding accuracy for action was observed in the anterior intraparietal sulcus (aIPS) and the ventral premotor cortex (PMv). Moreover, the decoder for action could be correctly generalized for images with high dissimilarity in the parietal and premotor region, but not in the visual area. Our study indicates that the parietal and premotor regions encode observed actions independent of retinal variations, which may subserve our capacity for invariant action recognition of others. \textcopyright{} 2010 Elsevier Inc. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2011 - Ogawa, Inui - Neural representation of observed actions in the parietal and premotor cortex.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Ogunmolu,
  title = {Nonlinear {{Systems Identification Using Deep Dynamic Neural Networks}}},
  author = {Ogunmolu, Olalekan and Gu, Xuejun and Jiang, Steve and Gans, Nicholas},
  pages = {8},
  abstract = {Neural networks are known to be effective function approximators. Recently, deep neural networks have proven to be very effective in pattern recognition, classification tasks and human-level control to model highly nonlinear realworld systems. This paper investigates the effectiveness of deep neural networks in the modeling of dynamical systems with complex behavior. Three deep neural network structures are trained on sequential data, and we investigate the effectiveness of these networks in modeling associated characteristics of the underlying dynamical systems. We carry out similar evaluations on select publicly available system identification datasets. We demonstrate that deep neural networks are effective model estimators from input-output data.},
  file = {/Users/qualia/Documents/Papers/Ogunmolu et al. - Nonlinear Systems Identiﬁcation Using Deep Dynamic.pdf},
  language = {en}
}

@article{OHare2017,
  title = {Striatal Fast-Spiking Interneurons Selectively Modulate Circuit Output and Are Required for Habitual Behavior},
  author = {O'Hare, Justin K and Li, Haofang and Kim, Namsoo and Gaidis, Erin and Ade, Kristen and Beck, Jeff and Yin, Henry and Calakos, Nicole},
  year = {2017},
  month = sep,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.26231},
  file = {/Users/qualia/Documents/Papers/O'Hare et al. - 2017 - Striatal fast-spiking interneurons selectively mod.pdf},
  journal = {eLife},
  language = {en}
}

@article{Ohyama2015,
  title = {A Multilevel Multimodal Circuit Enhances Action Selection in {{Drosophila}}},
  author = {Ohyama, Tomoko and {Schneider-Mizell}, Casey M. and Fetter, Richard D. and Aleman, Javier Valdes and Franconville, Romain and {Rivera-Alba}, Marta and Mensh, Brett D. and Branson, Kristin M. and Simpson, Julie H. and Truman, James W. and Cardona, Albert and Zlatic, Marta},
  year = {2015},
  month = apr,
  volume = {520},
  pages = {633--639},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14297},
  file = {/Users/qualia/Documents/Papers/Ohyama et al. - 2015 - A multilevel multimodal circuit enhances action se.pdf},
  journal = {Nature},
  language = {en},
  number = {7549}
}

@article{OKeeffe2015,
  title = {Synchronization as {{Aggregation}}: {{Cluster Kinetics}} of {{Pulse}}-{{Coupled Oscillators}}},
  shorttitle = {Synchronization as {{Aggregation}}},
  author = {O'Keeffe, Kevin P. and Krapivsky, P. L. and Strogatz, Steven H.},
  year = {2015},
  month = aug,
  volume = {115},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.115.064101},
  file = {/Users/qualia/Documents/Papers/2015 - O'Keeffe, Krapivsky, Strogatz - Synchronization as Aggregation Cluster Kinetics of Pulse-Coupled Oscillators.pdf;/Users/qualia/Documents/Papers/O’Keeffe et al. - 2015 - Synchronization as Aggregation Cluster Kinetics o.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {6}
}

@article{OKeeffe2016,
  title = {Dynamics of a Population of Oscillatory and Excitable Elements},
  author = {O'Keeffe, Kevin P. and Strogatz, Steven H.},
  year = {2016},
  month = jun,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.062203},
  file = {/Users/qualia/Documents/Papers/O'Keeffe and Strogatz - 2016 - Dynamics of a population of oscillatory and excita.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{OKeeffe2017,
  title = {Oscillators That Sync and Swarm},
  author = {O'Keeffe, Kevin P. and Hong, Hyunsuk and Strogatz, Steven H.},
  year = {2017},
  month = dec,
  volume = {8},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-01190-3},
  file = {/Users/qualia/Documents/Papers/O’Keeffe et al. - 2017 - Oscillators that sync and swarm.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Okun2015,
  title = {Diverse Coupling of Neurons to Populations in Sensory Cortex},
  author = {Okun, Michael and Steinmetz, Nicholas A. and Cossell, Lee and Iacaruso, M. Florencia and Ko, Ho and Barth{\'o}, P{\'e}ter and Moore, Tirin and Hofer, Sonja B. and {Mrsic-Flogel}, Thomas D. and Carandini, Matteo and Harris, Kenneth D.},
  year = {2015},
  month = may,
  volume = {521},
  pages = {511--515},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14273},
  file = {/Users/qualia/Documents/Papers/Okun et al. - 2015 - Diverse coupling of neurons to populations in sens.pdf},
  journal = {Nature},
  language = {en},
  number = {7553}
}

@article{OLeary2011,
  title = {Neuronal Homeostasis: Time for a Change?: {{Neuronal}} Homeostasis: Time for a Change?},
  shorttitle = {Neuronal Homeostasis},
  author = {O'Leary, Timothy and Wyllie, David J. A.},
  year = {2011},
  month = oct,
  volume = {589},
  pages = {4811--4826},
  issn = {00223751},
  doi = {10.1113/jphysiol.2011.210179},
  abstract = {Homeostatic processes that regulate electrical activity in neurones are now an established aspect of physiology and rest on a large body of experimental evidence that points to roles in development, learning and memory, and disease. However, the concepts underlying homeostasis are too often summarized in ways that restrict their explanatory power and obviate important subtleties. Here, we present a review of the underlying theory of homeostasis \textendash{} control theory \textendash{} in an attempt to reconcile some existing conceptual problems in the context of neuronal physiology. In addition to clarifying the underlying theory, this review highlights the remaining challenges posed when analysing homeostatic phenomena that underlie the regulation of neuronal excitability. Moreover, we suggest approaches for future experimental and computational work that will further our understanding of neuronal homeostasis and the fundamental neurophysiological functions it serves.},
  file = {/Users/qualia/Documents/Papers/2011 - O'Leary, Wyllie - Neuronal homeostasis Time for a change.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {20}
}

@article{OLeary2013,
  title = {Correlations in Ion Channel Expression Emerge from Homeostatic Tuning Rules},
  author = {O'Leary, T. and Williams, A. H. and Caplan, J. S. and Marder, E.},
  year = {2013},
  month = jul,
  volume = {110},
  pages = {E2645-E2654},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1309966110},
  file = {/Users/qualia/Documents/Papers/2013 - O'Leary et al. - Correlations in ion channel expression emerge from homeostatic tuning rules.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {28}
}

@article{OLeary2014,
  title = {Cell {{Types}}, {{Network Homeostasis}}, and {{Pathological Compensation}} from a {{Biologically Plausible Ion Channel Expression Model}}},
  author = {O'Leary, Timothy and Williams, Alex H. and Franci, Alessio and Marder, Eve},
  year = {2014},
  month = may,
  volume = {82},
  pages = {809--821},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.04.002},
  abstract = {How do neurons develop, control, and maintain their electrical signaling properties in spite of ongoing protein turnover and perturbations to activity? From generic assumptions about the molecular biology underlying channel expression, we derive a simple model and show how it encodes an ``activity set point'' in single neurons. The model generates diverse self-regulating cell types and relates correlations in conductance expression observed in vivo to underlying channel expression rates. Synaptic as well as intrinsic conductances can be regulated to make a self-assembling central pattern generator network; thus, network-level homeostasis can emerge from cell-autonomous regulation rules. Finally, we demonstrate that the outcome of homeostatic regulation depends on the complement of ion channels expressed in cells: in some cases, loss of specific ion channels can be compensated; in others, the homeostatic mechanism itself causes pathological loss of function.},
  file = {/Users/qualia/Documents/Papers/2014 - O'Leary et al. - Cell Types, Network Homeostasis, and Pathological Compensation from a Biologically Plausible Ion Channel Express.pdf;/Users/qualia/Documents/Papers/O’Leary et al. - 2014 - Cell Types, Network Homeostasis, and Pathological .pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{OLeary2014a,
  title = {Temperature-Robust Neural Activity Using Feedback Control of Ion Channel Expression},
  author = {O'Leary, Timothy and Marder, Eve},
  year = {2014},
  month = jul,
  volume = {15},
  issn = {1471-2202},
  doi = {10.1186/1471-2202-15-S1-P103},
  file = {/Users/qualia/Documents/Papers/2014 - O’Leary, Marder - Temperature-robust neural activity using feedback control of ion channel expression.pdf;/Users/qualia/Documents/Papers/O’Leary and Marder - 2014 - Temperature-robust neural activity using feedback .pdf},
  journal = {BMC Neuroscience},
  language = {en},
  number = {S1}
}

@article{Oliveira2010,
  title = {Transcranial Magnetic Stimulation of Posterior Parietal Cortex Affects Decisions of Hand Choice},
  author = {Oliveira, Flavio T. P. and Diedrichsen, J{\"o}rn and Verstynen, Timothy and Duque, Julie and Ivry, Richard B.},
  year = {2010},
  month = oct,
  volume = {107},
  pages = {17751--17756},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1006223107},
  file = {/Users/qualia/Documents/Papers/2010 - Oliveira et al. - Transcranial magnetic stimulation of posterior parietal cortex affects decisions of hand choice.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {41}
}

@article{Olsen2012,
  title = {Gain Control by Layer Six in Cortical Circuits of Vision},
  author = {Olsen, Shawn R. and Bortone, Dante S. and Adesnik, Hillel and Scanziani, Massimo},
  year = {2012},
  month = mar,
  volume = {483},
  pages = {47--52},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature10835},
  file = {/Users/qualia/Documents/Papers/2012 - Olsen et al. - Gain control by layer six in cortical circuits of vision.pdf},
  journal = {Nature},
  language = {en},
  number = {7387}
}

@article{Onslow2014,
  title = {A {{Canonical Circuit}} for {{Generating Phase}}-{{Amplitude Coupling}}},
  author = {Onslow, Angela C. E. and Jones, Matthew W. and Bogacz, Rafal},
  editor = {Tort, Adriano B. L.},
  year = {2014},
  month = aug,
  volume = {9},
  pages = {e102591},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0102591},
  abstract = {Phase amplitude coupling' (PAC) in oscillatory neural activity describes a phenomenon whereby the amplitude of higher frequency activity is modulated by the phase of lower frequency activity. Such coupled oscillatory activity \textendash{} also referred to as `cross-frequency coupling' or `nested rhythms' \textendash{} has been shown to occur in a number of brain regions and at behaviorally relevant time points during cognitive tasks; this suggests functional relevance, but the circuit mechanisms of PAC generation remain unclear. In this paper we present a model of a canonical circuit for generating PAC activity, showing how interconnected excitatory and inhibitory neural populations can be periodically shifted in to and out of oscillatory firing patterns by afferent drive, hence generating higher frequency oscillations phase-locked to a lower frequency, oscillating input signal. Since many brain regions contain mutually connected excitatory-inhibitory populations receiving oscillatory input, the simplicity of the mechanism generating PAC in such networks may explain the ubiquity of PAC across diverse neural systems and behaviors. Analytic treatment of this circuit as a nonlinear dynamical system demonstrates how connection strengths and inputs to the populations can be varied in order to change the extent and nature of PAC activity, importantly which phase of the lower frequency rhythm the higher frequency activity is locked to. Consequently, this model can inform attempts to associate distinct types of PAC with different network topologies and physiologies in real data.},
  file = {/Users/qualia/Documents/Papers/Onslow et al. - 2014 - A Canonical Circuit for Generating Phase-Amplitude.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {8}
}

@article{Oosterhof2011,
  title = {A Comparison of Volume-Based and Surface-Based Multi-Voxel Pattern Analysis},
  author = {Oosterhof, Nikolaas N. and Wiestler, Tobias and Downing, Paul E. and Diedrichsen, J{\"o}rn},
  year = {2011},
  month = may,
  volume = {56},
  pages = {593--600},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.04.270},
  abstract = {For functional magnetic resonance imaging (fMRI), multi-voxel pattern analysis (MVPA) has been shown to be a sensitive method to detect areas that encode certain stimulus dimensions. By moving a searchlight through the volume of the brain, one can continuously map the information content about the experimental conditions of interest to the brain.},
  file = {/Users/qualia/Documents/Papers/2011 - Oosterhof et al. - A comparison of volume-based and surface-based multi-voxel pattern analysis.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{OpdeBeeck2010,
  title = {Probing the Mysterious Underpinnings of Multi-Voxel {{fMRI}} Analyses},
  author = {{Op de Beeck}, Hans P.},
  year = {2010},
  month = apr,
  volume = {50},
  pages = {567--571},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.12.072},
  abstract = {Various arguments have been proposed for or against sub-voxel sensitivity or hyperacuity in functional magnetic resonance imaging (fMRI) at standard resolution. Sub-voxel sensitivity might exist, but nevertheless the performance of multi-voxel fMRI analyses is very likely to be dominated by a largerscale organization, even if this organization is very weak. Up to now, most arguments are indirect in nature: they do not in themselves proof or contradict sub-voxel sensitivity, but they are suggestive, seem consistent or not with sub-voxel sensitivity, or show that the principle might or might not work. Here the previously proposed smoothing argument against hyperacuity is extended with simulations that include more realistic signal, noise, and analysis properties than any of the simulations presented before. These simulations confirm the relevance of the smoothing approach to find out the scale of the functional maps that underlie the outcome of multi-voxel analyses, at least in relative terms (differences in the scale of different maps). However, image smoothing, like most other arguments in the literature, is an indirect argument, and at the end of the day such arguments are not sufficient to decide the issue on whether and how much sub-voxel maps contribute. A few suggestions are made about the type of evidence that is needed to help us understand the as yet mysterious underpinnings of multi-voxel fMRI analyses.},
  file = {/Users/qualia/Documents/Papers/2010 - Op de Beeck - Probing the mysterious underpinnings of multi-voxel fMRI analyses.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Osband2019,
  title = {Behaviour {{Suite}} for {{Reinforcement Learning}}},
  author = {Osband, Ian and Doron, Yotam and Hessel, Matteo and Aslanides, John and Sezener, Eren and Saraiva, Andre and McKinney, Katrina and Lattimore, Tor and Szepezvari, Csaba and Singh, Satinder and Van Roy, Benjamin and Sutton, Richard and Silver, David and Van Hasselt, Hado},
  year = {2019},
  month = aug,
  abstract = {This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.},
  archivePrefix = {arXiv},
  eprint = {1908.03568},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Osband et al. - 2019 - Behaviour Suite for Reinforcement Learning.pdf},
  journal = {arXiv:1908.03568 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Osborne1977,
  title = {The Free Food (Contrafreeloading) Phenomenon: {{A}} Review and Analysis},
  shorttitle = {The Free Food (Contrafreeloading) Phenomenon},
  author = {Osborne, Steve R.},
  year = {1977},
  month = sep,
  volume = {5},
  pages = {221--235},
  issn = {0090-4996, 1532-5830},
  doi = {10.3758/BF03209232},
  file = {/Users/qualia/Documents/Papers/Osborne - 1977 - The free food (contrafreeloading) phenomenon A re.pdf},
  journal = {Animal Learning \& Behavior},
  language = {en},
  number = {3}
}

@article{Osborne1977a,
  title = {The Free Food (Contrafreeloading) Phenomenon: {{A}} Review and Analysis},
  shorttitle = {The Free Food (Contrafreeloading) Phenomenon},
  author = {Osborne, Steve R.},
  year = {1977},
  month = sep,
  volume = {5},
  pages = {221--235},
  issn = {0090-4996, 1532-5830},
  doi = {10.3758/BF03209232},
  file = {/Users/qualia/Documents/Papers/Osborne - 1977 - The free food (contrafreeloading) phenomenon A re 2.pdf},
  journal = {Animal Learning \& Behavior},
  language = {en},
  number = {3}
}

@article{Ostojic2011,
  title = {From {{Spiking Neuron Models}} to {{Linear}}-{{Nonlinear Models}}},
  author = {Ostojic, Srdjan and Brunel, Nicolas},
  editor = {Latham, Peter E.},
  year = {2011},
  month = jan,
  volume = {7},
  pages = {e1001056},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1001056},
  abstract = {Neurons transform time-varying inputs into action potentials emitted stochastically at a time dependent rate. The mapping from current input to output firing rate is often represented with the help of phenomenological models such as the linearnonlinear (LN) cascade, in which the output firing rate is estimated by applying to the input successively a linear temporal filter and a static non-linear transformation. These simplified models leave out the biophysical details of action potential generation. It is not a priori clear to which extent the input-output mapping of biophysically more realistic, spiking neuron models can be reduced to a simple linear-nonlinear cascade. Here we investigate this question for the leaky integrate-andfire (LIF), exponential integrate-and-fire (EIF) and conductance-based Wang-Buzsa\textasciiacute{}ki models in presence of background synaptic activity. We exploit available analytic results for these models to determine the corresponding linear filter and static non-linearity in a parameter-free form. We show that the obtained functions are identical to the linear filter and static nonlinearity determined using standard reverse correlation analysis. We then quantitatively compare the output of the corresponding linear-nonlinear cascade with numerical simulations of spiking neurons, systematically varying the parameters of input signal and background noise. We find that the LN cascade provides accurate estimates of the firing rates of spiking neurons in most of parameter space. For the EIF and Wang-Buzsa\textasciiacute{}ki models, we show that the LN cascade can be reduced to a firing rate model, the timescale of which we determine analytically. Finally we introduce an adaptive timescale rate model in which the timescale of the linear filter depends on the instantaneous firing rate. This model leads to highly accurate estimates of instantaneous firing rates.},
  file = {/Users/qualia/Documents/Papers/2011 - Ostojic, Brunel - From Spiking Neuron Models to Linear-Nonlinear Models.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {1}
}

@article{Ostojic2014,
  title = {Two Types of Asynchronous Activity in Networks of Excitatory and Inhibitory Spiking Neurons},
  author = {Ostojic, Srdjan},
  year = {2014},
  month = apr,
  volume = {17},
  pages = {594--600},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3658},
  file = {/Users/qualia/Documents/Papers/Ostojic - 2014 - Two types of asynchronous activity in networks of .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{OToole2005,
  title = {Partially {{Distributed Representations}} of {{Objects}} and {{Faces}} in {{Ventral Temporal Cortex}}},
  author = {O'Toole, Alice J. and Jiang, Fang and Abdi, Herv{\'e} and Haxby, James V.},
  year = {2005},
  month = apr,
  volume = {17},
  pages = {580--590},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/0898929053467550},
  file = {/Users/qualia/Documents/Papers/2005 - O'Toole et al. - Partially distributed representations of objects and faces in ventral temporal cortex.pdf},
  journal = {Journal of Cognitive Neuroscience},
  language = {en},
  number = {4}
}

@article{OToole2007,
  title = {Theoretical, {{Statistical}}, and {{Practical Perspectives}} on {{Pattern}}-Based {{Classification Approaches}} to the {{Analysis}} of {{Functional Neuroimaging Data}}},
  author = {O'Toole, Alice J. and Jiang, Fang and Abdi, Herv{\'e} and P{\'e}nard, Nils and Dunlop, Joseph P. and Parent, Marc A.},
  year = {2007},
  month = nov,
  volume = {19},
  pages = {1735--1752},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn.2007.19.11.1735},
  file = {/Users/qualia/Documents/Papers/2007 - O'Toole et al. - Theoretical, statistical, and practical perspectives on pattern-based classification approaches to the analysis.pdf},
  journal = {Journal of Cognitive Neuroscience},
  language = {en},
  number = {11}
}

@article{Ottino-Loffler,
  title = {Evolutionary Dynamics of Incubation Periods},
  author = {{Ottino-Loffler}, Bertrand and Scott, Jacob G and Strogatz, Steven H},
  pages = {28},
  file = {/Users/qualia/Documents/Papers/Ottino-Loffler et al. - Evolutionary dynamics of incubation periods.pdf},
  language = {en}
}

@article{Ottino-Loffler2016,
  title = {Comparing the Locking Threshold for Rings and Chains of Oscillators},
  author = {{Ottino-L{\"o}ffler}, Bertrand and Strogatz, Steven H.},
  year = {2016},
  month = dec,
  volume = {94},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.94.062203},
  file = {/Users/qualia/Documents/Papers/Ottino-Löffler and Strogatz - 2016 - Comparing the locking threshold for rings and chai.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Ottino-Loffler2016a,
  title = {Frequency Spirals},
  author = {{Ottino-L{\"o}ffler}, Bertrand and Strogatz, Steven H.},
  year = {2016},
  month = sep,
  volume = {26},
  pages = {094804},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4954038},
  file = {/Users/qualia/Documents/Papers/Ottino-Löffler and Strogatz - 2016 - Frequency spirals.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {9}
}

@article{Ottino-Loffler2016b,
  title = {Kuramoto Model with Uniformly Spaced Frequencies: {{Finite}}- {{N}} Asymptotics of the Locking Threshold},
  shorttitle = {Kuramoto Model with Uniformly Spaced Frequencies},
  author = {{Ottino-L{\"o}ffler}, Bertrand and Strogatz, Steven H.},
  year = {2016},
  month = jun,
  volume = {93},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.93.062220},
  file = {/Users/qualia/Documents/Papers/Ottino-Löffler and Strogatz - 2016 - Kuramoto model with uniformly spaced frequencies .pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Pajevic2014,
  title = {Role of Myelin Plasticity in Oscillations and Synchrony of Neuronal Activity},
  author = {Pajevic, S. and Basser, P.J. and Fields, R.D.},
  year = {2014},
  month = sep,
  volume = {276},
  pages = {135--147},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2013.11.007},
  abstract = {Conduction time is typically ignored in computational models of neural network function. Here we consider the effects of conduction delays on the synchrony of neuronal activity and neural oscillators, and evaluate the consequences of allowing conduction velocity (CV) to be regulated adaptively. We propose that CV variation, mediated by myelin, could provide an important mechanism of activity-dependent nervous system plasticity. Even small changes in CV, resulting from small changes in myelin thickness or nodal structure, could have profound effects on neuronal network function in terms of spike-time arrival, oscillation frequency, oscillator coupling, and propagation of brain waves. For example, a conduction delay of 5 ms could change interactions of two coupled oscillators at the upper end of the gamma frequency range ({$\sim$}100 Hz) from constructive to destructive interference; delays smaller than 1 ms could change the phase by 30\textdegree, significantly affecting signal amplitude. Myelin plasticity, as another form of activitydependent plasticity, is relevant not only to nervous system development but also to complex information processing tasks that involve coupling and synchrony among different brain rhythms. We use coupled oscillator models with time delays to explore the importance of adaptive time delays and adaptive synaptic strengths. The impairment of activity-dependent myelination and the loss of adaptive time delays may contribute to disorders where hyper- and hypo-synchrony of neuronal firing leads to dysfunction (e.g., dyslexia, schizophrenia, epilepsy).},
  file = {/Users/qualia/Documents/Papers/Pajevic et al. - 2014 - Role of myelin plasticity in oscillations and sync.pdf},
  journal = {Neuroscience},
  language = {en}
}

@article{Palmieri2015,
  title = {The Transfer Function of Neuron Spike},
  author = {Palmieri, Igor and Monteiro, Luiz H.A. and Miranda, Maria D.},
  year = {2015},
  month = aug,
  volume = {68},
  pages = {89--95},
  issn = {08936080},
  doi = {10.1016/j.neunet.2015.04.003},
  abstract = {The mathematical modeling of neuronal signals is a relevant problem in neuroscience. The complexity of the neuron behavior, however, makes this problem a particularly difficult task. Here, we propose a discrete-time linear time-invariant (LTI) model with a rational function in order to represent the neuronal spike detected by an electrode located in the surroundings of the nerve cell. The model is presented as a cascade association of two subsystems: one that generates an action potential from an input stimulus, and one that represents the medium between the cell and the electrode. The suggested approach employs system identification and signal processing concepts, and is dissociated from any considerations about the biophysical processes of the neuronal cell, providing a low-complexity alternative to model the neuronal spike. The model is validated by using in vivo experimental readings of intracellular and extracellular signals. A computational simulation of the model is presented in order to assess its proximity to the neuronal signal and to observe the variability of the estimated parameters. The implications of the results are discussed in the context of spike sorting.},
  file = {/Users/qualia/Documents/Papers/Palmieri et al. - 2015 - The transfer function of neuron spike.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Panzeri2008,
  title = {On the Use of Information Theory for the Analysis of the Relationship between Neural and Imaging Signals},
  author = {Panzeri, Stefano and Magri, Cesare and Logothetis, Nikos K.},
  year = {2008},
  month = sep,
  volume = {26},
  pages = {1015--1025},
  issn = {0730725X},
  doi = {10.1016/j.mri.2008.02.019},
  abstract = {Functional magnetic resonance imaging (fMRI) is a widely used method for studying the neural basis of cognition and of sensory function. A potential problem in the interpretation of fMRI data is that fMRI measures neural activity only indirectly, as a local change of deoxyhemoglobin concentration due to the metabolic demands of neural function. To build correct sensory and cognitive maps in the human brain, it is thus crucial to understand whether fMRI and neural activity convey the same type of information about external correlates. While a substantial experimental effort has been devoted to the simultaneous recordings of hemodynamic and neural signals, so far, the development of analysis methods that elucidate how neural and hemodynamic signals represent sensory information has received less attention. In this article, we critically review why the analytical framework of information theory, the mathematical theory of communication, is ideally suited to this purpose. We review the principles of information theory and explain how they could be applied to the analysis of fMRI and neural signals. We show that a critical advantage of information theory over more traditional analysis paradigms commonly used in the fMRI literature is that it can elucidate, within a single framework, whether an empirically observed correlation between neural and fMRI signals reflects either a similar stimulus tuning or a common source of variability unrelated to the external stimuli. In addition, information theory determines the extent to which these shared sources of stimulus signal and of variability lead fMRI and neural signals to convey similar information about external correlates. We then illustrate the formalism by applying it to the analysis of the information carried by different bands of the local field potential. We conclude by discussing the current methodological challenges that need to be addressed to make the information-theoretic approach more robustly applicable to the simultaneous recordings of neural and imaging data.},
  file = {/Users/qualia/Documents/Papers/2008 - Panzeri, Magri, Logothetis - On the use of information theory for the analysis of the relationship between neural and imaging sig.pdf},
  journal = {Magnetic Resonance Imaging},
  language = {en},
  number = {7}
}

@article{Panzeri2015,
  title = {Neural Population Coding: Combining Insights from Microscopic and Mass Signals},
  shorttitle = {Neural Population Coding},
  author = {Panzeri, Stefano and Macke, Jakob H. and Gross, Joachim and Kayser, Christoph},
  year = {2015},
  month = mar,
  volume = {19},
  pages = {162--172},
  issn = {13646613},
  doi = {10.1016/j.tics.2015.01.002},
  file = {/Users/qualia/Documents/Papers/2015 - Panzeri et al. - Neural population coding combining insights from microscopic and mass signals.pdf;/Users/qualia/Documents/Papers/Panzeri et al. - 2015 - Neural population coding combining insights from .pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {3}
}

@article{Papadopoulos,
  title = {Embedding of Biological Distribution Networks with Differing Environmental Constraints},
  author = {Papadopoulos, Lia and Blinder, Pablo and Ronellenfitsch, Henrik and Katifori, Eleni and Kleinfeld, David and Bassett, Danielle S},
  pages = {20},
  file = {/Users/qualia/Documents/Papers/Papadopoulos et al. - Embedding of biological distribution networks with.pdf},
  language = {en}
}

@article{Papadopoulos2017,
  title = {Development of Structural Correlations and Synchronization from Adaptive Rewiring in Networks of {{Kuramoto}} Oscillators},
  author = {Papadopoulos, Lia and Kim, Jason Z. and Kurths, J{\"u}rgen and Bassett, Danielle S.},
  year = {2017},
  month = jul,
  volume = {27},
  pages = {073115},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4994819},
  file = {/Users/qualia/Documents/Papers/Papadopoulos et al. - 2017 - Development of structural correlations and synchro.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {7}
}

@article{Papasavvas2015,
  title = {Gain Control through Divisive Inhibition Prevents Abrupt Transition to Chaos in a Neural Mass Model},
  author = {Papasavvas, Christoforos A. and Wang, Yujiang and Trevelyan, Andrew J. and Kaiser, Marcus},
  year = {2015},
  month = sep,
  volume = {92},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.92.032723},
  file = {/Users/qualia/Documents/Papers/2015 - Papasavvas et al. - Gain control through divisive inhibition prevents abrupt transition to chaos in a neural mass model.pdf;/Users/qualia/Documents/Papers/Papasavvas et al. - 2015 - Gain control through divisive inhibition prevents .pdf},
  journal = {Physical Review E},
  language = {en},
  number = {3}
}

@article{Papernot2017,
  title = {{{SEMI}}-{{SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNING FROM PRIVATE TRAINING DATA}}},
  author = {Papernot, Nicolas and Abadi, Mart\i{}n},
  year = {2017},
  pages = {14},
  abstract = {Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.},
  file = {/Users/qualia/Documents/Papers/Papernot and Abadi - 2017 - SEMI-SUPERVISED KNOWLEDGE TRANSFER FOR DEEP LEARNI.pdf},
  language = {en}
}

@techreport{Parameshwaran2017,
  title = {Modernization, Wealth and the Emergence of Strong Alpha Oscillations in the Human {{EEG}}},
  author = {Parameshwaran, Dhanya and Thiagarajan, Tara C.},
  year = {2017},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/125898},
  abstract = {Oscillations in the alpha range (8-15 Hz) have been found to appear prominently in the EEG signal when people are awake with their eyes closed, and since their discovery have been considered a fundamental cerebral rhythm. While the mechanism of this oscillation continues to be debated, it has been shown to bear positive relation to memory capacity, attention and a host of other cognitive outcomes. Here we show that this feature is largely undetected in the EEG of adults without post-primary education and access to modern technologies. Furthermore, we show that the spatial extent and energy of the oscillation have wide variation, with energy ranging over a thousand fold across the breath of humanity with no centralizing mean. This represents a divergence in a fundamental functional characteristic of an organ demonstrating both that modernization has had a profound influence on brain dynamics and that a meaningful `average' human brain does not exist in a dynamical sense.},
  file = {/Users/qualia/Documents/Papers/Parameshwaran and Thiagarajan - 2017 - Modernization, wealth and the emergence of strong .pdf},
  language = {en},
  type = {Preprint}
}

@article{Parent2007,
  title = {The Microcircuitry of Primate Subthalamic Nucleus},
  author = {Parent, Martin and Parent, Andr{\'e}},
  year = {2007},
  volume = {13},
  pages = {S292-S295},
  issn = {13538020},
  doi = {10.1016/S1353-8020(08)70018-X},
  abstract = {Single-cell labeling experiments in cynomolgus monkeys have revealed that the subthalamic nucleus (STN) harbors several subtypes of projection neurons, each endowed with a highly patterned set of axon collaterals. This organizational feature allows single STN neurons to act directly upon the two major output structures of the basal ganglia \textendash{} the substantia nigra pars reticulata and the internal pallidum \textendash{} and, at the same time, to exert a multifarious effect upon the external pallidum with which the STN is reciprocally connected. These findings have clarified the role of the STN in basal ganglia organization and led to the elaboration of more accurate computational models of deep brain stimulation, a therapeutic approach currently used to alleviate the motor symptoms of Parkinson's Disease.},
  file = {/Users/qualia/Documents/Papers/2007 - Parent, Parent - The microcircuitry of primate subthalamic nucleus.pdf},
  journal = {Parkinsonism \& Related Disorders},
  language = {en}
}

@article{Parikh2007,
  title = {Prefrontal {{Acetylcholine Release Controls Cue Detection}} on {{Multiple Timescales}}},
  author = {Parikh, Vinay and Kozak, Rouba and Martinez, Vicente and Sarter, Martin},
  year = {2007},
  month = oct,
  volume = {56},
  pages = {141--154},
  issn = {08966273},
  doi = {10.1016/j.neuron.2007.08.025},
  abstract = {Cholinergic neurons originating from the basal forebrain innervate the entire cortical mantle. Choline-sensitive microelectrodes were used to measure the synaptic release of cortical acetylcholine (ACh) at a sub-second resolution in rats performing a task involving the detection of cues. Cues that were detected, defined behaviorally, evoked transient increases in cholinergic activity (at the scale of seconds) in the medial prefrontal cortex (mPFC), but not in a non-associational control region (motor cortex). In trials involving missed cues, cholinergic transients were not observed. Cholinergic deafferentation of the mPFC, but not motor cortex, impaired cue detection. Furthermore, decreases and increases in pre-cue cholinergic activity predicted subsequent cue detection or misses, respectively. Finally, cue-evoked cholinergic transients were superimposed over slower (at the time scale of minutes) changes in cholinergic activity. Cortical cholinergic neurotransmission is regulated on multiple time scales to mediate the detection of behaviorally significant cues and to support cognitive performance.},
  file = {/Users/qualia/Documents/Papers/2007 - Parikh et al. - Prefrontal acetylcholine release controls cue detection on multiple time scales.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@techreport{Park2017,
  title = {Bayesian {{Efficient Coding}}},
  author = {Park, Il Memming and Pillow, Jonathan W.},
  year = {2017},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/178418},
  abstract = {The efficient coding hypothesis, which proposes that neurons are optimized to maximize information about the environment, has provided a guiding theoretical framework for sensory and systems neuroscience. More recently, a theory known as the Bayesian Brain hypothesis has focused on the brain's ability to integrate sensory and prior sources of information in order to perform Bayesian inference. However, there is as yet no comprehensive theory connecting these two theoretical frameworks. Here we bridge this gap by formalizing a Bayesian theory of efficient coding. We define Bayesian efficient codes in terms of four basic ingredients: (1) a stimulus prior distribution; (2) an encoding model; (3) a capacity constraint, specifying a neural resource limit; and (4) a loss function, quantifying the desirability or undesirability of various posterior distributions. Classic efficient codes can be seen as a special case in which the loss function is the posterior entropy, leading to a code that maximizes mutual information, but alternate loss functions give solutions that differ dramatically from information-maximizing codes. In particular, we show that decorrelation of sensory inputs, which is optimal under classic efficient codes in low-noise settings, can be disadvantageous for loss functions that penalize large errors. Bayesian efficient coding therefore enlarges the family of normatively optimal codes and provides a more general framework for understanding the design principles of sensory systems. We examine Bayesian efficient codes for linear receptive fields and nonlinear input-output functions, and show that our theory invites reinterpretation of Laughlin's seminal analysis of efficient coding in the blowfly visual system.},
  file = {/Users/qualia/Documents/Papers/Park and Pillow - 2017 - Bayesian Efficient Coding.pdf},
  language = {en},
  type = {Preprint}
}

@article{Parkes2004,
  title = {Reduced {{BOLD}} Response to Periodic Visual Stimulation},
  author = {Parkes, Laura M and Fries, Pascal and Kerskens, Christian M and Norris, David G},
  year = {2004},
  month = jan,
  volume = {21},
  pages = {236--243},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2003.08.025},
  file = {/Users/qualia/Documents/Papers/2004 - Parkes et al. - Reduced BOLD response to periodic visual stimulation.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Parkes2009,
  title = {Multivoxel {{fMRI}} Analysis of Color Tuning in Human Primary Visual Cortex},
  author = {Parkes, L. M. and Marsman, J. B. C. and Oxley, D. C. and Goulermas, J. Y. and Wuerger, S. M.},
  year = {2009},
  month = jan,
  volume = {9},
  pages = {1--1},
  issn = {1534-7362},
  doi = {10.1167/9.1.1},
  abstract = {We use multivoxel pattern analysis (MVPA) to study the spatial clustering of color-selective neurons in the human brain. Our main objective was to investigate whether MVPA reveals the spatial arrangements of color-selective neurons in human primary visual cortex (V1). We measured the distributed fMRI activation patterns for different color stimuli (Experiment 1: cardinal colors (to which the LGN is known to be tuned), Experiment 2: perceptual hues) in V1. Our two main findings were that (i) cone-opponent cardinal color modulations produce highly reproducible patterns of activity in V1, but these were not unique to each color. This suggests that V1 neurons with tuning characteristics similar to those found in LGN are not spatially clustered. (ii) Unique activation patterns for perceptual hues in V1 support current evidence for a spatially clustered hue map. We believe that our work is the first to show evidence of spatial clustering of neurons with similar color preferences in human V1.},
  file = {/Users/qualia/Documents/Papers/2009 - Parkes, Marsman - Multivoxel fMRI analysis of color tuning in human primary visual cortex.pdf},
  journal = {Journal of Vision},
  language = {en},
  number = {1}
}

@article{Parpart2018,
  title = {Heuristics as {{Bayesian}} Inference under Extreme Priors},
  author = {Parpart, Paula and Jones, Matt and Love, Bradley C.},
  year = {2018},
  month = may,
  volume = {102},
  pages = {127--144},
  issn = {00100285},
  doi = {10.1016/j.cogpsych.2017.11.006},
  abstract = {Simple heuristics are often regarded as tractable decision strategies because they ignore a great deal of information in the input data. One puzzle is why heuristics can outperform full-information models, such as linear regression, which make full use of the available information. These ``lessis-more'' effects, in which a relatively simpler model outperforms a more complex model, are prevalent throughout cognitive science, and are frequently argued to demonstrate an inherent advantage of simplifying computation or ignoring information. In contrast, we show at the computational level (where algorithmic restrictions are set aside) that it is never optimal to discard information. Through a formal Bayesian analysis, we prove that popular heuristics, such as tallying and take-the-best, are formally equivalent to Bayesian inference under the limit of infinitely strong priors. Varying the strength of the prior yields a continuum of Bayesian models with the heuristics at one end and ordinary regression at the other. Critically, intermediate models perform better across all our simulations, suggesting that down-weighting information with the appropriate prior is preferable to entirely ignoring it. Rather than because of their simplicity, our analyses suggest heuristics perform well because they implement strong priors that approximate the actual structure of the environment. We end by considering how new heuristics could be derived by infinitely strengthening the priors of other Bayesian models. These formal results have implications for work in psychology, machine learning and economics.},
  file = {/Users/qualia/Documents/Papers/Parpart et al. - 2018 - Heuristics as Bayesian inference under extreme pri.pdf;/Users/qualia/Documents/Papers/Parpart et al. - Heuristics as Bayesian inference under extreme pri.pdf},
  journal = {Cognitive Psychology},
  language = {en}
}

@article{Pascanu2013,
  title = {Revisiting {{Natural Gradient}} for {{Deep Networks}}},
  author = {Pascanu, Razvan and Bengio, Yoshua},
  year = {2013},
  month = jan,
  abstract = {We evaluate natural gradient descent, an algorithm originally proposed in Amari (1997), for learning deep models. The contributions of this paper are as follows. We show the connection between natural gradient descent and three other recently proposed methods for training deep models: Hessian-Free Optimization (Martens, 2010), Krylov Subspace Descent (Vinyals and Povey, 2012) and TONGA (Le Roux et al., 2008). We describe how one can use unlabeled data to improve the generalization error obtained by natural gradient and empirically evaluate the robustness of the algorithm to the ordering of the training set compared to stochastic gradient descent. Finally we extend natural gradient descent to incorporate second order information alongside the manifold information and provide a benchmark of the new algorithm using a truncated Newton approach for inverting the metric matrix instead of using a diagonal approximation of it.},
  archivePrefix = {arXiv},
  eprint = {1301.3584},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Pascanu and Bengio - 2013 - Revisiting Natural Gradient for Deep Networks.pdf},
  journal = {arXiv:1301.3584 [cs]},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{Pathak2017,
  title = {Curiosity-{{Driven Exploration}} by {{Self}}-{{Supervised Prediction}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A. and Darrell, Trevor},
  year = {2017},
  month = jul,
  pages = {488--489},
  publisher = {{IEEE}},
  address = {{Honolulu, HI, USA}},
  doi = {10.1109/CVPRW.2017.70},
  abstract = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch.},
  file = {/Users/qualia/Documents/Papers/Pathak et al. - 2017 - Curiosity-Driven Exploration by Self-Supervised Pr.pdf},
  isbn = {978-1-5386-0733-6},
  language = {en}
}

@article{Pathak2018,
  title = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}: {{A Reservoir Computing Approach}}},
  shorttitle = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}},
  author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  year = {2018},
  month = jan,
  volume = {120},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.120.024102},
  file = {/Users/qualia/Documents/Papers/Pathak et al. - 2018 - Model-Free Prediction of Large Spatiotemporally Ch.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {2}
}

@article{Pathak2019,
  title = {Self-{{Supervised Exploration}} via {{Disagreement}}},
  author = {Pathak, Deepak and Gandhi, Dhiraj and Gupta, Abhinav},
  year = {2019},
  pages = {10},
  abstract = {Efficient exploration is a long-standing problem in sensorimotor learning. Major advances have been demonstrated in noise-free, non-stochastic domains such as video games and simulation. However, most of these formulations either get stuck in environments with stochastic dynamics or are too inefficient to be scalable to real robotics setups. In this paper, we propose a formulation for exploration inspired by the work in active learning literature. Specifically, we train an ensemble of dynamics models and incentivize the agent to explore such that the disagreement of those ensembles is maximized. This allows the agent to learn skills by exploring in a self-supervised manner without any external reward. Notably, we further leverage the disagreement objective to optimize the agent's policy in a differentiable manner, without using reinforcement learning, which results in a sample-efficient exploration. We demonstrate the efficacy of this formulation across a variety of benchmark environments including stochastic-Atari, Mujoco and Unity. Finally, we implement our differentiable exploration on a real robot which learns to interact with objects completely from scratch. Project videos and code are at https://pathak22.github. io/exploration-by-disagreement/.},
  file = {/Users/qualia/Documents/Papers/Pathak et al. - Self-Supervised Exploration via Disagreement.pdf},
  journal = {Proceedings of the 36th International Conference on Machine Learning},
  language = {en}
}

@article{Pavlides2015,
  title = {Computational {{Models Describing Possible Mechanisms}} for {{Generation}} of {{Excessive Beta Oscillations}} in {{Parkinson}}'s {{Disease}}},
  author = {Pavlides, Alex and Hogan, S. John and Bogacz, Rafal},
  editor = {Graham, Lyle J.},
  year = {2015},
  month = dec,
  volume = {11},
  pages = {e1004609},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004609},
  file = {/Users/qualia/Documents/Papers/2015 - Pavlides, Hogan, Bogacz - Computational Models Describing Possible Mechanisms for Generation of Excessive Beta Oscillations in.pdf;/Users/qualia/Documents/Papers/Pavlides et al. - 2015 - Computational Models Describing Possible Mechanism.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{Pearl,
  title = {Theoretical {{Impediments}} to {{Machine Learning}}},
  author = {Pearl, Judea},
  pages = {5},
  abstract = {Current machine learning systems operate, almost exclusively, in a purely statistical mode, which puts severe theoretical limits on their performance. We consider the feasibility of leveraging counterfactual reasoning in machine learning tasks, and to identify areas where such reasoning could lead to major breakthroughs in machine learning applications.},
  file = {/Users/qualia/Documents/Papers/Pearl - Theoretical Impediments to Machine Learning.pdf},
  language = {en}
}

@inproceedings{Pearl2018,
  title = {Theoretical {{Impediments}} to {{Machine Learning With Seven Sparks}} from the {{Causal Revolution}}},
  booktitle = {Proceedings of the {{Eleventh ACM International Conference}} on {{Web Search}} and {{Data Mining}}  - {{WSDM}} '18},
  author = {Pearl, Judea},
  year = {2018},
  pages = {3--3},
  publisher = {{ACM Press}},
  address = {{Marina Del Rey, CA, USA}},
  doi = {10.1145/3159652.3176182},
  abstract = {Current machine learning systems operate, almost exclusively, in a statistical, or modelblind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.},
  file = {/Users/qualia/Documents/Papers/Pearl - 2018 - Theoretical Impediments to Machine Learning With S.pdf},
  isbn = {978-1-4503-5581-0},
  language = {en}
}

@techreport{Pebay2008,
  title = {Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments.},
  author = {Pebay, Philippe Pierre},
  year = {2008},
  month = sep,
  doi = {10.2172/1028931},
  abstract = {We present a formula for the pairwise update of arbitrary-order centered statistical moments. This formula is of particular interest to compute such moments in parallel for large-scale, distributed data sets. As a corollary, we indicate a specialization of this formula for incremental updates, of particular interest to streaming implementations. Finally, we provide pairwise and incremental update formulas for the covariance.},
  file = {/Users/qualia/Documents/Papers/2008 - Pébay - Formulas for Robust, One-Pass Parallel Computation of Covariances and Arbitrary-Order Statistical Moments.pdf},
  language = {en},
  number = {SAND2008-6212, 1028931}
}

@techreport{Peles2019,
  title = {Phase-Specific Microstimulation in Brain-Machine Interface Setting Differentially Modulates Beta Oscillations and Affects Behavior},
  author = {Peles, Oren and {Werner-Reiss}, Uri and Bergman, Hagai and Israel, Zvi and Vaadia, Eilon},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/622787},
  abstract = {It is widely accepted that beta-band oscillations play a role in sensorimotor behavior. To further explore this role, we developed a novel hybrid platform to combine operant conditioning and phasespecific intracortical microstimulation (ICMS). We trained monkeys, implanted with 96 electrodes arrays in motor cortex, to volitionally enhance local field potential (LFP) beta-band (20-30Hz) activity at selected sites using a brain-machine interface (BMI). We demonstrate that beta oscillations of LFP and single-unit spiking activity increased dramatically with BMI training, and that pre-movement Beta-power was anti-correlated with task performance. We also show that phase-specific ICMS modulated the power and phase of oscillations, shifting local networks between oscillatory and non-oscillatory states. Furthermore, ICMS induced phase-dependent effects in animal reaction times and success rates. These findings contribute to unraveling of the functional role of cortical oscillations, and to future development of clinical tools for ameliorating abnormal neuronal activities in brain diseases.},
  file = {/Users/qualia/Documents/Papers/Peles et al. - 2019 - Phase-specific microstimulation in brain-machine i.pdf},
  language = {en},
  type = {Preprint}
}

@article{Pena2017,
  title = {Particle Swarm Optimization for Programming Deep Brain Stimulation Arrays},
  author = {Pe{\~n}a, Edgar and Zhang, Simeng and Deyo, Steve and Xiao, YiZi and Johnson, Matthew D},
  year = {2017},
  month = feb,
  volume = {14},
  pages = {016014},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2552/aa52d1},
  abstract = {Objective. Deep brain stimulation (DBS) therapy relies on both precise neurosurgical targeting and systematic optimization of stimulation settings to achieve beneficial clinical outcomes. One recent advance to improve targeting is the development of DBS arrays (DBSAs) with electrodes segmented both along and around the DBS lead. However, increasing the number of independent electrodes creates the logistical challenge of optimizing stimulation parameters efficiently. Approach. Solving such complex problems with multiple solutions and objectives is well known to occur in biology, in which complex collective behaviors emerge out of swarms of individual organisms engaged in learning through social interactions. Here, we developed a particle swarm optimization (PSO) algorithm to program DBSAs using a swarm of individual particles representing electrode configurations and stimulation amplitudes. Using a finite element model of motor thalamic DBS, we demonstrate how the PSO algorithm can efficiently optimize a multi-objective function that maximizes predictions of axonal activation in regions of interest (ROI, cerebellar-receiving area of motor thalamus), minimizes predictions of axonal activation in regions of avoidance (ROA, somatosensory thalamus), and minimizes power consumption. Main results. The algorithm solved the multi-objective problem by producing a Pareto front. ROI and ROA activation predictions were consistent across swarms ({$<$}1\% median discrepancy in axon activation). The algorithm was able to accommodate for (1) lead displacement (1 mm) with relatively small ROI ({$\leqslant$}9.2\%) and ROA ({$\leqslant$}1\%) activation changes, irrespective of shift direction; (2) reduction in maximum per-electrode current (by 50\% and 80\%) with ROI activation decreasing by 5.6\% and 16\%, respectively; and (3) disabling electrodes (n = 3 and 12) with ROI activation reduction by 1.8\% and 14\%, respectively. Additionally, comparison between PSO predictions and multicompartment axon model simulations showed discrepancies of {$<$}1\% between approaches. Significance. The PSO algorithm provides a computationally efficient way to program DBS systems especially those with higher electrode counts.},
  file = {/Users/qualia/Documents/Papers/Peña et al. - 2017 - Particle swarm optimization for programming deep b.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {1}
}

@article{Pereira2009,
  title = {Machine Learning Classifiers and {{fMRI}}: {{A}} Tutorial Overview},
  shorttitle = {Machine Learning Classifiers and {{fMRI}}},
  author = {Pereira, Francisco and Mitchell, Tom and Botvinick, Matthew},
  year = {2009},
  month = mar,
  volume = {45},
  pages = {S199-S209},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2008.11.007},
  abstract = {Interpreting brain image experiments requires analysis of complex, multivariate data. In recent years, one analysis approach that has grown in popularity is the use of machine learning algorithms to train classifiers to decode stimuli, mental states, behaviours and other variables of interest from fMRI data and thereby show the data contain information about them. In this tutorial overview we review some of the key choices faced in using this approach as well as how to derive statistically significant results, illustrating each point from a case study. Furthermore, we show how, in addition to answering the question of `is there information about a variable of interest' (pattern discrimination), classifiers can be used to tackle other classes of question, namely `where is the information' (pattern localization) and `how is that information encoded' (pattern characterization).},
  file = {/Users/qualia/Documents/Papers/2009 - Pereira, Mitchell, Botvinick - Machine learning classifiers and fMRI a tutorial overview.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Pereira2010,
  title = {Astrocytes and Human Cognition: {{Modeling}} Information Integration and Modulation of Neuronal Activity},
  shorttitle = {Astrocytes and Human Cognition},
  author = {Pereira, Alfredo and Furlan, F{\'a}bio Augusto},
  year = {2010},
  month = nov,
  volume = {92},
  pages = {405--420},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2010.07.001},
  abstract = {Recent research focusing on the participation of astrocytes in glutamatergic tripartite synapses has revealed mechanisms that support cognitive functions common to human and other mammalian species, such as learning, perception, conscious integration, memory formation/retrieval and the control of voluntary behavior. Astrocytes can modulate neuronal activity by means of release of glutamate, D-serine, adenosine triphosphate and other signaling molecules, contributing to sustain, reinforce or depress pre- and post-synaptic membranes. We review molecular mechanisms present in tripartite synapses and model the cognitive role of astrocytes. Single protoplasmic astrocytes operate as a ``Local Hub'', integrating information patterns from neuronal and glial populations. Two mechanisms, here modeled as the ``domino'' and ``carousel'' effects, contribute to the formation of intercellular calcium waves. As waves propagate through gap junctions and reach other types of astrocytes (interlaminar, polarized, fibrous and varicose projection), the active astroglial network functions as a ``Master Hub'' that integrates results of distributed processing from several brain areas and supports conscious states. Response of this network would define the effect exerted on neuronal plasticity (membrane potentiation or depression), behavior and psychosomatic processes. Theoretical results of our modeling can contribute to the development of new experimental research programs to test cognitive functions of astrocytes.},
  file = {/Users/qualia/Documents/Papers/Pereira and Furlan - 2010 - Astrocytes and human cognition Modeling informati.pdf},
  journal = {Progress in Neurobiology},
  language = {en},
  number = {3}
}

@article{Pereira2010a,
  title = {Astrocytes and Human Cognition: {{Modeling}} Information Integration and Modulation of Neuronal Activity},
  shorttitle = {Astrocytes and Human Cognition},
  author = {Pereira, Alfredo and Furlan, F{\'a}bio Augusto},
  year = {2010},
  month = nov,
  volume = {92},
  pages = {405--420},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2010.07.001},
  abstract = {Recent research focusing on the participation of astrocytes in glutamatergic tripartite synapses has revealed mechanisms that support cognitive functions common to human and other mammalian species, such as learning, perception, conscious integration, memory formation/retrieval and the control of voluntary behavior. Astrocytes can modulate neuronal activity by means of release of glutamate, D-serine, adenosine triphosphate and other signaling molecules, contributing to sustain, reinforce or depress pre- and post-synaptic membranes. We review molecular mechanisms present in tripartite synapses and model the cognitive role of astrocytes. Single protoplasmic astrocytes operate as a ``Local Hub'', integrating information patterns from neuronal and glial populations. Two mechanisms, here modeled as the ``domino'' and ``carousel'' effects, contribute to the formation of intercellular calcium waves. As waves propagate through gap junctions and reach other types of astrocytes (interlaminar, polarized, fibrous and varicose projection), the active astroglial network functions as a ``Master Hub'' that integrates results of distributed processing from several brain areas and supports conscious states. Response of this network would define the effect exerted on neuronal plasticity (membrane potentiation or depression), behavior and psychosomatic processes. Theoretical results of our modeling can contribute to the development of new experimental research programs to test cognitive functions of astrocytes.},
  file = {/Users/qualia/Documents/Papers/Pereira and Furlan - 2010 - Astrocytes and human cognition Modeling informati 2.pdf},
  journal = {Progress in Neurobiology},
  language = {en},
  number = {3}
}

@article{Pereira2010b,
  title = {Astrocytes and Human Cognition: {{Modeling}} Information Integration and Modulation of Neuronal Activity},
  shorttitle = {Astrocytes and Human Cognition},
  author = {Pereira, Alfredo and Furlan, F{\'a}bio Augusto},
  year = {2010},
  month = nov,
  volume = {92},
  pages = {405--420},
  issn = {03010082},
  doi = {10.1016/j.pneurobio.2010.07.001},
  abstract = {Recent research focusing on the participation of astrocytes in glutamatergic tripartite synapses has revealed mechanisms that support cognitive functions common to human and other mammalian species, such as learning, perception, conscious integration, memory formation/retrieval and the control of voluntary behavior. Astrocytes can modulate neuronal activity by means of release of glutamate, D-serine, adenosine triphosphate and other signaling molecules, contributing to sustain, reinforce or depress pre- and post-synaptic membranes. We review molecular mechanisms present in tripartite synapses and model the cognitive role of astrocytes. Single protoplasmic astrocytes operate as a ``Local Hub'', integrating information patterns from neuronal and glial populations. Two mechanisms, here modeled as the ``domino'' and ``carousel'' effects, contribute to the formation of intercellular calcium waves. As waves propagate through gap junctions and reach other types of astrocytes (interlaminar, polarized, fibrous and varicose projection), the active astroglial network functions as a ``Master Hub'' that integrates results of distributed processing from several brain areas and supports conscious states. Response of this network would define the effect exerted on neuronal plasticity (membrane potentiation or depression), behavior and psychosomatic processes. Theoretical results of our modeling can contribute to the development of new experimental research programs to test cognitive functions of astrocytes.},
  file = {/Users/qualia/Zotero/storage/TVSA6HVX/Pereira and Furlan - 2010 - Astrocytes and human cognition Modeling informati.pdf},
  journal = {Progress in Neurobiology},
  language = {en},
  number = {3}
}

@article{Pereira2011,
  title = {Information Mapping with Pattern Classifiers: {{A}} Comparative Study},
  shorttitle = {Information Mapping with Pattern Classifiers},
  author = {Pereira, Francisco and Botvinick, Matthew},
  year = {2011},
  month = may,
  volume = {56},
  pages = {476--496},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.026},
  abstract = {Information mapping using pattern classifiers has become increasingly popular in recent years, although without a clear consensus on which classifier(s) ought to be used or how results should be tested. This paper addresses each of these questions, both analytically and through comparative analyses on five empirical datasets. We also describe how information maps in multiple class situations can provide information concerning the content of neural representations. Finally, we introduce a publically available software toolbox designed specifically for information mapping.},
  file = {/Users/qualia/Documents/Papers/2011 - Pereira, Botvinick - Information mapping with pattern classifiers a comparative study.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Perlstein2001,
  title = {Relation of {{Prefrontal Cortex Dysfunction}} to {{Working Memory}} and {{Symptoms}} in {{Schizophrenia}}},
  author = {Perlstein, William M. and Carter, Cameron S. and Noll, Douglas C. and Cohen, Jonathan D.},
  year = {2001},
  month = jul,
  volume = {158},
  pages = {1105--1113},
  issn = {0002-953X, 1535-7228},
  doi = {10.1176/appi.ajp.158.7.1105},
  file = {/Users/qualia/Documents/Papers/2001 - Perlstein et al. - Relation of Prefrontal Cortex Dysfunction to Working Memory and Symptoms in Schizophrenia.pdf},
  journal = {American Journal of Psychiatry},
  language = {en},
  number = {7}
}

@article{Peron2013,
  title = {Subthalamic Nucleus: {{A}} Key Structure for Emotional Component Synchronization in Humans},
  shorttitle = {Subthalamic Nucleus},
  author = {P{\'e}ron, Julie and Fr{\"u}hholz, Sascha and V{\'e}rin, Marc and Grandjean, Didier},
  year = {2013},
  month = mar,
  volume = {37},
  pages = {358--373},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2013.01.001},
  abstract = {Affective neuroscience is concerned with identifying the neural bases of emotion. For historical and methodological reasons, models describing the brain architecture that supports emotional processes in humans have tended to neglect the basal ganglia, focusing instead on cortical and amygdalar mechanisms. Now, however, deep brain stimulation (DBS) of the subthalamic nucleus (STN), a neurosurgical treatment for Parkinson's disease and obsessive\textendash{}compulsive disorder, is helping researchers explore the possible functional role of this particular basal ganglion in emotional processes. After reviewing studies that have used DBS in this way, we propose a model in which the STN plays a crucial role in producing temporally organized neural co-activation patterns at the cortical and subcortical levels that are essential for generating emotions and related feelings.},
  file = {/Users/qualia/Documents/Papers/2013 - Péron et al. - Subthalamic nucleus A key structure for emotional component synchronization in humans.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en},
  number = {3}
}

@article{Pessoa2006,
  title = {Decoding {{Near}}-{{Threshold Perception}} of {{Fear}} from {{Distributed Single}}-{{Trial Brain Activation}}},
  author = {Pessoa, L. and Padmala, S.},
  year = {2006},
  month = mar,
  volume = {17},
  pages = {691--701},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhk020},
  abstract = {Instead of contrasting functional magnetic resonance imaging (fMRI) signals associated with 2 conditions, as customarily done in neuroimaging, we reversed the direction of analysis and probed whether brain signals could be used to ``predict'' perceptual states. We probed the neural correlates of perceptual decisions by ``decoding'' brain states during near-threshold fear detection. Decoding was attempted by using support vector machines and other related techniques. Although previous decoding studies have employed relatively ``blocked'' data, our objective was to probe how the ``moment-to-moment'' fluctuation in fMRI signals across a population of voxels reflected the participant's perceptual decision. Accuracy increased from when 1 region was considered (\textasciitilde{}64\%) to when 10 regions were used (\textasciitilde{}78\%). When the best classifications per subject were averaged, accuracy levels ranged between 74\% and 86\% correct. An information theoretic analysis revealed that the information carried by pairs of regions reliably exceeded the sum of the information carried by individual regions, suggesting that information was combined ``synergistically'' across regions. Our results indicate that the representation of behavioral choice is ``distributed'' across several brain regions. Such distributed encoding may help prepare the organism to appropriately handle emotional stimuli and regulate the associated emotional response upon the conscious decision that a fearful face is present. In addition, the results show that challenging brain states can be decoded with high accuracy even when ``single-trial'' data are employed and suggest that multivariate analysis strategies have considerable potential in helping to elucidate the neural correlates of visual awareness and the encoding of perceptual decisions.},
  file = {/Users/qualia/Documents/Papers/2007 - Pessoa, Padmala - Decoding near-threshold perception of fear from distributed single-trial brain activation.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Peters2016,
  title = {Evaluating Gambles Using Dynamics},
  author = {Peters, O. and {Gell-Mann}, M.},
  year = {2016},
  month = feb,
  volume = {26},
  pages = {023103},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4940236},
  file = {/Users/qualia/Documents/Papers/Peters and Gell-Mann - 2016 - Evaluating gambles using dynamics.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {2}
}

@article{Peterson2015,
  title = {Balanced {{Oscillatory Coupling Improves Information Flow}}},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2015},
  month = oct,
  doi = {10.1101/030304},
  abstract = {All animals are able to rapidly change their behavior. The neural basis of such flexibility requires that groups of distant neural ensembles rapidly alter communications with selectivity and fidelity. Low frequency oscillations are a strong candidate for how neurons coordinate communication via the dynamic instantiation of functional networks. These dynamic networks are argued to rapidly guide the flow of information, with the presumption that stronger oscillations more strongly influence information flow. Surprisingly, there is scant evidence or theoretical support for how oscillatory activity might enhance information flow. Here we introduce a novel computational model for oscillatory neural communication and show that, rather than the strength of the oscillation, it is the balance between excitatory and inhibitory neuronal activity that has the largest effect on information flow. When coupling between an oscillation and spiking has balanced excitatory-inhibitory inputs, information flow is enhanced via improved discriminability between signal and noise. In contrast, when coupling is unbalanced, driven either by excessive excitation or inhibition, information flow is obstructed, regardless of the strength of the oscillation. A multitude of neuropathologies, including Parkinson's disease, schizophrenia, and autism, are associated with oscillatory disruptions and excitation-inhibition imbalances. Our results show that understanding the distinction between balanced and unbalanced oscillatory coupling offers a unifying mechanistic framework for understanding effective neural communication and its disruption in neuropathology.},
  file = {/Users/qualia/Documents/Papers/2015 - Peterson, Voytek - Balanced Oscillatory Coupling Improves Information Flow.pdf;/Users/qualia/Documents/Papers/Peterson and Voytek - 2015 - Balanced Oscillatory Coupling Improves Information.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Peterson2015a,
  title = {Balanced {{Oscillatory Coupling Improves Information Flow}}},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2015},
  month = oct,
  institution = {{Neuroscience}},
  doi = {10.1101/030304},
  abstract = {All animals are able to rapidly change their behavior. The neural basis of such flexibility requires that groups of distant neural ensembles rapidly alter communications with selectivity and fidelity. Low frequency oscillations are a strong candidate for how neurons coordinate communication via the dynamic instantiation of functional networks. These dynamic networks are argued to rapidly guide the flow of information, with the presumption that stronger oscillations more strongly influence information flow. Surprisingly, there is scant evidence or theoretical support for how oscillatory activity might enhance information flow. Here we introduce a novel computational model for oscillatory neural communication and show that, rather than the strength of the oscillation, it is the balance between excitatory and inhibitory neuronal activity that has the largest effect on information flow. When coupling between an oscillation and spiking has balanced excitatory-inhibitory inputs, information flow is enhanced via improved discriminability between signal and noise. In contrast, when coupling is unbalanced, driven either by excessive excitation or inhibition, information flow is obstructed, regardless of the strength of the oscillation. A multitude of neuropathologies, including Parkinson's disease, schizophrenia, and autism, are associated with oscillatory disruptions and excitation-inhibition imbalances. Our results show that understanding the distinction between balanced and unbalanced oscillatory coupling offers a unifying mechanistic framework for understanding effective neural communication and its disruption in neuropathology.},
  file = {/Users/qualia/Documents/Papers/Peterson and Voytek - 2015 - Balanced Oscillatory Coupling Improves Information 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Peterson2017,
  title = {Alpha Oscillations Control Cortical Gain by Modulating Excitatory-Inhibitory Background Activity.},
  author = {Peterson, Erik J. and Voytek, Bradley},
  year = {2017},
  month = sep,
  volume = {185074},
  doi = {10.1101/185074},
  abstract = {The first recordings of human brain activity in 1929 revealed a striking 8-12 Hz oscillation in the visual cortex. During the intervening 90 years, these alpha oscillations have been linked to numerous physiological and cognitive processes. However, because of the vast and seemingly contradictory cognitive and physiological processes to which it has been related, the physiological function of alpha remains unclear. We identify a novel neural circuit mechanism\textemdash{}the modulation of both excitatory and inhibitory neurons in a balanced configuration\textemdash{}by which alpha can modulate gain. We find that this model naturally unifies the prior, highly diverse reports on alpha dynamics, while making the novel prediction that alpha rhythms have two functional roles: a sustained high-power mode that suppresses cortical gain and a weak, bursting mode that enhances gain.},
  file = {/Users/qualia/Documents/Papers/Peterson and Voytek - 2017 - Alpha oscillations control cortical gain by modula.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Peterson2017a,
  title = {1/ {\emph{f}} Neural Noise Is a Better Predictor of Schizophrenia than Neural Oscillations},
  author = {Peterson, Erik J. and Rosen, Burke Q. and Campbell, Alana M. and Belger, Aysenil and Voytek, Bradley},
  year = {2017},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/113449},
  abstract = {Schizophrenia has been associated with separate irregularities in several neural oscillatory frequency bands, including theta, alpha, and gamma. Our multivariate classification of human EEG suggests that instead of irregularities in many frequency bands, schizophrenia-related electrophysiological differences may better be explained by an overall shift in neural noise, reflected by a change in the 1/f slope of the power spectrum.},
  file = {/Users/qualia/Documents/Papers/Peterson et al. - 2017 - 1 ifi neural noise is a better predictor of .pdf},
  language = {en},
  type = {Preprint}
}

@article{Peterson2018,
  title = {Keep It Stupid Simple},
  author = {Peterson, Erik J. and M{\"u}yesser, Necati Alp and Verstynen, Timothy and Dunovan, Kyle},
  year = {2018},
  month = sep,
  abstract = {Deep reinforcement learning can match and exceed human performance, but if even minor changes are introduced to the environment artificial networks often can't adapt. Humans meanwhile are quite adaptable. We hypothesize that this is partly because of how humans use heuristics, and partly because humans can imagine new and more challenging environments to learn from. We've developed a model of hierarchical reinforcement learning that combines both these elements into a stumbler-strategist network. We test transfer performance of this network using Wythoff's game, a gridworld environment with a known optimal strategy. We show that combining imagined play with a heuristic\textendash{}labeling each position as ``good'' or ``bad''\textendash{}both accelerates learning and promotes transfer to novel games, while also improving model interpretability.},
  archivePrefix = {arXiv},
  eprint = {1809.03406},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Peterson et al. - 2018 - Keep it stupid simple.pdf},
  journal = {arXiv:1809.03406 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@techreport{Peterson2018a,
  title = {Healthy Oscillatory Coordination Is Bounded by Single-Unit Computation.},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/309427},
  abstract = {Oscillations can improve neural coding by grouping action potentials into synchronous windows of activity, but this same effect can harm coding when action potentials become over-synchronized. Diseases ranging from Parkinson's to epilepsy suggest that over-synchronization leads to pathology, but the precise boundary separating healthy from pathological synchrony remains an open theoretical problem. Here we study a simple model that shows how error in individual cells' computations is traded for population-level synchronization. To put the in biological terms accessible to the cell we conceive of a ''voltage budget'' where instantaneous moments of membrane voltage can be partitioned into oscillatory and computational terms. By comparing these budget terms we derive a new set of biologically measurable inequalities that separate healthy from pathological synchrony. Finally, we derive an optimal non-biological algorithm for exchanging computational error with population synchrony.},
  file = {/Users/qualia/Documents/Papers/Peterson and Voytek - 2018 - Healthy oscillatory coordination is bounded by sin.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2018b,
  title = {Healthy Oscillatory Coordination Is Bounded by Single-Unit Computation},
  author = {Peterson, Erik J and Voytek, Bradley},
  year = {2018},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/309427},
  abstract = {Oscillations can improve neural coding by grouping action potentials into synchronous windows of activity, but this same effect can harm coding when action potentials become over-synchronized. Diseases ranging from Parkinson's to epilepsy suggest that oversynchronization leads to pathology, but the precise boundary separating healthy from pathological synchrony remains an open theoretical problem. Here we study a simple model that shows how error in individual cells' computations is traded for population-level synchronization. To put the in biological terms accessible to the cell we conceive of a ``voltage budget'' where instantaneous moments of membrane voltage can be partitioned into oscillatory and computational terms. By comparing these budget terms we derive a new set of biologically measurable inequalities that bound healthy from pathological synchrony. Finally, we derive an optimal non-biological algorithm for exchanging computational error with population synchrony.},
  file = {/Users/qualia/Documents/Papers/Peterson and Voytek - 2018 - Healthy oscillatory coordination is bounded by sin 2.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2019,
  title = {A Way around the Exploration-Exploitation Dilemma},
  author = {Peterson, Erik J and Verstynen, Timothy D},
  year = {2019},
  month = jun,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/671362},
  abstract = {The optimal decision to exploit existing rewards, or explore looking for larger rewards, is known to be a mathematically intractable problem. Here we challenge this fundamental result in the learning and decision sciences by showing that there is an optimal solution if exploitation and exploration are treated as independent, but competing, objectives. To make it independent we re-imagine exploration as an open-ended search for any new information, whose value we define with a new set of universal axioms. We prove a reward-information competition can be solved by a deterministic winner-take-all algorithm. This algorithm has the properties expected of an ideal solution to the original dilemma, maximizing total value with no regret. Our work suggests an answer to all exploration-exploitation questions can found by exploring simply to learn, and not to go looking for reward.},
  file = {/Users/qualia/Zotero/storage/ENLK2CIB/Peterson and Verstynen - 2019 - A way around the exploration-exploitation dilemma.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2019a,
  title = {A Way around the Exploration-Exploitation Dilemma},
  author = {Peterson, Erik J and Verstynen, Timothy D},
  year = {2019},
  month = jun,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/671362},
  abstract = {For all animals the decision to explore comes with a risk of getting less. For example, a foraging bee might find less nectar, or hunting hawk less prey. This loss is often formalized as regret. It's been mathematically proven that exploring an uncertain world with a specific goal always has some regret. This is why exploration-exploitation can be a dilemma. Given this proof we wondered if the common advice to ``focus on learning and not the goal'' might have mathematical merit. So we re-imagined exploration in the dilemma as an open ended search for any new information. We then developed a new minimal description of information value, which generalizes existing ideas like curiosity, novelty and information gain. We use this description to model the dilemma as a competition between strategies that maximize reward and information independently. Here we prove this competition has a no regret solution. When we study this solution in simulation \textendash{} using classic bandit tasks \textendash{} it outperforms standard approaches, especially when rewards are sparse.},
  file = {/Users/qualia/Documents/Papers/Peterson and Verstynen - 2019 - A way around the exploration-exploitation dilemma.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2019b,
  title = {Homeostatic Mechanisms May Shape the Type and Duration of Oscillatory Modulation},
  author = {Peterson, Erik J. and Voytek, Bradley},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/615450},
  abstract = {Neural oscillations are observed ubiquitously in the mammalian brain. However the stability of oscillations is highly variable. Some oscillations are tonic, lasting for seconds or even minutes; others are unstable, appearing only as a single-cycle burst. In a model of hippocampal neurons, we use numerical simulations to show how these different forms of rhythm stability can interact with activity-dependent homeostasis to profoundly alter the modulatory effect of neural oscillations. Under homeostasis, tonic oscillations that are synaptically excitatory have a paradoxical effect; they decrease excitability and desynchronizing firing. Tonic oscillations that are synaptically inhibitory\textendash{}like those in a real hippocampus\textendash{}fail to generate new action potentials and so provoke no homeostatic response. This may explain why the theta rhythm in hippocampus is synaptically inhibitory: inhibitory oscillations don't raise the firing threshold, as excitatory oscillations do, and so can preserve each cell's dynamic range. Based on these simulations, we also speculate that homeostasis may explain why excitatory intra-cortical and intra-layer oscillations often appear as bursts. In our model bursts minimally interact with the slow homeostasis time constant and so retain typical excitatory effects.},
  file = {/Users/qualia/Documents/Papers/Peterson and Voytek - 2019 - Homeostatic mechanisms may shape the type and dura.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Peterson2019c,
  title = {A Way around the Exploration-Exploitation Dilemma},
  author = {Peterson, Erik J and Verstynen, Timothy D},
  year = {2019},
  month = jun,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/671362},
  abstract = {For all animals the decision to explore comes with a risk of getting less. For example, a foraging bee might find less nectar, or hunting hawk less prey. This loss is often formalized as regret. It's been mathematically proven that exploring an uncertain world with a specific goal always has some regret. This is why exploration-exploitation can be a dilemma. Given this proof we wondered if the common advice to ``focus on learning and not the goal'' might have mathematical merit. So we re-imagined exploration in the dilemma as an open ended search for any new information. We then developed a new minimal description of information value, which generalizes existing ideas like curiosity, novelty and information gain. We use this description to model the dilemma as a competition between strategies that maximize reward and information independently. Here we prove this competition has a no regret solution. When we study this solution in simulation \textendash{} using classic bandit tasks \textendash{} it outperforms standard approaches, especially when rewards are sparse.},
  file = {/Users/qualia/Documents/Papers/Peterson and Verstynen - 2019 - A way around the exploration-exploitation dilemma 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Pfurtscheller1996,
  title = {Event-Related Synchronization ({{ERS}}) in the Alpha Band \textemdash{} an Electrophysiological Correlate of Cortical Idling: {{A}} Review},
  shorttitle = {Event-Related Synchronization ({{ERS}}) in the Alpha Band \textemdash{} an Electrophysiological Correlate of Cortical Idling},
  author = {Pfurtscheller, G. and Stanc{\'a}k, A. and Neuper, Ch.},
  year = {1996},
  month = nov,
  volume = {24},
  pages = {39--46},
  issn = {01678760},
  doi = {10.1016/S0167-8760(96)00066-9},
  abstract = {EEG desynchronization is a reliable correlate of excited neural structures or activated cortical areas. EEG synchronization within the alpha band may be an electrophysiological correlate of deactivated cortical areas. Such areas are not processing sensory information or motor output and can be considered to be in an idling state. One example of such an idling cortical area is the enhancement of mu rhythms in the primary hand area during visual processing or during foot movement. In both circumstances, the neurons in the hand area are not needed for visual processing or preparation for foot movement. As a result of this, an enhanced hand area mu rhythm can be observed.},
  file = {/Users/qualia/Documents/Papers/1996 - Pfurtscheller, Stancák, Neuper - Event-related synchronization (ERS) in the alpha band - An electrophysiological correlate of co.pdf},
  journal = {International Journal of Psychophysiology},
  language = {en},
  number = {1-2}
}

@article{Phillips2003,
  title = {Convergence of Biological and Psychological Perspectives on Cognitive Coordination in Schizophrenia},
  author = {Phillips, William A. and Silverstein, Steven M.},
  year = {2003},
  month = feb,
  volume = {26},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X03000025},
  abstract = {The concept of locally specialized functions dominates research on higher brain function and its disorders. Locally specialized functions must be complemented by processes that coordinate those functions, however, and impairment of coordinating processes may be central to some psychotic conditions. Evidence for processes that coordinate activity is provided by neurobiological and psychological studies of contextual disambiguation and dynamic grouping. Mechanisms by which this important class of cognitive functions could be achieved include those long-range connections within and between cortical regions that activate synaptic channels via NMDAreceptors, and which control gain through their voltage-dependent mode of operation. An impairment of these mechanisms is central to PCP-psychosis, and the cognitive capabilities that they could provide are impaired in some forms of schizophrenia. We conclude that impaired cognitive coordination due to reduced ion flow through NMDA-channels is involved in schizophrenia, and we suggest that it may also be involved in other disorders. This perspective suggests several ways in which further research could enhance our understanding of cognitive coordination, its neural basis, and its relevance to psychopathology.},
  file = {/Users/qualia/Documents/Papers/2003 - Phillips, Silverstein - Convergence of biological and psychological perspectives on cognitive coordination in schizophrenia.pdf},
  journal = {Behavioral and Brain Sciences},
  language = {en},
  number = {01}
}

@article{Phillips2018,
  title = {Face Recognition Accuracy of Forensic Examiners, Superrecognizers, and Face Recognition Algorithms},
  author = {Phillips, P. Jonathon and Yates, Amy N. and Hu, Ying and Hahn, Carina A. and Noyes, Eilidh and Jackson, Kelsey and Cavazos, Jacqueline G. and Jeckeln, G{\'e}raldine and Ranjan, Rajeev and Sankaranarayanan, Swami and Chen, Jun-Cheng and Castillo, Carlos D. and Chellappa, Rama and White, David and O'Toole, Alice J.},
  year = {2018},
  month = jun,
  volume = {115},
  pages = {6171--6176},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1721355115},
  file = {/Users/qualia/Documents/Papers/Phillips et al. - 2018 - Face recognition accuracy of forensic examiners, s.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {24}
}

@book{Pierce2002,
  title = {Types and Programming Languages},
  author = {Pierce, Benjamin C.},
  year = {2002},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {/Users/qualia/Documents/Papers/2000 - Ferreira - IUuI for Programming Languages.pdf},
  isbn = {978-0-262-16209-8},
  keywords = {Programming languages (Electronic computers)},
  language = {en},
  lccn = {QA76.7 .P54 2002}
}

@article{Pinotsis2011,
  title = {Neural Fields, Spectral Responses and Lateral Connections},
  author = {Pinotsis, D.A. and Friston, K.J.},
  year = {2011},
  month = mar,
  volume = {55},
  pages = {39--48},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.11.081},
  abstract = {This paper describes a neural field model for local (mesoscopic) dynamics on the cortical surface. Our focus is on sparse intrinsic connections that are characteristic of real cortical microcircuits. This sparsity is modelled with radial connectivity functions or kernels with non-central peaks. The ensuing analysis allows one to generate or predict spectral responses to known exogenous input or random fluctuations. Here, we characterise the effect of different connectivity architectures (the range, dispersion and propagation speed of intrinsic or lateral connections) and synaptic gains on spatiotemporal dynamics. Specifically, we look at spectral responses to random fluctuations and examine the ability of synaptic gain and connectivity parameters to induce Turing instabilities. We find that although the spatial deployment and speed of lateral connections can have a profound affect on the behaviour of spatial modes over different scales, only synaptic gain is capable of producing phase-transitions. We discuss the implications of these findings for the use of neural fields as generative models in dynamic causal modeling (DCM).},
  file = {/Users/qualia/Documents/Papers/2011 - Pinotsis, Friston - Neural fields, spectral responses and lateral connections.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Pinotsis2013,
  title = {On Conductance-Based Neural Field Models},
  author = {Pinotsis, Dimitris A. and Leite, Marco and Friston, Karl J.},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00158},
  file = {/Users/qualia/Documents/Papers/2013 - Pinotsis, Leite, Friston - On conductance-based neural field models.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Pinotsis2014,
  title = {Neural Masses and Fields: Modeling the Dynamics of Brain Activity},
  shorttitle = {Neural Masses and Fields},
  author = {Pinotsis, Dimitris and Robinson, Peter and {beim Graben}, Peter and Friston, Karl},
  year = {2014},
  month = nov,
  volume = {8},
  issn = {1662-5188},
  doi = {10.3389/fncom.2014.00149},
  file = {/Users/qualia/Documents/Papers/2014 - Pinotsis et al. - Neural masses and fields modeling the dynamics of brain activity.pdf;/Users/qualia/Documents/Papers/Pinotsis et al. - 2014 - Neural masses and fields modeling the dynamics of.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@techreport{Pisupati2019,
  title = {Lapses in Perceptual Judgments Reflect Exploration},
  author = {Pisupati, Sashank and {Chartarifsky-Lynn}, Lital and Khanal, Anup and Churchland, Anne K.},
  year = {2019},
  month = apr,
  institution = {{Neuroscience}},
  doi = {10.1101/613828},
  abstract = {ABSTRACT
          During perceptual decision making, subjects often display a constant rate of errors independent of evidence strength, referred to as ``lapses''. Their proper treatment is crucial for accurate estimation of perceptual parameters, however they are often treated as a nuisance arising from motor errors or inattention. Here, we propose that lapses can instead reflect a dynamic form of exploration. We demonstrate that perceptual uncertainty modulates the probability of lapses both across and within modalities on a multisensory discrimination task in rats. These effects cannot be accounted for by inattention or motor error, however they are concisely explained by uncertainty-guided exploration. We confirm the predictions of the exploration model by showing that changing the magnitude or probability of reward associated with one of the decisions selectively affects the lapses associated with that decision in uncertain conditions, while leaving ``sure-bet'' decisions unchanged, as predicted by the model. Finally, we demonstrate that muscimol inactivations of secondary motor cortex and posterior striatum affect lapses asymmetrically across modalities. The inactivations can be captured by a devaluation of actions corresponding to the inactivated side, and do not affect ``sure-bet'' decisions. Together, our results suggest that far from being a nuisance, lapses are informative about subjects' action values and deficits thereof during perceptual decisions.},
  file = {/Users/qualia/Documents/Papers/Pisupati et al. - 2019 - Lapses in perceptual judgments reflect exploration.pdf},
  language = {en},
  type = {Preprint}
}

@article{Plenz1999,
  title = {A Basal Ganglia Pacemaker Formed by the Subthalamic Nucleus and External Globus Pallidus},
  author = {Plenz, Dietmar and Kital, Stephen T.},
  year = {1999},
  month = aug,
  volume = {400},
  pages = {677--682},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/23281},
  file = {/Users/qualia/Documents/Papers/1999 - Plenz, Kital - A basal ganglia pacemaker formed by the subthalamic nucleus and external globus pallidus.pdf},
  journal = {Nature},
  language = {en},
  number = {6745}
}

@article{Pluta2015,
  title = {A Direct Translaminar Inhibitory Circuit Tunes Cortical Output},
  author = {Pluta, Scott and Naka, Alexander and Veit, Julia and Telian, Gregory and Yao, Lucille and Hakim, Richard and Taylor, David and Adesnik, Hillel},
  year = {2015},
  month = nov,
  volume = {18},
  pages = {1631--1640},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4123},
  file = {/Users/qualia/Documents/Papers/2015 - Pluta et al. - A direct translaminar inhibitory circuit tunes cortical output.pdf;/Users/qualia/Documents/Papers/Pluta et al. - 2015 - A direct translaminar inhibitory circuit tunes cor.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Podlaski2017,
  title = {Mapping the Function of Neuronal Ion Channels in Model and Experiment},
  author = {Podlaski, William F and Seeholzer, Alexander and Groschner, Lukas N and Miesenb{\"o}ck, Gero and Ranjan, Rajnish and Vogels, Tim P},
  year = {2017},
  month = mar,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.22152},
  file = {/Users/qualia/Documents/Papers/Podlaski et al. - 2017 - Mapping the function of neuronal ion channels in m.pdf},
  journal = {eLife},
  language = {en}
}

@article{Podvalny2015,
  title = {A Unifying Principle Underlying the Extracellular Field Potential Spectral Responses in the Human Cortex},
  author = {Podvalny, Ella and Noy, Niv and Harel, Michal and Bickel, Stephan and Chechik, Gal and Schroeder, Charles E. and Mehta, Ashesh D. and Tsodyks, Misha and Malach, Rafael},
  year = {2015},
  month = jul,
  volume = {114},
  pages = {505--519},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00943.2014},
  file = {/Users/qualia/Documents/Papers/Podvalny et al. - 2015 - A unifying principle underlying the extracellular  2.pdf;/Users/qualia/Documents/Papers/Podvalny et al. - 2015 - A unifying principle underlying the extracellular .pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Poggio2017,
  title = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality: {{A}} Review},
  shorttitle = {Why and When Can Deep-but Not Shallow-Networks Avoid the Curse of Dimensionality},
  author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  year = {2017},
  month = oct,
  volume = {14},
  pages = {503--519},
  issn = {1476-8186, 1751-8520},
  doi = {10.1007/s11633-017-1054-2},
  abstract = {The paper characterizes classes of functions for which deep learning can be exponentially better than shallow learning. Deep convolutional networks are a special case of these conditions, though weight sharing is not the main reason for their exponential advantage.},
  file = {/Users/qualia/Documents/Papers/Poggio et al. - 2017 - Why and when can deep-but not shallow-networks avo.pdf},
  journal = {International Journal of Automation and Computing},
  language = {en},
  number = {5}
}

@article{Pogosyan2009,
  title = {Boosting {{Cortical Activity}} at {{Beta}}-{{Band Frequencies Slows Movement}} in {{Humans}}},
  author = {Pogosyan, Alek and Gaynor, Louise Doyle and Eusebio, Alexandre and Brown, Peter},
  year = {2009},
  month = oct,
  volume = {19},
  pages = {1637--1641},
  issn = {09609822},
  doi = {10.1016/j.cub.2009.07.074},
  abstract = {Neurons have a striking tendency to engage in oscillatory activities. One important type of oscillatory activity prevalent in the motor system occurs in the beta frequency band, at about 20 Hz. It is manifest during the maintenance of tonic contractions and is suppressed prior to and during voluntary movement [1\textendash{}7]. This and other correlative evidence suggests that beta activity might promote tonic contraction, while impairing motor processing related to new movements [3, 8, 9]. Hence, bursts of beta activity in the cortex are associated with a strengthening of the motor effects of sensory feedback during tonic contraction and with reductions in the velocity of voluntary movements [9\textendash{}11]. Moreover, beta activity is increased when movement has to be resisted or voluntarily suppressed [7, 12, 13]. Here we use imperceptible transcranial alternating-current stimulation to entrain cortical activity at 20 Hz in healthy subjects and show that this slows voluntary movement. The present findings are the first direct evidence of causality between any physiological oscillatory brain activity and concurrent motor behavior in the healthy human and help explain how the exaggerated beta activity found in Parkinson's disease can lead to motor slowing in this illness [14].},
  file = {/Users/qualia/Documents/Papers/2009 - Pogosyan et al. - Boosting Cortical Activity at Beta-Band Frequencies Slows Movement in Humans.pdf},
  journal = {Current Biology},
  language = {en},
  number = {19}
}

@article{Poirazi2003,
  title = {Pyramidal {{Neuron}} as {{Two}}-{{Layer Neural Network}}},
  author = {Poirazi, Panayiota and Brannon, Terrence and Mel, Bartlett W.},
  year = {2003},
  month = mar,
  volume = {37},
  pages = {989--999},
  issn = {08966273},
  doi = {10.1016/S0896-6273(03)00149-1},
  abstract = {The pyramidal neuron is the principal cell type in the mammalian forebrain, but its function remains poorly understood. Using a detailed compartmental model of a hippocampal CA1 pyramidal cell, we recorded responses to complex stimuli consisting of dozens of high-frequency activated synapses distributed throughout the apical dendrites. We found the cell's firing rate could be predicted by a simple formula that maps the physical components of the cell onto those of an abstract two-layer ``neural network.'' In the first layer, synaptic inputs drive independent sigmoidal subunits corresponding to the cell's several dozen long, thin terminal dendrites. The subunit outputs are then summed within the main trunk and cell body prior to final thresholding. We conclude that insofar as the neural code is mediated by average firing rate, a twolayer neural network may provide a useful abstraction for the computing function of the individual pyramidal neuron.},
  file = {/Users/qualia/Documents/Papers/2003 - Poirazi, Brannon, Mel - Pyramidal Neuron as Two-Layered Neural Network.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@incollection{Polani2001,
  title = {An {{Information}}-{{Theoretic Approach}} for the {{Quantification}} of {{Relevance}}},
  booktitle = {Advances in {{Artificial Life}}},
  author = {Polani, Daniel and Martinetz, Thomas and Kim, Jan},
  editor = {Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Kelemen, Jozef and Sos{\'i}k, Petr},
  year = {2001},
  volume = {2159},
  pages = {704--713},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-44811-X_82},
  file = {/Users/qualia/Documents/Papers/1999 - Monmarche, Venturini - Advances in Artificial Life.pdf},
  isbn = {978-3-540-42567-0 978-3-540-44811-2}
}

@article{Pollard2016,
  title = {A {{Second Law}} for {{Open Markov Processes}}},
  author = {Pollard, Blake S.},
  year = {2016},
  month = mar,
  volume = {23},
  pages = {1650006},
  issn = {1230-1612, 1793-7191},
  doi = {10.1142/S1230161216500062},
  abstract = {In this paper we define the notion of an open Markov process. An open Markov process is a generalization of an ordinary Markov process in which populations are allowed to flow in and out of the system at certain boundary states. We show that the rate of change of relative entropy in an open Markov process is less than or equal to the flow of relative entropy through its boundary states. This can be viewed as a generalization of the Second Law for open Markov processes. In the case of a Markov process whose equilibrium obeys detailed balance, this inequality puts an upper bound on the rate of change of the free energy for any non-equilibrium distribution.},
  archivePrefix = {arXiv},
  eprint = {1410.6531},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Pollard - 2016 - A Second Law for Open Markov Processes.pdf},
  journal = {Open Systems \& Information Dynamics},
  keywords = {Condensed Matter - Statistical Mechanics},
  language = {en},
  number = {01}
}

@article{Polsky2004,
  title = {Computational Subunits in Thin Dendrites of Pyramidal Cells},
  author = {Polsky, Alon and Mel, Bartlett W and Schiller, Jackie},
  year = {2004},
  month = jun,
  volume = {7},
  pages = {621--627},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1253},
  file = {/Users/qualia/Documents/Papers/2004 - Polsky, Mel, Schiller - Computational subunits in thin dendrites of pyramidal cells.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Poo2009,
  title = {Odor {{Representations}} in {{Olfactory Cortex}}: ``{{Sparse}}'' {{Coding}}, {{Global Inhibition}}, and {{Oscillations}}},
  shorttitle = {Odor {{Representations}} in {{Olfactory Cortex}}},
  author = {Poo, Cindy and Isaacson, Jeffry S.},
  year = {2009},
  month = jun,
  volume = {62},
  pages = {850--861},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.05.022},
  abstract = {The properties of cortical circuits underlying central representations of sensory stimuli are poorly understood. Here we use in vivo cell-attached and wholecell voltage-clamp recordings to reveal how excitatory and inhibitory synaptic input govern odor representations in rat primary olfactory (piriform) cortex. We show that odors evoke spiking activity that is sparse across the cortical population. We find that unbalanced synaptic excitation and inhibition underlie sparse activity: inhibition is widespread and broadly tuned, while excitation is less common and odor-specific. ``Global'' inhibition can be explained by local interneurons that receive ubiquitous and nonselective odor-evoked excitation. In the temporal domain, while respiration imposes a slow rhythm to olfactory cortical responses, odors evoke fast (15-30 Hz) oscillations in synaptic activity. Oscillatory excitation precedes inhibition, generating brief time windows for precise and temporally sparse spike output. Together, our results reveal that global inhibition and oscillations are major synaptic mechanisms shaping odor representations in olfactory cortex.},
  file = {/Users/qualia/Documents/Papers/2009 - Poo, Isaacson - Odor Representations in Olfactory Cortex Sparse Coding, Global Inhibition, and Oscillations.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Popov,
  title = {Alpha Oscillations Govern Interhemispheric Spike Timing Coordination in the Honey Bee Brain},
  author = {Popov, Tzvetan and Szyszka, Paul},
  pages = {10},
  abstract = {In 1929 Hans Berger discovered the alpha oscillations: a prominent, ongoing 10 Hz rhythm in the human EEG. These alpha oscillations are amongst the most widely studied cerebral signals, related to cognitive phenomena such as attention, memory and consciousness. However, the mechanisms by which alpha oscillations affect cognition await demonstration. Here we provide a novel model system from an adequately described complex neural circuit of the honey bee (Apis mellifera), that exhibits properties of the alpha oscillations. We found a prominent alpha wavelike ongoing neural activity (\textasciitilde{} 18 Hz) that is reduced in amplitude upon stimulus presentation. The phase of this alpha activity biased both neuronal spikes and amplitude of high frequency gamma activity ({$>$} 30 Hz). These results suggest a common role of oscillatory neuronal activity across phyla and provide an unprecedented new venue for causal studies on the relationship between neuronal spikes, brain oscillations and cognition.},
  file = {/Users/qualia/Documents/Papers/Popov and Szyszka - Alpha oscillations govern interhemispheric spike t.pdf},
  language = {en}
}

@article{Popova,
  title = {Alpha Oscillations Govern Interhemispheric Spike Timing Coordination in the Honey Bee Brain},
  author = {Popov, Tzvetan and Szyszka, Paul},
  pages = {7},
  file = {/Users/qualia/Documents/Papers/Popov and Szyszka - Alpha oscillations govern interhemispheric spike t 2.pdf},
  language = {en}
}

@article{Porter2014,
  title = {Dynamical {{Systems}} on {{Networks}}: {{A Tutorial}}},
  shorttitle = {Dynamical {{Systems}} on {{Networks}}},
  author = {Porter, Mason A. and Gleeson, James P.},
  year = {2014},
  month = mar,
  abstract = {We give a tutorial for the study of dynamical systems on networks. We focus especially on "simple" situations that are tractable analytically, because they can be very insightful and provide useful springboards for the study of more complicated scenarios. We briefly motivate why examining dynamical systems on networks is interesting and important, and we then give several fascinating examples and discuss some theoretical results. We also briefly discuss dynamical systems on dynamical (i.e., time-dependent) networks, overview software implementations, and give an outlook on the field.},
  archivePrefix = {arXiv},
  eprint = {1403.7663},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Porter and Gleeson - 2014 - Dynamical Systems on Networks A Tutorial.pdf},
  journal = {arXiv:1403.7663 [cond-mat, physics:nlin, physics:physics]},
  keywords = {Computer Science - Social and Information Networks,Condensed Matter - Disordered Systems and Neural Networks,Condensed Matter - Statistical Mechanics,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Physics and Society},
  language = {en},
  primaryClass = {cond-mat, physics:nlin, physics:physics}
}

@article{Porto-Pazos2011,
  title = {Artificial {{Astrocytes Improve Neural Network Performance}}},
  author = {{Porto-Pazos}, Ana B. and Veiguela, Noha and Mesejo, Pablo and Navarrete, Marta and Alvarellos, Alberto and Ib{\'a}{\~n}ez, Oscar and Pazos, Alejandro and Araque, Alfonso},
  editor = {Am{\'e}d{\'e}e, Thierry},
  year = {2011},
  month = apr,
  volume = {6},
  pages = {e19109},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019109},
  abstract = {Compelling evidence indicates the existence of bidirectional communication between astrocytes and neurons. Astrocytes, a type of glial cells classically considered to be passive supportive cells, have been recently demonstrated to be actively involved in the processing and regulation of synaptic information, suggesting that brain function arises from the activity of neuron-glia networks. However, the actual impact of astrocytes in neural network function is largely unknown and its application in artificial intelligence remains untested. We have investigated the consequences of including artificial astrocytes, which present the biologically defined properties involved in astrocyte-neuron communication, on artificial neural network performance. Using connectionist systems and evolutionary algorithms, we have compared the performance of artificial neural networks (NN) and artificial neuron-glia networks (NGN) to solve classification problems. We show that the degree of success of NGN is superior to NN. Analysis of performances of NN with different number of neurons or different architectures indicate that the effects of NGN cannot be accounted for an increased number of network elements, but rather they are specifically due to astrocytes. Furthermore, the relative efficacy of NGN vs. NN increases as the complexity of the network increases. These results indicate that artificial astrocytes improve neural network performance, and established the concept of Artificial Neuron-Glia Networks, which represents a novel concept in Artificial Intelligence with implications in computational science as well as in the understanding of brain function.},
  file = {/Users/qualia/Documents/Papers/Porto-Pazos et al. - 2011 - Artificial Astrocytes Improve Neural Network Perfo.PDF},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Porto-Pazos2011a,
  title = {Artificial {{Astrocytes Improve Neural Network Performance}}},
  author = {{Porto-Pazos}, Ana B. and Veiguela, Noha and Mesejo, Pablo and Navarrete, Marta and Alvarellos, Alberto and Ib{\'a}{\~n}ez, Oscar and Pazos, Alejandro and Araque, Alfonso},
  editor = {Am{\'e}d{\'e}e, Thierry},
  year = {2011},
  month = apr,
  volume = {6},
  pages = {e19109},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0019109},
  abstract = {Compelling evidence indicates the existence of bidirectional communication between astrocytes and neurons. Astrocytes, a type of glial cells classically considered to be passive supportive cells, have been recently demonstrated to be actively involved in the processing and regulation of synaptic information, suggesting that brain function arises from the activity of neuron-glia networks. However, the actual impact of astrocytes in neural network function is largely unknown and its application in artificial intelligence remains untested. We have investigated the consequences of including artificial astrocytes, which present the biologically defined properties involved in astrocyte-neuron communication, on artificial neural network performance. Using connectionist systems and evolutionary algorithms, we have compared the performance of artificial neural networks (NN) and artificial neuron-glia networks (NGN) to solve classification problems. We show that the degree of success of NGN is superior to NN. Analysis of performances of NN with different number of neurons or different architectures indicate that the effects of NGN cannot be accounted for an increased number of network elements, but rather they are specifically due to astrocytes. Furthermore, the relative efficacy of NGN vs. NN increases as the complexity of the network increases. These results indicate that artificial astrocytes improve neural network performance, and established the concept of Artificial Neuron-Glia Networks, which represents a novel concept in Artificial Intelligence with implications in computational science as well as in the understanding of brain function.},
  file = {/Users/qualia/Documents/Papers/Porto-Pazos et al. - 2011 - Artificial Astrocytes Improve Neural Network Perfo 2.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Posner1980,
  title = {Orienting of Attention},
  author = {Posner, Michael I.},
  year = {1980},
  month = feb,
  volume = {32},
  pages = {3--25},
  issn = {0033-555X},
  doi = {10.1080/00335558008248231},
  file = {/Users/qualia/Documents/Papers/1980 - Posner - ORIENTING OF ATTENTION.pdf;/Users/qualia/Documents/Papers/Posner - 1980 - Orienting of attention.pdf},
  journal = {Quarterly Journal of Experimental Psychology},
  language = {en},
  number = {1}
}

@article{Potjans2014,
  title = {The {{Cell}}-{{Type Specific Cortical Microcircuit}}: {{Relating Structure}} and {{Activity}} in a {{Full}}-{{Scale Spiking Network Model}}},
  shorttitle = {The {{Cell}}-{{Type Specific Cortical Microcircuit}}},
  author = {Potjans, Tobias C. and Diesmann, Markus},
  year = {2014},
  month = mar,
  volume = {24},
  pages = {785--806},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhs358},
  abstract = {In the past decade, the cell-type specific connectivity and activity of local cortical networks have been characterized experimentally to some detail. In parallel, modeling has been established as a tool to relate network structure to activity dynamics. While available comprehensive connectivity maps (Thomson, West, et al. 2002; Binzegger et al. 2004) have been used in various computational studies, prominent features of the simulated activity such as the spontaneous firing rates do not match the experimental findings. Here, we analyze the properties of these maps to compile an integrated connectivity map, which additionally incorporates insights on the specific selection of target types. Based on this integrated map, we build a full-scale spiking network model of the local cortical microcircuit. The simulated spontaneous activity is asynchronous irregular and cell-type specific firing rates are in agreement with in vivo recordings in awake animals, including the low rate of layer 2/3 excitatory cells. The interplay of excitation and inhibition captures the flow of activity through cortical layers after transient thalamic stimulation. In conclusion, the integration of a large body of the available connectivity data enables us to expose the dynamical consequences of the cortical microcircuitry.},
  file = {/Users/qualia/Documents/Papers/2014 - Potjans, Diesmann - The cell-type specific cortical microcircuit Relating structure and activity in a full-scale spiking network.pdf;/Users/qualia/Documents/Papers/Potjans and Diesmann - 2014 - The Cell-Type Specific Cortical Microcircuit Rela.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {3}
}

@article{Poucet1993,
  title = {Spatial Cognitive Maps in Animals: New Hypotheses on Their Structure and Neural Mechanisms.},
  author = {Poucet, B},
  year = {1993},
  volume = {100},
  pages = {162--183},
  file = {/Users/qualia/Documents/Papers/Poucet1993.pdf},
  journal = {Psycholocial Review},
  number = {1}
}

@article{Pouget2000,
  title = {Information Processing with Population Codes},
  author = {Pouget, Alexandre and Dayan, Peter and Zemel, Richard},
  year = {2000},
  month = nov,
  volume = {1},
  pages = {125--132},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/35039062},
  file = {/Users/qualia/Documents/Papers/Pouget et al. - 2000 - Information processing with population codes.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {2}
}

@techreport{Prat-Carrabin2019,
  title = {Human Inference in Changing Environments},
  author = {{Prat-Carrabin}, Arthur and Wilson, Robert C. and Cohen, Jonathan D. and {da Silveira}, Rava Azeredo},
  year = {2019},
  month = jul,
  institution = {{Neuroscience}},
  doi = {10.1101/720516},
  abstract = {In past decades, the Bayesian paradigm has gained traction as a principled account of human behavior in inference tasks. Yet this success is tainted by the ubiquity of behavioral suboptimality and variability. We explore these discrepancies using an online inference task, in which we modulate the temporal statistics of hidden change points. We show that humans adapt their inference process to the implicit temporal statistics of stimuli, thereby behaving in an approximate Bayesian fashion. However, they exhibit biases and variability, and these depend on the history of stimuli. A systematic study of a broad family of optimal and suboptimal models indicates that noise arises `internally'\textemdash{}in the inference process itself\textemdash{}rather than at the behavioral output. Specifically, we argue that humans mimic Bayesian inference by approximating the posterior with a modest number of samples. Our results contribute to a growing literature on sample-based cognition and compression by stochastic pruning.},
  file = {/Users/qualia/Documents/Papers/Prat-Carrabin et al. - 2019 - Human inference in changing environments.pdf},
  language = {en},
  type = {Preprint}
}

@article{Prescott2006,
  title = {A Robot Model of the Basal Ganglia: {{Behavior}} and Intrinsic Processing},
  shorttitle = {A Robot Model of the Basal Ganglia},
  author = {Prescott, Tony J. and Montes Gonz{\'a}lez, Fernando M. and Gurney, Kevin and Humphries, Mark D. and Redgrave, Peter},
  year = {2006},
  month = jan,
  volume = {19},
  pages = {31--61},
  issn = {08936080},
  doi = {10.1016/j.neunet.2005.06.049},
  abstract = {The existence of multiple parallel loops connecting sensorimotor systems to the basal ganglia has given rise to proposals that these nuclei serve as a selection mechanism resolving competitions between the alternative actions available in a given context. A strong test of this hypothesis is to require a computational model of the basal ganglia to generate integrated selection sequences in an autonomous agent, we therefore describe a robot architecture into which such a model is embedded, and require it to control action selection in a robotic task inspired by animal observations. Our results demonstrate effective action selection by the embedded model under a wide range of sensory and motivational conditions. When confronted with multiple, high salience alternatives, the robot also exhibits forms of behavioral disintegration that show similarities to animal behavior in conflict situations. The model is shown to cast light on recent neurobiological findings concerning behavioral switching and sequencing.},
  file = {/Users/qualia/Documents/Papers/2006 - Prescott et al. - A robot model of the basal ganglia Behavior and intrinsic processing.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {1}
}

@article{Preston2008,
  title = {Multivoxel {{Pattern Selectivity}} for {{Perceptually Relevant Binocular Disparities}} in the {{Human Brain}}},
  author = {Preston, T. J. and Li, S. and Kourtzi, Z. and Welchman, A. E.},
  year = {2008},
  month = oct,
  volume = {28},
  pages = {11315--11327},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2728-08.2008},
  file = {/Users/qualia/Documents/Papers/2008 - Preston et al. - Multivoxel pattern selectivity for perceptually relevant binocular disparities in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {44}
}

@article{Principe2015,
  title = {Representing and Decomposing Neural Potential Signals},
  author = {Principe, Jose C and Brockmeier, Austin J},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {13--17},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.07.023},
  file = {/Users/qualia/Documents/Papers/Principe and Brockmeier - 2015 - Representing and decomposing neural potential sign.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Prinz2004,
  title = {The Dynamic Clamp Comes of Age},
  author = {Prinz, Astrid A and Abbott, L.F and Marder, Eve},
  year = {2004},
  month = apr,
  volume = {27},
  pages = {218--224},
  issn = {01662236},
  doi = {10.1016/j.tins.2004.02.004},
  file = {/Users/qualia/Documents/Papers/2004 - Prinz, Abbott, Marder - The dynamic clamp comes of age.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {4}
}

@article{Prinz2007,
  title = {Shaul {{Druckmann}} 1,{${_\ast}$}, {{Yoav Banitt}} 2, {{Albert Gidon}} 2, {{Felix Schu}}\textasciidieresis{} Rmann 3, {{Henry Markram Idan Segev}}},
  author = {Prinz, Astrid},
  year = {2007},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/2007 - Druckmann et al. - Idan Segev.pdf},
  journal = {Frontiers in Neuroscience},
  language = {en}
}

@article{Pritchett2015,
  title = {For Things Needing Your Attention: The Role of Neocortical Gamma in Sensory Perception},
  shorttitle = {For Things Needing Your Attention},
  author = {Pritchett, Dominique L and Siegle, Joshua H and Deister, Christopher A and Moore, Christopher I},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {254--263},
  issn = {09594388},
  doi = {10.1016/j.conb.2015.02.004},
  file = {/Users/qualia/Documents/Papers/Pritchett et al. - 2015 - For things needing your attention the role of neo.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@inproceedings{Pugh2015,
  title = {Confronting the {{Challenge}} of {{Quality Diversity}}},
  booktitle = {Proceedings of the 2015 on {{Genetic}} and {{Evolutionary Computation Conference}} - {{GECCO}} '15},
  author = {Pugh, Justin K. and Soros, L. B. and Szerlip, Paul A. and Stanley, Kenneth O.},
  year = {2015},
  pages = {967--974},
  publisher = {{ACM Press}},
  address = {{Madrid, Spain}},
  doi = {10.1145/2739480.2754664},
  abstract = {In contrast to the conventional role of evolution in evolutionary computation (EC) as an optimization algorithm, a new class of evolutionary algorithms has emerged in recent years that instead aim to accumulate as diverse a collection of discoveries as possible, yet where each variant in the collection is as fit as it can be. Often applied in both neuroevolution and morphological evolution, these new quality diversity (QD) algorithms are particularly well-suited to evolution's inherent strengths, thereby offering a promising niche for EC within the broader field of machine learning. However, because QD algorithms are so new, until now no comprehensive study has yet attempted to systematically elucidate their relative strengths and weaknesses under different conditions. Taking a first step in this direction, this paper introduces a new benchmark domain designed specifically to compare and contrast QD algorithms. It then shows how the degree of alignment between the measure of quality and the behavior characterization (which is an essential component of all QD algorithms to date) impacts the ultimate performance of different such algorithms. The hope is that this initial study will help to stimulate interest in QD and begin to unify the disparate ideas in the area.},
  file = {/Users/qualia/Documents/Papers/Pugh et al. - 2015 - Confronting the Challenge of Quality Diversity.pdf},
  isbn = {978-1-4503-3472-3},
  language = {en}
}

@incollection{Pugh2016,
  title = {Searching for {{Quality Diversity When Diversity}} Is {{Unaligned}} with {{Quality}}},
  booktitle = {Parallel {{Problem Solving}} from {{Nature}} \textendash{} {{PPSN XIV}}},
  author = {Pugh, Justin K. and Soros, L. B. and Stanley, Kenneth O.},
  editor = {Handl, Julia and Hart, Emma and Lewis, Peter R. and {L{\'o}pez-Ib{\'a}{\~n}ez}, Manuel and Ochoa, Gabriela and Paechter, Ben},
  year = {2016},
  volume = {9921},
  pages = {880--889},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-45823-6_82},
  abstract = {Inspired by natural evolution's affinity for discovering a wide variety of successful organisms, a new evolutionary search paradigm has emerged wherein the goal is not to find the single best solution but rather to collect a diversity of unique phenotypes where each variant is as good as it can be. These quality diversity (QD) algorithms therefore must explore multiple promising niches simultaneously. A QD algorithm's diversity component, formalized by specifying a behavior characterization (BC), not only generates diversity but also promotes quality by helping to overcome deception in the fitness landscape. However, some BCs (particularly those that are unaligned with the notion of quality) do not adequately mitigate deception, rendering QD algorithms unable to discover the best-performing solutions on difficult problems. This paper introduces a solution that enables QD algorithms to pursue arbitrary notions of diversity without compromising their ability to solve hard problems: driving search with multiple BCs simultaneously.},
  file = {/Users/qualia/Documents/Papers/Pugh et al. - 2016 - Searching for Quality Diversity When Diversity is .pdf},
  isbn = {978-3-319-45822-9 978-3-319-45823-6},
  language = {en}
}

@article{Putney,
  title = {Precise Timing Is Ubiquitous, Consistent, and Coordinated across a Comprehensive, Spike-Resolved Flight Motor Program},
  author = {Putney, Joy and Conn, Rachel and Sponberg, Simon},
  pages = {10},
  file = {/Users/qualia/Documents/Papers/Putney et al. - Precise timing is ubiquitous, consistent, and coor.pdf},
  journal = {COMPUTATIONAL BIOLOGY},
  language = {en}
}

@article{Qi2013,
  title = {Firing Patterns in a Conductance-Based Neuron Model: Bifurcation, Phase Diagram, and Chaos},
  shorttitle = {Firing Patterns in a Conductance-Based Neuron Model},
  author = {Qi, Y. and Watts, A. L. and Kim, J. W. and Robinson, P. A.},
  year = {2013},
  month = feb,
  volume = {107},
  pages = {15--24},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-012-0520-8},
  abstract = {Responding to various stimuli, some neurons either remain resting or can fire several distinct patterns of action potentials, such as spiking, bursting, subthreshold oscillations, and chaotic firing. In particular, Wilson's conductance-based neocortical neuron model, derived from the Hodgkin\textendash{}Huxley model, is explored to understand underlying mechanisms of the firing patterns. Phase diagrams describing boundaries between the domains of different firing patterns are obtained via extensive numerical computations. The boundaries are further studied by standard instability analyses, which demonstrates that the chaotic neural firing could develop via period-doubling and/or periodadding cascades. Sequences of the firing patterns often observed in many neural experiments are also discussed in the phase diagram framework developed. Our results lay the groundwork for wider use of the model, especially for incorporating it into neural field modeling of the brain.},
  file = {/Users/qualia/Documents/Papers/2013 - Qi et al. - Firing patterns in a conductance-based neuron model bifurcation, phase diagram, and chaos.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {1}
}

@article{Qi2020,
  title = {Using Machine Learning to Predict Extreme Events in Complex Systems},
  author = {Qi, Di and Majda, Andrew J.},
  year = {2020},
  month = jan,
  volume = {117},
  pages = {52--59},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1917285117},
  abstract = {Extreme events and the related anomalous statistics are ubiquitously observed in many natural systems, and the development of efficient methods to understand and accurately predict such representative features remains a grand challenge. Here, we investigate the skill of deep learning strategies in the prediction of extreme events in complex turbulent dynamical systems. Deep neural networks have been successfully applied to many imaging processing problems involving big data, and have recently shown potential for the study of dynamical systems. We propose to use a densely connected mixed-scale network model to capture the extreme events appearing in a truncated Korteweg\textendash{}de Vries (tKdV) statistical framework, which creates anomalous skewed distributions consistent with recent laboratory experiments for shallow water waves across an abrupt depth change, where a remarkable statistical phase transition is generated by varying the inverse temperature parameter in the corresponding Gibbs invariant measures. The neural network is trained using data without knowing the explicit model dynamics, and the training data are only drawn from the near-Gaussian regime of the tKdV model solutions without the occurrence of large extreme values. A relative entropy loss function, together with empirical partition functions, is proposed for measuring the accuracy of the network output where the dominant structures in the turbulent field are emphasized. The optimized network is shown to gain uniformly high skill in accurately predicting the solutions in a wide variety of statistical regimes, including highly skewed extreme events. The technique is promising to be further applied to other complicated high-dimensional systems.},
  file = {/Users/qualia/Documents/Papers/Qi and Majda - 2020 - Using machine learning to predict extreme events i.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {1}
}

@article{Quinn2011,
  title = {Data Assimilation Using a {{GPU}} Accelerated Path Integral {{Monte Carlo}} Approach},
  author = {Quinn, John C. and Abarbanel, Henry D.I.},
  year = {2011},
  month = sep,
  volume = {230},
  pages = {8168--8178},
  issn = {00219991},
  doi = {10.1016/j.jcp.2011.07.015},
  abstract = {The answers to data assimilation questions can be expressed as path integrals over all possible state and parameter histories. We show how these path integrals can be evaluated numerically using a Markov Chain Monte Carlo method designed to run in parallel on a graphics processing unit (GPU). We demonstrate the application of the method to an example with a transmembrane voltage time series of a simulated neuron as an input, and using a Hodgkin\textendash{}Huxley neuron model. By taking advantage of GPU computing, we gain a parallel speedup factor of up to about 300, compared to an equivalent serial computation on a CPU, with performance increasing as the length of the observation time used for data assimilation increases.},
  file = {/Users/qualia/Documents/Papers/2011 - Quinn, Abarbanel - Data assimilation using a GPU accelerated path integral Monte Carlo approach.pdf},
  journal = {Journal of Computational Physics},
  language = {en},
  number = {22}
}

@article{Radulescu2019,
  title = {Holistic {{Reinforcement Learning}}: {{The Role}} of {{Structure}} and {{Attention}}},
  shorttitle = {Holistic {{Reinforcement Learning}}},
  author = {Radulescu, Angela and Niv, Yael and Ballard, Ian},
  year = {2019},
  month = apr,
  volume = {23},
  pages = {278--292},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.01.010},
  file = {/Users/qualia/Documents/Papers/Radulescu et al. - 2019 - Holistic Reinforcement Learning The Role of Struc.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {4}
}

@article{Rafols1976,
  title = {The Neurons in the Primate Subthalamic Nucleus: {{A Golgi}} and Electron Microscopic Study},
  shorttitle = {The Neurons in the Primate Subthalamic Nucleus},
  author = {Rafols, Jos\dbend{} A. and Fox, Clement A.},
  year = {1976},
  month = jul,
  volume = {168},
  pages = {75--111},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/cne.901680105},
  abstract = {In Golgi preparations of the adult monkey (Macaca mulatta) local interneurons and two varieties of principal neurons, radiating and elongated fusiform, are found in the subthalamic nucleus. The cell bodies of the radiating neurons have a few delicate, somatic spines some of which are occasionally bilobed and trilobed. Five to eight dendritic trunks give rise to branching, tapering dendrites, which may extend for over 400 microns. These dendrites are much thinner than the dendrites in the globus pallidus and the substantia nigra. Some neurons have many and some neurons have few dendritic spines. When numerous the dendritic spines are concentrated on the dendritic trunks and proximal dendrites. The relatively few elongated fusiform neurons are found not only in the capsule but also i n the center of the nucleus. Most dendrites emerge from the opposite poles of their smooth surfaced cell bodies. They have a few dendritic spines. Some of these dendrites extend for more than 750 microns. In 1-micron thick plastic sections lipofuscin granules are present i n some but not all principal neuron cell bodies of the monkey (Macaca mulatta); but these granules are present i n all principal neuron cell bodies of the pig-tail monkey (Macaca nemestrina) and of the squirrel monkey ( S a i m i r i sciureus).},
  file = {/Users/qualia/Documents/Papers/1976 - Rafols, Fox - The neurons in the primate subthalamic nucleus a Golgi and electron microscopic study.pdf;/Users/qualia/Documents/Papers/Rafols and Fox - 1976 - The neurons in the primate subthalamic nucleus A .pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Raghu,
  title = {Can {{Deep Reinforcement Learning Solve Erdos}}-{{Selfridge}}-{{Spencer Games}}?},
  author = {Raghu, Maithra and Irpan, Alex and Andreas, Jacob and Kleinberg, Robert and Le, Quoc and Kleinberg, Jon},
  pages = {13},
  abstract = {Deep reinforcement learning has achieved many recent successes, but our understanding of its strengths and limitations is hampered by the lack of rich environments in which we can fully characterize optimal behavior, and correspondingly diagnose individual actions against such a characterization. Here we consider a family of combinatorial games, arising from work of Erdos, Selfridge, and Spencer, and we propose their use as environments for evaluating and comparing different approaches to reinforcement learning. These games have a number of appealing features: they are challenging for current learning approaches, but they form (i) a low-dimensional, simply parametrized environment where (ii) there is a linear closed form solution for optimal behavior from any state, and (iii) the difficulty of the game can be tuned by changing environment parameters in an interpretable way. We use these Erdos-Selfridge-Spencer games not only to compare different algorithms, but test for generalization, make comparisons to supervised learning, analyze multiagent play, and even develop a self play algorithm.},
  file = {/Users/qualia/Documents/Papers/Raghu et al. - Can Deep Reinforcement Learning Solve Erdos-Selfri.pdf},
  language = {en}
}

@article{Rahnev2011,
  title = {Prior {{Expectation Modulates}} the {{Interaction}} between {{Sensory}} and {{Prefrontal Regions}} in the {{Human Brain}}},
  author = {Rahnev, D. and Lau, H. and {de Lange}, F. P.},
  year = {2011},
  month = jul,
  volume = {31},
  pages = {10741--10748},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1478-11.2011},
  file = {/Users/qualia/Documents/Papers/2011 - Rahnev, Lau, de Lange - Prior expectation modulates the interaction between sensory and prefrontal regions in the human brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {29}
}

@techreport{Rahnev2016,
  title = {Suboptimality in {{Perceptual Decision Making}}},
  author = {Rahnev, Dobromir and Denison, Rachel N},
  year = {2016},
  month = jun,
  institution = {{Neuroscience}},
  doi = {10.1101/060194},
  abstract = {Human perceptual decisions are often described as optimal. Critics of this view have argued that claims of optimality are overly flexible and lack explanatory power. Meanwhile, advocates for optimality have countered that such criticisms single out a few selected papers. To elucidate the issue of optimality in perceptual decision making, we review the extensive literature on suboptimal performance in perceptual tasks. We discuss eight different classes of suboptimal perceptual decisions, including improper placement, maintenance, and adjustment of perceptual criteria, inadequate tradeoff between speed and accuracy, inappropriate confidence ratings, misweightings in cue combination, and findings related to various perceptual illusions and biases. In addition, we discuss conceptual shortcomings of a focus on optimality, such as definitional difficulties and the limited value of optimality claims in and of themselves. We therefore advocate that the field drop its emphasis on whether observed behavior is optimal and instead concentrate on building and testing detailed observer models that explain behavior across a wide range of tasks. To facilitate this transition, we compile the proposed hypotheses regarding the origins of suboptimal perceptual decisions reviewed here. We argue that verifying, rejecting, and expanding these explanations for suboptimal behavior - rather than assessing optimality per se - should be among the major goals of the science of perceptual decision making.},
  file = {/Users/qualia/Documents/Papers/Rahnev and Denison - 2016 - Suboptimality in Perceptual Decision Making.pdf},
  language = {en},
  type = {Preprint}
}

@article{Rahnev2018,
  title = {Suboptimality in Perceptual Decision Making},
  author = {Rahnev, Dobromir and Denison, Rachel N.},
  year = {2018},
  volume = {41},
  pages = {e223},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X18000936},
  abstract = {To deny that human perception is optimal is not to claim that it is suboptimal. Rahnev \& Denison point out that optimality is often ill-defined. The fundamental issue is framing perception as a statistical inference problem. Outside the lab, the real perceptual challenge is to determine the lawful structure of the world, not variables of a predetermined statistical model.},
  file = {/Users/qualia/Documents/Papers/Rahnev and Denison - 2018 - Suboptimality in perceptual decision making.pdf},
  journal = {Behavioral and Brain Sciences},
  language = {en}
}

@article{Raizada,
  title = {What {{Makes Different People's Representations Alike}}: {{Neural Similarity Space Solves}} the {{Problem}} of {{Across}}-Subject {{fMRI Decoding}}},
  author = {Raizada, Rajeev D S and Connolly, Andrew C},
  volume = {24},
  pages = {10},
  file = {/Users/qualia/Documents/Papers/2012 - Raizada, Connolly - What Makes Different Peopleʼs Representations Alike Neural Similarity Space Solves the Problem of Across-su.pdf},
  language = {en},
  number = {4}
}

@article{Raizada2010,
  title = {Linking Brain-Wide Multivoxel Activation Patterns to Behaviour: {{Examples}} from Language and Math},
  shorttitle = {Linking Brain-Wide Multivoxel Activation Patterns to Behaviour},
  author = {Raizada, Rajeev D.S. and Tsao, Feng-Ming and Liu, Huei-Mei and Holloway, Ian D. and Ansari, Daniel and Kuhl, Patricia K.},
  year = {2010},
  month = may,
  volume = {51},
  pages = {462--471},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.01.080},
  abstract = {A key goal of cognitive neuroscience is to find simple and direct connections between brain and behaviour. However, fMRI analysis typically involves choices between many possible options, with each choice potentially biasing any brain\textendash{}behaviour correlations that emerge. Standard methods of fMRI analysis assess each voxel individually, but then face the problem of selection bias when combining those voxels into a region-of-interest, or ROI. Multivariate pattern-based fMRI analysis methods use classifiers to analyse multiple voxels together, but can also introduce selection bias via data-reduction steps as feature selection of voxels, pre-selecting activated regions, or principal components analysis. We show here that strong brain\textendash{}behaviour links can be revealed without any voxel selection or data reduction, using just plain linear regression as a classifier applied to the whole brain at once, i.e. treating each entire brain volume as a single multi-voxel pattern. The brain\textendash{}behaviour correlations emerged despite the fact that the classifier was not provided with any information at all about subjects' behaviour, but instead was given only the neural data and its condition-labels. Surprisingly, more powerful classifiers such as a linear SVM and regularised logistic regression produce very similar results. We discuss some possible reasons why the very simple brain-wide linear regression model is able to find correlations with behaviour that are as strong as those obtained on the one hand from a specific ROI and on the other hand from more complex classifiers. In a manner which is unencumbered by arbitrary choices, our approach offers a method for investigating connections between brain and behaviour which is simple, rigorous and direct.},
  file = {/Users/qualia/Documents/Papers/2010 - Raizada et al. - Linking brain-wide multivoxel activation patterns to behaviour Examples from language and math.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Rajan2006,
  title = {Eigenvalue {{Spectra}} of {{Random Matrices}} for {{Neural Networks}}},
  author = {Rajan, Kanaka and Abbott, L. F.},
  year = {2006},
  month = nov,
  volume = {97},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.188104},
  file = {/Users/qualia/Documents/Papers/2006 - Rajan, Abbott - Eigenvalue spectra of random matrices for neural networks.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {18}
}

@article{Rajan2016,
  title = {Recurrent {{Network Models}} of {{Sequence Generation}} and {{Memory}}},
  author = {Rajan, Kanaka and Harvey, Christopher D. and Tank, David W.},
  year = {2016},
  month = apr,
  volume = {90},
  pages = {128--142},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.02.009},
  abstract = {Sequential activation of neurons is a common feature of network activity during a variety of behaviors, including working memory and decision making. Previous network models for sequences and memory emphasized specialized architectures in which a principled mechanism is pre-wired into their connectivity. Here we demonstrate that, starting from random connectivity and modifying a small fraction of connections, a largely disordered recurrent network can produce sequences and implement working memory efficiently. We use this process, called Partial In-Network Training (PINning), to model and match cellular resolution imaging data from the posterior parietal cortex during a virtual memoryguided two-alternative forced-choice task. Analysis of the connectivity reveals that sequences propagate by the cooperation between recurrent synaptic interactions and external inputs, rather than through feedforward or asymmetric connections. Together our results suggest that neural sequences may emerge through learning from largely unstructured network architectures.},
  file = {/Users/qualia/Documents/Papers/Rajan et al. - 2016 - Recurrent Network Models of Sequence Generation an.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Ramanujan2019,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2019},
  month = nov,
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 [28] we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [8] trained on ImageNet [3]. Not only do these ``untrained subnetworks'' exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an ``untrained subnetwork'' approaches a network with learned weights in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Ramanujan et al. - 2019 - What's Hidden in a Randomly Weighted Neural Networ.pdf},
  journal = {arXiv:1911.13299 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Ramanujan2020,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2020},
  month = mar,
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 [28] we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [8] trained on ImageNet [3]. Not only do these ``untrained subnetworks'' exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an ``untrained subnetwork'' approaches a network with learned weights in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf},
  journal = {arXiv:1911.13299 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Ramscar2014,
  title = {The {{Myth}} of {{Cognitive Decline}}: {{Non}}-{{Linear Dynamics}} of {{Lifelong Learning}}},
  shorttitle = {The {{Myth}} of {{Cognitive Decline}}},
  author = {Ramscar, Michael and Hendrix, Peter and Shaoul, Cyrus and Milin, Petar and Baayen, Harald},
  year = {2014},
  month = jan,
  volume = {6},
  pages = {5--42},
  issn = {17568757},
  doi = {10.1111/tops.12078},
  abstract = {As adults age, their performance on many psychometric tests changes systematically, a finding that is widely taken to reveal that cognitive information-processing capacities decline across adulthood. Contrary to this, we suggest that older adults' changing performance reflects memory search demands, which escalate as experience grows. A series of simulations show how the performance patterns observed across adulthood emerge naturally in learning models as they acquire knowledge. The simulations correctly identify greater variation in the cognitive performance of older adults, and successfully predict that older adults will show greater sensitivity to fine-grained differences in the properties of test stimuli than younger adults. Our results indicate that older adults' performance on cognitive tests reflects the predictable consequences of learning on informationprocessing, and not cognitive decline. We consider the implications of this for our scientific and cultural understanding of aging.},
  file = {/Users/qualia/Documents/Papers/Ramscar et al. - 2014 - The Myth of Cognitive Decline Non-Linear Dynamics.pdf},
  journal = {Topics in Cognitive Science},
  language = {en},
  number = {1}
}

@techreport{Rashid2020,
  title = {The Dendritic Spatial Code: Branch-Specific Place Tuning and Its Experience-Dependent Decoupling},
  shorttitle = {The Dendritic Spatial Code},
  author = {Rashid, Shannon K. and Pedrosa, Victor and Dufour, Martial A. and Moore, Jason J. and Chavlis, Spyridon and Delatorre, Rodrigo G. and Poirazi, Panayiota and Clopath, Claudia and Basu, Jayeeta},
  year = {2020},
  month = jan,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.01.24.916643},
  abstract = {Abstract
          
            Dendrites of pyramidal neurons integrate different sensory inputs, and non-linear dendritic computations drive feature selective tuning and plasticity. Yet little is known about how dendrites themselves represent the environment, the degree to which they are coupled to their soma, and how that coupling is sculpted with experience. In order to answer these questions, we developed a novel preparation in which we image soma and connected dendrites in a single plane across days using
            in vivo
            two-photon microscopy. Using this preparation, we monitored spatially tuned activity in area CA3 of the hippocampus in head-fixed mice running on a linear track. We identified ``place dendrites'', which can stably and precisely represent both familiar and novel spatial environments. Dendrites could display place tuning independent of their connected soma and even their sister dendritic branches, the first evidence for branch-specific tuning in the hippocampus. In a familiar environment, spatially tuned somata were more decoupled from their dendrites as compared to non-tuned somata. This relationship was absent in a novel environment, suggesting an experience dependent selective gating of dendritic spatial inputs. We then built a data-driven multicompartment computational model that could capture the experimentally observed correlations. Our model predicts that place cells exhibiting branch-specific tuning have more flexible place fields, while neurons with homogenous or co-tuned dendritic branches have higher place field stability. These findings demonstrate that spatial representation is organized in a branch-specific manner within dendrites of hippocampal pyramidal cells. Further, spatial inputs from dendrites to soma are selectively and dynamically gated in an experience-dependent manner, endowing both flexibility and stability to the cognitive map of space.
          
          
            One sentence summary
            Hippocampal pyramidal cells show branch-specific tuning for different place fields, and their coupling to their soma changes with experience of an environment.},
  file = {/Users/qualia/Documents/Papers/Rashid et al. - 2020 - The dendritic spatial code branch-specific place .pdf},
  language = {en},
  type = {Preprint}
}

@article{Rasmussen,
  title = {A Neural Reinforcement Learning Model for Tasks with Unknown Time Delays},
  author = {Rasmussen, Daniel and Eliasmith, Chris},
  pages = {6},
  abstract = {We present a biologically based neural model capable of performing reinforcement learning in complex tasks. The model is unique in its ability to solve tasks that require the agent to make a sequence of unrewarded actions in order to reach the goal, in an environment where there are unknown and variable time delays between actions, state transitions, and rewards. Specifically, this is the first neural model of reinforcement learning able to function within a Semi-Markov Decision Process (SMDP) framework. We believe that this extension of current modelling efforts lays the groundwork for increasingly sophisticated models of human decision making.},
  file = {/Users/qualia/Documents/Papers/1994 - Rasmussen, Eliasmith - A neural reinforcement learning model for tasks with unknown time delays.pdf;/Users/qualia/Documents/Papers/1994 - Rasmussen, Eliasmith - A neural reinforcement learning model for tasks with unknown time delays(2).pdf},
  language = {en}
}

@article{Rasmussen2011,
  title = {Visualization of Nonlinear Kernel Models in Neuroimaging by Sensitivity Maps},
  author = {Rasmussen, Peter Mondrup and Madsen, Kristoffer Hougaard and Lund, Torben Ellegaard and Hansen, Lars Kai},
  year = {2011},
  month = apr,
  volume = {55},
  pages = {1120--1131},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.12.035},
  abstract = {There is significant current interest in decoding mental states from neuroimages. In this context kernel methods, e.g., support vector machines (SVM) are frequently adopted to learn statistical relations between patterns of brain activation and experimental conditions. In this paper we focus on visualization of such nonlinear kernel models. Specifically, we investigate the sensitivity map as a technique for generation of global summary maps of kernel classification models. We illustrate the performance of the sensitivity map on functional magnetic resonance (fMRI) data based on visual stimuli. We show that the performance of linear models is reduced for certain scan labelings/categorizations in this data set, while the nonlinear models provide more flexibility. We show that the sensitivity map can be used to visualize nonlinear versions of kernel logistic regression, the kernel Fisher discriminant, and the SVM, and conclude that the sensitivity map is a versatile and computationally efficient tool for visualization of nonlinear kernel models in neuroimaging. \textcopyright{} 2010 Elsevier Inc. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2011 - Rasmussen et al. - Visualization of nonlinear kernel models in neuroimaging by sensitivity maps.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Rasmussena,
  title = {The {{Infinite Gaussian Mixture Model}}},
  author = {Rasmussen, Carl Edward},
  pages = {7},
  abstract = {In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the ``right'' number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.},
  file = {/Users/qualia/Documents/Papers/2000 - Rasmussen - The Infinite Gaussian Mixture Model.pdf},
  language = {en}
}

@article{Ratcliff,
  title = {Retrieval {{Processes}} in {{Recognition Memory}}},
  author = {Ratcliff, Roger and Murdock, Bennet B},
  pages = {25},
  file = {/Users/qualia/Documents/Papers/1976 - Ratcliff, Murdock - Retrieval processes in recognition memory.pdf},
  language = {en}
}

@article{Rattay1989,
  title = {Analysis of Models for Extracellular Fiber Stimulation},
  author = {Rattay, F.},
  year = {1989},
  month = jul,
  volume = {36},
  pages = {676--682},
  issn = {00189294},
  doi = {10.1109/10.32099},
  abstract = {This paper presents the mathematical basis for analysis as well as for the computer simulation of the stimulus/response characteristics of nerve or muscle fibers. The results follow from the extracellular potential along the fiber as a function of electrode geometry. The theory is of a general nature but special investigations are made on monopolar, bipolar, and ring electrodes. Stimulations with monopolar electrodes show better recruitment characteristics than ring electrodes.},
  file = {/Users/qualia/Documents/Papers/1989 - Rattay - Analysis of Models for Extracellular Fiber Stimulation.pdf;/Users/qualia/Documents/Papers/Rattay - 1989 - Analysis of models for extracellular fiber stimula.pdf},
  journal = {IEEE Transactions on Biomedical Engineering},
  language = {en},
  number = {7}
}

@article{Ravi2018,
  title = {Homeostatic Feedback Modulates the Development of Two-State Patterned Activity in a Model Serotonin Motor Circuit In},
  author = {Ravi, Bhavya and Garcia, Jessica and Collins, Kevin M.},
  year = {2018},
  month = may,
  doi = {10.1101/202507},
  abstract = {Neuron activity accompanies synapse formation and maintenance, but how early circuit activity contributes to behavior development is not well understood. Here, we use the
            
            egg-laying motor circuit as a model to understand how coordinated cell and circuit activity develops and drives a robust two-state behavior in adults. Using calcium imaging in behaving animals, we find the serotonergic Hermaphrodite Specific Neurons (HSNs) and vulval muscles show rhythmic calcium transients in L4 larvae before eggs are produced. HSN activity in L4 is tonic and lacks the alternating burst-firing/quiescent pattern seen in egg-laying adults. Vulval muscle activity in L4 is initially uncoordinated but becomes synchronous as the anterior and posterior muscle arms meet at HSN synaptic release sites. However, coordinated muscle activity does not require presynaptic HSN input. Using reversible silencing experiments, we show that neuronal and vulval muscle activity in L4 is not required for the onset of adult behavior. Instead, the accumulation of eggs in the adult uterus renders the muscles sensitive to HSN input. Sterilization or acute electrical silencing of the vulval muscles inhibits presynaptic HSN activity, and reversal of muscle silencing triggers a homeostatic increase in HSN activity and egg release that maintains \textasciitilde{}12-15 eggs in the uterus. Feedback of egg accumulation depends upon the vulval muscle postsynaptic terminus, suggesting a retrograde signal sustains HSN synaptic activity and egg release. Our results show that egg-laying behavior in
            
            is driven by a homeostat that scales serotonin motor neuron activity in response to postsynaptic muscle feedback.},
  file = {/Users/qualia/Documents/Papers/Ravi et al. - 2018 - Homeostatic feedback modulates the development of .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Ray2015,
  title = {Challenges in the Quantification and Interpretation of Spike-{{LFP}} Relationships},
  author = {Ray, Supratim},
  year = {2015},
  month = apr,
  volume = {31},
  pages = {111--118},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.09.004},
  file = {/Users/qualia/Documents/Papers/Ray - 2015 - Challenges in the quantification and interpretatio.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@techreport{Razo-Mejia2019,
  title = {First-Principles Prediction of the Information Processing Capacity of a Simple Genetic Circuit},
  author = {{Razo-Mejia}, Manuel and Phillips, Rob},
  year = {2019},
  month = mar,
  institution = {{Biophysics}},
  doi = {10.1101/594325},
  abstract = {Abstract
          
            Given the stochastic nature of gene expression, genetically identical cells exposed to the same environmental inputs will produce different outputs. This heterogeneity has consequences for how cells are able to survive in changing environments. Recent work has explored the use of information theory as a framework to understand the accuracy with which cells can ascertain the state of their surroundings. Yet the predictive power of these approaches is limited and has not been rigorously tested using precision measurements. To that end, we generate a minimal model for a simple genetic circuit in which all parameter values for the model come from independently published data sets. We then predict the information processing capacity of the genetic circuit for a suite of biophysical parameters such as protein copy number and protein-DNA affinity. We compare these parameter-free predictions with an experimental determination of the information processing capacity of
            E. coli
            cells, and find that our minimal model accurately captures the experimental data.},
  file = {/Users/qualia/Documents/Papers/Razo-Mejia and Phillips - 2019 - First-principles prediction of the information pro.pdf},
  language = {en},
  type = {Preprint}
}

@article{Reato2015,
  title = {Lasting Modulation of in Vitro Oscillatory Activity with Weak Direct Current Stimulation},
  author = {Reato, Davide and Bikson, Marom and Parra, Lucas C.},
  year = {2015},
  month = mar,
  volume = {113},
  pages = {1334--1341},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00208.2014},
  file = {/Users/qualia/Documents/Papers/Reato et al. - 2015 - Lasting modulation of in vitro oscillatory activit.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {5}
}

@article{Reddy2010,
  title = {Reading the Mind's Eye: {{Decoding}} Category Information during Mental Imagery},
  shorttitle = {Reading the Mind's Eye},
  author = {Reddy, Leila and Tsuchiya, Naotsugu and Serre, Thomas},
  year = {2010},
  month = apr,
  volume = {50},
  pages = {818--825},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.11.084},
  abstract = {Category information for visually presented objects can be read out from multi-voxel patterns of fMRI activity in ventral\textendash{}temporal cortex. What is the nature and reliability of these patterns in the absence of any bottom\textendash{}up visual input, for example, during visual imagery? Here, we first ask how well category information can be decoded for imagined objects and then compare the representations evoked during imagery and actual viewing. In an fMRI study, four object categories (food, tools, faces, buildings) were either visually presented to subjects, or imagined by them. Using pattern classification techniques, we could reliably decode category information (including for non-special categories, i.e., food and tools) from ventral\textendash{}temporal cortex in both conditions, but only during actual viewing from retinotopic areas. Interestingly, in temporal cortex when the classifier was trained on the viewed condition and tested on the imagery condition, or vice versa, classification performance was comparable to within the imagery condition. The above results held even when we did not use information in the specialized category-selective areas. Thus, the patterns of representation during imagery and actual viewing are in fact surprisingly similar to each other. Consistent with this observation, the maps of ``diagnostic voxels'' (i.e., the classifier weights) for the perception and imagery classifiers were more similar in ventral\textendash{}temporal cortex than in retinotopic cortex. These results suggest that in the absence of any bottom\textendash{}up input, cortical back projections can selectively re-activate specific patterns of neural activity.},
  file = {/Users/qualia/Documents/Papers/2010 - Reddy, Tsuchiya, Serre - Reading the mind's eye decoding category information during mental imagery.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Reddy2016,
  title = {Infomax Strategies for an Optimal Balance between Exploration and Exploitation},
  author = {Reddy, Gautam and Celani, Antonio and Vergassola, Massimo},
  year = {2016},
  month = jun,
  volume = {163},
  pages = {1454--1476},
  issn = {0022-4715, 1572-9613},
  doi = {10.1007/s10955-016-1521-0},
  abstract = {Proper balance between exploitation and exploration is what makes good decisions, which achieve high rewards like payoff or evolutionary fitness. The Infomax principle postulates that maximization of information directs the function of diverse systems, from living systems to artificial neural networks. While specific applications are successful, the validity of information as a proxy for reward remains unclear. Here, we consider the multi-armed bandit decision problem, which features arms (slot-machines) of unknown probabilities of success and a player trying to maximize cumulative payoff by choosing the sequence of arms to play. We show that an Infomax strategy (Info-p) which optimally gathers information on the highest mean reward among the arms saturates known optimal bounds and compares favorably to existing policies. The highest mean reward considered by Info-p is not the quantity actually needed for the choice of the arm to play, yet it allows for optimal tradeoffs between exploration and exploitation.},
  archivePrefix = {arXiv},
  eprint = {1601.03073},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Reddy et al. - 2016 - Infomax strategies for an optimal balance between .pdf},
  journal = {Journal of Statistical Physics},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Populations and Evolution,Statistics - Machine Learning},
  language = {en},
  number = {6}
}

@article{Rehan2013,
  title = {Modeling and {{Automatic Feedback Control}} of {{Tremor}}: {{Adaptive Estimation}} of {{Deep Brain Stimulation}}},
  shorttitle = {Modeling and {{Automatic Feedback Control}} of {{Tremor}}},
  author = {Rehan, Muhammad and Hong, Keum-Shik},
  editor = {Androulakis, Ioannis P.},
  year = {2013},
  month = apr,
  volume = {8},
  pages = {e62888},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0062888},
  abstract = {This paper discusses modeling and automatic feedback control of (postural and rest) tremor for adaptive-controlmethodology-based estimation of deep brain stimulation (DBS) parameters. The simplest linear oscillator-based tremor model, between stimulation amplitude and tremor, is investigated by utilizing input-output knowledge. Further, a nonlinear generalization of the oscillator-based tremor model, useful for derivation of a control strategy involving incorporation of parametric-bound knowledge, is provided. Using the Lyapunov method, a robust adaptive output feedback control law, based on measurement of the tremor signal from the fingers of a patient, is formulated to estimate the stimulation amplitude required to control the tremor. By means of the proposed control strategy, an algorithm is developed for estimation of DBS parameters such as amplitude, frequency and pulse width, which provides a framework for development of an automatic clinical device for control of motor symptoms. The DBS parameter estimation results for the proposed control scheme are verified through numerical simulations.},
  file = {/Users/qualia/Documents/Papers/2013 - Rehan, Hong - Modeling and Automatic Feedback Control of Tremor Adaptive Estimation of Deep Brain Stimulation.pdf;/Users/qualia/Documents/Papers/2013 - Rehan, Hong - Modeling and Automatic Feedback Control of Tremor Adaptive Estimation of Deep Brain Stimulation(2).pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {4}
}

@article{Reimann2017,
  title = {Cliques of {{Neurons Bound}} into {{Cavities Provide}} a {{Missing Link}} between {{Structure}} and {{Function}}},
  author = {Reimann, Michael W. and Nolte, Max and Scolamiero, Martina and Turner, Katharine and Perin, Rodrigo and Chindemi, Giuseppe and D{\l}otko, Pawe{\l} and Levi, Ran and Hess, Kathryn and Markram, Henry},
  year = {2017},
  month = jun,
  volume = {11},
  issn = {1662-5188},
  doi = {10.3389/fncom.2017.00048},
  abstract = {The lack of a formal link between neural network structure and its emergent function has hampered our understanding of how the brain processes information. We have now come closer to describing such a link by taking the direction of synaptic transmission into account, constructing graphs of a network that reflect the direction of information flow, and analyzing these directed graphs using algebraic topology. Applying this approach to a local network of neurons in the neocortex revealed a remarkably intricate and previously unseen topology of synaptic connectivity. The synaptic network contains an abundance of cliques of neurons bound into cavities that guide the emergence of correlated activity. In response to stimuli, correlated activity binds synaptically connected neurons into functional cliques and cavities that evolve in a stereotypical sequence toward peak complexity. We propose that the brain processes stimuli by forming increasingly complex functional cliques and cavities.},
  file = {/Users/qualia/Documents/Papers/Reimann et al. - 2017 - Cliques of Neurons Bound into Cavities Provide a M 2.pdf;/Users/qualia/Documents/Papers/Reimann et al. - 2017 - Cliques of Neurons Bound into Cavities Provide a M.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Renart2014,
  title = {Variability in Neural Activity and Behavior},
  author = {Renart, Alfonso and Machens, Christian K},
  year = {2014},
  month = apr,
  volume = {25},
  pages = {211--220},
  issn = {09594388},
  doi = {10.1016/j.conb.2014.02.013},
  file = {/Users/qualia/Documents/Papers/Renart and Machens - 2014 - Variability in neural activity and behavior.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Rey2014,
  title = {Using Waveform Information in Nonlinear Data Assimilation},
  author = {Rey, Daniel and Eldridge, Michael and Morone, Uriel and Abarbanel, Henry D. I. and Parlitz, Ulrich and {Schumann-Bischoff}, Jan},
  year = {2014},
  month = dec,
  volume = {90},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.90.062916},
  file = {/Users/qualia/Documents/Papers/2014 - Rey et al. - Using waveform information in nonlinear data assimilation.pdf;/Users/qualia/Documents/Papers/Rey et al. - 2014 - Using waveform information in nonlinear data assim.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@inproceedings{Ribeiro2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  booktitle = {Proceedings of the 22nd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}} - {{KDD}} '16},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  pages = {1135--1144},
  publisher = {{ACM Press}},
  address = {{San Francisco, California, USA}},
  doi = {10.1145/2939672.2939778},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  file = {/Users/qualia/Documents/Papers/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf},
  isbn = {978-1-4503-4232-2},
  language = {en}
}

@article{Rich2019,
  title = {Lessons for Artificial Intelligence from the Study of Natural Stupidity},
  author = {Rich, Alexander S. and Gureckis, Todd M.},
  year = {2019},
  month = apr,
  volume = {1},
  pages = {174--180},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0038-z},
  file = {/Users/qualia/Documents/Papers/Rich and Gureckis - 2019 - Lessons for artificial intelligence from the study.pdf},
  journal = {Nature Machine Intelligence},
  language = {en},
  number = {4}
}

@article{Richards2019,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and {de Berker}, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  year = {2019},
  month = nov,
  volume = {22},
  pages = {1761--1770},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0520-2},
  file = {/Users/qualia/Documents/Papers/Richards et al. - 2019 - A deep learning framework for neuroscience.pdf},
  journal = {Nat Neurosci},
  language = {en},
  number = {11}
}

@article{Riedl2013,
  title = {Practical Considerations of Permutation Entropy: {{A}} Tutorial Review},
  shorttitle = {Practical Considerations of Permutation Entropy},
  author = {Riedl, M. and M{\"u}ller, A. and Wessel, N.},
  year = {2013},
  month = jun,
  volume = {222},
  pages = {249--262},
  issn = {1951-6355, 1951-6401},
  doi = {10.1140/epjst/e2013-01862-7},
  abstract = {More than ten years ago Bandt and Pompe introduced a new measure to quantify complexity in measured time series. During these ten years, this measure has been modified and extended. In this review we will give a brief introduction to permutation entropy, explore the different fields of utilization where permutation entropy has been applied and provide a guide on how to choose appropriate parameters for different applications of permutation entropy.},
  file = {/Users/qualia/Documents/Papers/2013 - Riedl, Mller, Wessel - Practical considerations of permutation entropy A tutorial review.pdf},
  journal = {The European Physical Journal Special Topics},
  language = {en},
  number = {2}
}

@article{Rieskamp2006,
  title = {{{SSL}}: {{A Theory}} of {{How People Learn}} to {{Select Strategies}}.},
  shorttitle = {{{SSL}}},
  author = {Rieskamp, J{\"o}rg and Otto, Philipp E.},
  year = {2006},
  volume = {135},
  pages = {207--236},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/0096-3445.135.2.207},
  abstract = {The assumption that people possess a repertoire of strategies to solve the inference problems they face has been raised repeatedly. However, a computational model specifying how people select strategies from their repertoire is still lacking. The proposed strategy selection learning (SSL) theory predicts a strategy selection process on the basis of reinforcement learning. The theory assumes that individuals develop subjective expectations for the strategies they have and select strategies proportional to their expectations, which are then updated on the basis of subsequent experience. The learning assumption was supported in 4 experimental studies. Participants substantially improved their inferences through feedback. In all 4 studies, the best-performing strategy from the participants' repertoires most accurately predicted the inferences after sufficient learning opportunities. When testing SSL against 3 models representing extensions of SSL and against an exemplar model assuming a memory-based inference process, the authors found that SSL predicted the inferences most accurately.},
  file = {/Users/qualia/Documents/Papers/2006 - Rieskamp, Otto - SSL A theory of how people learn to select strategies.pdf},
  journal = {Journal of Experimental Psychology: General},
  language = {en},
  number = {2}
}

@article{Risi2019,
  title = {Deep {{Neuroevolution}} of {{Recurrent}} and {{Discrete World Models}}},
  author = {Risi, Sebastian and Stanley, Kenneth O},
  year = {2019},
  pages = {7},
  abstract = {Neural architectures inspired by our own human cognitive system, such as the recently introduced world models, have been shown to outperform traditional deep reinforcement learning (RL) methods in a variety of different domains. Instead of the relatively simple architectures employed in most RL experiments, world models rely on multiple different neural components that are responsible for visual information processing, memory, and decision-making. However, so far the components of these models have to be trained separately and through a variety of specialized training methods. This paper demonstrates the surprising finding that models with the same precise parts can be instead efficiently trained end-to-end through a genetic algorithm (GA), reaching a comparable performance to the original world model by solving a challenging car racing task. An analysis of the evolved visual and memory system indicates that they include a similar effective representation to the system trained through gradient descent. Additionally, in contrast to gradient descent methods that struggle with discrete variables, GAs also work directly with such representations, opening up opportunities for classical planning in latent space. This paper adds additional evidence on the effectiveness of deep neuroevolution for tasks that require the intricate orchestration of multiple components in complex heterogeneous architectures.},
  file = {/Users/qualia/Documents/Papers/Risi and Stanley - 2019 - Deep Neuroevolution of Recurrent and Discrete Worl.pdf},
  language = {en}
}

@article{Robert1976,
  title = {A {{Unifying Tool}} for {{Linear Multivariate Statistical Methods}}: {{The RV}}- {{Coefficient}}},
  shorttitle = {A {{Unifying Tool}} for {{Linear Multivariate Statistical Methods}}},
  author = {Robert, P. and Escoufier, Y.},
  year = {1976},
  volume = {25},
  pages = {257},
  issn = {00359254},
  doi = {10.2307/2347233},
  abstract = {Considertwodata matriceosn thesamesampleofn individualsX, (p x n), Y(q x n). Fromthesematricesg,eometricarlepresentatioonfsthesampleare obtainedas two configurationosf n points,in RP and \_?q.It is shownthat the RV-coefficient (Escoufier1,970,1973)canbeusedas a measureofsimilaritoyfthetwoconfigurations, takingintoaccountthepossiblydistincmt etrictso be usedon themto measurethe distancesbetweenpoints.The purposeofthispaperis to showthatmostclassical methodsoflinearmultivariatsteatisticaalnalysiscan beinterpreteads thesearchfor optimallineartransformatioonrs, equivalentlyt,hesearchforoptimalmetricsto applyon twodatamatriceosnthesamesample;theoptimalitiys definedintermsof thesimilaritoyfthecorrespondincgonfiguratioonfspoints,whichi,n turn,callsfor themaximizatioonftheassociatedR V-coefficienTth. emethodstudiedareprincipal componentps,rincipaclomponentosfinstrumentvaalriablesm, ultivariatregression, canonicalvariables,discriminanatnalysis;theyare differentiatbeyd the possible relationshipesxistingbetweenthe two data matricesinvolvedand by additional constrainutsnderwhichthemaximumofR Vistobe obtained.It is alsoshownthatthe RV-coefficiecnatn be usedas a measureofgoodnessofa solutionto theproblemof discardinvgariables.},
  file = {/Users/qualia/Documents/Papers/2012 - Society, Statistics - Tool for Linear Multivariate A Unifying The RV-Coefficient Methods Statistical.pdf},
  journal = {Applied Statistics},
  language = {en},
  number = {3}
}

@article{Robinson2012,
  title = {Spike, Rate, Field, and Hybrid Methods for Treating Neuronal Dynamics and Interactions},
  author = {Robinson, P.A. and Kim, J.W.},
  year = {2012},
  month = apr,
  volume = {205},
  pages = {283--294},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2012.01.018},
  abstract = {Spike-, rate-, and field-based approaches to neural dynamics are adapted and hybridized to provide new methods of analyzing dynamics of single neurons and large neuronal systems, to elucidate the relationships and intermediate forms between these limiting cases, and to enable faster simulations with reduced memory requirements. At the single-neuron level, the new approaches involve reformulation of dynamics in synapses, dendrites, cell bodies, and axons to enable new types of analysis, longer numerical timesteps, and demonstration that rate-based methods can predict spike times. In multineuron systems, hybrids and intermediates between spike-based and field-based coupling between neurons are used to provide stepping stones between descriptions based on pairwise spike-based interactions between neurons and ones based on neural field-based interactions within and between populations, including arbitrary spatial structure and temporal delays in the connections in general. In particular, a new neuronin-cell approach is introduced that is a hybrid between neural field theory and spiking-neuron models in analogy to particle-in-cell methods in plasma physics. This approach enables large speedups in computations while preserving spike shapes and times. Various approaches are illustrated numerically for specific cases.},
  file = {/Users/qualia/Documents/Papers/2012 - Robinson, Kim - Spike, rate, field, and hybrid methods for treating neuronal dynamics and interactions.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en},
  number = {2}
}

@article{Robinson2015,
  title = {Short {{Stimulus}}, {{Long Response}}: {{Sodium}} and {{Calcium Dynamics Explain Persistent Neuronal Firing}}},
  shorttitle = {Short {{Stimulus}}, {{Long Response}}},
  author = {Robinson, Richard},
  year = {2015},
  month = dec,
  volume = {13},
  pages = {e1002320},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.1002320},
  file = {/Users/qualia/Documents/Papers/2015 - Robinson - Short Stimulus, Long Response Sodium and Calcium Dynamics Explain Persistent Neuronal Firing.pdf;/Users/qualia/Documents/Papers/Robinson - 2015 - Short Stimulus, Long Response Sodium and Calcium .pdf},
  journal = {PLOS Biology},
  language = {en},
  number = {12}
}

@inproceedings{Rocktaschel2014,
  title = {Low-{{Dimensional Embeddings}} of {{Logic}}},
  booktitle = {Proceedings of the {{ACL}} 2014 {{Workshop}} on {{Semantic Parsing}}},
  author = {Rockt{\"a}schel, Tim and Bo{\v s}njak, Matko and Singh, Sameer and Riedel, Sebastian},
  year = {2014},
  pages = {45--49},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, MD}},
  doi = {10.3115/v1/W14-2409},
  abstract = {Many machine reading approaches, from shallow information extraction to deep semantic parsing, map natural language to symbolic representations of meaning. Representations such as first-order logic capture the richness of natural language and support complex reasoning, but often fail in practice due to their reliance on logical background knowledge and the difficulty of scaling up inference. In contrast, low-dimensional embeddings (i.e. distributional representations) are efficient and enable generalization, but it is unclear how reasoning with embeddings could support the full power of symbolic representations such as first-order logic. In this proof-ofconcept paper we address this by learning embeddings that simulate the behavior of first-order logic.},
  file = {/Users/qualia/Documents/Papers/2014 - Rocktäschel et al. - Low-Dimensional Embeddings of Logic.pdf;/Users/qualia/Documents/Papers/Rocktäschel et al. - 2014 - Low-Dimensional Embeddings of Logic.pdf},
  language = {en}
}

@article{Rodrigues2010,
  title = {Mappings between a Macroscopic Neural-Mass Model and a Reduced Conductance-Based Model},
  author = {Rodrigues, Serafim and Chizhov, Anton V. and Marten, Frank and Terry, John R.},
  year = {2010},
  month = may,
  volume = {102},
  pages = {361--371},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-010-0372-z},
  abstract = {We present two alternative mappings between macroscopic neuronal models and a reduction of a conductance-based model. These provide possible explanations of the relationship between parameters of these two different approaches to modelling neuronal activity. Obtaining a physical interpretation of neural-mass models is of fundamental importance as they could provide direct and accessible tools for use in diagnosing neurological conditions. Detailed consideration of the assumptions required for the validity of each mapping elucidates strengths and weaknesses of each macroscopic model and suggests improvements for future development.},
  file = {/Users/qualia/Documents/Papers/2010 - Rodrigues et al. - Mappings between a macroscopic neural-mass model and a reduced conductance-based model.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {5}
}

@article{Rogalsky2011,
  title = {Functional {{Anatomy}} of {{Language}} and {{Music Perception}}: {{Temporal}} and {{Structural Factors Investigated Using Functional Magnetic Resonance Imaging}}},
  shorttitle = {Functional {{Anatomy}} of {{Language}} and {{Music Perception}}},
  author = {Rogalsky, C. and Rong, F. and Saberi, K. and Hickok, G.},
  year = {2011},
  month = mar,
  volume = {31},
  pages = {3843--3852},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4515-10.2011},
  file = {/Users/qualia/Documents/Papers/2011 - Rogalsky et al. - Functional anatomy of language and music perception temporal and structural factors investigated using function.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {10}
}

@techreport{Roseberry2019,
  title = {Locomotor Suppression by a Monosynaptic Amygdala to Brainstem Circuit},
  author = {Roseberry, Thomas K. and Lalive, Arnaud L. and Margolin, Benjamin D. and Kreitzer, Anatol C.},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/724252},
  abstract = {The control of locomotion is fundamental to vertebrate animal survival. Defensive situations require an animal to rapidly decide whether to run away or suppress locomotor activity to avoid detection. While much of the neural circuitry involved in defensive action selection has been elucidated, top-down modulation of brainstem locomotor circuitry remains unclear. Here we provide evidence for the existence and functionality of a monosynaptic connection from the central amygdala (CeA) to the mesencephalic locomotor region (MLR) that inhibits locomotion in unconditioned and conditioned defensive behavior in mice. We show that locomotion stimulated by airpuff coincides with increased activity of MLR glutamatergic neurons. Using retrograde tracing and ex vivo electrophysiology, we find that the CeA makes a monosynaptic connection with the MLR. In the open field, in vivo stimulation of this projection suppressed spontaneous locomotion, whereas inhibition of this projection had no effect. However, inhibiting CeA terminals within the MLR increased both neural activity and locomotor responses to airpuff. Finally, using a conditioned avoidance paradigm known to activate CeA neurons, we find that inhibition of the CeA projection increased successful escape, whereas activating the projection reduced escape. Together these results provide evidence for a new circuit substrate influencing locomotion and defensive behaviors.},
  file = {/Users/qualia/Documents/Papers/Roseberry et al. - 2019 - Locomotor suppression by a monosynaptic amygdala t.pdf},
  language = {en},
  type = {Preprint}
}

@article{Rosenblatt,
  title = {A {{COMPARISON OF SEVERAL PERCEPTRON MODELS}} \textbullet{}},
  author = {Rosenblatt, Frank and Ym, New},
  pages = {23},
  file = {/Users/qualia/Documents/Papers/1981 - Rosenblatt - A comparison of several perceptron models.pdf;/Users/qualia/Documents/Papers/Rosenblatt and Ym - A COMPARISON OF SEVERAL PERCEPTRON MODELS •.pdf},
  language = {en}
}

@article{Ross,
  title = {A {{Bayesian Approach}} for {{Learning}} and {{Planning}} in {{Partially Observable Markov Decision Processes}}},
  author = {Ross, Stephane and Pineau, Joelle and {Chaib-draa}, Brahim and Kreitmann, Pierre},
  pages = {45},
  abstract = {Bayesian learning methods have recently been shown to provide an elegant solution to the exploration-exploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience.},
  file = {/Users/qualia/Documents/Papers/2011 - Ross, Pineau - A Bayesian approach for learning and planning in partially observable Markov decision processes.pdf},
  language = {en}
}

@article{Rossert,
  title = {Automated Point-Neuron Simplification of Data-Driven Microcircuit Models},
  author = {Rossert, Christian and Pozzorini, Christian and Chindemi, Giuseppe and Eroe, Csaba and King, James and Newton, Taylor H and Nolte, Max and Reimann, Michael W and Gewaltig, Marc-Oliver and Gerstner, Wulfram and Markram, Henry and Segev, Idan and Muller, Eilif},
  pages = {26},
  abstract = {A method is presented for the reduction of morphologically detailed microcircuit models to a point-neuron representation without human intervention. The simplification occurs in a modular workflow, in the neighborhood of a user specified network activity state for the reference model, the ``operating point''. First, synapses are moved to the soma, correcting for dendritic filtering by low-pass filtering the delivered synaptic current. Filter parameters are computed numerically and independently for inhibitory and excitatory input on the basal and apical dendrites, respectively, in a distance dependent and post-synaptic m-type specific manner. Next, point-neuron models for each neuron in the microcircuit are fit to their respective morphologically detailed counterparts. Here, generalized integrate-and-fire point neuron models are used, leveraging a recently published fitting toolbox. The fits are constrained by currents and voltages computed in the morphologically detailed partner neurons with soma corrected synapses at three depolarizations about the user specified operating point. The result is a simplified circuit which is well constrained by the reference circuit, and can be continuously updated as the latter iteratively integrates new data. The modularity of the approach makes it applicable also for other point-neuron and synapse models.},
  file = {/Users/qualia/Documents/Papers/Rossert et al. - Automated point-neuron simpliﬁcation of data-drive.pdf},
  language = {en}
}

@article{Rottenstreich2006,
  title = {On Decision Making without Likelihood Judgment},
  author = {Rottenstreich, Yuval and Kivetz, Ran},
  year = {2006},
  month = sep,
  volume = {101},
  pages = {74--88},
  issn = {07495978},
  doi = {10.1016/j.obhdp.2006.06.004},
  abstract = {Subjective expected utility, prospect theory and most other formal models of decision making under uncertainty are probabilistic: they assume that in making choices people judge the likelihood of relevant uncertainties. Clearly, in many situations people do indeed judge likelihood. However, we present studies suggesting that there are also many situations in which people do not judge likelihood and instead base their decisions on intuitively generated, non-probabilistic rules or rationales. Thus, we argue that real-world situations are of two types. In situations eliciting a probabilistic mindset, people rely on judgments of likelihood. In situations eliciting a non-probabilistic mindset, they neglect judgments of likelihood. We suggest three factors that may inXuence the tendency towards either probabilistic or non-probabilistic mindsets. We also outline how extant probabilistic theories may be complemented by non-probabilistic models.},
  file = {/Users/qualia/Documents/Papers/2006 - Rottenstreich, Kivetz - On decision making without likelihood judgment.pdf},
  journal = {Organizational Behavior and Human Decision Processes},
  language = {en},
  number = {1}
}

@book{Roughgarden2019,
  title = {Algorithms {{Illuminated}} ({{Part}} 3): {{Greedy Algorithms}} and {{Dynamic Programming}}},
  author = {Roughgarden, Tim},
  year = {2019},
  volume = {1},
  number = {3}
}

@techreport{Roussel2019,
  title = {Acoustic Contamination of Electrophysiological Brain Signals during Speech Production and Sound Perception},
  author = {Roussel, Phil{\'e}mon and Bocquelet, Florent and Palma, Marie and Kahane, Philippe and Chabard{\`e}s, St{\'e}phan and Yvert, Blaise},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/722207},
  abstract = {A current challenge of neurotechnologies is the development of speech brain-computer interfaces to restore communication in people unable to speak. To achieve a proof of concept of such system, neural activity can be investigated in patients implanted for clinical reasons while they speak. Using such simultaneously recorded audio and neural data, decoders can be built to predict speech features using features extracted from brain signals. A typical neural feature is the spectral power of field potentials in the high-gamma frequency band (between 70 and 200 Hz), a range that happen to overlap with the fundamental frequency of speech. Here, we analyzed human electrocorticographic (ECoG) and intracortical recordings during speech production and perception as well as rat microelectrocorticographic (\textmu{}-ECoG) recordings during sound perception. We observed that electrophysiological recordings, obtained with different recording setups, often contain spectrotemporal features of the sound, especially within the high-gamma band. Further analysis and in vitro replication suggest that these correlations are caused by a microphonic effect, transforming sound vibrations into an undesired electrical noise that contaminates the biopotential measurements. This study does not question the existence of relevant physiological neural information underlying speech production or sound perception in the high-gamma frequency band, but alerts on the fact that care should be taken to evaluate and eliminate any possible acoustic contamination of neural signals to investigate cortical dynamics underlying speech production and auditory perception.},
  file = {/Users/qualia/Documents/Papers/Roussel et al. - 2019 - Acoustic contamination of electrophysiological bra.pdf},
  language = {en},
  type = {Preprint}
}

@article{Roux2013,
  title = {The {{Phase}} of {{Thalamic Alpha Activity Modulates Cortical Gamma}}-{{Band Activity}}: {{Evidence}} from {{Resting}}-{{State MEG Recordings}}},
  shorttitle = {The {{Phase}} of {{Thalamic Alpha Activity Modulates Cortical Gamma}}-{{Band Activity}}},
  author = {Roux, F. and Wibral, M. and Singer, W. and Aru, J. and Uhlhaas, P. J.},
  year = {2013},
  month = nov,
  volume = {33},
  pages = {17827--17835},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5778-12.2013},
  file = {/Users/qualia/Documents/Papers/2013 - Wibral et al. - The Phase of Thalamic Alpha Activity Modulates Cortical Gamma-Band Activity Evidence from Resting-State MEG Recor.pdf;/Users/qualia/Documents/Papers/Roux et al. - 2013 - The Phase of Thalamic Alpha Activity Modulates Cor.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {45}
}

@article{Roux2015,
  title = {Tasks for Inhibitory Interneurons in Intact Brain Circuits},
  author = {Roux, Lisa and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {2015},
  month = jan,
  volume = {88},
  pages = {10--23},
  issn = {00283908},
  doi = {10.1016/j.neuropharm.2014.09.011},
  abstract = {Synaptic inhibition, brought about by a rich variety of interneuron types, counters excitation, modulates the gain, timing, tuning, bursting properties of principal cell firing, and exerts selective filtering of synaptic excitation. At the network level, it allows for coordinating transient interactions among the principal cells to form cooperative assemblies for efficient transmission of information and routing of excitatory activity across networks, typically in the form of brain oscillations. Recent techniques based on targeted expression of neuronal activity modulators, such as optogenetics, allow physiological identification and perturbation of specific interneuron subtypes in the intact brain. Combined with large-scale recordings or imaging techniques, these approaches facilitate our understanding of the multiple roles of inhibitory interneurons in shaping circuit functions.},
  file = {/Users/qualia/Documents/Papers/Roux and Buzsáki - 2015 - Tasks for inhibitory interneurons in intact brain .pdf},
  journal = {Neuropharmacology},
  language = {en}
}

@article{Rowan2013,
  title = {Synaptic {{Scaling Balances Learning}} in a {{Spiking Model}} of {{Neocortex}}},
  author = {Rowan, Mark and Neymotin, Samuel},
  year = {2013},
  month = apr,
  abstract = {Learning in the brain requires complementary mechanisms: potentiation and activity-dependent homeostatic scaling. We introduce synaptic scaling to a biologically-realistic spiking model of neocortex which can learn changes in oscillatory rhythms using STDP, and show that scaling is necessary to balance both positive and negative changes in input from potentiation and atrophy. We discuss some of the issues that arise when considering synaptic scaling in such a model, and show that scaling regulates activity whilst allowing learning to remain unaltered.},
  archivePrefix = {arXiv},
  eprint = {1304.2266},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2013 - Rowan, Neymotin - Synaptic Scaling Balances Learning in a Spiking Model of Neocortex.pdf},
  journal = {arXiv:1304.2266 [cs, q-bio]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{Rowe2010,
  title = {Action Selection: {{A}} Race Model for Selected and Non-Selected Actions Distinguishes the Contribution of Premotor and Prefrontal Areas},
  shorttitle = {Action Selection},
  author = {Rowe, J.B. and Hughes, L. and {Nimmo-Smith}, I.},
  year = {2010},
  month = jun,
  volume = {51},
  pages = {888--896},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.02.045},
  abstract = {Race models have been used to explain perceptual, motor and oculomotor decisions. Here we developed a race model to explain how human subjects select actions when there are no overt rewards and no external cues to specify which action to make. Critically, we were able to estimate the cumulative activity of neuronal decision-units for selected and non-selected actions. We used functional magnetic resonance imaging (fMRI) to test for regional brain activity that correlated with the predictions of this race model. Activity in the preSMA, cingulate motor and premotor areas correlated with prospective selection between responses according to the race model. Activity in the lateral prefrontal cortex did not correlate with the race model, even though this area was active during action selection. This activity related to the degree to which individuals switched between alternative actions. Crucially, a follow-up experiment showed that it was not present on the first trial. Taken together, these results suggest that the lateral prefrontal cortex is not the source for the generation of action. It is more likely that it is involved in switching to alternatives or monitoring previous actions. Thus, our experiment shows the power of the race model in distinguishing the contribution of different areas in the selection of action.},
  file = {/Users/qualia/Documents/Papers/2010 - Rowe, Hughes, Nimmo-Smith - Action selection a race model for selected and non-selected actions distinguishes the contribution of.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Rubin2004,
  title = {High {{Frequency Stimulation}} of the {{Subthalamic Nucleus Eliminates Pathological Thalamic Rhythmicity}} in a {{Computational Model}}},
  author = {Rubin, Jonathan E. and Terman, David},
  year = {2004},
  month = may,
  volume = {16},
  pages = {211--235},
  issn = {0929-5313},
  doi = {10.1023/B:JCNS.0000025686.47117.67},
  abstract = {Deep brain stimulation (DBS) of the subthalamic nucleus (STN) or the internal segment of the globus pallidus (GPi) has recently been recognized as an important form of intervention for alleviating motor symptoms associated with Parkinson's disease, but the mechanism underlying its effectiveness remains unknown. Using a computational model, this paper considers the hypothesis that DBS works by replacing pathologically rhythmic basal ganglia output with tonic, high frequency firing. In our simulations of parkinsonian conditions, rhythmic inhibition from GPi to the thalamus compromises the ability of thalamocortical relay (TC) cells to respond to depolarizing inputs, such as sensorimotor signals. High frequency stimulation of STN regularizes GPi firing, and this restores TC responsiveness, despite the increased frequency and amplitude of GPi inhibition to thalamus that result. We provide a mathematical phase plane analysis of the mechanisms that determine TC relay capabilities in normal, parkinsonian, and DBS states in a reduced model. This analysis highlights the differences in deinactivation of the low-threshold calcium T -current that we observe in TC cells in these different conditions. Alternative scenarios involving convergence of thalamic signals in the cortex are also discussed, and predictions associated with these results, including the occurrence of rhythmic rebound bursts in certain TC cells in parkinsonian states and their drastic reduction by DBS, are stated. These results demonstrate how DBS could work by increasing firing rates of target cells, rather than shutting them down.},
  file = {/Users/qualia/Documents/Papers/2004 - Rubin, Terman - High Frequency Stimulation of the Subthalamic Nucleus Eliminates .pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Rubin2017,
  title = {Decoding Brain Activity Using a Large-Scale Probabilistic Functional-Anatomical Atlas of Human Cognition},
  author = {Rubin, Timothy N. and Koyejo, Oluwasanmi and Gorgolewski, Krzysztof J. and Jones, Michael N. and Poldrack, Russell A. and Yarkoni, Tal},
  editor = {Gershman, Samuel J.},
  year = {2017},
  month = oct,
  volume = {13},
  pages = {e1005649},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005649},
  abstract = {A central goal of cognitive neuroscience is to decode human brain activity\textemdash{}that is, to infer mental processes from observed patterns of whole-brain activation. Previous decoding efforts have focused on classifying brain activity into a small set of discrete cognitive states. To attain maximal utility, a decoding framework must be open-ended, systematic, and context-sensitive\textemdash{}that is, capable of interpreting numerous brain states, presented in arbitrary combinations, in light of prior information. Here we take steps towards this objective by introducing a probabilistic decoding framework based on a novel topic model\textemdash{}Generalized Correspondence Latent Dirichlet Allocation\textemdash{}that learns latent topics from a database of over 11,000 published fMRI studies. The model produces highly interpretable, spatially-circumscribed topics that enable flexible decoding of whole-brain images. Importantly, the Bayesian nature of the model allows one to ``seed'' decoder priors with arbitrary images and text\textemdash{}enabling researchers, for the first time, to generate quantitative, context-sensitive interpretations of whole-brain patterns of brain activity.},
  file = {/Users/qualia/Documents/Papers/Rubin et al. - 2017 - Decoding brain activity using a large-scale probab.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {10}
}

@article{Ruder,
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  pages = {14},
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  file = {/Users/qualia/Documents/Papers/Ruder - An overview of gradient descent optimization algor.pdf},
  language = {en}
}

@article{Runyan2017,
  title = {Distinct Timescales of Population Coding across Cortex},
  author = {Runyan, Caroline A. and Piasini, Eugenio and Panzeri, Stefano and Harvey, Christopher D.},
  year = {2017},
  month = aug,
  volume = {548},
  pages = {92--96},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature23020},
  file = {/Users/qualia/Documents/Papers/Runyan et al. - 2017 - Distinct timescales of population coding across co.pdf},
  journal = {Nature},
  language = {en},
  number = {7665}
}

@article{Rutstrom2009,
  title = {Stated Beliefs versus Inferred Beliefs: {{A}} Methodological Inquiry and Experimental Test},
  shorttitle = {Stated Beliefs versus Inferred Beliefs},
  author = {Rutstr{\"o}m, E. Elisabet and Wilcox, Nathaniel T.},
  year = {2009},
  month = nov,
  volume = {67},
  pages = {616--632},
  issn = {08998256},
  doi = {10.1016/j.geb.2009.04.001},
  abstract = {If asking subjects their beliefs during repeated game play changes the way those subjects play, using those stated beliefs to evaluate and compare theories of strategic behavior is problematic. We experimentally verify that belief elicitation can alter paths of play in a repeated asymmetric matching pennies game. In this setting, belief elicitation improves the goodness of fit of structural models of belief learning, and the prior beliefs implied by such structural models are both stronger and more realistic when beliefs are elicited than when they are not. These effects are, however, confined to the player type who sees a strong asymmetry between payoff possibilities for her two strategies in the game. We also find that ``inferred beliefs'' (beliefs estimated from past observed actions of opponents) can be better predictors of observed actions than the ``stated beliefs'' resulting from belief elicitation.},
  file = {/Users/qualia/Documents/Papers/2008 - Rutstrom, Wilcox - Stated Beliefs Versus Inferred Beliefs.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {2}
}

@article{Ryglewski2014,
  title = {Dendrites Are Dispensable for Basic Motoneuron Function but Essential for Fine Tuning of Behavior},
  author = {Ryglewski, Stefanie and Kadas, Dimitrios and Hutchinson, Katie and Schuetzler, Natalie and Vonhoff, Fernando and Duch, Carsten},
  year = {2014},
  month = dec,
  volume = {111},
  pages = {18049--18054},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1416247111},
  file = {/Users/qualia/Documents/Papers/Ryglewski et al. - 2014 - Dendrites are dispensable for basic motoneuron fun.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {50}
}

@article{Saalmann2009,
  title = {Gain Control in the Visual Thalamus during Perception and Cognition},
  author = {Saalmann, Yuri B and Kastner, Sabine},
  year = {2009},
  month = aug,
  volume = {19},
  pages = {408--414},
  issn = {09594388},
  doi = {10.1016/j.conb.2009.05.007},
  file = {/Users/qualia/Documents/Papers/2009 - Saalmann, Kastner - Gain control in the visual thalamus during perception and cognition.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en},
  number = {4}
}

@article{Saalmann2012,
  title = {The {{Pulvinar Regulates Information Transmission Between Cortical Areas Based}} on {{Attention Demands}}},
  author = {Saalmann, Y. B. and Pinsk, M. A. and Wang, L. and Li, X. and Kastner, S.},
  year = {2012},
  month = aug,
  volume = {337},
  pages = {753--756},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1223082},
  file = {/Users/qualia/Documents/Papers/2012 - Saalmann et al. - The Pulvinar Regulates Information Transmission Between Cortical Areas Based on Attention Demands.pdf},
  journal = {Science},
  language = {en},
  number = {6095}
}

@article{Saalmann2014,
  title = {Intralaminar and Medial Thalamic Influence on Cortical Synchrony, Information Transmission and Cognition},
  author = {Saalmann, Yuri B.},
  year = {2014},
  month = may,
  volume = {8},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2014.00083},
  abstract = {The intralaminar and medial thalamic nuclei are part of the higher-order thalamus, which receives little sensory input, and instead forms extensive cortico-thalamo-cortical pathways. The large mediodorsal thalamic nucleus predominantly connects with the prefrontal cortex, the adjacent intralaminar nuclei connect with fronto-parietal cortex, and the midline thalamic nuclei connect with medial prefrontal cortex and medial temporal lobe. Taking into account this connectivity pattern, it is not surprising that the intralaminar and medial thalamus has been implicated in a variety of cognitive functions, including memory processing, attention and orienting, as well as reward-based behavior. This review addresses how the intralaminar and medial thalamus may regulate information transmission in cortical circuits. A key neural mechanism may involve intralaminar and medial thalamic neurons modulating the degree of synchrony between different groups of cortical neurons according to behavioral demands. Such a thalamic-mediated synchronization mechanism may give rise to large-scale integration of information across multiple cortical circuits, consequently influencing the level of arousal and consciousness. Overall, the growing evidence supports a general role for the higher-order thalamus in the control of cortical information transmission and cognitive processing.},
  file = {/Users/qualia/Documents/Papers/Saalmann - 2014 - Intralaminar and medial thalamic influence on cort.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Sabuncu2010,
  title = {Function-Based {{Intersubject Alignment}} of {{Human Cortical Anatomy}}},
  author = {Sabuncu, Mert R. and Singer, Benjamin D. and Conroy, Bryan and Bryan, Ronald E. and Ramadge, Peter J. and Haxby, James V.},
  year = {2010},
  month = jan,
  volume = {20},
  pages = {130--140},
  issn = {1460-2199, 1047-3211},
  doi = {10.1093/cercor/bhp085},
  file = {/Users/qualia/Documents/Papers/2010 - Sabuncu et al. - Function-based intersubject alignment of human cortical anatomy.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Sacramento2017,
  title = {Dendritic Error Backpropagation in Deep Cortical Microcircuits},
  author = {Sacramento, Jo{\~a}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
  year = {2017},
  month = dec,
  abstract = {Animal behaviour depends on learning to associate sensory stimuli with the desired motor command. Understanding how the brain orchestrates the necessary synaptic modifications across different brain areas has remained a longstanding puzzle. Here, we introduce a multi-area neuronal network model in which synaptic plasticity continuously adapts the network towards a global desired output. In this model synaptic learning is driven by a local dendritic prediction error that arises from a failure to predict the top-down input given the bottom-up activities. Such errors occur at apical dendrites of pyramidal neurons where both long-range excitatory feedback and local inhibitory predictions are integrated. When local inhibition fails to match excitatory feedback an error occurs which triggers plasticity at bottom-up synapses at basal dendrites of the same pyramidal neurons. We demonstrate the learning capabilities of the model in a number of tasks and show that it approximates the classical error backpropagation algorithm. Finally, complementing this cortical circuit with a disinhibitory mechanism enables attention-like stimulus denoising and generation. Our framework makes several experimental predictions on the function of dendritic integration and cortical microcircuits, is consistent with recent observations of cross-area learning, and suggests a biological implementation of deep learning.},
  archivePrefix = {arXiv},
  eprint = {1801.00062},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Sacramento et al. - 2017 - Dendritic error backpropagation in deep cortical m.pdf},
  journal = {arXiv:1801.00062 [cs, q-bio]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{Sacramento2018,
  title = {Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm},
  author = {Sacramento, Jo{\~a}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
  year = {2018},
  month = oct,
  abstract = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances \textendash{} error backpropagation \textendash{} appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
  archivePrefix = {arXiv},
  eprint = {1810.11393},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Sacramento et al. - 2018 - Dendritic cortical microcircuits approximate the b.pdf},
  journal = {arXiv:1810.11393 [cs, q-bio]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{Saeb2017,
  title = {The Need to Approximate the Use-Case in Clinical Machine Learning},
  author = {Saeb, Sohrab and Lonini, Luca and Jayaraman, Arun and Mohr, David C. and Kording, Konrad P.},
  year = {2017},
  month = may,
  volume = {6},
  issn = {2047-217X},
  doi = {10.1093/gigascience/gix019},
  abstract = {Background. The availability of smartphone and wearable sensor technology is leading to a rapid accumulation of human subject data, and machine learning is emerging as a technique to map that data into clinical predictions. As machine learning algorithms are increasingly used to support clinical decision making, it is vital to reliably quantify their prediction accuracy. Cross-validation is the standard approach where the accuracy of such algorithms is evaluated on data the algorithm has not seen during training. However, for this procedure to be meaningful, the relationship between the training and validation set should mimic the relationship between the training set and the dataset expected for the clinical use. Here we compared two popular cross-validation methods: record-wise and subject-wise. The subjectwise procedure mirrors the clinically relevant use-case scenario of diagnosing/identifying patterns in newly recruited subjects. The record-wise strategy has no such interpretation.
Results. Using both a publicly available dataset and a simulation, we found that record-wise crossvalidation often massively overestimates the prediction accuracy of the algorithms. We also conducted a systematic review of the relevant literature, and found that this overly optimistic method is used by almost half of the retrieved studies that used accelerometers, wearable sensors, or smartphones to predict clinical outcomes.
Conclusions. As we move towards an era of machine learning based diagnosis and treatment, using proper methods to evaluate their accuracy is crucial, as results that are overly optimistic can mislead both clinicians and data scientists.},
  file = {/Users/qualia/Documents/Papers/Saeb et al. - 2017 - The need to approximate the use-case in clinical m.pdf},
  journal = {GigaScience},
  language = {en},
  number = {5}
}

@article{Saenger2017,
  title = {Uncovering the Underlying Mechanisms and Whole-Brain Dynamics of Deep Brain Stimulation for {{Parkinson}}'s Disease},
  author = {Saenger, Victor M. and Kahan, Joshua and Foltynie, Tom and Friston, Karl and Aziz, Tipu Z. and Green, Alexander L. and {van Hartevelt}, Tim J. and Cabral, Joana and Stevner, Angus B. A. and Fernandes, Henrique M. and Mancini, Laura and Thornton, John and Yousry, Tarek and Limousin, Patricia and Zrinzo, Ludvic and Hariz, Marwan and Marques, Paulo and Sousa, Nuno and Kringelbach, Morten L. and Deco, Gustavo},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-10003-y},
  file = {/Users/qualia/Documents/Papers/Saenger et al. - 2017 - Uncovering the underlying mechanisms and whole-bra.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Said2010,
  title = {Distributed Representations of Dynamic Facial Expressions in the Superior Temporal Sulcus},
  author = {Said, C. P. and Moore, C. D. and Engell, A. D. and Todorov, A. and Haxby, J. V.},
  year = {2010},
  month = may,
  volume = {10},
  pages = {11--11},
  issn = {1534-7362},
  doi = {10.1167/10.5.11},
  abstract = {Previous research on the superior temporal sulcus (STS) has shown that it responds more to facial expressions than to neutral faces. Here, we extend our understanding of the STS in two ways. First, using targeted high-resolution fMRI measurements of the lateral cortex and multivoxel pattern analysis, we show that the response to seven categories of dynamic facial expressions can be decoded in both the posterior STS (pSTS) and anterior STS (aSTS). We were also able to decode patterns corresponding to these expressions in the frontal operculum (FO), a structure that has also been shown to respond to facial expressions. Second, we measured the similarity structure of these representations and found that the similarity structure in the pSTS significantly correlated with the perceptual similarity structure of the expressions. This was the case regardless of whether we used pattern classification or more traditional correlation techniques to extract the neural similarity structure. These results suggest that distributed representations in the pSTS could underlie the perception of facial expressions.},
  file = {/Users/qualia/Documents/Papers/2010 - Said, Moore - Distributed representations of dynamic facial expressions in the superior temporal sulcus.pdf},
  journal = {Journal of Vision},
  language = {en},
  number = {5}
}

@article{Saleem2016,
  title = {Subcortical Source and Modulation of the Narrowband Gamma Oscillation in Mouse Visual Cortex},
  author = {Saleem, Aman B and Lien, Anthony D and Krumin, Michael and Haider, Bilal and Roman Roson, Miroslav and Ayaz, Asli and Reinhold, Kimberley and Busse, Laura and Carandini, Matteo and Harris, Kenneth D},
  year = {2016},
  month = oct,
  doi = {10.1101/050245},
  abstract = {Visual cortex (V1) exhibits two types of gamma oscillation: a well-characterized broadband (30-90Hz) rhythm, and a narrowband oscillation occurring at frequencies close to 60 Hz in mice. We investigated the source of narrowband gamma, the factors modulating its strength, and its relationship to broadband gamma. Narrowband and broadband gamma power were uncorrelated. Increasing visual contrast had opposite effects on the two kinds of gamma activity: it increased broadband power, but suppressed the narrowband oscillation. Narrowband power was strongest in layer 4, and was mediated primarily by excitatory currents entrained by rhythmically firing neuronal ensembles in the lateral geniculate nucleus (LGN). Silencing the cortex optogenetically did not affect narrowband rhythmicity in either LGN spike trains or cortical EPSCs, suggesting that this oscillation reflects unidirectional flow of information from LGN to V1.},
  file = {/Users/qualia/Documents/Papers/Saleem et al. - 2016 - Subcortical source and modulation of the narrowban.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Salkoff2015,
  title = {Synaptic {{Mechanisms}} of {{Tight Spike Synchrony}} at {{Gamma Frequency}} in {{Cerebral Cortex}}},
  author = {Salkoff, D. B. and Zagha, E. and Yuzgec, O. and McCormick, D. A.},
  year = {2015},
  month = jul,
  volume = {35},
  pages = {10236--10251},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0828-15.2015},
  file = {/Users/qualia/Documents/Papers/Salkoff et al. - 2015 - Synaptic Mechanisms of Tight Spike Synchrony at Ga.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {28}
}

@article{SalthouseandWemaraLichty,
  title = {Testsof {{theNeuralNoiseHypothesiosf Age}}-{{RelatedCognitiveChange}}'},
  author = {SalthouseandWemaraLichty, TimothyA},
  pages = {8},
  file = {/Users/qualia/Documents/Papers/1985 - Salthouse, Lichty - of the Neural Noise Hypothesis Cognitive Change ' J T Jrj J '.pdf},
  language = {en}
}

@article{Samaha2015,
  title = {The {{Speed}} of {{Alpha}}-{{Band Oscillations Predicts}} the {{Temporal Resolution}} of {{Visual Perception}}},
  author = {Samaha, Jason and Postle, Bradley R.},
  year = {2015},
  month = nov,
  volume = {25},
  pages = {2985--2990},
  issn = {09609822},
  doi = {10.1016/j.cub.2015.10.007},
  abstract = {Evidence suggests that scalp-recorded occipital alpha-band (8\textendash{}13 Hz) oscillations reflect phasic information transfer in thalamocortical neurons projecting from lateral geniculate nucleus to visual cortex [1\textendash{}5]. In animals, the phase of ongoing alpha oscillations has been shown to modulate stimulus discrimination and neuronal spiking [6]. Human research has shown that alpha phase predicts visual perception of nearthreshold stimuli [7\textendash{}11] and subsequent neural activity [12\textendash{}14] and that the frequency of these oscillations predicts reaction times [15], as well as the maximum temporal interval necessary for perceived simultaneity [16]. These phasic effects have led to the hypothesis that conscious perception occurs in discrete temporal windows, clocked by the frequency of alpha oscillations [17\textendash{}21]. Under this hypothesis, variation in the frequency of occipital alpha oscillations should predict variation in the temporal resolution of visual perception. Specifically, when two stimuli fall within the same alpha cycle, they may be perceived as a single stimulus, resulting in perception with lower temporal resolution when alpha frequency is lower. We tested this by assessing the relationship between two-flash fusion thresholds (a measure of the temporal resolution of visual perception) and the frequency of eyes-closed and task-related alpha rhythms. We found, both between and within subjects, that faster alpha frequencies predicted more accurate flash discrimination, providing novel evidence linking alpha frequency to the temporal resolution of perception.},
  file = {/Users/qualia/Documents/Papers/2015 - Samaha, Postle - The Speed of Alpha-Band Oscillations Predicts the Temporal Resolution of Visual Perception.pdf},
  journal = {Current Biology},
  language = {en},
  number = {22}
}

@article{Samaha2016,
  title = {Decoding and {{Reconstructing}} the {{Focus}} of {{Spatial Attention}} from the {{Topography}} of {{Alpha}}-Band {{Oscillations}}},
  author = {Samaha, Jason and Sprague, Thomas C. and Postle, Bradley R.},
  year = {2016},
  month = aug,
  volume = {28},
  pages = {1090--1097},
  issn = {0898-929X, 1530-8898},
  doi = {10.1162/jocn_a_00955},
  file = {/Users/qualia/Documents/Papers/Samaha et al. - 2016 - Decoding and Reconstructing the Focus of Spatial A.pdf},
  journal = {Journal of Cognitive Neuroscience},
  language = {en},
  number = {8}
}

@article{Sanborn2016,
  title = {Bayesian {{Brains}} without {{Probabilities}}},
  author = {Sanborn, Adam N. and Chater, Nick},
  year = {2016},
  month = dec,
  volume = {20},
  pages = {883--893},
  issn = {13646613},
  doi = {10.1016/j.tics.2016.10.003},
  file = {/Users/qualia/Documents/Papers/Sanborn and Chater - 2016 - Bayesian Brains without Probabilities.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {12}
}

@article{Santi2014,
  title = {Quantifying the Benefits of Vehicle Pooling with Shareability Networks},
  author = {Santi, Paolo and Resta, Giovanni and Szell, Michael and Sobolevsky, Stanislav and Strogatz, Steven H. and Ratti, Carlo},
  year = {2014},
  month = sep,
  volume = {111},
  pages = {13290--13294},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1403657111},
  file = {/Users/qualia/Documents/Papers/2013 - Santi et al. - Quantifying the benefits of vehicle pooling with shareability networks.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {37}
}

@article{Sapountzis2010,
  title = {A Comparison of {{fMRI}} Adaptation and Multivariate Pattern Classification Analysis in Visual Cortex},
  author = {Sapountzis, Panagiotis and Schluppeck, Denis and Bowtell, Richard and Peirce, Jonathan W.},
  year = {2010},
  month = jan,
  volume = {49},
  pages = {1632--1640},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.09.066},
  abstract = {Functional magnetic resonance imaging (fMRI) has become a ubiquitous tool in cognitive neuroscience. The technique allows noninvasive measurements of cortical responses in the human brain, but only on the millimeter scale. Because a typical voxel contains many thousands of neurons with varied properties, establishing the selectivity of their responses directly is impossible. In recent years, two methods using fMRI aimed at studying the selectivity of neuronal populations on a `subvoxel' scale have been heavily used. The first technique, fMRI adaptation, relies on the observation that the blood oxygen level-dependent (BOLD) response in a given voxel is reduced after prolonged presentation of a stimulus, and that this reduction is selective to the characteristics of the repeated stimuli (adapters). The second technique, multivariate pattern analysis (MVPA), makes use of multivariate statistics to recover small biases in individual voxels in their responses to different stimuli. It is thought that these biases arise due to the uneven distribution of neurons (with different properties) sampled by the many voxels in the imaged volume. These two techniques have not been compared explicitly, however, and little is known about their relative sensitivities. Here, we compared fMRI results from orientation-specific visual adaptation and orientation\textendash{}classification by MVPA, using optimized experimental designs for each, and found that the multivariate pattern classification approach was more sensitive to small differences in stimulus orientation than the adaptation paradigm. Estimates of orientation selectivity obtained with the two methods were, however, very highly correlated across visual areas.},
  file = {/Users/qualia/Documents/Papers/2010 - Sapountzis et al. - A comparison of fMRI adaptation and multivariate pattern classification analysis in visual cortex.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Sarin1999,
  title = {Payoff {{Assessments}} without {{Probabilities}}: {{A Simple Dynamic Model}} of {{Choice}}},
  shorttitle = {Payoff {{Assessments}} without {{Probabilities}}},
  author = {Sarin, Rajiv and Vahid, Farshid},
  year = {1999},
  month = aug,
  volume = {28},
  pages = {294--309},
  issn = {08998256},
  doi = {10.1006/game.1998.0702},
  file = {/Users/qualia/Documents/Papers/1999 - Sarin, Vahid - Payoff assessments without probabilities A simple dynamic model of choice.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {2}
}

@article{Sarin2001,
  title = {Predicting {{How People Play Games}}: {{A Simple Dynamic Model}} of {{Choice}}},
  shorttitle = {Predicting {{How People Play Games}}},
  author = {Sarin, Rajiv and Vahid, Farshid},
  year = {2001},
  month = jan,
  volume = {34},
  pages = {104--122},
  issn = {08998256},
  doi = {10.1006/game.1999.0783},
  file = {/Users/qualia/Documents/Papers/1999 - Sarin, Vahid - Predicting How People Play Games Suhglfwlqj Krz Shrsoh Sod Jdphv = D Vlpsoh.pdf},
  journal = {Games and Economic Behavior},
  language = {en},
  number = {1}
}

@article{Sato2000,
  title = {Axonal Branching Pattern of Neurons of the Subthalamic Nucleus in Primates},
  author = {Sato, Fumi and Parent, Martin and Levesque, Martin and Parent, Andre},
  year = {2000},
  month = aug,
  volume = {424},
  pages = {142--152},
  issn = {0021-9967, 1096-9861},
  doi = {10.1002/1096-9861(20000814)424:1<142::AID-CNE10>3.0.CO;2-8},
  abstract = {Axonal projections arising from the subthalamic nucleus (STN) in cynomolgus monkeys (Macaca fascicularis) were traced after labeling small pools (5\textendash{}15 cells) of neurons with biotinylated dextran amine. Seventy-five single axons were reconstructed from serial sagittal sections with a camera lucida. Most of the STN labeled cells displayed five to eight long, sparsely spined dendrites that arborized mostly along the main axis of the nucleus. Based on their axonal targets, five distinct types of STN projection neurons have been identified: 1) neurons projecting to the substantia nigra pars reticulata (SNr), the internal (GPi) and external (GPe) segments of the globus pallidus (21.3\%); 2) neurons targeting SNr and GPe (2.7\%); 3) neurons projecting to GPi and GPe (48\%); 4) neurons targeting GPe only (10.7 \%); and 5) neurons with axons that coursed toward the sriatum, but whose terminal arborization could not be visualized in detail (17.3\%). Axons of the first two types bifurcated into rostral subthalamopallidal and caudal pallidonigral branches. However, the majority of STN axons had only a single branch that coursed rostrally toward the pallidum and striatum. These results reveal that, in contrast to current beliefs, the primate STN is not a monolithic entity. This nucleus harbors several subtypes of projection neurons, each endowed with a highly patterned set of collaterals. This organization allows STN neurons to exert a multifarious effect not only on the GPe, with which the STN is reciprocally connected, but also on the two major output structures of the basal ganglia, the SNr and the GPi. J. Comp. Neurol. 424: 142\textendash{}152, 2000. \textcopyright{} 2000 Wiley-Liss, Inc.},
  file = {/Users/qualia/Documents/Papers/2000 - Sato et al. - Axonal branching pattern of neurons of the subthalamic nucleus in primates.pdf},
  journal = {The Journal of Comparative Neurology},
  language = {en},
  number = {1}
}

@article{Sato2001,
  title = {Search {{Efficiency}} but {{Not Response Interference Affects Visual Selection}} in {{Frontal Eye Field}}},
  author = {Sato, Takashi and Murthy, Aditya and Thompson, Kirk G. and Schall, Jeffrey D.},
  year = {2001},
  month = may,
  volume = {30},
  pages = {583--591},
  issn = {08966273},
  doi = {10.1016/S0896-6273(01)00304-X},
  abstract = {Two manipulations of a visual search task were used to test the hypothesis that the discrimination of a target from distractors by visually responsive neurons in the frontal eye field (FEF) marks the outcome and conclusion of visual processing instead of saccade preparation. First, search efficiency was reduced by increasing the similarity of the distractors to the target. Second, response interference was introduced by infrequently changing the location of the target in the array. Both manipulations increased reaction time, but only the change in search efficiency affected the time needed to select the target by visually responsive neurons. This result indicates that visually responsive neurons in FEF form an explicit representation of the location of the target in the image.},
  file = {/Users/qualia/Documents/Papers/2001 - Sato et al. - Search efficiency but not response interference affects visual selection in frontal eye field.pdf},
  journal = {Neuron},
  language = {en},
  number = {2}
}

@article{Saxe2013,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2013},
  month = dec,
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archivePrefix = {arXiv},
  eprint = {1312.6120},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2013 - Saxe, McClelland, Ganguli - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf},
  journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cond-mat, q-bio, stat}
}

@article{Scemes2006,
  title = {Astrocyte Calcium Waves: {{What}} They Are and What They Do},
  shorttitle = {Astrocyte Calcium Waves},
  author = {Scemes, Eliana and Giaume, Christian},
  year = {2006},
  month = nov,
  volume = {54},
  pages = {716--725},
  issn = {0894-1491, 1098-1136},
  doi = {10.1002/glia.20374},
  abstract = {Several lines of evidence indicate that the elaborated calcium signals and the occurrence of calcium waves in astrocytes provide these cells with a specific form of excitability. The identification of the cellular and molecular steps involved in the triggering and transmission of Ca21 waves between astrocytes resulted in the identification of two pathways mediating this form of intercellular communication. One of them involves the direct communication between the cytosols of two adjoining cells through gap junction channels, while the other depends upon the release of ``gliotransmitters'' that activates membrane receptors on neighboring cells. In this review we summarize evidence in favor of these two mechanisms of Ca21 wave transmission and we discuss that they may not be mutually exclusive, but are likely to work in conjunction to coordinate the activity of a group of cells. To address a key question regarding the functional consequences following the passage of a Ca21 wave, we list, in this review, some of the potential intracellular targets of these Ca21 transients in astrocytes, and discuss the functional consequences of the activation of these targets for the interactions that astrocytes maintain with themselves and with other cellular partners, including those at the glial/vasculature interface and at perisynaptic sites where astrocytic processes tightly interact with neurons. VC 2006 Wiley-Liss, Inc.},
  file = {/Users/qualia/Documents/Papers/Scemes and Giaume - 2006 - Astrocyte calcium waves What they are and what th.pdf},
  journal = {Glia},
  language = {en},
  number = {7}
}

@article{Schaffer2013,
  title = {A {{Complex}}-{{Valued Firing}}-{{Rate Model That Approximates}} the {{Dynamics}} of {{Spiking Networks}}},
  author = {Schaffer, Evan S. and Ostojic, Srdjan and Abbott, L. F.},
  editor = {Ermentrout, Bard},
  year = {2013},
  month = oct,
  volume = {9},
  pages = {e1003301},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003301},
  abstract = {Firing-rate models provide an attractive approach for studying large neural networks because they can be simulated rapidly and are amenable to mathematical analysis. Traditional firing-rate models assume a simple form in which the dynamics are governed by a single time constant. These models fail to replicate certain dynamic features of populations of spiking neurons, especially those involving synchronization. We present a complex-valued firing-rate model derived from an eigenfunction expansion of the Fokker-Planck equation and apply it to the linear, quadratic and exponential integrate-andfire models. Despite being almost as simple as a traditional firing-rate description, this model can reproduce firing-rate dynamics due to partial synchronization of the action potentials in a spiking model, and it successfully predicts the transition to spike synchronization in networks of coupled excitatory and inhibitory neurons.},
  file = {/Users/qualia/Documents/Papers/2013 - Schaffer, Ostojic, Abbott - A Complex-Valued Firing-Rate Model That Approximates the Dynamics of Spiking Networks.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Schaworonkow2015,
  title = {Power-Law Dynamics in Neuronal and Behavioral Data Introduce Spurious Correlations: {{Power}}-{{Law Dynamics}} in {{Neuronal}} and {{Behavioral Data}}},
  shorttitle = {Power-Law Dynamics in Neuronal and Behavioral Data Introduce Spurious Correlations},
  author = {Schaworonkow, Natalie and Blythe, Duncan A.J. and Kegeles, Jewgeni and Curio, Gabriel and Nikulin, Vadim V.},
  year = {2015},
  month = aug,
  volume = {36},
  pages = {2901--2914},
  issn = {10659471},
  doi = {10.1002/hbm.22816},
  abstract = {Relating behavioral and neuroimaging measures is essential to understanding human brain function. Often, this is achieved by computing a correlation between behavioral measures, e.g., reaction times, and neurophysiological recordings, e.g., prestimulus EEG alpha-power, on a single-trial-basis. This approach treats individual trials as independent measurements and ignores the fact that data are acquired in a temporal order. It has already been shown that behavioral measures as well as neurophysiological recordings display power-law dynamics, which implies that trials are not in fact independent. Critically, computing the correlation coefficient between two measures exhibiting long-range temporal dependencies may introduce spurious correlations, thus leading to erroneous conclusions about the relationship between brain activity and behavioral measures. Here, we address data-analytic pitfalls which may arise when long-range temporal dependencies in neural as well as behavioral measures are ignored. We quantify the influence of temporal dependencies of neural and behavioral measures on the observed correlations through simulations. Results are further supported in analysis of real EEG data recorded in a simple reaction time task, where the aim is to predict the latency of responses on the basis of prestimulus alpha oscillations. We show that it is possible to "predict" reaction times from one subject on the basis of EEG activity recorded in another subject simply owing to the fact that both measures display power-law dynamics. The same is true when correlating EEG activity obtained from different subjects. A surrogatedata procedure is described which correctly tests for the presence of correlation while controlling for the effect of power-law dynamics. Hum Brain Mapp 00:000\textendash{}000, 2015. VC 2015 Wiley Periodicals, Inc.},
  file = {/Users/qualia/Documents/Papers/Schaworonkow et al. - 2015 - Power-law dynamics in neuronal and behavioral data.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {8}
}

@article{Schillebeeckx2013,
  title = {The Missing Piece to Changing the University Culture},
  author = {Schillebeeckx, Maximiliaan and Maricque, Brett and Lewis, Cory},
  year = {2013},
  month = oct,
  volume = {31},
  pages = {938--941},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt.2706},
  file = {/Users/qualia/Documents/Papers/2013 - Schillebeeckx, Maricque, Lewis - The missing piece to changing the university culture.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {10}
}

@article{Schmah2010,
  title = {Comparing {{Classification Methods}} for {{Longitudinal fMRI Studies}}},
  author = {Schmah, Tanya and Yourganov, Grigori and Zemel, Richard S. and Hinton, Geoffrey E. and Small, Steven L. and Strother, Stephen C.},
  year = {2010},
  month = nov,
  volume = {22},
  pages = {2729--2762},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00024},
  file = {/Users/qualia/Documents/Papers/2010 - Schmah et al. - Comparing classification methods for longitudinal fMRI studies.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {11}
}

@article{Schmidhuber,
  title = {A {{Possibility}} for {{Implementing Curiosity}} and {{Boredom}} in {{Model}}-{{Building Neural Controllers}}},
  author = {Schmidhuber, Jurgen},
  pages = {7},
  file = {/Users/qualia/Documents/Papers/1991 - Urgen Schmidhuber - A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers.pdf},
  language = {en}
}

@article{Schmidhuber1991,
  title = {A Possibility for Implementing Curiosity and Boredom in Model-Building Neural Controllers},
  author = {Schmidhuber},
  year = {1991},
  pages = {222--227},
  file = {/Users/qualia/Documents/Papers/1991 - Schmidhuber - A possibility for implementing curiosity and boredom in model-building neural controllers.pdf},
  journal = {Proc. of the international conference on simulation of adaptive behavior: From animals to animats}
}

@article{Schmidhuber2008,
  title = {Driven by {{Compression Progress}}: {{A Simple Principle Explains Essential Aspects}} of {{Subjective Beauty}}, {{Novelty}}, {{Surprise}}, {{Interestingness}}, {{Attention}}, {{Curiosity}}, {{Creativity}}, {{Art}}, {{Science}}, {{Music}}, {{Jokes}}},
  shorttitle = {Driven by {{Compression Progress}}},
  author = {Schmidhuber, Juergen},
  year = {2008},
  month = dec,
  abstract = {I argue that data becomes temporarily interesting by itself to some self-improving, but computationally limited, subjective observer once he learns to predict or compress the data in a better way, thus making it subjectively simpler and more beautiful. Curiosity is the desire to create or discover more non-random, nonarbitrary, regular data that is novel and surprising not in the traditional sense of Boltzmann and Shannon but in the sense that it allows for compression progress because its regularity was not yet known. This drive maximizes interestingness, the first derivative of subjective beauty or compressibility, that is, the steepness of the learning curve. It motivates exploring infants, pure mathematicians, composers, artists, dancers, comedians, yourself, and (since 1990) artificial systems.},
  archivePrefix = {arXiv},
  eprint = {0812.4360},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Schmidhuber - 2008 - Driven by Compression Progress A Simple Principle.pdf},
  journal = {arXiv:0812.4360 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Schmidhuber2010,
  title = {Formal {{Theory}} of {{Creativity}}, {{Fun}}, and {{Intrinsic Motivation}} (1990\textendash{}2010)},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = sep,
  volume = {2},
  pages = {230--247},
  issn = {1943-0604, 1943-0612},
  doi = {10.1109/TAMD.2010.2056368},
  abstract = {The simple but general formal theory of fun \& intrinsic motivation \& creativity (1990-) is based on the concept of maximizing intrinsic reward for the active creation or discovery of novel, surprising patterns allowing for improved prediction or data compression. It generalizes the traditional field of active learning, and is related to old but less formal ideas in aesthetics theory and developmental psychology. It has been argued that the theory explains many essential aspects of intelligence including autonomous development, science, art, music, humor. This overview first describes theoretically optimal (but not necessarily practical) ways of implementing the basic computational principles on exploratory, intrinsically motivated agents or robots, encouraging them to provoke event sequences exhibiting previously unknown but learnable algorithmic regularities. Emphasis is put on the importance of limited computational resources for online prediction and compression. Discrete and continuous time formulations are given. Previous practical but non-optimal implementations (1991, 1995, 1997-2002) are reviewed, as well as several recent variants by others (2005-). A simplified typology addresses current confusion concerning the precise nature of intrinsic motivation.},
  file = {/Users/qualia/Documents/Papers/Schmidhuber - 2010 - Formal Theory of Creativity, Fun, and Intrinsic Mo.pdf},
  journal = {IEEE Transactions on Autonomous Mental Development},
  language = {en},
  number = {3}
}

@article{Schmidhuber2015,
  title = {On {{Learning}} to {{Think}}: {{Algorithmic Information Theory}} for {{Novel Combinations}} of {{Reinforcement Learning Controllers}} and {{Recurrent Neural World Models}}},
  shorttitle = {On {{Learning}} to {{Think}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = nov,
  abstract = {This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially ``learning to think.'' The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as ``mirror neurons.'' Experimental results will be described in separate papers.},
  archivePrefix = {arXiv},
  eprint = {1511.09249},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Schmidhuber - On Learning to Think Algorithmic Information Theory for Novel Combinations of Reinforcement Learning Controllers an.pdf},
  journal = {arXiv:1511.09249 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Schmidhuber2015a,
  title = {On {{Learning}} to {{Think}}: {{Algorithmic Information Theory}} for {{Novel Combinations}} of {{Reinforcement Learning Controllers}} and {{Recurrent Neural World Models}}},
  shorttitle = {On {{Learning}} to {{Think}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = nov,
  abstract = {This paper addresses the general problem of reinforcement learning (RL) in partially observable environments. In 2013, our large RL recurrent neural networks (RNNs) learned from scratch to drive simulated cars from high-dimensional video input. However, real brains are more powerful in many ways. In particular, they learn a predictive model of their initially unknown environment, and somehow use it for abstract (e.g., hierarchical) planning and reasoning. Guided by algorithmic information theory, we describe RNN-based AIs (RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending sequences of tasks, some of them provided by the user, others invented by the RNNAI itself in a curious, playful fashion, to improve its RNN-based world model. Unlike our previous model-building RNN-based RL machines dating back to 1990, the RNNAI learns to actively query its model for abstract reasoning and planning and decision making, essentially ``learning to think.'' The basic ideas of this report can be applied to many other cases where one RNN-like system exploits the algorithmic information content of another. They are taken from a grant proposal submitted in Fall 2014, and also explain concepts such as ``mirror neurons.'' Experimental results will be described in separate papers.},
  archivePrefix = {arXiv},
  eprint = {1511.09249},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Schmidhuber - 2015 - On Learning to Think Algorithmic Information Theo.pdf},
  journal = {arXiv:1511.09249 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Schmidhuber2019,
  title = {Unsupervised {{Minimax}}: {{Adversarial Curiosity}}, {{Generative Adversarial Networks}}, and {{Predictability Minimization}}},
  shorttitle = {Unsupervised {{Minimax}}},
  author = {Schmidhuber, Juergen},
  year = {2019},
  month = jun,
  abstract = {Generative Adversarial Networks (GANs) learn to model data distributions through two unsupervised neural networks, each minimizing the objective function maximized by the other. We relate this game theoretic strategy to earlier neural networks playing unsupervised minimax games. (i) GANs can be formulated as a special case of Adversarial Curiosity (1990) based on a minimax duel between two networks, one generating data through its probabilistic actions, the other predicting consequences thereof. (ii) We correct a previously published claim that Predictability Minimization (PM, 1990s) is not based on a minimax game. PM models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components.},
  archivePrefix = {arXiv},
  eprint = {1906.04493},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Schmidhuber - 2019 - Unsupervised Minimax Adversarial Curiosity, Gener.pdf},
  journal = {arXiv:1906.04493 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Schmidhuber2019a,
  title = {Unsupervised {{Minimax}}: {{Adversarial Curiosity}}, {{Generative Adversarial Networks}}, and {{Predictability Minimization}}},
  shorttitle = {Unsupervised {{Minimax}}},
  author = {Schmidhuber, Juergen},
  year = {2019},
  month = jun,
  abstract = {Generative Adversarial Networks (GANs) learn to model data distributions through two unsupervised neural networks, each minimizing the objective function maximized by the other. We relate this game theoretic strategy to earlier neural networks playing unsupervised minimax games. (i) GANs can be formulated as a special case of Adversarial Curiosity (1990) based on a minimax duel between two networks, one generating data through its probabilistic actions, the other predicting consequences thereof. (ii) We correct a previously published claim that Predictability Minimization (PM, 1990s) is not based on a minimax game. PM models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components.},
  archivePrefix = {arXiv},
  eprint = {1906.04493},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Schmidhuber - 2019 - Unsupervised Minimax Adversarial Curiosity, Gener 2.pdf},
  journal = {arXiv:1906.04493 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Schmidhuber2019b,
  title = {Reinforcement {{Learning Upside Down}}: {{Don}}'t {{Predict Rewards}} -- {{Just Map Them}} to {{Actions}}},
  shorttitle = {Reinforcement {{Learning Upside Down}}},
  author = {Schmidhuber, Juergen},
  year = {2019},
  month = dec,
  abstract = {We transform reinforcement learning (RL) into a form of supervised learning (SL) by turning traditional RL on its head, calling this or Upside Down RL (UDRL). Standard RL predicts rewards, while instead uses rewards as task-defining inputs, together with representations of time horizons and other computable functions of historic and desired future data. learns to interpret these input observations as commands, mapping them to actions (or action probabilities) through SL on past (possibly accidental) experience. generalizes to achieve high rewards or other goals, through input commands such as: get lots of reward within at most so much time! A separate paper [61] on first experiments with shows that even a pilot version of can outperform traditional baseline algorithms on certain challenging RL problems.},
  archivePrefix = {arXiv},
  eprint = {1912.02875},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Schmidhuber - 2019 - Reinforcement Learning Upside Down Don't Predict .pdf},
  journal = {arXiv:1912.02875 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Schmidhubera,
  title = {{{EVALUATING BENCHMARK PROBLEMS BY RANDOM GUESSING}}},
  author = {Schmidhuber, Jurgen and Hochreiter, Sepp and Bengio, Yoshua},
  pages = {12},
  file = {/Users/qualia/Documents/Papers/Schmidhuber et al. - EVALUATING BENCHMARK PROBLEMS BY RANDOM GUESSING.pdf},
  language = {en}
}

@article{Schmidt2013,
  title = {Patterned {{Brain Stimulation}}, {{What}} a {{Framework}} with {{Rhythmic}} and {{Noisy Components Might Tell Us}} about {{Recovery Maximization}}},
  author = {Schmidt, Sein and Scholz, Michael and Obermayer, Klaus and Brandt, Stephan A.},
  year = {2013},
  volume = {7},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00325},
  abstract = {Brain stimulation is having remarkable impact on clinical neurology. Brain stimulation can modulate neuronal activity in functionally segregated circumscribed regions of the human brain. Polarity, frequency, and noise specific stimulation can induce specific manipulations on neural activity. In contrast to neocortical stimulation, deep-brain stimulation has become a tool that can dramatically improve the impact clinicians can possibly have on movement disorders. In contrast, neocortical brain stimulation is proving to be remarkably susceptible to intrinsic brain-states. Although evidence is accumulating that brain stimulation can facilitate recovery processes in patients with cerebral stroke, the high variability of results impedes successful clinical implementation. Interestingly, recent data in healthy subjects suggests that brain-state dependent patterned stimulation might help resolve some of the intrinsic variability found in previous studies. In parallel, other studies suggest that noisy ``stochastic resonance'' (SR)-like processes are a non-negligible component in non-invasive brain stimulation studies. The hypothesis developed in this manuscript is that stimulation patterning with noisy and oscillatory components will help patients recover from stroke related deficits more reliably. To address this hypothesis we focus on two factors common to both neural computation (intrinsic variables) as well as brain stimulation (extrinsic variables): noise and oscillation. We review diverse theoretical and experimental evidence that demonstrates that subject-function specific brain-states are associated with specific oscillatory activity patterns. These states are transient and can be maintained by noisy processes. The resulting control procedures can resemble homeostatic or SR processes. In this context we try to extend awareness for inter-individual differences and the use of individualized stimulation in the recovery maximization of stroke patients.},
  file = {/Users/qualia/Documents/Papers/2013 - Schmidt et al. - Patterned Brain Stimulation, What a Framework with Rhythmic and Noisy Components Might Tell Us about Recovery Ma.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Schmidt2019,
  title = {Self-{{Play Learning Without}} a {{Reward Metric}}},
  author = {Schmidt, Dan and Moran, Nick and Rosenfeld, Jonathan S. and Rosenthal, Jonathan and Yedidia, Jonathan},
  year = {2019},
  month = dec,
  abstract = {The AlphaZero algorithm for the learning of strategy games via self-play, which has produced superhuman ability in the games of Go, chess, and shogi, uses a quantitative reward function for game outcomes, requiring the users of the algorithm to explicitly balance different components of the reward against each other, such as the game winner and margin of victory. We present a modification to the AlphaZero algorithm that requires only a total ordering over game outcomes, obviating the need to perform any quantitative balancing of reward components. We demonstrate that this system learns optimal play in a comparable amount of time to AlphaZero on a sample game.},
  archivePrefix = {arXiv},
  eprint = {1912.07557},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Schmidt et al. - 2019 - Self-Play Learning Without a Reward Metric.pdf},
  journal = {arXiv:1912.07557 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Schneider2014,
  title = {Linking {{Macroscopic}} with {{Microscopic Neuroanatomy Using Synthetic Neuronal Populations}}},
  author = {Schneider, Calvin J. and Cuntz, Hermann and Soltesz, Ivan},
  editor = {Hilgetag, Claus C.},
  year = {2014},
  month = oct,
  volume = {10},
  pages = {e1003921},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003921},
  abstract = {Dendritic morphology has been shown to have a dramatic impact on neuronal function. However, population features such as the inherent variability in dendritic morphology between cells belonging to the same neuronal type are often overlooked when studying computation in neural networks. While detailed models for morphology and electrophysiology exist for many types of single neurons, the role of detailed single cell morphology in the population has not been studied quantitatively or computationally. Here we use the structural context of the neural tissue in which dendritic trees exist to drive their generation in silico. We synthesize the entire population of dentate gyrus granule cells, the most numerous cell type in the hippocampus, by growing their dendritic trees within their characteristic dendritic fields bounded by the realistic structural context of (1) the granule cell layer that contains all somata and (2) the molecular layer that contains the dendritic forest. This process enables branching statistics to be linked to larger scale neuroanatomical features. We find large differences in dendritic total length and individual path length measures as a function of location in the dentate gyrus and of somatic depth in the granule cell layer. We also predict the number of unique granule cell dendrites invading a given volume in the molecular layer. This work enables the complete population-level study of morphological properties and provides a framework to develop complex and realistic neural network models.},
  file = {/Users/qualia/Documents/Papers/Schneider et al. - 2014 - Linking Macroscopic with Microscopic Neuroanatomy .pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {10}
}

@article{Schoenholz2017,
  title = {{{DEEP INFORMATION PROPAGATION}}},
  author = {Schoenholz, Samuel S and Gilmer, Justin and Ganguli, Surya and {Sohl-Dickstein}, Jascha},
  year = {2017},
  pages = {18},
  abstract = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
  file = {/Users/qualia/Documents/Papers/Schoenholz et al. - 2017 - DEEP INFORMATION PROPAGATION.pdf},
  language = {en}
}

@article{Schoenick2017,
  title = {Moving beyond the {{Turing Test}} with the {{Allen AI Science Challenge}}},
  author = {Schoenick, Carissa and Clark, Peter and Tafjord, Oyvind and Turney, Peter and Etzioni, Oren},
  year = {2017},
  month = aug,
  volume = {60},
  pages = {60--64},
  issn = {00010782},
  doi = {10.1145/3122814},
  file = {/Users/qualia/Documents/Papers/Schoenick et al. - 2017 - Moving beyond the Turing Test with the Allen AI Sc.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {9}
}

@article{Scholte2017,
  title = {Visual Pathways from the Perspective of Cost Functions and Multi-Task Deep Neural Networks},
  author = {Scholte, H. Steven and Losch, Max M. and Ramakrishnan, Kandan and {de Haan}, Edward and Bohte, Sander},
  year = {2017},
  month = sep,
  doi = {10.1101/146472},
  abstract = {Vision research has been shaped by the seminal insight that we can understand higher-tier visual cortex from the perspective of multiple functional pathways with different goals. In this paper we try to give a computational account of the functional organization of this system by reasoning from the perspective of multi-task deep neural networks. Machine learning has shown that tasks become easier to solve when they are decomposed into subtasks with their own cost function. We hypothesise that the visual system optimizes multiple cost functions of unrelated tasks and this causes the emergence of the ventral pathway, dedicated to vision for perception and dorsal pathway, dedicated to vision for action. To evaluate the functional organization in multi-task deep neural networks we propose a method that measures the contribution of a unit towards each task and apply it to two networks that have been trained on either two related or two unrelated tasks using an identical stimulus set. Results show that the network trained on the unrelated tasks shows a decreasing degree of feature representation sharing towards higher-tier layers while the network trained on related tasks uniformly shows high degree of sharing. We conjecture that the method we propose can be used to reason about the anatomical and functional organization of the visual system and beyond as we predict that the degree to which tasks are related is a good descriptor of the degree to which they can share downstream corticalunits.},
  file = {/Users/qualia/Documents/Papers/Scholte et al. - 2017 - Visual pathways from the perspective of cost funct.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Schomburg,
  title = {{{BIOPHYSICAL AND NETWORK MECHANISMS OF HIGH FREQUENCY EXTRACELLULAR POTENTIALS IN THE RAT HIPPOCAMPUS}}},
  author = {Schomburg, Erik W},
  pages = {230},
  file = {/Users/qualia/Documents/Papers/2014 - Unknown - Thesis by.pdf;/Users/qualia/Documents/Papers/Schomburg - BIOPHYSICAL AND NETWORK MECHANISMS OF HIGH FREQUEN.pdf},
  language = {en}
}

@article{Schreckenberger2004,
  title = {The Thalamus as the Generator and Modulator of {{EEG}} Alpha Rhythm: A Combined {{PET}}/{{EEG}} Study with Lorazepam Challenge in Humans},
  shorttitle = {The Thalamus as the Generator and Modulator of {{EEG}} Alpha Rhythm},
  author = {Schreckenberger, Mathias and {Lange-Asschenfeld}, Christian and Lochmann, Matthias and Mann, Klaus and Siessmeier, Thomas and Buchholz, Hans-Georg and Bartenstein, Peter and Gr{\"u}nder, Gerhard},
  year = {2004},
  month = jun,
  volume = {22},
  pages = {637--644},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2004.01.047},
  file = {/Users/qualia/Documents/Papers/2004 - Schreckenberger et al. - The thalamus as the generator and modulator of EEG alpha rhythm A combined PETEEG study with lorazepam c.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Schulman,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  pages = {12},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ``surrogate'' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  file = {/Users/qualia/Documents/Papers/Schulman et al. - Proximal Policy Optimization Algorithms.pdf},
  language = {en}
}

@article{Schulman2015,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2015},
  month = jun,
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD({$\lambda$}). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.},
  archivePrefix = {arXiv},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Schulman et al. - High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf},
  journal = {arXiv:1506.02438 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,Computer Science - Systems and Control},
  language = {en},
  primaryClass = {cs}
}

@article{Schulman2015a,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2015},
  month = feb,
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archivePrefix = {arXiv},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Schulman et al. - Trust Region Policy Optimization.pdf},
  journal = {arXiv:1502.05477 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Schultz1998,
  title = {Predictive {{Reward Signal}} of {{Dopamine Neurons}}},
  author = {Schultz, Wolfram},
  year = {1998},
  month = jul,
  volume = {80},
  pages = {1--27},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.80.1.1},
  file = {/Users/qualia/Documents/Papers/Schultz - 1998 - Predictive Reward Signal of Dopamine Neurons.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Schulz,
  title = {Searching for {{Rewards Like}} a {{Child Means Less Generalization}} and {{More Directed Exploration}}},
  author = {Schulz, Eric and Wu, Charley M and Ruggeri, Azzurra and Meder, Bj{\"o}rn},
  pages = {12},
  abstract = {How do children and adults differ in their search for rewards? We considered three different hypotheses that attribute developmental differences to (a) children's increased random sampling, (b) more directed exploration toward uncertain options, or (c) narrower generalization. Using a search task in which noisy rewards were spatially correlated on a grid, we compared the ability of 55 younger children (ages 7 and 8 years), 55 older children (ages 9\textendash{}11 years), and 50 adults (ages 19\textendash{}55 years) to successfully generalize about unobserved outcomes and balance the exploration\textendash{}exploitation dilemma. Our results show that children explore more eagerly than adults but obtain lower rewards. We built a predictive model of search to disentangle the unique contributions of the three hypotheses of developmental differences and found robust and recoverable parameter estimates indicating that children generalize less and rely on directed exploration more than adults. We did not, however, find reliable differences in terms of random sampling.},
  file = {/Users/qualia/Documents/Papers/Schulz et al. - Searching for Rewards Like a Child Means Less Gene.pdf},
  language = {en}
}

@article{Schulz2006,
  title = {Plasticity and Stability in Neuronal Output via Changes in Intrinsic Excitability: It's What's inside That Counts},
  shorttitle = {Plasticity and Stability in Neuronal Output via Changes in Intrinsic Excitability},
  author = {Schulz, D. J.},
  year = {2006},
  month = dec,
  volume = {209},
  pages = {4821--4827},
  issn = {0022-0949, 1477-9145},
  doi = {10.1242/jeb.02567},
  abstract = {Summary The nervous system faces an extremely difficult task. It must be flexible, both during development and in adult life, so that it can respond to a variety of environmental demands and produce adaptive behavior. At the same time the nervous system must be stable, so that the neural circuits that produce behavior function throughout the lifetime of the animal and that changes produced by learning endure. We are only beginning to understand how neural networks strike a balance between altering individual neurons in the name of plasticity, while maintaining long-term stability in neural system function. The balance of this plasticity and stability in neural networks undoubtedly plays a critical role in the normal functioning of the nervous system. While mechanisms of synaptic plasticity have garnered extensive study over the past three decades, it is only recently that more attention has been turned to plasticity of intrinsic excitability as a key player in neural network function. This review will focus on this emerging area of research that undoubtedly will contribute a great deal to our understanding of the functionality of the nervous system.},
  file = {/Users/qualia/Documents/Papers/2006 - Schulz - Plasticity and stability in neuronal output via changes in intrinsic excitability it's what's inside that counts.pdf},
  journal = {Journal of Experimental Biology},
  language = {en},
  number = {24}
}

@article{Schulz2018,
  title = {Finding Structure in Multi-Armed Bandits},
  author = {Schulz, Eric and Franklin, Nicholas T and Gershman, Samuel J},
  year = {2018},
  month = dec,
  doi = {10.1101/432534},
  abstract = {How do humans search for rewards? This question is commonly studied using multi-armed bandit tasks, which require participants to trade off exploration and exploitation. Standard multi-armed bandits assume that each option has an independent reward distribution. However, learning about options independently is unrealistic, since in the real world options often share an underlying structure. We introduce a class of structured bandit tasks, which we use to probe how generalization guides exploration. In a structured multi-armed bandit, options have a correlation structure dictated by a latent function. We focus on bandits in which rewards are linear functions of an option's spatial position. Across 5 experiments, we find evidence that participants utilize functional structure to guide their exploration, and also exhibit a learning-to-learn effect across rounds, becoming progressively faster at identifying the latent function. The experiments rule out several heuristic explanations, and show that the same findings obtain with non-linear functions. Comparing several models of learning and decision making, we find that the best model of human behavior in our tasks combines three computational mechanisms: (1) function learning, (2) clustering of reward distributions across rounds, and (3) uncertainty-guided exploration. Our results suggest that human reinforcement learning can utilize latent structure in sophisticated ways to improve efficiency.},
  file = {/Users/qualia/Documents/Papers/Schulz et al. - 2018 - Finding structure in multi-armed bandits.pdf},
  journal = {bioRxiv},
  language = {en}
}

@techreport{Schulz2018a,
  title = {Exploration in the Wild},
  author = {Schulz, Eric and Bhui, Rahul and Love, Bradley C and Brier, Bastien and Todd, Michael T and Gershman, Samuel J},
  year = {2018},
  month = dec,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/492058},
  abstract = {Making good decisions requires people to appropriately explore their available options and generalize what they have learned. While computational models have successfully explained exploratory behavior in constrained laboratory tasks, it is unclear to what extent these models generalize to complex real world choice problems. We investigate the factors guiding exploratory behavior in a data set consisting of 195,333 customers placing 1,613,967 orders from a large online food delivery service. We find important hallmarks of adaptive exploration and generalization, which we analyze using computational models. We find evidence for several theoretical predictions: (1) customers engage in uncertainty-directed exploration, (2) they adjust their level of exploration to the average restaurant quality in a city, and (3) they use feature-based generalization to guide exploration towards promising restaurants. Our results provide new evidence that people use sophisticated strategies to explore complex, real-world environments.},
  file = {/Users/qualia/Documents/Papers/Schulz et al. - 2018 - Exploration in the wild.pdf},
  language = {en},
  type = {Preprint}
}

@article{Schulze2018,
  title = {Active {{Reinforcement Learning}} with {{Monte}}-{{Carlo Tree Search}}},
  author = {Schulze, Sebastian and Evans, Owain},
  year = {2018},
  month = mar,
  abstract = {Active Reinforcement Learning (ARL) is a twist on RL where the agent observes reward information only if it pays a cost. This subtle change makes exploration substantially more challenging. Powerful principles in RL like optimism, Thompson sampling, and random exploration do not help with ARL. We relate ARL in tabular environments to BayesAdaptive MDPs. We provide an ARL algorithm using Monte-Carlo Tree Search that is asymptotically Bayes optimal. Experimentally, this algorithm is near-optimal on small Bandit problems and MDPs. On larger MDPs it outperforms a Q-learner augmented with specialised heuristics for ARL. By analysing exploration behaviour in detail, we uncover obstacles to scaling up simulation-based algorithms for ARL.},
  archivePrefix = {arXiv},
  eprint = {1803.04926},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Schulze and Evans - 2018 - Active Reinforcement Learning with Monte-Carlo Tre.pdf},
  journal = {arXiv:1803.04926 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Schuman2019,
  title = {Four {{Unique Interneuron Populations Reside}} in {{Neocortical Layer}} 1},
  author = {Schuman, Benjamin and Machold, Robert P. and Hashikawa, Yoshiko and Fuzik, J{\'a}nos and Fishell, Gord J. and Rudy, Bernardo},
  year = {2019},
  month = jan,
  volume = {39},
  pages = {125--139},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1613-18.2018},
  file = {/Users/qualia/Documents/Papers/Schuman et al. - 2019 - Four Unique Interneuron Populations Reside in Neoc.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {1}
}

@article{Schurger2010,
  title = {Reproducibility {{Distinguishes Conscious}} from {{Nonconscious Neural Representations}}},
  author = {Schurger, A. and Pereira, F. and Treisman, A. and Cohen, J. D.},
  year = {2010},
  month = jan,
  volume = {327},
  pages = {97--99},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1180029},
  file = {/Users/qualia/Documents/Papers/2010 - Schurger et al. - Reproducibility distinguishes conscious from nonconscious neural representations.pdf},
  journal = {Science},
  language = {en},
  number = {5961}
}

@article{Schwabe2006,
  title = {The {{Role}} of {{Feedback}} in {{Shaping}} the {{Extra}}-{{Classical Receptive Field}} of {{Cortical Neurons}}: {{A Recurrent Network Model}}},
  shorttitle = {The {{Role}} of {{Feedback}} in {{Shaping}} the {{Extra}}-{{Classical Receptive Field}} of {{Cortical Neurons}}},
  author = {Schwabe, L.},
  year = {2006},
  month = sep,
  volume = {26},
  pages = {9117--9129},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1253-06.2006},
  file = {/Users/qualia/Documents/Papers/2006 - Schwabe et al. - The role of feedback in shaping the extra-classical receptive field of cortical neurons a recurrent network mode.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {36}
}

@article{Schwartenbeck2019,
  title = {Computational Mechanisms of Curiosity and Goal-Directed Exploration},
  author = {Schwartenbeck, Philipp and Passecker, Johannes and Hauser, Tobias U and FitzGerald, Thomas HB and Kronbichler, Martin and Friston, Karl J},
  year = {2019},
  pages = {45},
  file = {/Users/qualia/Documents/Papers/Schwartenbeck et al. - Computational mechanisms of curiosity and goal-dir.pdf},
  journal = {eLife},
  language = {en},
  number = {e41703}
}

@article{Schwartz-Ziv,
  title = {Opening the Black Box of {{Deep Neural Networks}} via {{Information}}},
  author = {{Schwartz-Ziv}, Ravid and Tishby, Naftali},
  pages = {19},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work [Tishby and Zaslavsky (2015)] proposed to analyze DNNs in the Information Plane; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer.},
  file = {/Users/qualia/Documents/Papers/Schwartz-Ziv and Tishby - Opening the black box of Deep Neural Networks via .pdf},
  language = {en}
}

@article{Schwemmer2015,
  title = {Constructing {{Precisely Computing Networks}} with {{Biophysical Spiking Neurons}}},
  author = {Schwemmer, M. A. and Fairhall, A. L. and Deneve, S. and {Shea-Brown}, E. T.},
  year = {2015},
  month = jul,
  volume = {35},
  pages = {10112--10134},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4951-14.2015},
  file = {/Users/qualia/Documents/Papers/Schwemmer et al. - 2015 - Constructing Precisely Computing Networks with Bio.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {28}
}

@article{Scott2014,
  title = {Voltage {{Imaging}} of {{Waking Mouse Cortex Reveals Emergence}} of {{Critical Neuronal Dynamics}}},
  author = {Scott, G. and Fagerholm, E. D. and Mutoh, H. and Leech, R. and Sharp, D. J. and Shew, W. L. and Knopfel, T.},
  year = {2014},
  month = dec,
  volume = {34},
  pages = {16611--16620},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3474-14.2014},
  file = {/Users/qualia/Documents/Papers/Scott et al. - 2014 - Voltage Imaging of Waking Mouse Cortex Reveals Eme.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {50}
}

@article{Searcy,
  title = {Bird {{Song}} and the {{Problem}} of {{Honest Communication}}},
  author = {Searcy, William A and Nowicki, Stephen},
  pages = {9},
  file = {/Users/qualia/Documents/Papers/Searcy and Nowicki - Bird Song and the Problem of Honest Communication.pdf},
  language = {en}
}

@article{Sebastian-Gonzalez2019,
  title = {The Extent, Frequency and Ecological Functions of Food Wasting by Parrots},
  author = {{Sebasti{\'a}n-Gonz{\'a}lez}, Esther and Hiraldo, Fernando and Blanco, Guillermo and {Hern{\'a}ndez-Brito}, Dailos and {Romero-Vidal}, Pedro and Carrete, Martina and {G{\'o}mez-Llanos}, Eduardo and Pac{\'i}fico, Erica C. and {D{\'i}az-Luque}, Jos{\'e} A. and D{\'e}nes, Francisco V. and Tella, Jos{\'e} L.},
  year = {2019},
  month = dec,
  volume = {9},
  pages = {15280},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-51430-3},
  file = {/Users/qualia/Documents/Papers/Sebastián-González et al. - 2019 - The extent, frequency and ecological functions of .pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Sedigh-Sarvestani2012,
  title = {Reconstructing {{Mammalian Sleep Dynamics}} with {{Data Assimilation}}},
  author = {{Sedigh-Sarvestani}, Madineh and Schiff, Steven J. and Gluckman, Bruce J.},
  editor = {Gutkin, Boris S.},
  year = {2012},
  month = nov,
  volume = {8},
  pages = {e1002788},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1002788},
  abstract = {Data assimilation is a valuable tool in the study of any complex system, where measurements are incomplete, uncertain, or both. It enables the user to take advantage of all available information including experimental measurements and shortterm model forecasts of a system. Although data assimilation has been used to study other biological systems, the study of the sleep-wake regulatory network has yet to benefit from this toolset. We present a data assimilation framework based on the unscented Kalman filter (UKF) for combining sparse measurements together with a relatively high-dimensional nonlinear computational model to estimate the state of a model of the sleep-wake regulatory system. We demonstrate with simulation studies that a few noisy variables can be used to accurately reconstruct the remaining hidden variables. We introduce a metric for ranking relative partial observability of computational models, within the UKF framework, that allows us to choose the optimal variables for measurement and also provides a methodology for optimizing framework parameters such as UKF covariance inflation. In addition, we demonstrate a parameter estimation method that allows us to track nonstationary model parameters and accommodate slow dynamics not included in the UKF filter model. Finally, we show that we can even use observed discretized sleep-state, which is not one of the model variables, to reconstruct model state and estimate unknown parameters. Sleep is implicated in many neurological disorders from epilepsy to schizophrenia, but simultaneous observation of the many brain components that regulate this behavior is difficult. We anticipate that this data assimilation framework will enable better understanding of the detailed interactions governing sleep and wake behavior and provide for better, more targeted, therapies.},
  file = {/Users/qualia/Documents/Papers/2012 - Sedigh-Sarvestani, Schiff, Gluckman - Reconstructing Mammalian Sleep Dynamics with Data Assimilation.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {11}
}

@article{Seeds2014,
  title = {A Suppression Hierarchy among Competing Motor Programs Drives Sequential Grooming in {{Drosophila}}},
  author = {Seeds, Andrew M and Ravbar, Primoz and Chung, Phuong and Hampel, Stefanie and Midgley, Frank M and Mensh, Brett D and Simpson, Julie H},
  year = {2014},
  month = aug,
  volume = {3},
  pages = {e02951},
  issn = {2050-084X},
  doi = {10.7554/eLife.02951},
  abstract = {Motor sequences are formed through the serial execution of different movements, but how nervous systems implement this process remains largely unknown. We determined the organizational principles governing how dirty fruit flies groom their bodies with sequential movements. Using genetically targeted activation of neural subsets, we drove distinct motor programs that clean individual body parts. This enabled competition experiments revealing that the motor programs are organized into a suppression hierarchy; motor programs that occur first suppress those that occur later. Cleaning one body part reduces the sensory drive to its motor program, which relieves suppression of the next movement, allowing the grooming sequence to progress down the hierarchy. A model featuring independently evoked cleaning movements activated in parallel, but selected serially through hierarchical suppression, was successful in reproducing the grooming sequence. This provides the first example of an innate motor sequence implemented by the prevailing model for generating human action sequences.
          , 
            Anyone who has ever lived with a cat is familiar with its grooming behavior. This innate behavior follows a particular sequence as the cat methodically cleans its body parts one-by-one. Many animals also have grooming habits, even insects such as fruit flies. The fact that grooming sequences are seen across such different species suggests that this behavior is important for survival. Nevertheless, how the brain organizes grooming sequences, or other behaviors that involve a sequence of tasks, is not well understood.
            Fruit flies make a good model for studying grooming behavior for a couple of reasons. First, they are fastidious cleaners. When coated with dust they will faithfully carry out a series of cleaning tasks to clean each body part. Second, there are many genetic tools and techniques that researchers can use to manipulate the fruit flies' behaviors. One technique allows specific brain cells to be targeted and activated to trigger particular behaviors.
            Seeds et al. used these sophisticated techniques, computer modeling, and behavioral observations to uncover how the brains of fruit flies orchestrate a grooming sequence. Dust-covered flies follow a predictable sequence of cleaning tasks: beginning by using their front legs to clean their eyes, they then clean their antennae and head. This likely helps to protect their sensory organs. Next, they move on to the abdomen, possibly to ensure that dust doesn't interfere with their ability to breathe. Wings and thorax follow last. Periodically, the flies stop to rub their legs together to remove any accumulated dust before resuming the cleaning sequence.
            Seeds et al. activated different sets of brain cells one-by-one to see if they could trigger a particular grooming task and found that individual cleaning tasks could be triggered, in the absence of dust, by stimulating a specific group of brain cells. This suggests each cleaning task is a discrete behavior controlled by a subset of cells. Then Seeds et al. tried to stimulate more than one cleaning behavior at a time; they discovered that wing-cleaning suppressed thorax-cleaning, abdomen-cleaning suppressed both of these, and head-cleaning suppressed all the others. This suggests that a `hierarchy' exists in the brain that exactly matches the sequence that flies normally follow as they clean their body parts.
            By learning more about how the brain coordinates grooming sequences, the findings of Seeds et al. may also provide insights into other behaviors that involve a sequence of tasks, such as nest building in animals or typing in humans. Following on from this work, one of the next challenges will be to see if such behaviors also use a `suppression hierarchy' to ensure that individual tasks are carried out in the right order.},
  file = {/Users/qualia/Documents/Papers/Seeds et al. - 2014 - A suppression hierarchy among competing motor prog.pdf},
  journal = {eLife},
  language = {en}
}

@article{Sehnke2010,
  title = {Parameter-Exploring Policy Gradients},
  author = {Sehnke, Frank and Osendorfer, Christian and R{\"u}ckstie{\ss}, Thomas and Graves, Alex and Peters, Jan and Schmidhuber, J{\"u}rgen},
  year = {2010},
  month = may,
  volume = {23},
  pages = {551--559},
  issn = {08936080},
  doi = {10.1016/j.neunet.2009.12.004},
  abstract = {We present a model-free reinforcement learning method for partially observable Markov decision problems. Our method estimates a likelihood gradient by sampling directly in parameter space, which leads to lower variance gradient estimates than obtained by regular policy gradient methods. We show that for several complex control tasks, including robust standing with a humanoid robot, this method outperforms well-known algorithms from the fields of standard policy gradients, finite difference methods and population based heuristics. We also show that the improvement is largest when the parameter samples are drawn symmetrically. Lastly we analyse the importance of the individual components of our method by incrementally incorporating them into the other algorithms, and measuring the gain in performance after each step.},
  file = {/Users/qualia/Documents/Papers/Sehnke et al. - 2010 - Parameter-exploring policy gradients.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4}
}

@article{Seldin,
  title = {{{PAC}}-{{Bayesian Analysis}} of {{Contextual Bandits}}},
  author = {Seldin, Yevgeny and Auer, Peter and {Shawe-taylor}, John S and Ortner, Ronald and Laviolette, Fran{\c c}ois},
  pages = {9},
  abstract = {We derive an instantaneous (per-round) data-dependent regret bound for stochastic multiarmed bandits with side information (also known as contextual bandits). The scaling of our regret bound with the number of states (contexts) N goes as pN I{$\dashrightarrow$} (S; A), where I{$\dashrightarrow$} (S; A) is the mutual information between states and act t tions (the side information) used by the algorithm at roupnd t. If the algorithm uses all the side information, the regret bound scales as N ln K, where K is the number of actions (arms). However, if the side information I{$\dashrightarrow$} (S; A) is not t fully used, the regret bound is significantly tighter. In the extreme case, when I{$\dashrightarrow$} (S; A) = 0, the dependence on the number of states reduces from linear to t logarithmic. Our analysis allows to provide the algorithm large amount of side information, let the algorithm to decide which side information is relevant for the task, and penalize the algorithm only for the side information that it is using de facto. We also present an algorithm for multiarmed bandits with side information with O(K) computational complexity per game round.},
  file = {/Users/qualia/Documents/Papers/Seldin et al. - PAC-Bayesian Analysis of Contextual Bandits.pdf},
  language = {en}
}

@article{Serences2004,
  title = {A Comparison of Methods for Characterizing the Event-Related {{BOLD}} Timeseries in Rapid {{fMRI}}},
  author = {Serences, John T.},
  year = {2004},
  month = apr,
  volume = {21},
  pages = {1690--1700},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2003.12.021},
  file = {/Users/qualia/Documents/Papers/2004 - Serences - A comparison of methods for characterizing the event-related BOLD timeseries in rapid fMRI.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Serences2009,
  title = {Stimulus-{{Specific Delay Activity}} in {{Human Primary Visual Cortex}}},
  author = {Serences, John T. and Ester, Edward F. and Vogel, Edward K. and Awh, Edward},
  year = {2009},
  month = feb,
  volume = {20},
  pages = {207--214},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2009.02276.x},
  abstract = {Working memory (WM) involves maintaining information in an on-line state. One emerging view is that information in WM is maintained via sensory recruitment, such that information is stored via sustained activity in the sensory areas that encode the to-be-remembered information. Using functional magnetic resonance imaging, we observed that key sensory regions such as primary visual cortex (V1) showed little evidence of sustained increases in mean activation during a WM delay period, though such amplitude increases have typically been used to determine whether a region is involved in on-line maintenance. However, a multivoxel pattern analysis of delay-period activity revealed a sustained pattern of activation in V1 that represented only the intentionally stored feature of a multifeature object. Moreover, the pattern of delay activity was qualitatively similar to that observed during the discrimination of sensory stimuli, suggesting that WM representations in V1 are reasonable ``copies'' of those evoked during pure sensory processing.},
  file = {/Users/qualia/Documents/Papers/2009 - Serences et al. - Stimulus-specific delay activity in human primary visual cortex.pdf},
  journal = {Psychological Science},
  language = {en},
  number = {2}
}

@article{Serences2011,
  title = {Mechanisms of {{Selective Attention}}: {{Response Enhancement}}, {{Noise Reduction}}, and {{Efficient Pooling}} of {{Sensory Responses}}},
  shorttitle = {Mechanisms of {{Selective Attention}}},
  author = {Serences, John T.},
  year = {2011},
  month = dec,
  volume = {72},
  pages = {685--687},
  issn = {08966273},
  doi = {10.1016/j.neuron.2011.11.005},
  file = {/Users/qualia/Documents/Papers/2011 - Serences - Mechanisms of selective attention Response enhancement, noise reduction, and efficient pooling of sensory responses.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Serrano2013,
  title = {Gain {{Control Network Conditions}} in {{Early Sensory Coding}}},
  author = {Serrano, Eduardo and Nowotny, Thomas and Levi, Rafael and Smith, Brian H. and Huerta, Ram{\'o}n},
  editor = {Sporns, Olaf},
  year = {2013},
  month = jul,
  volume = {9},
  pages = {e1003133},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003133},
  abstract = {Gain control is essential for the proper function of any sensory system. However, the precise mechanisms for achieving effective gain control in the brain are unknown. Based on our understanding of the existence and strength of connections in the insect olfactory system, we analyze the conditions that lead to controlled gain in a randomly connected network of excitatory and inhibitory neurons. We consider two scenarios for the variation of input into the system. In the first case, the intensity of the sensory input controls the input currents to a fixed proportion of neurons of the excitatory and inhibitory populations. In the second case, increasing intensity of the sensory stimulus will both, recruit an increasing number of neurons that receive input and change the input current that they receive. Using a mean field approximation for the network activity we derive relationships between the parameters of the network that ensure that the overall level of activity of the excitatory population remains unchanged for increasing intensity of the external stimulation. We find that, first, the main parameters that regulate network gain are the probabilities of connections from the inhibitory population to the excitatory population and of the connections within the inhibitory population. Second, we show that strict gain control is not achievable in a random network in the second case, when the input recruits an increasing number of neurons. Finally, we confirm that the gain control conditions derived from the mean field approximation are valid in simulations of firing rate models and Hodgkin-Huxley conductance based models.},
  file = {/Users/qualia/Documents/Papers/2013 - Serrano et al. - Gain Control Network Conditions in Early Sensory Coding.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {7}
}

@article{Seung2003,
  title = {Learning in {{Spiking Neural Networks}} by {{Reinforcement}} of {{Stochastic Synaptic Transmission}}},
  author = {Seung, H.Sebastian},
  year = {2003},
  month = dec,
  volume = {40},
  pages = {1063--1073},
  issn = {08966273},
  doi = {10.1016/S0896-6273(03)00761-X},
  abstract = {It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are ``hedonistic,'' responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-andfire model neurons.},
  file = {/Users/qualia/Documents/Papers/2003 - Seung - Learning in Spiking Neural Networks by Reinforcement of Stochastics Transmission.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Shafto2011,
  title = {A Probabilistic Model of Cross-Categorization},
  author = {Shafto, Patrick and Kemp, Charles and Mansinghka, Vikash and Tenenbaum, Joshua B.},
  year = {2011},
  month = jul,
  volume = {120},
  pages = {1--25},
  issn = {00100277},
  doi = {10.1016/j.cognition.2011.02.010},
  abstract = {Most natural domains can be represented in multiple ways: we can categorize foods in terms of their nutritional content or social role, animals in terms of their taxonomic groupings or their ecological niches, and musical instruments in terms of their taxonomic categories or social uses. Previous approaches to modeling human categorization have largely ignored the problem of cross-categorization, focusing on learning just a single system of categories that explains all of the features. Cross-categorization presents a difficult problem: how can we infer categories without first knowing which features the categories are meant to explain? We present a novel model that suggests that human cross-categorization is a result of joint inference about multiple systems of categories and the features that they explain. We also formalize two commonly proposed alternative explanations for cross-categorization behavior: a features-first and an objects-first approach. The features-first approach suggests that cross-categorization is a consequence of attentional processes, where features are selected by an attentional mechanism first and categories are derived second. The objects-first approach suggests that cross-categorization is a consequence of repeated, sequential attempts to explain features, where categories are derived first, then features that are poorly explained are recategorized. We present two sets of simulations and experiments testing the models' predictions about human categorization. We find that an approach based on joint inference provides the best fit to human categorization behavior, and we suggest that a full account of human category learning will need to incorporate something akin to these capabilities.},
  file = {/Users/qualia/Documents/Papers/2011 - Shafto et al. - A probabilistic model of cross-categorization.pdf},
  journal = {Cognition},
  language = {en},
  number = {1}
}

@article{Shannon1948,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, Claude},
  year = {1948},
  volume = {27},
  pages = {379--423, 623--656,},
  file = {/Users/qualia/Documents/Papers/1948 - Shannon - A Mathematical Theory of Communication.pdf},
  journal = {The Bell System Technical Journal}
}

@techreport{Sharpe2019,
  title = {Dopamine Transients Delivered in Learning Contexts Do Not Act as Model-Free Prediction Errors},
  author = {Sharpe, Melissa J. and Batchelor, Hannah M. and Mueller, Lauren E. and Chang, Chun Yun and Maes, Etienne J.P. and Niv, Yael and Schoenbaum, Geoffrey},
  year = {2019},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/574541},
  abstract = {Dopamine neurons fire transiently in response to unexpected rewards. These neural correlates are proposed to signal the reward prediction error described in model-free reinforcement learning algorithms. This error term represents the unpredicted or `excess' value of the rewarding event. In model-free reinforcement learning, this value is then stored as part of the learned value of any antecedent cues, contexts or events, making them intrinsically valuable, independent of the specific rewarding event that caused the prediction error. In support of equivalence between dopamine transients and this model-free error term, proponents cite causal optogenetic studies showing that artificially induced dopamine transients cause lasting changes in behavior. Yet none of these studies directly demonstrate the presence of cached value under conditions appropriate for associative learning. To address this gap in our knowledge, we conducted three studies where we optogenetically activated dopamine neurons while rats were learning associative relationships, both with and without reward. In each experiment, the antecedent cues failed to acquired value and instead entered into value-independent associative relationships with the other cues or rewards. These results show that dopamine transients, constrained within appropriate learning situations, support valueless associative learning.},
  file = {/Users/qualia/Documents/Papers/Sharpe et al. - 2019 - Dopamine transients delivered in learning contexts.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Sharpe2019a,
  title = {Dopamine Transients Delivered in Learning Contexts Do Not Act as Model-Free Prediction Errors},
  author = {Sharpe, Melissa J. and Batchelor, Hannah M. and Mueller, Lauren E. and Chang, Chun Yun and Maes, Etienne J.P. and Niv, Yael and Schoenbaum, Geoffrey},
  year = {2019},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/574541},
  abstract = {Dopamine neurons fire transiently in response to unexpected rewards. These neural correlates are proposed to signal the reward prediction error described in model-free reinforcement learning algorithms. This error term represents the unpredicted or `excess' value of the rewarding event. In model-free reinforcement learning, this value is then stored as part of the learned value of any antecedent cues, contexts or events, making them intrinsically valuable, independent of the specific rewarding event that caused the prediction error. In support of equivalence between dopamine transients and this model-free error term, proponents cite causal optogenetic studies showing that artificially induced dopamine transients cause lasting changes in behavior. Yet none of these studies directly demonstrate the presence of cached value under conditions appropriate for associative learning. To address this gap in our knowledge, we conducted three studies where we optogenetically activated dopamine neurons while rats were learning associative relationships, both with and without reward. In each experiment, the antecedent cues failed to acquired value and instead entered into value-independent associative relationships with the other cues or rewards. These results show that dopamine transients, constrained within appropriate learning situations, support valueless associative learning.},
  file = {/Users/qualia/Documents/Papers/Sharpe et al. - 2019 - Dopamine transients delivered in learning contexts 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Sharpe2020,
  title = {Dopamine Transients Do Not Act as Model-Free Prediction Errors during Associative Learning},
  author = {Sharpe, Melissa J. and Batchelor, Hannah M. and Mueller, Lauren E. and Yun Chang, Chun and Maes, Etienne J. P. and Niv, Yael and Schoenbaum, Geoffrey},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {106},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13953-1},
  file = {/Users/qualia/Documents/Papers/Sharpe et al. - 2020 - Dopamine transients do not act as model-free predi.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Sharpee2014,
  title = {Toward {{Functional Classification}} of {{Neuronal Types}}},
  author = {Sharpee, Tatyana O.},
  year = {2014},
  month = sep,
  volume = {83},
  pages = {1329--1334},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.08.040},
  file = {/Users/qualia/Documents/Papers/Sharpee - 2014 - Toward Functional Classification of Neuronal Types.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Shattuck2008,
  title = {Construction of a {{3D}} Probabilistic Atlas of Human Cortical Structures},
  author = {Shattuck, David W. and Mirza, Mubeena and Adisetiyo, Vitria and Hojatkashani, Cornelius and Salamon, Georges and Narr, Katherine L. and Poldrack, Russell A. and Bilder, Robert M. and Toga, Arthur W.},
  year = {2008},
  month = feb,
  volume = {39},
  pages = {1064--1080},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2007.09.031},
  file = {/Users/qualia/Documents/Papers/2008 - Shattuck et al. - Construction of a 3D probabilistic atlas of human cortical structures.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Shazeer2017,
  title = {{{OUTRAGEOUSLY LARGE NEURAL NETWORKS}}: {{THE SPARSELY}}-{{GATED MIXTURE}}-{{OF}}-{{EXPERTS LAYER}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Dean, Jeff},
  year = {2017},
  pages = {19},
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  file = {/Users/qualia/Documents/Papers/Shazeer et al. - 2017 - OUTRAGEOUSLY LARGE NEURAL NETWORKS THE SPARSELY-G.pdf},
  language = {en}
}

@article{Shepherd1987,
  title = {Logic Operations Are Properties of Computer-Simulated Interactions between Excitable Dendritic Spines},
  author = {Shepherd, Gordon M. and Brayton, Robert K.},
  year = {1987},
  month = apr,
  volume = {21},
  pages = {151--165},
  issn = {03064522},
  doi = {10.1016/0306-4522(87)90329-0},
  abstract = {Neurons in the central nervous system of mammals and many other species receive most of their synaptic inputs in their dendritic branches and spines, but the precise manner in which this information is processed in the dendrites is not understood. In order to gain insight into these mechanisms, simulations of interactions between distal dendritic spines with an excitable membrane have been carried out, using an electrical circuit analysis program for the compartmental representation of a dendrite and several spines. Interactions between responses to single and paired excitatory and inhibitory synaptic inputs have been analyzed. Basic logic operations, including AND gates, OR gates and ANDNOT gates, arise from these interactions.},
  file = {/Users/qualia/Documents/Papers/1987 - Shepherd, Brayton - M. and.pdf;/Users/qualia/Documents/Papers/Shepherd and Brayton - 1987 - Logic operations are properties of computer-simula.pdf},
  journal = {Neuroscience},
  language = {en},
  number = {1}
}

@article{Sheremet2016,
  title = {Movement {{Enhances}} the {{Nonlinearity}} of {{Hippocampal Theta}}},
  author = {Sheremet, A. and Burke, S. N. and Maurer, A. P.},
  year = {2016},
  month = apr,
  volume = {36},
  pages = {4218--4230},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3564-15.2016},
  file = {/Users/qualia/Documents/Papers/Sheremet et al. - 2016 - Movement Enhances the Nonlinearity of Hippocampal .pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {15}
}

@article{Sherman1998,
  title = {On the Actions That One Nerve Cell Can Have on Another: {{Distinguishing}} "Drivers" from "Modulators"},
  shorttitle = {On the Actions That One Nerve Cell Can Have on Another},
  author = {Sherman, S. M. and Guillery, R. W.},
  year = {1998},
  month = jun,
  volume = {95},
  pages = {7121--7126},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.95.12.7121},
  abstract = {When one nerve cell acts on another, its postsynaptic effect can vary greatly. In sensory systems, inputs from ``drivers'' can be differentiated from those of ``modulators.'' The driver can be identified as the transmitter of receptive field properties; the modulator can be identified as altering the probability of certain aspects of that transmission. Where receptive fields are not available, the distinction is more difficult and currently is undefined. We use the visual pathways, particularly the thalamic geniculate relay for which much relevant evidence is available, to explore ways in which drivers can be distinguished from modulators. The extent to which the distinction may apply first to other parts of the thalamus and then, possibly, to other parts of the brain is considered. We suggest the following distinctions: Crosscorrelograms from driver inputs have sharper peaks than those from modulators; there are likely to be few drivers but many modulators for any one cell; and drivers are likely to act only through ionotropic receptors having a fast postsynaptic effect whereas modulators also are likely to activate metabotropic receptors having a slow and prolonged postsynaptic effect.},
  file = {/Users/qualia/Documents/Papers/1998 - Sherman, Guillery - On the actions that one nerve cell can have on another distinguishing drivers from modulators.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {12}
}

@article{Sherman2016,
  title = {Neural Mechanisms of Transient Neocortical Beta Rhythms: {{Converging}} Evidence from Humans, Computational Modeling, Monkeys, and Mice},
  shorttitle = {Neural Mechanisms of Transient Neocortical Beta Rhythms},
  author = {Sherman, Maxwell A. and Lee, Shane and Law, Robert and Haegens, Saskia and Thorn, Catherine A. and H{\"a}m{\"a}l{\"a}inen, Matti S. and Moore, Christopher I. and Jones, Stephanie R.},
  year = {2016},
  month = aug,
  volume = {113},
  pages = {E4885-E4894},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1604135113},
  file = {/Users/qualia/Documents/Papers/Sherman et al. - 2016 - Neural mechanisms of transient neocortical beta rh.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {33}
}

@article{Shimazaki,
  title = {Neurons as an {{Information}}-Theoretic {{Engine}}},
  author = {Shimazaki, Hideaki},
  pages = {16},
  abstract = {We show that dynamical gain modulation of neurons' stimulus response is described as an information-theoretic cycle that generates entropy associated with the stimulus-related activity from entropy produced by the modulation. To articulate this theory, we describe stimulus-evoked activity of a neural population based on the maximum entropy principle with constraints on two types of overlapping activities, one that is controlled by stimulus conditions and the other, termed internal activity, that is regulated internally in an organism. We demonstrate that modulation of the internal activity realises gain control of stimulus response, and controls stimulus information. A cycle of neural dynamics is then introduced to model information processing by the neurons during which the stimulus information is dynamically enhanced by the internal gain-modulation mechanism. Based on the conservation law for entropy production, we demonstrate that the cycle generates entropy ascribed to the stimulus-related activity using entropy supplied by the internal mechanism, analogously to a heat engine that produces work from heat. We provide an efficient cycle that achieves the highest entropic efficiency to retain the stimulus information. The theory allows us to quantify efficiency of the internal computation and its theoretical limit.},
  file = {/Users/qualia/Documents/Papers/2015 - Shimazaki - Neurons as an Information-theoretic Engine.pdf;/Users/qualia/Documents/Papers/Shimazaki - Neurons as an Information-theoretic Engine.pdf},
  language = {en}
}

@article{Shin2017,
  title = {The Rate of Transient Beta Frequency Events Predicts Behavior across Tasks and Species},
  author = {Shin, Hyeyoung and Law, Robert and Tsutsui, Shawn and Moore, Christopher I and Jones, Stephanie R},
  year = {2017},
  month = nov,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.29086},
  file = {/Users/qualia/Documents/Papers/Shin et al. - 2017 - The rate of transient beta frequency events predic.pdf},
  journal = {eLife},
  language = {en}
}

@article{Shinkareva2012,
  title = {Exploring Commonalities across Participants in the Neural Representation of Objects},
  author = {Shinkareva, Svetlana V. and Malave, Vicente L. and Just, Marcel Adam and Mitchell, Tom M.},
  year = {2012},
  month = jun,
  volume = {33},
  pages = {1375--1383},
  issn = {10659471},
  doi = {10.1002/hbm.21296},
  abstract = {The question of whether the neural encodings of objects are similar across different people is one of the key questions in cognitive neuroscience. This article examines the commonalities in the internal representation of objects, as measured with fMRI, across individuals in two complementary ways. First, we examine the commonalities in the internal representation of objects across people at the level of interobject distances, derived from whole brain fMRI data, and second, at the level of spatially localized anatomical brain regions that contain sufficient information for identification of object categories, without making the assumption that their voxel patterns are spatially matched in a common space. We examine the commonalities in internal representation of objects on 3T fMRI data collected while participants viewed line drawings depicting various tools and dwellings. This exploratory study revealed the extent to which the representation of individual concepts, and their mutual similarity, is shared across participants. Hum Brain Mapp 33:1375\textendash{}1383, 2012. VC 2011 Wiley Periodicals, Inc.},
  file = {/Users/qualia/Documents/Papers/2012 - Shinkareva et al. - Exploring commonalities across participants in the neural representation of objects.pdf},
  journal = {Human Brain Mapping},
  language = {en},
  number = {6}
}

@article{Shmuel2007,
  title = {Spatio-Temporal Point-Spread Function of {{fMRI}} Signal in Human Gray Matter at 7 {{Tesla}}},
  author = {Shmuel, Amir and Yacoub, Essa and Chaimow, Denis and Logothetis, Nikos K. and Ugurbil, Kamil},
  year = {2007},
  month = apr,
  volume = {35},
  pages = {539--552},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.12.030},
  file = {/Users/qualia/Documents/Papers/2007 - Shmuel et al. - Spatio-temporal point-spread function of fMRI signal in human gray matter at 7 Tesla.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Shoham,
  title = {Multi-{{Agent Reinforcement Learning}}: A Critical Survey},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  pages = {13},
  abstract = {We survey the recent work in AI on multi-agent reinforcement learning (that is, learning in stochastic games). We then argue that, while exciting, this work is flawed. The fundamental flaw is unclarity about the problem or problems being addressed. After tracing a representative sample of the recent literature, we identify four well-defined problems in multi-agent reinforcement learning, single out the problem that in our view is most suitable for AI, and make some remarks about how we believe progress is to be made on this problem.},
  file = {/Users/qualia/Documents/Papers/2003 - Shoham, Powers, Grenager - Multi-agent reinforcement learning a critical survey.pdf},
  language = {en}
}

@article{Shoham2007,
  title = {If Multi-Agent Learning Is the Answer, What Is the Question?},
  author = {Shoham, Yoav and Powers, Rob and Grenager, Trond},
  year = {2007},
  month = may,
  volume = {171},
  pages = {365--377},
  issn = {00043702},
  doi = {10.1016/j.artint.2006.02.006},
  file = {/Users/qualia/Documents/Papers/2007 - Shoham, Powers, Grenager - If multi-agent learning is the answer, what is the question.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {7}
}

@article{Shouval2002,
  title = {Converging Evidence for a Simplified Biophysical Model of Synaptic Plasticity},
  author = {Shouval, Harel Z. and Castellani, Gastone C. and Blais, Brian S. and Yeung, Luk C. and Cooper, Leon N},
  year = {2002},
  month = dec,
  volume = {87},
  pages = {383--391},
  issn = {03401200},
  doi = {10.1007/s00422-002-0362-x},
  abstract = {Different mechanisms that could form the molecular basis for bi-directional synaptic plasticity have been identified experimentally and corresponding biophysical models can be constructed. However, such models are complex and therefore it is hard to deduce their consequences to compare them to existing abstract models of synaptic plasticity. In this paper we examine two such models: a phenomenological one inspired by the phenomena of AMPA receptor insertion, and a more complex biophysical model based on the phenomena of AMPA receptor phosphorylation. We show that under certain approximations both these models can be mapped on to an equivalent, calcium-dependent, differential equation. Intracellular calcium concentration varies locally in each postsynaptic compartment, thus the plasticity rule we extract is a single-synapse rule. We convert this single synapse plasticity equation to a multisynapse rule by incorporating a model of the NMDA receptor. Finally we suggest a mathematical embodiment of metaplasticity, which is consistent with observations on NMDA receptor properties and dependence on cellular activity. These results, in combination with some of our previous results, produce converging evidence for the calcium control hypothesis including a dependence of synaptic plasticity on the level of intercellular calcium as well as on the temporal pattern of calcium transients.},
  file = {/Users/qualia/Documents/Papers/2002 - Shouval et al. - Converging evidence for a simplified biophysical model of synaptic plasticity.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {5-6}
}

@article{Shriki2003,
  title = {Rate {{Models}} for {{Conductance}}-{{Based Cortical Neuronal Networks}}},
  author = {Shriki, Oren and Hansel, David and Sompolinsky, Haim},
  year = {2003},
  month = aug,
  volume = {15},
  pages = {1809--1841},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/08997660360675053},
  file = {/Users/qualia/Documents/Papers/2003 - Shriki, Hansel, Sompolinsky - Rate models for conductance-based cortical neuronal networks.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {8}
}

@article{Shyam2018,
  title = {Model-{{Based Active Exploration}}},
  author = {Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  year = {2018},
  month = oct,
  abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to highdimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  archivePrefix = {arXiv},
  eprint = {1810.12162},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Shyam et al. - 2018 - Model-Based Active Exploration.pdf},
  journal = {arXiv:1810.12162 [cs, math, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{Siegel1994,
  title = {Activity-Dependent Current Distributions in Model Neurons.},
  author = {Siegel, M. and Marder, E. and Abbott, L. F.},
  year = {1994},
  month = nov,
  volume = {91},
  pages = {11308--11312},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.91.24.11308},
  abstract = {The electrical activity of a neuron can affect its intrinsic physiological characteristics through a wide range of processes. We study a computer-simulated multicompartment model neuron In which channel density depends on local Ca2+ concentrations. This has three interesting consequences for the spatial distribution of conductances and the physiological behavior ofthe neuron: (i) the model neuron spontaneously develops a realistic, nonuniform distribution of conductances that is linked both to the morphology of the neuron and to the pattern of synaptic input that it receives, (i) the response to synaptic Input reveals a form of intrinsic localized plasticity that balances the synaptic contribution from dendritic regions receiving unequal stimulation, and (i) intrinsic plasticity establishes a biophysical gain control that restores the neuron to its optimal firing range after synapses are strengthened by "Hebbian" long-term potentiation.},
  file = {/Users/qualia/Documents/Papers/1994 - Siegel, Marder, Abbottt - Activity-dependent current distributions in model neurons.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {24}
}

@article{Siegel2012,
  title = {Spectral Fingerprints of Large-Scale Neuronal Interactions},
  author = {Siegel, Markus and Donner, Tobias H. and Engel, Andreas K.},
  year = {2012},
  month = feb,
  volume = {13},
  pages = {121--134},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3137},
  abstract = {Cognition results from interactions among functionally specialized but widely distributed brain regions; however, neuroscience has so far largely focused on characterizing the function of individual brain regions and neurons therein. Here we discuss recent studies that have instead investigated the interactions between brain regions during cognitive processes by assessing correlations between neuronal oscillations in different regions of the primate cerebral cortex. These studies have opened a new window onto the large-scale circuit mechanisms underlying sensorimotor decision-making and top-down attention. We propose that frequency-specific neuronal correlations in large-scale cortical networks may be `fingerprints' of canonical neuronal computations underlying cognitive processes.},
  file = {/Users/qualia/Documents/Papers/2012 - Siegel, Donner, Engel - Spectral fingerprints of large-scale neuronal interactions.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {2}
}

@article{Siegel2015,
  title = {Cortical Information Flow during Flexible Sensorimotor Decisions},
  author = {Siegel, M. and Buschman, T. J. and Miller, E. K.},
  year = {2015},
  month = jun,
  volume = {348},
  pages = {1352--1355},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aab0551},
  abstract = {IT and V4 first extracted task information from the cues along with the encoding of cue identity. After this transient burst, there was a flow of sustained task information from PFC and LIP across the entire sensorimotor hierarchy.},
  file = {/Users/qualia/Documents/Papers/Siegel et al. - 2015 - Cortical information flow during flexible sensorim.pdf},
  journal = {Science},
  language = {en},
  number = {6241}
}

@article{Siero2013,
  title = {{{BOLD Consistently Matches Electrophysiology}} in {{Human Sensorimotor Cortex}} at {{Increasing Movement Rates}}: {{A Combined 7T fMRI}} and {{ECoG Study}} on {{Neurovascular Coupling}}},
  shorttitle = {{{BOLD Consistently Matches Electrophysiology}} in {{Human Sensorimotor Cortex}} at {{Increasing Movement Rates}}},
  author = {Siero, Jeroen CW and Hermes, Dora and Hoogduin, Hans and Luijten, Peter R and Petridou, Natalia and Ramsey, Nick F},
  year = {2013},
  month = sep,
  volume = {33},
  pages = {1448--1456},
  issn = {0271-678X, 1559-7016},
  doi = {10.1038/jcbfm.2013.97},
  file = {/Users/qualia/Documents/Papers/2013 - Siero et al. - BOLD Consistently Matches Electrophysiology in Human Sensorimotor Cortex at Increasing Movement Rates A Combined 7.pdf;/Users/qualia/Documents/Papers/Siero et al. - 2013 - BOLD Consistently Matches Electrophysiology in Hum.pdf},
  journal = {Journal of Cerebral Blood Flow \& Metabolism},
  language = {en},
  number = {9}
}

@article{Sigeti1987,
  title = {High-Frequency Power Spectra for Systems Subject to Noise},
  author = {Sigeti, D. and Horsthemke, W.},
  year = {1987},
  month = mar,
  volume = {35},
  pages = {2276--2282},
  issn = {0556-2791},
  doi = {10.1103/PhysRevA.35.2276},
  file = {/Users/qualia/Documents/Papers/1987 - Sigeti, Horsthemke - High-frequency power spectra for systems subject to noise.pdf},
  journal = {Physical Review A},
  language = {en},
  number = {5}
}

@article{Silberberg2004,
  title = {Synaptic Dynamics Control the Timing of Neuronal Excitation in the Activated Neocortical Microcircuit: {{Synaptic}} Dynamics Shape Cross-Correlations},
  shorttitle = {Synaptic Dynamics Control the Timing of Neuronal Excitation in the Activated Neocortical Microcircuit},
  author = {Silberberg, Gilad and Wu, Caizhi and Markram, Henry},
  year = {2004},
  month = apr,
  volume = {556},
  pages = {19--27},
  issn = {00223751},
  doi = {10.1113/jphysiol.2004.060962},
  file = {/Users/qualia/Documents/Papers/2004 - Silberberg, Wu, Markram - Synaptic dynamics control the timing of neuronal excitation in the activated neocortical microcircuit.pdf},
  journal = {The Journal of Physiology},
  language = {en},
  number = {1}
}

@article{Silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {/Users/qualia/Documents/Papers/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{Silver2016a,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {/Users/qualia/Documents/Papers/Silver et al. - 2016 - Mastering the game of Go with deep neural networks 2.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{Silver2016b,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {/Users/qualia/Documents/Papers/Silver et al. - 2016 - Mastering the game of Go with deep neural networks 3.pdf},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{Silver2018,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  volume = {362},
  pages = {1140--1144},
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  file = {/Users/qualia/Documents/Papers/Silver et al. - Mastering Chess and Shogi by Self-Play with a Gene.pdf},
  journal = {Science},
  language = {en},
  number = {6419}
}

@article{Sims2016,
  title = {Rate\textendash{}Distortion Theory and Human Perception},
  author = {Sims, Chris R.},
  year = {2016},
  month = jul,
  volume = {152},
  pages = {181--198},
  issn = {00100277},
  doi = {10.1016/j.cognition.2016.03.020},
  abstract = {The fundamental goal of perception is to aid in the achievement of behavioral objectives. This requires extracting and communicating useful information from noisy and uncertain sensory signals. At the same time, given the complexity of sensory information and the limitations of biological information processing, it is necessary that some information must be lost or discarded in the act of perception. Under these circumstances, what constitutes an `optimal' perceptual system? This paper describes the mathematical framework of rate\textendash{}distortion theory as the optimal solution to the problem of minimizing the costs of perceptual error subject to strong constraints on the ability to communicate or transmit information. Rate\textendash{}distortion theory offers a general and principled theoretical framework for developing computational-level models of human perception (Marr, 1982). Models developed in this framework are capable of producing quantitatively precise explanations for human perceptual performance, while yielding new insights regarding the nature and goals of perception. This paper demonstrates the application of rate\textendash{}distortion theory to two benchmark domains where capacity limits are especially salient in human perception: discrete categorization of stimuli (also known as absolute identification) and visual working memory. A software package written for the R statistical programming language is described that aids in the development of models based on rate\textendash{}distortion theory.},
  file = {/Users/qualia/Documents/Papers/Sims - 2016 - Rate–distortion theory and human perception.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Sims2018,
  title = {Efficient Coding Explains the Universal Law of Generalization in Human Perception},
  author = {Sims, Chris R.},
  year = {2018},
  month = may,
  volume = {360},
  pages = {652--656},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaq1118},
  file = {/Users/qualia/Documents/Papers/Sims - 2018 - Efficient coding explains the universal law of gen.pdf},
  journal = {Science},
  language = {en},
  number = {6389}
}

@article{Sinervo1996,
  title = {The Rock\textendash{}Paper\textendash{}Scissors Game and the Evolution of Alternative Male Strategies},
  author = {Sinervo, B. and Lively, C. M.},
  year = {1996},
  month = mar,
  volume = {380},
  pages = {240--243},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/380240a0},
  file = {/Users/qualia/Documents/Papers/Sinervo and Lively - 1996 - The rock–paper–scissors game and the evolution of .pdf},
  journal = {Nature},
  language = {en},
  number = {6571}
}

@article{Singh1970,
  title = {Preference for Bar Pressing to Obtain Reward over Freeloading in Rats and Children.},
  author = {Singh, Devendra},
  year = {1970},
  volume = {73},
  pages = {320--327},
  issn = {0021-9940},
  doi = {10.1037/h0030222},
  file = {/Users/qualia/Documents/Papers/Singh - 1970 - Preference for bar pressing to obtain reward over .pdf},
  journal = {Journal of Comparative and Physiological Psychology},
  language = {en},
  number = {2}
}

@article{Singh1970a,
  title = {Preference for Bar Pressing to Obtain Reward over Freeloading in Rats and Children.},
  author = {Singh, Devendra},
  year = {1970},
  volume = {73},
  pages = {320--327},
  issn = {0021-9940},
  doi = {10.1037/h0030222},
  file = {/Users/qualia/Documents/Papers/Singh - 1970 - Preference for bar pressing to obtain reward over  2.pdf},
  journal = {Journal of Comparative and Physiological Psychology},
  language = {en},
  number = {2}
}

@article{Sinha2014,
  title = {Autism as a Disorder of Prediction},
  author = {Sinha, Pawan and Kjelgaard, Margaret M. and Gandhi, Tapan K. and Tsourides, Kleovoulos and Cardinaux, Annie L. and Pantazis, Dimitrios and Diamond, Sidney P. and Held, Richard M.},
  year = {2014},
  month = oct,
  volume = {111},
  pages = {15220--15225},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1416797111},
  file = {/Users/qualia/Documents/Papers/Sinha et al. - 2014 - Autism as a disorder of prediction.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {42}
}

@article{Skarda1987,
  title = {How Brains Make Chaos in Order to Make Sense of the World},
  author = {Skarda, Christine A. and Freeman, Walter J.},
  year = {1987},
  month = jun,
  volume = {10},
  pages = {161},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X00047336},
  abstract = {Recent "connectionist" models provide a new explanatory alternative to the digital computer as a model for brain function. Evidence from our EEG research on the olfactory bulb suggests that the brain may indeed use computational mechanisms like those found in connectionist models. In the present paper we discuss our data and develop a model to describe the neural dynamics responsible for odor recognition and discrimination. The results indicate the existence of sensory- and motor-specific information in the spatial dimension of EEG activity and call for new physiological metaphors and techniques of analysis. Special emphasis is placed in our model on chaotic neural activity. We hypothesize that chaotic behavior serves as the essential ground state for the neural perceptual apparatus, and we propose a mechanism for acquiring new forms of patterned activity corresponding to new learned odors. Finally, some of the implications of our neural model for behavioral theories are briefly discussed. Our research, in concert with the connectionist work, encourages a reevaluation of explanatory models that are based only on the digital computer metaphor.},
  file = {/Users/qualia/Documents/Papers/1987 - Skarda, Freeman - How brains make chaos in order to make sense of the world.pdf;/Users/qualia/Documents/Papers/Skarda and Freeman - 1987 - How brains make chaos in order to make sense of th.pdf},
  journal = {Behavioral and Brain Sciences},
  language = {en},
  number = {02}
}

@article{Slezak2019,
  title = {Distinct {{Mechanisms}} for {{Visual}} and {{Motor}}-{{Related Astrocyte Responses}} in {{Mouse Visual Cortex}}},
  author = {Slezak, Michal and Kandler, Steffen and Van Veldhoven, Paul P. and {Van den Haute}, Chris and Bonin, Vincent and Holt, Matthew G.},
  year = {2019},
  month = sep,
  pages = {S0960982219309583},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.07.078},
  file = {/Users/qualia/Documents/Papers/Slezak et al. - 2019 - Distinct Mechanisms for Visual and Motor-Related A.pdf},
  journal = {Current Biology},
  language = {en}
}

@article{Smith2011,
  title = {The Confounding Effect of Response Amplitude on {{MVPA}} Performance Measures},
  author = {Smith, A.T. and Kosillo, P. and Williams, A.L.},
  year = {2011},
  month = may,
  volume = {56},
  pages = {525--530},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.05.079},
  abstract = {Multi-voxel pattern analysis (MVPA) is proving very powerful in the analysis of fMRI timeseries data, yielding surprising sensitivity, in many different contexts, to the response characteristics of neurons in a given brain region. However, MVPA yields a metric (classification performance) that does not readily lend itself to quantitative comparisons across experimental conditions, brain regions or people. This is because performance is influenced by a number of factors other than the sensitivity of neurons to the experimental manipulation. One such factor that varies widely but has been largely ignored in MVPA studies is the amplitude of the response being decoded. In a noisy system, it is expected that measured classification performance will decline with declining response amplitude, even if the underlying neuronal specificity is constant. We document the relationship between response amplitude and classification performance in the context of orientation decoding in the visual cortex. Flickering sine gratings were presented at each of two orthogonal orientations in a block design (multivariate experiment) or an event-related design (univariate experiment). Response amplitude was manipulated by varying stimulus contrast. Orientation classification performance in retinotopically defined occipital area V1 increased approximately linearly with the logarithm of stimulus contrast. As expected, univariate response amplitude also increased with contrast. Similar results were obtained in V2, V3 and V3A. Plotting classification performance against response amplitude gave a function with a compressive non-linearity that was well fit by a power function. Knowledge of this function potentially allows adjustment of classification performance to take account of the effect of response size, making comparisons across brain areas, categories or people more meaningful.},
  file = {/Users/qualia/Documents/Papers/2011 - Smith, Kosillo, Williams - The confounding effect of response amplitude on MVPA performance measures.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Snell2017,
  title = {Prototypical {{Networks}} for {{Few}}-Shot {{Learning}}},
  author = {Snell, Jake and Swersky, Kevin and Zemel, Richard S.},
  year = {2017},
  month = jun,
  abstract = {We propose prototypical networks for the problem of few-shot classification, where a classifier must generalize to new classes not seen in the training set, given only a small number of examples of each new class. Prototypical networks learn a metric space in which classification can be performed by computing distances to prototype representations of each class. Compared to recent approaches for few-shot learning, they reflect a simpler inductive bias that is beneficial in this limited-data regime, and achieve excellent results. We provide an analysis showing that some simple design decisions can yield substantial improvements over recent approaches involving complicated architectural choices and meta-learning. We further extend prototypical networks to zero-shot learning and achieve state-of-theart results on the CU-Birds dataset.},
  archivePrefix = {arXiv},
  eprint = {1703.05175},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Snell et al. - 2017 - Prototypical Networks for Few-shot Learning.pdf},
  journal = {arXiv:1703.05175 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Snyder2015,
  title = {Global Network Influences on Local Functional Connectivity},
  author = {Snyder, Adam C and Morais, Michael J and Willis, Cory M and Smith, Matthew A},
  year = {2015},
  month = may,
  volume = {18},
  pages = {736--743},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3979},
  file = {/Users/qualia/Documents/Papers/2015 - Snyder et al. - Global network influences on local functional connectivity.pdf;/Users/qualia/Documents/Papers/Snyder et al. - 2015 - Global network influences on local functional conn.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{So2012,
  title = {Relative Contributions of Local Cell and Passing Fiber Activation and Silencing to Changes in Thalamic Fidelity during Deep Brain Stimulation and Lesioning: A Computational Modeling Study},
  shorttitle = {Relative Contributions of Local Cell and Passing Fiber Activation and Silencing to Changes in Thalamic Fidelity during Deep Brain Stimulation and Lesioning},
  author = {So, Rosa Q. and Kent, Alexander R. and Grill, Warren M.},
  year = {2012},
  month = jun,
  volume = {32},
  pages = {499--519},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-011-0366-4},
  abstract = {Deep brain stimulation (DBS) and lesioning are two surgical techniques used in the treatment of advanced Parkinson's disease (PD) in patients whose symptoms are not well controlled by drugs, or who experience dyskinesias as a side effect of medications. Although these treatments have been widely practiced, the mechanisms behind DBS and lesioning are still not well understood. The subthalamic nucleus (STN) and globus pallidus pars interna (GPi) are two common targets for both DBS and lesioning. Previous studies have indicated that DBS not only affects local cells within the target, but also passing axons within neighboring regions. Using a computational model of the basal ganglia-thalamic network, we studied the relative contributions of activation and silencing of local cells (LCs) and fibers of passage (FOPs) to changes in the accuracy of information transmission through the thalamus (thalamic fidelity), which is correlated with the effectiveness of DBS. Activation of both LCs and FOPs during STN and GPi-DBS were beneficial to the outcome of stimulation. During STN and GPi lesioning, effects of silencing LCs and FOPs were different between the two types of lesioning. For STN lesioning, silencing GPi FOPs mainly contributed to its effectiveness, while silencing only STN LCs did not improve thalamic fidelity. In contrast, silencing both GPi LCs and GPe FOPs during GPi lesioning contributed to improvements in thalamic fidelity. Thus, two distinct mechanisms produced comparable improvements in thalamic function: driving the output of the basal ganglia to produce tonic inhibition and silencing the output of the basal ganglia to produce tonic disinhibition. These results show the importance of considering effects of activating or silencing fibers passing close to the nucleus when deciding upon a target location for DBS or lesioning.},
  file = {/Users/qualia/Documents/Papers/2013 - Rosa Q. So, Alexander R. Kent - Relative contributions of local cell and passing fiber activation and silencing to changes in tha.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Sohal2016,
  title = {How {{Close Are We}} to {{Understanding What}} (If {{Anything}}) {{Oscillations Do}} in {{Cortical Circuits}}?},
  author = {Sohal, V. S.},
  year = {2016},
  month = oct,
  volume = {36},
  pages = {10489--10495},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0990-16.2016},
  file = {/Users/qualia/Documents/Papers/Sohal - 2016 - How Close Are We to Understanding What (if Anythin.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Sohn2019,
  title = {Bayesian {{Computation}} through {{Cortical Latent Dynamics}}},
  author = {Sohn, Hansem and Narain, Devika and Meirhaeghe, Nicolas and Jazayeri, Mehrdad},
  year = {2019},
  month = sep,
  volume = {103},
  pages = {934-947.e5},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.06.012},
  abstract = {Statistical regularities in the environment create prior beliefs that we rely on to optimize our behavior when sensory information is uncertain. Bayesian theory formalizes how prior beliefs can be leveraged and has had a major impact on models of perception, sensorimotor function, and cognition. However, it is not known how recurrent interactions among neurons mediate Bayesian integration. By using a timeinterval reproduction task in monkeys, we found that prior statistics warp neural representations in the frontal cortex, allowing the mapping of sensory inputs to motor outputs to incorporate prior statistics in accordance with Bayesian inference. Analysis of recurrent neural network models performing the task revealed that this warping was enabled by a low-dimensional curved manifold and allowed us to further probe the potential causal underpinnings of this computational strategy. These results uncover a simple and general principle whereby prior beliefs exert their influence on behavior by sculpting cortical latent dynamics.},
  file = {/Users/qualia/Documents/Papers/Sohn et al. - 2019 - Bayesian Computation through Cortical Latent Dynam.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Sokal1962,
  title = {The {{Comparison}} of {{Dendrograms}} by {{Objective Methods}}},
  author = {Sokal, Robert R. and Rohlf, F. James},
  year = {1962},
  month = feb,
  volume = {11},
  pages = {33},
  issn = {00400262},
  doi = {10.2307/1217208},
  file = {/Users/qualia/Documents/Papers/2012 - Taxonomy - No Title.pdf},
  journal = {Taxon},
  language = {en},
  number = {2}
}

@article{Solari2011,
  title = {Cognitive Consilience: {{Primate}} Non-Primary Neuroanatomical Circuits Underlying Cognition},
  shorttitle = {Cognitive Consilience},
  author = {{Solari}},
  year = {2011},
  issn = {16625129},
  doi = {10.3389/fnana.2011.00065},
  abstract = {Interactions between the cerebral cortex, thalamus, and basal ganglia form the basis of cognitive information processing in the mammalian brain. Understanding the principles of neuroanatomical organization in these structures is critical to understanding the functions they perform and ultimately how the human brain works. We have manually distilled and synthesized hundreds of primate neuroanatomy facts into a single interactive visualization. The resulting picture represents the fundamental neuroanatomical blueprint upon which cognitive functions must be implemented. Within this framework we hypothesize and detail 7 functional circuits corresponding to psychological perspectives on the brain: consolidated long-term declarative memory, short-term declarative memory, working memory/information processing, behavioral memory selection, behavioral memory output, cognitive control, and cortical information flow regulation. Each circuit is described in terms of distinguishable neuronal groups including the cerebral isocortex (9 pyramidal neuronal groups), parahippocampal gyrus and hippocampus, thalamus (4 neuronal groups), basal ganglia (7 neuronal groups), metencephalon, basal forebrain, and other subcortical nuclei. We focus on neuroanatomy related to primate nonprimary cortical systems to elucidate the basis underlying the distinct homotypical cognitive architecture. To display the breadth of this review, we introduce a novel method of integrating and presenting data in multiple independent visualizations: an interactive website (http://www.frontiersin.org/files/cognitiveconsilience/index.html) and standalone iPhone and iPad applications. With these tools we present a unique, annotated view of neuroanatomical consilience (integration of knowledge).},
  file = {/Users/qualia/Documents/Papers/2011 - Solari, Stoner - Cognitive consilience primate non-primary neuroanatomical circuits underlying cognition.pdf},
  journal = {Frontiers in Neuroanatomy},
  language = {en}
}

@article{Soltoggio2013,
  title = {Rare {{Neural Correlations Implement Robotic Conditioning}} with {{Delayed Rewards}} and {{Disturbances}}},
  author = {Soltoggio, Andrea and Lemme, Andre and Reinhart, Felix and Steil, Jochen J.},
  year = {2013},
  volume = {7},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2013.00006},
  abstract = {Neural conditioning associates cues and actions with following rewards. The environments in which robots operate, however, are pervaded by a variety of disturbing stimuli and uncertain timing. In particular, variable reward delays make it difficult to reconstruct which previous actions are responsible for following rewards. Such an uncertainty is handled by biological neural networks, but represents a challenge for computational models, suggesting the lack of a satisfactory theory for robotic neural conditioning. The present study demonstrates the use of rare neural correlations in making correct associations between rewards and previous cues or actions. Rare correlations are functional in selecting sparse synapses to be eligible for later weight updates if a reward occurs. The repetition of this process singles out the associating and reward-triggering pathways, and thereby copes with distal rewards. The neural network displays macro-level classical and operant conditioning, which is demonstrated in an interactive real-life human-robot interaction. The proposed mechanism models realistic conditioning in humans and animals and implements similar behaviors in neuro-robotic platforms.},
  file = {/Users/qualia/Documents/Papers/2013 - Soltoggio et al. - Rare neural correlations implement robotic conditioning with delayed rewards and disturbances.pdf;/Users/qualia/Documents/Papers/Soltoggio et al. - 2013 - Rare Neural Correlations Implement Robotic Conditi.pdf},
  journal = {Frontiers in Neurorobotics},
  language = {en}
}

@article{Soltoggio2013a,
  title = {Solving the {{Distal Reward Problem}} with {{Rare Correlations}}},
  author = {Soltoggio, Andrea and Steil, Jochen J.},
  year = {2013},
  month = apr,
  volume = {25},
  pages = {940--978},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00419},
  file = {/Users/qualia/Documents/Papers/2013 - Soltoggio, Steil - Solving the distal reward problem with rare correlations.pdf;/Users/qualia/Documents/Papers/Soltoggio and Steil - 2013 - Solving the Distal Reward Problem with Rare Correl.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{Sompolinsky1988,
  title = {Chaos in {{Random Neural Networks}}},
  author = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  year = {1988},
  month = jul,
  volume = {61},
  pages = {259--262},
  issn = {0031-9007},
  doi = {10.1103/PhysRevLett.61.259},
  file = {/Users/qualia/Documents/Papers/1988 - Sompolinsky, Crisanti, Sommers - Chaos in random neural networks.pdf;/Users/qualia/Documents/Papers/Sompolinsky et al. - 1988 - Chaos in Random Neural Networks.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {3}
}

@article{Song,
  title = {Reward-Based Training of Recurrent Neural Networks for Cognitive and Value-Based Tasks},
  author = {Song, H Francis and Yang, Guangyu R and Wang, Xiao-Jing},
  pages = {59},
  abstract = {Trained neural network models, which exhibit many features observed in neural recordings from behaving animals and whose activity and connectivity can be fully analyzed, may provide insights into neural mechanisms. In contrast to commonly used methods for supervised learning from graded error signals, however, animals learn from reward feedback on definite actions through reinforcement learning. Reward maximization is particularly relevant when the optimal behavior depends on an animal's internal judgment of confidence or subjective preferences. Here, we describe reward-based training of recurrent neural networks in which a value network guides learning by using the selected actions and activity of the policy network to predict future reward. We show that such models capture both behavioral and electrophysiological findings from well-known experimental paradigms. Our results provide a unified framework for investigating diverse cognitive and value-based computations, including a role for value representation that is essential for learning, but not executing, a task.},
  file = {/Users/qualia/Documents/Papers/Song et al. - Reward-based training of recurrent neural networks.pdf},
  language = {en}
}

@article{Song2000,
  title = {Competitive {{Hebbian}} Learning through Spike-Timing-Dependent Synaptic Plasticity},
  author = {Song, Sen and Miller, Kenneth D. and Abbott, L. F.},
  year = {2000},
  month = sep,
  volume = {3},
  pages = {919--926},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/78829},
  file = {/Users/qualia/Documents/Papers/2000 - Song, Miller, Abbott - Competitive Hebbian Learning through Spike-Time Dependent Synaptic Plasticity.pdf;/Users/qualia/Documents/Papers/2000 - Song, Miller, Abbott - Competitive Hebbian learning through spike-timing-dependent synaptic plasticity.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Song2005,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  author = {Song, Sen and Sj{\"o}str{\"o}m, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B},
  editor = {Friston, Karl J.},
  year = {2005},
  month = mar,
  volume = {3},
  pages = {e68},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0030068},
  file = {/Users/qualia/Documents/Papers/2005 - Song et al. - Highly nonrandom features of synaptic connectivity in local cortical circuits.pdf},
  journal = {PLoS Biology},
  language = {en},
  number = {3}
}

@article{Song2005a,
  title = {Highly {{Nonrandom Features}} of {{Synaptic Connectivity}} in {{Local Cortical Circuits}}},
  author = {Song, Sen and Sj{\"o}str{\"o}m, Per Jesper and Reigl, Markus and Nelson, Sacha and Chklovskii, Dmitri B},
  editor = {Friston, Karl J.},
  year = {2005},
  month = mar,
  volume = {3},
  pages = {e68},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.0030068},
  file = {/Users/qualia/Documents/Papers/Song et al. - 2005 - Highly Nonrandom Features of Synaptic Connectivity.PDF},
  journal = {PLoS Biology},
  language = {en},
  number = {3}
}

@article{Song2016,
  title = {Training {{Excitatory}}-{{Inhibitory Recurrent Neural Networks}} for {{Cognitive Tasks}}: {{A Simple}} and {{Flexible Framework}}},
  shorttitle = {Training {{Excitatory}}-{{Inhibitory Recurrent Neural Networks}} for {{Cognitive Tasks}}},
  author = {Song, H. Francis and Yang, Guangyu R. and Wang, Xiao-Jing},
  editor = {Sporns, Olaf},
  year = {2016},
  month = feb,
  volume = {12},
  pages = {e1004792},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004792},
  file = {/Users/qualia/Documents/Papers/Song et al. - 2016 - Training Excitatory-Inhibitory Recurrent Neural Ne.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {2}
}

@article{Song2019,
  title = {Sources of Suboptimality in a Minimalistic Explore\textendash{}Exploit Task},
  author = {Song, Mingyu and Bnaya, Zahy and Ma, Wei Ji},
  year = {2019},
  month = apr,
  volume = {3},
  pages = {361--368},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0526-x},
  file = {/Users/qualia/Documents/Papers/Song et al. - 2019 - Sources of suboptimality in a minimalistic explore.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {4}
}

@article{Song2019a,
  title = {Sources of Suboptimality in a Minimalistic Explore\textendash{}Exploit Task},
  author = {Song, Mingyu and Bnaya, Zahy and Ma, Wei Ji},
  year = {2019},
  month = apr,
  volume = {3},
  pages = {361--368},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0526-x},
  file = {/Users/qualia/Documents/Papers/Song et al. - 2019 - Sources of suboptimality in a minimalistic explore.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {4}
}

@article{Soon2008,
  title = {Unconscious Determinants of Free Decisions in the Human Brain},
  author = {Soon, Chun Siong and Brass, Marcel and Heinze, Hans-Jochen and Haynes, John-Dylan},
  year = {2008},
  month = may,
  volume = {11},
  pages = {543--545},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2112},
  file = {/Users/qualia/Documents/Papers/2008 - Soon et al. - Unconscious determinants of free decisions in the human brain.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {5}
}

@article{Sotero2007,
  title = {Realistically {{Coupled Neural Mass Models Can Generate EEG Rhythms}}},
  author = {Sotero, Roberto C. and {Trujillo-Barreto}, Nelson J. and {Iturria-Medina}, Yasser and Carbonell, Felix and Jimenez, Juan C.},
  year = {2007},
  month = feb,
  volume = {19},
  pages = {478--512},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2007.19.2.478},
  file = {/Users/qualia/Documents/Papers/2007 - Sotero, Trujillo-barreto - Realistically Coupled Neural Mass Models Can Generate EEG Rhythms.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {2}
}

@article{Sotero2015,
  title = {Laminar {{Distribution}} of {{Phase}}-{{Amplitude Coupling}} of {{Spontaneous Current Sources}} and {{Sinks}}},
  author = {Sotero, Roberto C. and Bortel, Aleksandra and Naaman, Shmuel and Mocanu, Victor M. and Kropf, Pascal and Villeneuve, Martin and Shmuel, Amir},
  year = {2015},
  month = dec,
  volume = {9},
  issn = {1662-453X},
  doi = {10.3389/fnins.2015.00454},
  abstract = {Although resting-state functional connectivity is a commonly used neuroimaging paradigm, the underlying mechanisms remain unknown. Thalamo-cortical and cortico-cortical circuits generate oscillations at different frequencies during spontaneous activity. However, it remains unclear how the various rhythms interact and whether their interactions are lamina-specific. Here we investigated intra- and inter-laminar spontaneous phase-amplitude coupling (PAC). We recorded local-field potentials using laminar probes inserted in the forelimb representation of rat area S1. We then computed time-series of frequency-band- and lamina-specific current source density (CSD), and PACs of CSD for all possible pairs of the classical frequency bands in the range of 1\textendash{}150 Hz. We observed both intra- and inter-laminar spontaneous PAC. Of 18 possible combinations, 12 showed PAC, with the highest measures of interaction obtained for the pairs of the theta/gamma and delta/gamma bands. Intra- and inter-laminar PACs involving layers 2/3\textendash{}5a were higher than those involving layer 6. Current sinks (sources) in the delta band were associated with increased (decreased) amplitudes of high-frequency signals in the beta to fast gamma bands throughout layers 2/3\textendash{}6. Spontaneous sinks (sources) of the theta and alpha bands in layers 2/3\textendash{}4 were on average linked to dipoles completed by sources (sinks) in layer 6, associated with high (low) amplitudes of the beta to fast-gamma bands in the entire cortical column. Our findings show that during spontaneous activity, delta, theta, and alpha oscillations are associated with periodic excitability, which for the theta and alpha bands is lamina-dependent. They further emphasize the differences between the function of layer 6 and that of the superficial layers, and the role of layer 6 in controlling activity in those layers. Our study links theories on the involvement of PAC in resting-state functional connectivity with previous work that revealed lamina-specific anatomical thalamo-cortico-cortical connections.},
  file = {/Users/qualia/Documents/Papers/Sotero et al. - 2015 - Laminar Distribution of Phase-Amplitude Coupling o.pdf},
  journal = {Frontiers in Neuroscience},
  language = {en}
}

@article{Sotero2016,
  title = {Topology, Cross-Frequency, and Same-Frequency Band Interactions Shape the Generation of Phase-Amplitude Coupling in a Neural Mass Model of a Cortical Column},
  author = {Sotero, Roberto},
  year = {2016},
  month = feb,
  doi = {10.1101/023291},
  abstract = {Phase-amplitude coupling (PAC), the phenomenon where the phase of a low-frequency rhythm modulates the amplitude of a higher frequency, is becoming an important neurophysiological indicator of short- and long-range information transmission in the brain. Although recent evidence suggests that PAC might play a functional role during sensorimotor, and cognitive events, the neurobiological mechanisms underlying its generation remain imprecise. Thus, a realistic but simple enough computational model of the phenomenon is needed. Here we propose a neural mass model of a cortical column, comprising fourteen neuronal populations distributed across four layers (L2/3, L4, L5 and L6). While experimental studies often focus in only one or two PAC combinations (e.g., theta-gamma or alpha-gamma) our simulations show that the cortical column can generate almost all possible couplings of phases and amplitudes, which are influenced by connectivity parameters, time constants, and external inputs. Furthermore, our simulations suggest that the effective connectivity between neuronal populations can result in the emergence of PAC combinations with frequencies different from the natural frequencies of the oscillators involved. For instance, simulations of oscillators with natural frequencies in the theta, alpha and gamma bands, were able to produce significant PAC combinations involving delta and beta bands.},
  file = {/Users/qualia/Documents/Papers/2015 - Sotero - Generation of phase-amplitude coupling of neurophysiological signals in a neural mass model of a cortical column(2).pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Spencer2004,
  title = {Neural Synchrony Indexes Disordered Perception and Cognition in Schizophrenia},
  author = {Spencer, K. M. and Nestor, P. G. and Perlmutter, R. and Niznikiewicz, M. A. and Klump, M. C. and Frumin, M. and Shenton, M. E. and McCarley, R. W.},
  year = {2004},
  month = dec,
  volume = {101},
  pages = {17288--17293},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0406074101},
  file = {/Users/qualia/Documents/Papers/2004 - Spencer et al. - Neural synchrony indexes disordered perception and cognition in schizophrenia.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {49}
}

@book{Spiegler2012,
  title = {Dynamics of Biologically Informed Neurals Mass Models of the Brain},
  author = {Spiegler, Andreas and Haueisen, Jens and Kn{\"o}sche, Thomas R. and Jirsa, Viktor K.},
  year = {2012},
  publisher = {{Univ.-Verl. Ilmenau}},
  address = {{Ilmenau}},
  abstract = {Die vorliegende Arbeit stellt einen Beitrag zur Entwicklung und Analyse von Computermodellen zum Verst{\"a}ndnis von Hirnfunktionen dar. Es wird die mittlere Aktivit{\"a}t eines Hirnareals analytisch einfach und dabei biologisch plausibel modelliert. Auf Grundlage eines Neuronalen Massenmodells (NMM) werden die Wechsel zwischen Oszillationsregimen (z.B. durch pharmakologisch, epilepsie-, schlaf- oder kontextbedingte Zustands{\"a}nderungen) als geordnete Folge beschrieben und Resonanzph{\"a}nomene in einem Photic-Driving-Experiment erkl{\"a}rt. Dieses NMM kann sehr komplexe Dynamiken (z.B. Chaos) innerhalb biologisch plausibler Parameterbereiche hervorbringen. Um das Verhalten abzusch{\"a}tzen, wird das NMM als Funktion konstanter Eingangsgr{\"o}{\ss}en und charakteristischer Zeitenkonstanten vollst{\"a}ndig auf Bifurkationen untersucht und klassifiziert. Dies erm{\"o}glicht die Beschreibung wechselnder Regime als geordnete Folge durch spezifische Eingangstrajektorien. Es wird ein Prinzip vorgestellt, um komplexe Ph{\"a}nomene durch Prozesse verschiedener Zeitskalen darzustellen. Da aufgrund rhythmischer Stimuli und der intrinsischen Rhythmen von Neuronenverb{\"a}nden die Eingangsgr{\"o}{\ss}en h{\"a}ufig periodisch sind, wird das Verhalten des NMM als Funktion der Intensit{\"a}t und Frequenz einer periodischen Stimulation mittels der zugeh{\"o}rigen Lyapunov-Spektren und der Zeitreihen charakterisiert. Auf der Basis der gr{\"o}{\ss}ten Lyapunov-Exponenten wird das NMM mit dem Photic-Driving-Experiment {\"u}berein gebracht. Dieses Experiment findet routinem{\"a}{\ss}ige Anwendung in der Diagnostik verschiedener Erkrankungen wie Epilepsie, Migr{\"a}ne, Schizophrenie und Depression. Durch die Anwendung des vorgestellten NMM wird der f{\"u}r die Diagnostik entscheidende Mitnahmeeffekt reproduziert und es werden Vorhersagen f{\"u}r eine Verbesserung der Indikation getroffen},
  file = {/Users/qualia/Documents/Papers/2011 - Spiegler - Dynamics of biologically informed neural mass models of the brain.pdf},
  isbn = {978-3-86360-024-2},
  language = {en},
  note = {OCLC: 855873253}
}

@article{Spiliotis,
  title = {Micro to {{Macro Equation}}-{{Free Bifurcation Analysis}} of {{Neuronal Random Graphs}}: {{Symmetry Breaking}} of {{Majority Rule Dynamics}}},
  author = {Spiliotis, Konstantinos and Russo, Lucia and Siettos, Constantinos I},
  pages = {6},
  file = {/Users/qualia/Documents/Papers/2003 - Spiliotis et al. - Micro to Macro Equation-Free Bifurcation Analysis of Neuronal Random Graphs Symmetry Breaking of Majority Rul.pdf},
  language = {en}
}

@article{Spiliotis2010,
  title = {{{MULTISCALE COMPUTATIONS ON NEURAL NETWORKS}}: {{FROM THE INDIVIDUAL NEURON INTERACTIONS TO THE MACROSCOPIC}}-{{LEVEL ANALYSIS}}},
  shorttitle = {{{MULTISCALE COMPUTATIONS ON NEURAL NETWORKS}}},
  author = {Spiliotis, Konstantinos G. and Siettos, Constantinos I.},
  year = {2010},
  month = jan,
  volume = {20},
  pages = {121--134},
  issn = {0218-1274, 1793-6551},
  doi = {10.1142/S0218127410025442},
  abstract = {We show how the ``Equation-Free'' approach for multi-scale computations can be exploited to systematically study the dynamics of neural interactions on a random regular connected graph under a pairwise representation perspective. Using an individual-based microscopic simulator as a black box coarse-grained timestepper and with the aid of simulated annealing we compute the coarse-grained equilibrium bifurcation diagram and analyze the stability of the stationary states sidestepping the necessity of obtaining explicit closures at the macroscopic level. We also exploit the scheme to perform a rare-events analysis by estimating an effective Fokker-Planck describing the evolving probability density function of the corresponding coarse-grained observables.},
  file = {/Users/qualia/Documents/Papers/2010 - Spiliotis, Siettos - Multiscale Computations On Neural Networks From The Individual Neuron Interactions To The Macroscopic-Level.pdf},
  journal = {International Journal of Bifurcation and Chaos},
  language = {en},
  number = {01}
}

@article{Spoerer2017,
  title = {Recurrent Convolutional Neural Networks: A Better Model of Biological Object Recognition},
  shorttitle = {Recurrent Convolutional Neural Networks},
  author = {Spoerer, Courtney J and McClure, Patrick and Kriegeskorte, Nikolaus},
  year = {2017},
  month = aug,
  doi = {10.1101/133330},
  abstract = {Feedforward neural networks provide the dominant model of how the brain performs visual object recognition. However, these networks lack the lateral and feedback connections, and the resulting recurrent neuronal dynamics, of the ventral visual pathway in the human and nonhuman primate brain. Here we investigate recurrent convolutional neural networks with bottom-up (B), lateral (L), and top-down (T) connections. Combining these types of connections yields four architectures (B, BT, BL, and BLT), which we systematically test and compare. We hypothesized that recurrent dynamics might improve recognition performance in the challenging scenario of partial occlusion. We introduce two novel occluded object recognition tasks to test the efficacy of the models, digit clutter (where multiple target digits occlude one another) and digit debris (where target digits are occluded by digit fragments). We find that recurrent neural networks outperform feedforward control models (approximately matched in parametric complexity) at recognising objects, both in the absence of occlusion and in all occlusion conditions. Recurrent networks were also found to be more robust to the inclusion of additive Gaussian noise. Recurrent neural networks are better in two respects: (1) they are more neurobiologically realistic than their feedforward counterparts; (2) they are better in terms of their ability to recognise objects, especially under challenging conditions. This work shows that computer vision can benefit from using recurrent convolutional architectures and suggests that the ubiquitous recurrent connections in biological brains are essential for task performance.},
  file = {/Users/qualia/Documents/Papers/Spoerer et al. - 2017 - Recurrent convolutional neural networks a better  2.pdf;/Users/qualia/Documents/Papers/Spoerer et al. - 2017 - Recurrent convolutional neural networks a better .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Sprague2013,
  title = {Attention Modulates Spatial Priority Maps in the Human Occipital, Parietal and Frontal Cortices},
  author = {Sprague, Thomas C and Serences, John T},
  year = {2013},
  month = dec,
  volume = {16},
  pages = {1879--1887},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3574},
  file = {/Users/qualia/Documents/Papers/2013 - Sprague, Serences - Attention modulates spatial priority maps in the human occipital, parietal and frontal cortices.pdf;/Users/qualia/Documents/Papers/Sprague and Serences - 2013 - Attention modulates spatial priority maps in the h.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {12}
}

@article{Srivastava,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  pages = {30},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ``thinned'' networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/Users/qualia/Documents/Papers/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf},
  language = {en}
}

@article{Stacy2005,
  title = {Identification of Motor and Nonmotor Wearing-off in {{Parkinson}}'s Disease: {{Comparison}} of a Patient Questionnaire versus a Clinician Assessment},
  shorttitle = {Identification of Motor and Nonmotor Wearing-off in {{Parkinson}}'s Disease},
  author = {Stacy, Mark and Bowron, Annette and Guttman, Mark and Hauser, Robert and Hughes, Kim and Larsen, Jan Petter and LeWitt, Peter and Oertel, Wolfgang and Quinn, Niall and Sethi, Kapil and Stocchi, Fabrizio},
  year = {2005},
  month = jun,
  volume = {20},
  pages = {726--733},
  issn = {0885-3185, 1531-8257},
  doi = {10.1002/mds.20383},
  file = {/Users/qualia/Documents/Papers/2005 - Stacy et al. - Identification of motor and nonmotor wearing-off in Parkinson's disease Comparison of a patient questionnaire vers.pdf},
  journal = {Movement Disorders},
  language = {en},
  number = {6}
}

@article{Stamatakis2013,
  title = {A {{Unique Population}} of {{Ventral Tegmental Area Neurons Inhibits}} the {{Lateral Habenula}} to {{Promote Reward}}},
  author = {Stamatakis, Alice M. and Jennings, Joshua H. and Ung, Randall L. and Blair, Grace A. and Weinberg, Richard J. and Neve, Rachael L. and Boyce, Frederick and Mattis, Joanna and Ramakrishnan, Charu and Deisseroth, Karl and Stuber, Garret D.},
  year = {2013},
  month = nov,
  volume = {80},
  pages = {1039--1053},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.08.023},
  abstract = {Lateral habenula (LHb) neurons convey aversive and negative reward conditions through potent indirect inhibition of ventral tegmental area (VTA) dopaminergic neurons. Although VTA dopaminergic neurons reciprocally project to the LHb, the electrophysiological properties and the behavioral consequences associated with selective manipulations of this circuit are unknown. Here, we identify an inhibitory input to the LHb arising from a unique population of VTA neurons expressing dopaminergic markers. Optogenetic activation of this circuit resulted in no detectable dopamine release in LHb brain slices. Instead, stimulation produced GABA-mediated inhibitory synaptic transmission, which suppressed the firing of postsynaptic LHb neurons in brain slices and increased the spontaneous firing rate of VTA dopaminergic neurons in vivo. Furthermore, in vivo activation of this pathway produced reward-related phenotypes that were dependent on intra-LHb GABAA receptor signaling. These results suggest that noncanonical inhibitory signaling by these hybrid dopaminergic-GABAergic neurons act to suppress LHb output under rewarding conditions.},
  file = {/Users/qualia/Documents/Papers/Stamatakis et al. - 2013 - A Unique Population of Ventral Tegmental Area Neur.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Stanley,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O and Miikkulainen, Risto},
  volume = {10},
  pages = {30},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  file = {/Users/qualia/Documents/Papers/Stanley and Miikkulainen - Evolving Neural Networks through Augmenting Topolo.pdf},
  language = {en},
  number = {2}
}

@article{Starkweather2017,
  title = {Dopamine Reward Prediction Errors Reflect Hidden-State Inference across Time},
  author = {Starkweather, Clara Kwon and Babayan, Benedicte M and Uchida, Naoshige and Gershman, Samuel J},
  year = {2017},
  month = apr,
  volume = {20},
  pages = {581--589},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4520},
  file = {/Users/qualia/Documents/Papers/Starkweather et al. - 2017 - Dopamine reward prediction errors reflect hidden-s.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Steinberg2013,
  title = {A Causal Link between Prediction Errors, Dopamine Neurons and Learning},
  author = {Steinberg, Elizabeth E and Keiflin, Ronald and Boivin, Josiah R and Witten, Ilana B and Deisseroth, Karl and Janak, Patricia H},
  year = {2013},
  month = jul,
  volume = {16},
  pages = {966--973},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3413},
  file = {/Users/qualia/Documents/Papers/Steinberg et al. - 2013 - A causal link between prediction errors, dopamine .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {7}
}

@article{Steinmetz2018,
  title = {Distributed Correlates of Visually-Guided Behavior across the Mouse Brain},
  author = {Steinmetz, Nicholas and {Zatka-Haas}, Peter and Carandini, Matteo and Harris, Kenneth},
  year = {2018},
  month = nov,
  doi = {10.1101/474437},
  abstract = {Behavior arises from neuronal activity, but it is not known how the active neurons are distributed across brain regions and how their activity unfolds in time. Here, we used high-density Neuropixels probes to record from \textasciitilde{}30,000 neurons in mice performing a visual contrast discrimination task. The task activated 60\% of the neurons, involving nearly all 42 recorded brain regions, well beyond the regions activated by passive visual stimulation. However, neurons selective for choice (left vs. right) were rare, and found mostly in midbrain, striatum, and frontal cortex. Those in midbrain were typically activated prior to contralateral choices and suppressed prior to ipsilateral choices, consistent with a competitive midbrain circuit for adjudicating the subject's choice. A brain-wide state shift distinguished trials in which visual stimuli led to movement. These results reveal concurrent representations of movement and choice in neurons widely distributed across the brain.},
  file = {/Users/qualia/Documents/Papers/Steinmetz et al. - 2018 - Distributed correlates of visually-guided behavior.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Stephens2008,
  title = {Decision Ecology: {{Foraging}} and the Ecology of Animal Decision Making},
  shorttitle = {Decision Ecology},
  author = {Stephens, D. W.},
  year = {2008},
  month = dec,
  volume = {8},
  pages = {475--484},
  issn = {1530-7026, 1531-135X},
  doi = {10.3758/CABN.8.4.475},
  file = {/Users/qualia/Documents/Papers/Stephens - 2008 - Decision ecology Foraging and the ecology of anim.pdf},
  journal = {Cognitive, Affective, \& Behavioral Neuroscience},
  language = {en},
  number = {4}
}

@article{Stephenson-Jones2011,
  title = {Evolutionary {{Conservation}} of the {{Basal Ganglia}} as a {{Common Vertebrate Mechanism}} for {{Action Selection}}},
  author = {{Stephenson-Jones}, Marcus and Samuelsson, Ebba and Ericsson, Jesper and Robertson, Brita and Grillner, Sten},
  year = {2011},
  month = jul,
  volume = {21},
  pages = {1081--1091},
  issn = {09609822},
  doi = {10.1016/j.cub.2011.05.001},
  abstract = {Background: Although the basal ganglia are thought to play a key role in action selection in mammals, it is unknown whether this mammalian circuitry is present in lower vertebrates as a conserved selection mechanism. We aim here, using lamprey, to elucidate the basal ganglia circuitry in the phylogenetically oldest group of vertebrates (cyclostomes) and determine how this selection architecture evolved to accommodate the increased behavioral repertoires of advanced vertebrates.
Results: We show, using immunohistochemistry, tract tracing, and whole-cell recordings, that all parts of the mammalian basal ganglia (striatum, globus pallidus interna [GPi] and externa [GPe], and subthalamic nucleus [STN]) are present in the lamprey forebrain. In addition, the circuit features, molecular markers, and physiological activity patterns are conserved. Thus, GABAergic striatal neurons expressing substance P project directly to the pallidal output layer, whereas enkephalin-expressing striatal neurons project indirectly via nuclei homologous to the GPe and STN. Moreover, pallidal output neurons tonically inhibit tectum, mesencephalic, and diencephalic motor regions.
Conclusions: These results show that the detailed basal ganglia circuitry is present in the phylogenetically oldest vertebrates and has been conserved, most likely as a mechanism for action selection used by all vertebrates, for over 560 million years. Our data also suggest that the mammalian basal ganglia evolved through a process of exaptation, where the ancestral core unit has been co-opted for multiple functions, allowing them to process cognitive, emotional, and motor information in parallel and control a broader range of behaviors.},
  file = {/Users/qualia/Documents/Papers/Stephenson-Jones et al. - 2011 - Evolutionary Conservation of the Basal Ganglia as .pdf},
  journal = {Current Biology},
  language = {en},
  number = {13}
}

@article{Stiefel2008,
  title = {Cholinergic {{Neuromodulation Changes Phase Response Curve Shape}} and {{Type}} in {{Cortical Pyramidal Neurons}}},
  author = {Stiefel, Klaus M. and Gutkin, Boris S. and Sejnowski, Terrence J.},
  editor = {Ermentrout, Bard},
  year = {2008},
  month = dec,
  volume = {3},
  pages = {e3947},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0003947},
  abstract = {Spike generation in cortical neurons depends on the interplay between diverse intrinsic conductances. The phase response curve (PRC) is a measure of the spike time shift caused by perturbations of the membrane potential as a function of the phase of the spike cycle of a neuron. Near the rheobase, purely positive (type I) phase-response curves are associated with an onset of repetitive firing through a saddle-node bifurcation, whereas biphasic (type II) phase-response curves point towards a transition based on a Hopf-Andronov bifurcation. In recordings from layer 2/3 pyramidal neurons in cortical slices, cholinergic action, consistent with down-regulation of slow voltage-dependent potassium currents such as the M-current, switched the PRC from type II to type I. This is the first report showing that cholinergic neuromodulation may cause a qualitative switch in the PRCs type implying a change in the fundamental dynamical mechanism of spike generation.},
  file = {/Users/qualia/Documents/Papers/2008 - Stiefel, Gutkin, Sejnowski - Cholinergic neuromodulation changes phase response curve shape and type in cortical pyramidal neuron.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {12}
}

@article{Stiefel2009,
  title = {The Effects of Cholinergic Neuromodulation on Neuronal Phase-Response Curves of Modeled Cortical Neurons},
  author = {Stiefel, Klaus M. and Gutkin, Boris S. and Sejnowski, Terrence J.},
  year = {2009},
  month = apr,
  volume = {26},
  pages = {289--301},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-008-0111-9},
  file = {/Users/qualia/Documents/Papers/2009 - Stiefel, Gutkin, Sejnowski - The effects of cholinergic neuromodulation on neuronal phase-response curves of modeled cortical neu.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {2}
}

@article{Still2012,
  title = {The Thermodynamics of Prediction},
  author = {Still, Susanne and Sivak, David A. and Bell, Anthony J. and Crooks, Gavin E.},
  year = {2012},
  month = sep,
  volume = {109},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.109.120604},
  abstract = {A system responding to a stochastic driving signal can be interpreted as computing, by means of its dynamics, an implicit model of the environmental variables. The system's state retains information about past environmental fluctuations, and a fraction of this information is predictive of future ones. The remaining nonpredictive information reflects model complexity that does not improve predictive power, and thus represents the ineffectiveness of the model. We expose the fundamental equivalence between this model inefficiency and thermodynamic inefficiency, measured by dissipation. Our results hold arbitrarily far from thermodynamic equilibrium and are applicable to a wide range of systems, including biomolecular machines. They highlight a profound connection between the effective use of information and efficient thermodynamic operation: any system constructed to keep memory about its environment and to operate with maximal energetic efficiency has to be predictive.},
  archivePrefix = {arXiv},
  eprint = {1203.3271},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2012 - Still et al. - Thermodynamics of prediction.pdf},
  journal = {Physical Review Letters},
  keywords = {Computer Science - Information Theory,Condensed Matter - Statistical Mechanics,Quantitative Biology - Quantitative Methods},
  language = {en},
  number = {12}
}

@article{Stimberg2014,
  title = {Equation-Oriented Specification of Neural Models for Simulations},
  author = {Stimberg, Marcel and Goodman, Dan F. M. and Benichoux, Victor and Brette, Romain},
  year = {2014},
  volume = {8},
  issn = {1662-5196},
  doi = {10.3389/fninf.2014.00006},
  abstract = {Simulating biological neuronal networks is a core method of research in computational neuroscience. A full specification of such a network model includes a description of the dynamics and state changes of neurons and synapses, as well as the synaptic connectivity patterns and the initial values of all parameters. A standard approach in neuronal modeling software is to build network models based on a library of pre-defined components and mechanisms; if a model component does not yet exist, it has to be defined in a special-purpose or general low-level language and potentially be compiled and linked with the simulator. Here we propose an alternative approach that allows flexible definition of models by writing textual descriptions based on mathematical notation. We demonstrate that this approach allows the definition of a wide range of models with minimal syntax. Furthermore, such explicit model descriptions allow the generation of executable code for various target languages and devices, since the description is not tied to an implementation. Finally, this approach also has advantages for readability and reproducibility, because the model description is fully explicit, and because it can be automatically parsed and transformed into formatted descriptions. The presented approach has been implemented in the Brian2 simulator.},
  file = {/Users/qualia/Documents/Papers/Stimberg et al. - 2014 - Equation-oriented specification of neural models f.pdf},
  journal = {Frontiers in Neuroinformatics},
  language = {en}
}

@article{Stimberg2017,
  title = {Modeling Neuron\textendash{}Glia Interactions with the                          Simulator},
  author = {Stimberg, Marcel and Goodman, Dan F. M. and Brette, Romain and De Pitt{\`a}, Maurizio},
  year = {2017},
  month = oct,
  doi = {10.1101/198366},
  abstract = {Despite compelling evidence that glial cells could crucially regulate neural network activity, the vast majority of available neural simulators ignores the possible contribution of glia to neuronal physiology. Here, we show how to model glial physiology and neuron-glia interactions in the Brian 2 simulator. Brian 2 offers facilities to explicitly describe any model in mathematical terms with limited and simple simulator-specific syntax, automatically generating high-performance code from the user-provided descriptions. The flexibility of this approach allows us to model not only networks of neurons, but also individual glial cells, electrical coupling of glial cells, and the interaction between glial cells and synapses. We therefore conclude that Brian 2 provides an ideal platform to efficiently simulate glial physiology, and specifically, the influence of astrocytes on neural activity.},
  file = {/Users/qualia/Documents/Papers/Stimberg et al. - 2017 - Modeling neuron–glia interactions with the        .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Stocchi2005,
  title = {Intermittent vs {{Continuous Levodopa Administration}} in {{Patients With Advanced Parkinson Disease}}: {{A Clinical}} and {{Pharmacokinetic Study}}},
  shorttitle = {Intermittent vs {{Continuous Levodopa Administration}} in {{Patients With Advanced Parkinson Disease}}},
  author = {Stocchi, Fabrizio and Vacca, Laura and Ruggieri, Stefano and Olanow, C. Warren},
  year = {2005},
  month = jun,
  volume = {62},
  issn = {0003-9942},
  doi = {10.1001/archneur.62.6.905},
  file = {/Users/qualia/Documents/Papers/2013 - Study - in Patients With Advanced Parkinson Disease.pdf;/Users/qualia/Documents/Papers/Stocchi et al. - 2005 - Intermittent vs Continuous Levodopa Administration.pdf},
  journal = {Archives of Neurology},
  language = {en},
  number = {6}
}

@article{Stokes2009,
  title = {Top-{{Down Activation}} of {{Shape}}-{{Specific Population Codes}} in {{Visual Cortex}} during {{Mental Imagery}}},
  author = {Stokes, M. and Thompson, R. and Cusack, R. and Duncan, J.},
  year = {2009},
  month = feb,
  volume = {29},
  pages = {1565--1572},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4657-08.2009},
  file = {/Users/qualia/Documents/Papers/2009 - Stokes et al. - Top-down activation of shape-specific population codes in visual cortex during mental imagery.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {5}
}

@article{Stokes2015,
  title = {`{{Activity}}-Silent' Working Memory in Prefrontal Cortex: A Dynamic Coding Framework},
  shorttitle = {`{{Activity}}-Silent' Working Memory in Prefrontal Cortex},
  author = {Stokes, Mark G.},
  year = {2015},
  month = jul,
  volume = {19},
  pages = {394--405},
  issn = {13646613},
  doi = {10.1016/j.tics.2015.05.004},
  file = {/Users/qualia/Documents/Papers/2015 - Stokes - ‘Activity-silent’ working memory in prefrontal cortex a dynamic coding framework.pdf;/Users/qualia/Documents/Papers/Stokes - 2015 - ‘Activity-silent’ working memory in prefrontal cor.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {7}
}

@article{Storch2013,
  title = {Nonmotor Fluctuations in {{Parkinson}} Disease: {{Severity}} and Correlation with Motor Complications},
  shorttitle = {Nonmotor Fluctuations in {{Parkinson}} Disease},
  author = {Storch, A. and Schneider, C. B. and Wolz, M. and Sturwald, Y. and Nebe, A. and Odin, P. and Mahler, A. and Fuchs, G. and Jost, W. H. and Chaudhuri, K. R. and Koch, R. and Reichmann, H. and Ebersbach, G.},
  year = {2013},
  month = feb,
  volume = {80},
  pages = {800--809},
  issn = {0028-3878, 1526-632X},
  doi = {10.1212/WNL.0b013e318285c0ed},
  abstract = {Objective: To evaluate frequency, severity, and correlation of nonmotor symptoms (NMS) with motor complications in fluctuating Parkinson disease (PD).
Methods: The Multicenter NonMotor Fluctuations in PD cross-sectional study used clinical examination of 10 NMS (dysphagia, anxiety, depression, fatigue, excessive sweating, inner restlessness, pain, concentration/attention, dizziness, bladder urgency) quantified using a visual analogue scale (VAS) in motor-defined on (NMSOn) and off state (NMSOff) combined with motor assessments and self-ratings at home in 100 patients with advanced PD.
Results: All NMS except dysphagia, excessive sweating, and bladder urgency fluctuated in conjunction to motor fluctuations with more frequent and severe symptoms in off compared to on state. The proportions of patients experiencing autonomic/sensory NMS in both motor states were similar to those with these NMS exclusively in off state (ratios 0.4\textendash{}1.3), while for mental/psychic NMS the proportions with exclusive manifestation in off state were higher (ratios 1.8\textendash{}3.1). Demographic and clinical characteristics correlated neither with NMS frequency patterns and severities nor with DNMSOn/Off severities (defined as the differences of VAS scores between on and off). Severities of NMSon, NMSOff, and DNMSOn/Off did not correlate with motor function. Presence of anxiety, depression, fatigue, and pain had negative impact on health-related quality of life (HRQOL) measured by Parkinson's Disease Questionnaire\textendash{}8 scoring independent of their occurrence with respect to motor state. Fluctuations of these NMS but not of fatigue deteriorated HRQOL.
Conclusion: Patterns of NMS fluctuations are heterogeneous and complex, but psychic NMS fluctuate more frequently and severely. Demographic parameters and motor function do not correlate with NMS or nonmotor fluctuation severities in fluctuating PD. Neurology{\^a} 2013;80:800\textendash{}809},
  file = {/Users/qualia/Documents/Papers/2013 - Storch et al. - Nonmotor fluctuations in Parkinson disease Severity and correlation with motor complications.pdf;/Users/qualia/Documents/Papers/Storch et al. - 2013 - Nonmotor fluctuations in Parkinson disease Severi.pdf},
  journal = {Neurology},
  language = {en},
  number = {9}
}

@article{Strehl,
  title = {Reinforcement {{Learning}} in {{Finite MDPs}}: {{PAC Analysis}}},
  author = {Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  pages = {32},
  abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These ``PAC-MDP'' algorithms include the wellknown E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
  file = {/Users/qualia/Documents/Papers/Strehl et al. - Reinforcement Learning in Finite MDPs PAC Analysi 2.pdf},
  language = {en}
}

@article{Strehl2009,
  title = {Reinforcement {{Learning}} in {{Finite MDPs}}: {{PAC Analysis}}},
  author = {Strehl, Alexander L and Li, Lihong and Littman, Michael L},
  year = {2009},
  volume = {10},
  pages = {1--32},
  abstract = {We study the problem of learning near-optimal behavior in finite Markov Decision Processes (MDPs) with a polynomial number of samples. These ``PAC-MDP'' algorithms include the wellknown E3 and R-MAX algorithms as well as the more recent Delayed Q-learning algorithm. We summarize the current state-of-the-art by presenting bounds for the problem in a unified theoretical framework. A more refined analysis for upper and lower bounds is presented to yield insight into the differences between the model-free Delayed Q-learning and the model-based R-MAX.},
  file = {/Users/qualia/Documents/Papers/Strehl et al. - Reinforcement Learning in Finite MDPs PAC Analysi.pdf},
  journal = {Journal of Machine Learning Research},
  language = {en}
}

@article{Stringer2016,
  title = {Inhibitory Control of Shared Variability in Cortical Networks},
  author = {Stringer, Carsen and Pachitariu, Marius and Okun, Michael and Bartho, Peter and Harris, Kenneth and Latham, Peter and Sahani, Maneesh and Lesica, Nicholas},
  year = {2016},
  month = jul,
  doi = {10.1101/041103},
  abstract = {Cortical networks exhibit intrinsic dynamics that drive coordinated, large-scale fluctuations across neuronal populations and create noise correlations that impact sensory coding. To investigate the network-level mechanisms that underlie these dynamics, we developed novel computational techniques to fit a deterministic spiking network model directly to multi-neuron recordings from different species, sensory modalities, and behavioral states. The model accurately reproduced the wide variety of activity patterns in our recordings, and analysis of its parameters suggested that differences in noise correlations across recordings were due primarily to differences in the strength of feedback inhibition. Further analysis of our recordings confirmed that putative inhibitory interneurons were indeed more active during desynchronized cortical states with weak noise correlations. Our results demonstrate the power of fitting spiking network models directly to multi-neuron recordings and suggest that inhibition modulates the interactions between intrinsic dynamics and sensory inputs by controlling network stability.},
  file = {/Users/qualia/Documents/Papers/Stringer et al. - 2016 - Inhibitory control of shared variability in cortic 2.pdf;/Users/qualia/Documents/Papers/Stringer et al. - 2016 - Inhibitory control of shared variability in cortic.pdf},
  journal = {bioRxiv},
  language = {en}
}

@book{Strogatz1994,
  title = {Nonlinear Dynamics and {{Chaos}}: With Applications to Physics, Biology, Chemistry, and Engineering},
  shorttitle = {Nonlinear Dynamics and {{Chaos}}},
  author = {Strogatz, Steven H.},
  year = {1994},
  publisher = {{Addison-Wesley Pub}},
  address = {{Reading, Mass}},
  file = {/Users/qualia/Documents/Papers/Strogatz - 1994 - Nonlinear dynamics and Chaos with applications to.pdf},
  isbn = {978-0-201-54344-5},
  keywords = {Chaotic behavior in systems,Dynamics,Nonlinear theories},
  language = {en},
  lccn = {Q172.5.C45 S767 1994},
  series = {Studies in Nonlinearity}
}

@article{Strouse2017,
  title = {The {{Deterministic Information Bottleneck}}},
  author = {Strouse, Dj and Schwab, David J.},
  year = {2017},
  month = jun,
  volume = {29},
  pages = {1611--1630},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00961},
  abstract = {Lossy compression and clustering fundamentally involve a decision about what features are relevant and which are not. The information bottleneck method (IB) by Tishby, Pereira, and Bialek formalized this notion as an information-theoretic optimization problem and proposed an optimal tradeoff between throwing away as many bits as possible, and selectively keeping those that are most important. In the IB, compression is measure my mutual information. Here, we introduce an alternative formulation that replaces mutual information with entropy, which we call the deterministic information bottleneck (DIB), that we argue better captures this notion of compression. As suggested by its name, the solution to the DIB problem turns out to be a deterministic encoder, or hard clustering, as opposed to the stochastic encoder, or soft clustering, that is optimal under the IB. We compare the IB and DIB on synthetic data, showing that the IB and DIB perform similarly in terms of the IB cost function, but that the DIB significantly outperforms the IB in terms of the DIB cost function. We also empirically find that the DIB offers a considerable gain in computational efficiency over the IB, over a range of convergence parameters. Our derivation of the DIB also suggests a method for continuously interpolating between the soft clustering of the IB and the hard clustering of the DIB.},
  file = {/Users/qualia/Documents/Papers/Strouse and Schwab - 2017 - The Deterministic Information Bottleneck 2.pdf;/Users/qualia/Documents/Papers/Strouse and Schwab - 2017 - The Deterministic Information Bottleneck.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@article{Such,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms}} Are a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O and Clune, Jeff},
  pages = {15},
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of techniques that have been developed in the neuroevolution community to improve performance on RL problems. To demonstrate the latter, we show that combining DNNs with novelty search, which was designed to encourage exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g. DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA parallelizes better than ES, A3C, and DQN, and enables a state-of-the-art compact encoding technique that can represent million-parameter DNNs in thousands of bytes.},
  file = {/Users/qualia/Documents/Papers/Such et al. - Deep Neuroevolution Genetic Algorithms are a Comp.pdf},
  language = {en}
}

@article{Sui2009,
  title = {An {{ICA}}-Based Method for the Identification of Optimal {{FMRI}} Features and Components Using Combined Group-Discriminative Techniques},
  author = {Sui, Jing and Adali, T{\"u}lay and Pearlson, Godfrey D. and Calhoun, Vince D.},
  year = {2009},
  month = may,
  volume = {46},
  pages = {73--86},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.01.026},
  abstract = {Extraction of relevant features from multitask functional MRI (fMRI) data in order to identify potential biomarkers for disease, is an attractive goal. In this paper, we introduce a novel feature-based framework, which is sensitive and accurate in detecting group differences (e.g. controls vs. patients) by proposing three key ideas. First, we integrate two goal-directed techniques: coefficient-constrained independent component analysis (CC-ICA) and principal component analysis with reference (PCA-R), both of which improve sensitivity to group differences. Secondly, an automated artifact-removal method is developed for selecting components of interest derived from CC-ICA, with an average accuracy of 91\%. Finally, we propose a strategy for optimal feature/component selection, aiming to identify optimal group-discriminative brain networks as well as the tasks within which these circuits are engaged. The group-discriminating performance is evaluated on 15 fMRI feature combinations (5 single features and 10 joint features) collected from 28 healthy control subjects and 25 schizophrenia patients. Results show that a feature from a sensorimotor task and a joint feature from a Sternberg working memory (probe) task and an auditory oddball (target) task are the top two feature combinations distinguishing groups. We identified three optimal features that best separate patients from controls, including brain networks consisting of temporal lobe, default mode and occipital lobe circuits, which when grouped together provide improved capability in classifying group membership. The proposed framework provides a general approach for selecting optimal brain networks which may serve as potential biomarkers of several brain diseases and thus has wide applicability in the neuroimaging research community.},
  file = {/Users/qualia/Documents/Papers/2009 - Sui et al. - An ICA-based method for the identification of optimal FMRI features and components using combined group-discriminati.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {1}
}

@article{Sumner2019,
  title = {The {{Exploration Advantage}}: {{Children}}'s Instinct to Explore Allows Them to Find Information That Adults Miss},
  author = {Sumner, Emily S and Li, Amy X and Perfors, Amy and Hayes, Brett K and Navarro, Danielle J and Sarnecka, Barbara W},
  year = {2019},
  volume = {h437v},
  pages = {11},
  abstract = {Humans have a long childhood in comparison to all other species. Across disciplines, researchers agree that humans' prolonged immaturity is integral to our unique intelligence. The studies presented here support the hypothesis that human beings' extended childhood pays off in the form of an ability to learn more about changing environments. Across two studies (n = 213), children and adults played a game where they chose among four different cartoon monsters yielding different numbers of star rewards. Adults focused on maximizing reward, while children chose to explore longer, even at the cost of earning fewer stars. As a result, adults won significantly more stars than children did. However, in the `dynamic' version of the task, the rewards given out by the monsters changed halfway through: the monster that had been giving out the fewest stars began giving out the most. Because children continued to explore whereas adults ignored the low-reward monster, children were much more likely than adults to detect the change. This illustrates that while exploration may be costly in the short term, it leads to a more flexible understanding of the world in the long term, particularly when that world is changing.},
  file = {/Users/qualia/Documents/Papers/Sumner et al. - The Exploration Advantage.pdf},
  journal = {PsyArxiv},
  language = {en}
}

@article{Sun2011,
  title = {Gamma Oscillations in Schizophrenia: {{Mechanisms}} and Clinical Significance},
  shorttitle = {Gamma Oscillations in Schizophrenia},
  author = {Sun, Yinming and Farzan, Faranak and Barr, Mera S. and Kirihara, Kenji and Fitzgerald, Paul B. and Light, Gregory A. and Daskalakis, Zafiris J.},
  year = {2011},
  month = sep,
  volume = {1413},
  pages = {98--114},
  issn = {00068993},
  doi = {10.1016/j.brainres.2011.06.065},
  abstract = {Brain oscillations are increasingly used for understanding complex psychiatric disorders. Gamma (30\textendash{}50 Hz) oscillations have warranted special attention due to their omnipresence in cognitive tasks. For patients with schizophrenia (SCZ), a disease associated with poor cognition, abnormal gamma oscillations have been reported in many experimental paradigms. The goal of this paper is to review the literature on gamma oscillations in SCZ. The review is structured into four sections. First, the functional role, neurobiology, and analysis of brain oscillations, especially gamma oscillations will be outlined. Second, the neurobiological abnormalities of SCZ in relation to gamma oscillations will be reviewed. Third, selected paradigms for investigating irregular gamma oscillations in SCZ will be discussed in detail. Finally, a discussion on the limitations of current findings and potential future research directions will be provided. The reviewed evidence suggests that gamma oscillations are disrupted in SCZ and could account for cognitive disturbances in this disorder. With additional analysis and experimentation, these indices may ultimately serve as endophenotypes that facilitate the development of etiologically based diagnostic methods, foster early identification and treatment, and advance our understanding of the complex genetic mechanisms involved in this disorder.},
  file = {/Users/qualia/Documents/Papers/2011 - Sun et al. - Gamma oscillations in schizophrenia Mechanisms and clinical significance.pdf},
  journal = {Brain Research},
  language = {en}
}

@book{Sundstrom2014,
  title = {Mathematical {{Reasoning}}: {{Writing}} and {{Proof}}},
  author = {Sundstrom, Ted},
  year = {2014},
  edition = {1.1},
  publisher = {{Pearson Education}},
  file = {/Users/qualia/Documents/Papers/Sundstrom - Mathematical Reasoning Writing and Proof.pdf},
  language = {en}
}

@article{Sussillo2009,
  title = {Generating {{Coherent Patterns}} of {{Activity}} from {{Chaotic Neural Networks}}},
  author = {Sussillo, David and Abbott, L.F.},
  year = {2009},
  month = aug,
  volume = {63},
  pages = {544--557},
  issn = {08966273},
  doi = {10.1016/j.neuron.2009.07.018},
  abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
  file = {/Users/qualia/Documents/Papers/2009 - Sussillo, Abbott - Generating Coherent Patterns of Activity from Chaotic Neural Networks.pdf},
  journal = {Neuron},
  language = {en},
  number = {4}
}

@article{Sussillo2013,
  title = {Opening the {{Black Box}}: {{Low}}-{{Dimensional Dynamics}} in {{High}}-{{Dimensional Recurrent Neural Networks}}},
  shorttitle = {Opening the {{Black Box}}},
  author = {Sussillo, David and Barak, Omri},
  year = {2013},
  month = mar,
  volume = {25},
  pages = {626--649},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00409},
  file = {/Users/qualia/Documents/Papers/2013 - Sussillo, Barak - Opening the black box low-dimensional dynamics in high-dimensional recurrent neural networks.pdf;/Users/qualia/Documents/Papers/Sussillo and Barak - 2013 - Opening the Black Box Low-Dimensional Dynamics in.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {3}
}

@article{Sussillo2015,
  title = {A Neural Network That Finds a Naturalistic Solution for the Production of Muscle Activity},
  author = {Sussillo, David and Churchland, Mark M and Kaufman, Matthew T and Shenoy, Krishna V},
  year = {2015},
  month = jul,
  volume = {18},
  pages = {1025--1033},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4042},
  file = {/Users/qualia/Documents/Papers/2015 - Sussillo et al. - A neural network that finds a naturalistic solution for the production of muscle activity.pdf;/Users/qualia/Documents/Papers/Sussillo et al. - 2015 - A neural network that finds a naturalistic solutio.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {7}
}

@article{Sutton,
  title = {{{IntegraBteadseAd rocnhiAtepctpurroexsimfoartiLnegaDrnyinnga}},{{mPiclaPnrnoingrga}}, {{manmdinRgeacting}}},
  author = {Sutton, Richard S},
  pages = {9},
  abstract = {This paper extends previous work with Dyna, a class of architectures for intelligent systems based on approximating dynamic programming methods. Dyna architectures integrate trial-and-error (reinforcement) learning and execution-time planning into a single process operating alternately on the world and on a learned model of the world. In this paper, I present and show results for two Dyna architectures. The Dyna-PI architecture is based on dynamic programming's policy iteration method and can be related to existing AI ideas such as evaluation functions and universal plans (reactive systems). Using a navigation task, results are shown for a simple Dyna-PI system that simultaneously learns by trial and error, learns a world model, and plans optimal routes using the evolving world model. The Dyna-Q architecture is based on Watkins's Q-learning, a new kind of reinforcement learning. Dyna-Q uses a less familiar set of data structures than does Dyna-PI, but is arguably simpler to implement and use. We show that Dyna-Q architectures are easy to adapt for use in changing environments.},
  file = {/Users/qualia/Documents/Papers/1990 - Sutton - Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming.pdf},
  language = {en}
}

@book{Sutton1998,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {/Users/qualia/Documents/Papers/Sutton and Barto - 1998 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-19398-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .S88 1998},
  series = {Adaptive Computation and Machine Learning}
}

@book{Sutton2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  file = {/Users/qualia/Documents/Papers/Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{Suttona,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  pages = {7},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  file = {/Users/qualia/Documents/Papers/1999 - Sutton et al. - Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf;/Users/qualia/Documents/Papers/Sutton et al. - Policy Gradient Methods for Reinforcement Learning.pdf},
  language = {en}
}

@article{Suzuki2011,
  title = {Astrocyte-{{Neuron Lactate Transport Is Required}} for {{Long}}-{{Term Memory Formation}}},
  author = {Suzuki, Akinobu and Stern, Sarah A. and Bozdagi, Ozlem and Huntley, George W. and Walker, Ruth H. and Magistretti, Pierre J. and Alberini, Cristina M.},
  year = {2011},
  month = mar,
  volume = {144},
  pages = {810--823},
  issn = {00928674},
  doi = {10.1016/j.cell.2011.02.018},
  abstract = {We report that, in the rat hippocampus, learning leads to a significant increase in extracellular lactate levels that derive from glycogen, an energy reserve selectively localized in astrocytes. Astrocytic glycogen breakdown and lactate release are essential for long-term but not short-term memory formation, and for the maintenance of long-term potentiation (LTP) of synaptic strength elicited in vivo. Disrupting the expression of the astrocytic lactate transporters monocarboxylate transporter 4 (MCT4) or MCT1 causes amnesia, which, like LTP impairment, is rescued by L-lactate but not equicaloric glucose. Disrupting the expression of the neuronal lactate transporter MCT2 also leads to amnesia that is unaffected by either L-lactate or glucose, suggesting that lactate import into neurons is necessary for long-term memory. Glycogenolysis and astrocytic lactate transporters are also critical for the induction of molecular changes required for memory formation, including the induction of phospho-CREB, Arc, and phospho-cofilin. We conclude that astrocyte-neuron lactate transport is required for long-term memory formation.},
  file = {/Users/qualia/Documents/Papers/Suzuki et al. - 2011 - Astrocyte-Neuron Lactate Transport Is Required for.pdf},
  journal = {Cell},
  language = {en},
  number = {5}
}

@article{Swisher2010,
  title = {Multiscale {{Pattern Analysis}} of {{Orientation}}-{{Selective Activity}} in the {{Primary Visual Cortex}}},
  author = {Swisher, J. D. and Gatenby, J. C. and Gore, J. C. and Wolfe, B. A. and Moon, C.-H. and Kim, S.-G. and Tong, F.},
  year = {2010},
  month = jan,
  volume = {30},
  pages = {325--330},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4811-09.2010},
  file = {/Users/qualia/Documents/Papers/2010 - Swisher et al. - Multiscale pattern analysis of orientation-selective activity in the primary visual cortex.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {1}
}

@article{Szegedy2013,
  title = {Intriguing Properties of Neural Networks},
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  year = {2013},
  month = dec,
  abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.},
  archivePrefix = {arXiv},
  eprint = {1312.6199},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2013 - Szegedy, Zaremba, Sutskever - Intriguing properties of neural networks.pdf;/Users/qualia/Documents/Papers/Szegedy et al. - 2013 - Intriguing properties of neural networks.pdf},
  journal = {arXiv:1312.6199 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Szepesvari2010,
  title = {Algorithms for {{Reinforcement Learning}}},
  author = {Szepesv{\'a}ri, Csaba},
  year = {2010},
  month = jan,
  volume = {4},
  pages = {1--103},
  issn = {1939-4608, 1939-4616},
  doi = {10.2200/S00268ED1V01Y201005AIM009},
  file = {/Users/qualia/Documents/Papers/2010 - Szepesvári - Algorithms for reinforcement learning.pdf},
  journal = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  language = {en},
  number = {1}
}

@article{Tachet2017,
  title = {Scaling {{Law}} of {{Urban Ride Sharing}}},
  author = {Tachet, R. and Sagarra, O. and Santi, P. and Resta, G. and Szell, M. and Strogatz, S. H. and Ratti, C.},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/srep42868},
  file = {/Users/qualia/Documents/Papers/Tachet et al. - 2017 - Scaling Law of Urban Ride Sharing.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Tachibana2011,
  title = {Subthalamo-Pallidal Interactions Underlying Parkinsonian Neuronal Oscillations in the Primate Basal Ganglia: {{BG}} Oscillations in {{Parkinson}}'s Disease},
  shorttitle = {Subthalamo-Pallidal Interactions Underlying Parkinsonian Neuronal Oscillations in the Primate Basal Ganglia},
  author = {Tachibana, Yoshihisa and Iwamuro, Hirokazu and Kita, Hitoshi and Takada, Masahiko and Nambu, Atsushi},
  year = {2011},
  month = nov,
  volume = {34},
  pages = {1470--1484},
  issn = {0953816X},
  doi = {10.1111/j.1460-9568.2011.07865.x},
  abstract = {Parkinson's disease is characterized by degeneration of nigral dopaminergic neurons, leading to a wide variety of psychomotor dysfunctions. Accumulated evidence suggests that abnormally synchronized oscillations in the basal ganglia contribute to the expression of parkinsonian motor symptoms. However, the mechanism that generates abnormal oscillations in a dopamine-depleted state remains poorly understood. We addressed this question by examining basal ganglia neuronal activity in two 1-methyl-4-phenyl1,2,3,6-tetrahydropyridine-treated parkinsonian monkeys. We found that systemic administration of l-3,4-dihydroxyphenylalanine (l-DOPA; dopamine precursor) decreased abnormal neuronal oscillations (8\textendash{}15 Hz) in the internal segment of the globus pallidus (GPi) and the subthalamic nucleus (STN) during the ON state when parkinsonian signs were alleviated and during l-DOPA-induced dyskinesia. GPi oscillations and parkinsonian signs were suppressed by silencing of the STN with infusion of muscimol (GABAA receptor agonist). Intrapallidal microinjection of a mixture of 3-(2-carboxypiperazin-4-yl)-propyl-1-phosphonic acid (CPP; N-methyl-daspartate receptor antagonist) and 1,2,3,4-tetrahydro-6-nitro-2,3-dioxo-benzo[f]quinoxaline-7-sulfonamide (NBQX; AMPA {$\fracslash$} kainate receptor antagonist) also decreased the oscillations in the GPi and the external segment of the globus pallidus (GPe). Neuronal oscillations in the STN were suppressed after intrasubthalamic microinjection of CPP {$\fracslash$} NBQX to block glutamatergic afferents of the STN. The STN oscillations were further reduced by muscimol inactivation of the GPe to block GABAergic inputs from the GPe. These results suggest that, in the dopamine-depleted state, glutamatergic inputs to the STN and reciprocal GPe\textendash{}STN interconnections are both important for the generation and amplification of the oscillatory activity of STN neurons, which is subsequently transmitted to the GPi, thus contributing to the symptomatic expression of Parkinson's disease.},
  file = {/Users/qualia/Documents/Papers/2011 - Tachibana et al. - Subthalamo-pallidal interactions underlying parkinsonian neuronal oscillations in the primate basal ganglia.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {9}
}

@article{Tan2016,
  title = {Decoding Gripping Force Based on Local Field Potentials Recorded from Subthalamic Nucleus in Humans},
  author = {Tan, Huiling and Pogosyan, Alek and Ashkan, Keyoumars and Green, Alexander L and Aziz, Tipu and Foltynie, Thomas and Limousin, Patricia and Zrinzo, Ludvic and Hariz, Marwan and Brown, Peter},
  year = {2016},
  month = nov,
  volume = {5},
  issn = {2050-084X},
  doi = {10.7554/eLife.19089},
  file = {/Users/qualia/Documents/Papers/Tan et al. - 2016 - Decoding gripping force based on local field poten.pdf},
  journal = {eLife},
  language = {en}
}

@article{Tang,
  title = {Control of {{Dynamics}} in {{Brain Networks}}},
  author = {Tang, Evelyn and Bassett, Danielle S},
  pages = {21},
  file = {/Users/qualia/Documents/Papers/Tang and Bassett - Control of Dynamics in Brain Networks.pdf},
  language = {en}
}

@article{Tao2010,
  title = {Outliers in the Spectrum of Iid Matrices with Bounded Rank Perturbations},
  author = {Tao, Terence},
  year = {2010},
  month = dec,
  abstract = {It is known that if one perturbs a large iid random matrix by a bounded rank error, then the majority of the eigenvalues will remain distributed according to the circular law. However, the bounded rank perturbation may also create one or more outlier eigenvalues. We show that if the perturbation is small, then the outlier eigenvalues are created next to the outlier eigenvalues of the bounded rank perturbation; but if the perturbation is large, then many more outliers can be created, and their law is governed by the zeroes of a random Laurent series with Gaussian coefficients. On the other hand, these outliers may be eliminated by enforcing a row sum condition on the final matrix.},
  archivePrefix = {arXiv},
  eprint = {1012.4818},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2013 - Tao - Outliers in the spectrum of iid matrices with bounded rank perturbations.pdf;/Users/qualia/Documents/Papers/Tao - 2010 - Outliers in the spectrum of iid matrices with boun.pdf},
  journal = {arXiv:1012.4818 [math]},
  keywords = {60B20,Mathematics - Probability},
  language = {en},
  primaryClass = {math}
}

@article{Tasic2017,
  title = {Shared and Distinct Transcriptomic Cell Types across Neocortical Areas},
  author = {Tasic, Bosiljka and Yao, Zizhen and Smith, Kimberly A and Graybuck, Lucas and Nguyen, Thuc Nghi and Bertagnolli, Darren and Goldy, Jeff and Garren, Emma and Economo, Michael N and Viswanathan, Sarada and Penn, Osnat and Bakken, Trygve and Menon, Vilas and Miller, Jeremy A and Fong, Olivia and Hirokawa, Karla E and Lathia, Kanan and Rimorin, Christine and Tieu, Michael and Larsen, Rachael and Casper, Tamara and Barkan, Eliza and Kroll, Matthew and Parry, Seana and Shapovalova, Nadiya V and Hirchstein, Daniel and Pendergraft, Julie and Kim, Tae Kyung and Szafer, Aaron and Dee, Nick and Groblewski, Peter and Wickersham, Ian and Cetin, Ali and Harris, Julie A and Levi, Boaz P and Sunkin, Susan M and Madisen, Linda and Daigle, Tanya L and Looger, Loren and Bernard, Amy and Phillips, John and Lein, Ed and Hawrylycz, Michael and Svoboda, Karel and Jones, Allan R and Koch, Christof and Zeng, Hongkui},
  year = {2017},
  month = dec,
  doi = {10.1101/229542},
  abstract = {Neocortex contains a multitude of cell types segregated into layers and functionally distinct regions. To investigate the diversity of cell types across the mouse neocortex, we analyzed 12,714 cells from the primary visual cortex (VISp), and 9,035 cells from the anterior lateral motor cortex (ALM) by deep single-cell RNA-sequencing (scRNA-seq), identifying 116 transcriptomic cell types. These two regions represent distant poles of the neocortex and perform distinct functions. We define 50 inhibitory transcriptomic cell types, all of which are shared across both cortical regions. In contrast, 49 of 52 excitatory transcriptomic types were found in either VISp or ALM, with only three present in both. By combining single cell RNA-seq and retrograde labeling, we demonstrate correspondence between excitatory transcriptomic types and their region-specific long-range target specificity. This study establishes a combined transcriptomic and projectional taxonomy of cortical cell types from functionally distinct regions of the mouse cortex.},
  file = {/Users/qualia/Documents/Papers/Tasic et al. - 2017 - Shared and distinct transcriptomic cell types acro.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Taylor1975,
  title = {Discriminability and the Contrafreeloading Phenomenon.},
  author = {Taylor, George T.},
  year = {1975},
  volume = {88},
  pages = {104--109},
  issn = {0021-9940},
  doi = {10.1037/h0076222},
  file = {/Users/qualia/Documents/Papers/Taylor - 1975 - Discriminability and the contrafreeloading phenome.pdf},
  journal = {Journal of Comparative and Physiological Psychology},
  language = {en},
  number = {1}
}

@article{Teglas2011,
  title = {Pure {{Reasoning}} in 12-{{Month}}-{{Old Infants}} as {{Probabilistic Inference}}},
  author = {Teglas, E. and Vul, E. and Girotto, V. and Gonzalez, M. and Tenenbaum, J. B. and Bonatti, L. L.},
  year = {2011},
  month = may,
  volume = {332},
  pages = {1054--1059},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1196404},
  file = {/Users/qualia/Documents/Papers/Teglas et al. - 2011 - Pure Reasoning in 12-Month-Old Infants as Probabil.pdf},
  journal = {Science},
  language = {en},
  number = {6033}
}

@article{Tenenbaum2006,
  title = {Theory-Based {{Bayesian}} Models of Inductive Learning and Reasoning},
  author = {Tenenbaum, Joshua B. and Griffiths, Thomas L. and Kemp, Charles},
  year = {2006},
  month = jul,
  volume = {10},
  pages = {309--318},
  issn = {13646613},
  doi = {10.1016/j.tics.2006.05.009},
  file = {/Users/qualia/Documents/Papers/Tenenbaum et al. - 2006 - Theory-based Bayesian models of inductive learning.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {7}
}

@article{Terman2002,
  title = {Activity {{Patterns}} in a {{Model}} for the {{Subthalamopallidal Network}} of the {{Basal Ganglia}}},
  author = {Terman, D. and Rubin, J. E. and Yew, A. C. and Wilson, C. J.},
  year = {2002},
  month = apr,
  volume = {22},
  pages = {2963--2976},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.22-07-02963.2002},
  file = {/Users/qualia/Documents/Papers/2002 - Terman et al. - Activity patterns in a model for the subthalamopallidal network of the basal ganglia.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Thalmeier2016,
  title = {Learning Universal Computations with Spikes},
  author = {Thalmeier, Dominik and Uhlmann, Marvin and Kappen, Hilbert J. and Memmesheimer, Raoul-Martin},
  year = {2016},
  month = jun,
  volume = {12},
  pages = {e1004895},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004895},
  abstract = {Providing the neurobiological basis of information processing in higher animals, spiking neural networks must be able to learn a variety of complicated computations, including the generation of appropriate, possibly delayed reactions to inputs and the self-sustained generation of complex activity patterns, e.g. for locomotion. Many such computations require previous building of intrinsic world models. Here we show how spiking neural networks may solve these different tasks. Firstly, we derive constraints under which classes of spiking neural networks lend themselves to substrates of powerful general purpose computing. The networks contain dendritic or synaptic nonlinearities and have a constrained connectivity. We then combine such networks with learning rules for outputs or recurrent connections. We show that this allows to learn even difficult benchmark tasks such as the self-sustained generation of desired low-dimensional chaotic dynamics or memory-dependent computations. Furthermore, we show how spiking networks can build models of external world systems and use the acquired knowledge to control them.},
  archivePrefix = {arXiv},
  eprint = {1505.07866},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2015 - Thalmeier et al. - Learning universal computations with spikes.pdf;/Users/qualia/Documents/Papers/Thalmeier et al. - 2016 - Learning universal computations with spikes.pdf},
  journal = {PLOS Computational Biology},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {6}
}

@article{Thirion2006,
  title = {Inverse Retinotopy: {{Inferring}} the Visual Content of Images from Brain Activation Patterns},
  shorttitle = {Inverse Retinotopy},
  author = {Thirion, Bertrand and Duchesnay, Edouard and Hubbard, Edward and Dubois, Jessica and Poline, Jean-Baptiste and Lebihan, Denis and Dehaene, Stanislas},
  year = {2006},
  month = dec,
  volume = {33},
  pages = {1104--1116},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2006.06.062},
  file = {/Users/qualia/Documents/Papers/2006 - Thirion et al. - Inverse retinotopy inferring the visual content of images from brain activation patterns.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Thomson2003,
  title = {Interlaminar {{Connections}} in the {{Neocortex}}},
  author = {Thomson, A. M.},
  year = {2003},
  month = jan,
  volume = {13},
  pages = {5--14},
  issn = {14602199},
  doi = {10.1093/cercor/13.1.5},
  file = {/Users/qualia/Documents/Papers/2003 - Thomson, Bannister - Interlaminar connections in the neocortex.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {1}
}

@article{Thomson2010,
  title = {Neocortical Layer 6, a Review},
  author = {{Thomson}},
  year = {2010},
  issn = {16625129},
  doi = {10.3389/fnana.2010.00013},
  abstract = {This review attempts to summarise some of the major areas of neocortical research as it pertains to neocortical layer 6. After a brief summary of the development of this intriguing layer, the major pyramidal cell classes to be found in layer 6 are described and compared.The connections made and received by these different classes of neurones are then discussed and the possible functions of these connections, with particular reference to the shaping of responses in visual cortex and thalamus. Inhibition in layer 6 is discussed where appropriate, but not in great detail. Many types of interneurones are to be found in each cortical layer and layer 6 is no exception, but the functions of each type remain to be elucidated (Gonchar et al., 2007).},
  file = {/Users/qualia/Documents/Papers/2010 - Thomson - Neocortical layer 6, a review.pdf},
  journal = {Frontiers in Neuroanatomy},
  language = {en}
}

@article{Thrun1992,
  title = {Eficient {{Exploration In Reinforcement Learning}}},
  author = {Thrun, Sebastian B},
  year = {1992},
  pages = {44},
  abstract = {Exploration plays a fundamental role in any active learning system. This study evaluates the role of exploration in active learning and describes several local techniques for exploration in nite, discrete domains, embedded in a reinforcement learning framework (delayed reinforcement).},
  file = {/Users/qualia/Documents/Papers/Thrun - E cient Exploration In Reinforcement Learning.pdf},
  journal = {NIPS},
  language = {en}
}

@article{Thrun1992a,
  title = {Active {{Exploration}} in {{Dynamic Environments}}},
  author = {Thrun, Sebastian and M{\"o}ller, Knut},
  year = {1992},
  pages = {531--538},
  abstract = {Vhenever an agent learns to control an unknown environment, two opposing principles have to be combined, namely: exploration (long-term optimization) and exploitation (short-term optimization). Many real-valued connectionist approaches to learning control realize exploration by randomness in action selection. This might be disadvantageous when costs are assigned to "negative experiences" . The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner. This is achieved by a so-called competence map, which is trained to predict the controller's accuracy, and is used for guiding exploration. Based on this, a bistable system enables smoothly switching attention between two behaviors - exploration and exploitation - depending on expected costs and knowledge gain.},
  file = {/Users/qualia/Documents/Papers/Thrun and Möller - Active Exploration in Dynamic Environments.pdf},
  journal = {Advances in neural information processing systems},
  language = {en}
}

@article{Thrun1992b,
  title = {Efficient {{Exploration In Reinforcement Learning}}},
  author = {Thrun, Sebastian B},
  year = {1992},
  pages = {1--44},
  abstract = {Exploration plays a fundamental role in any active learning system. This study evaluates the role of exploration in active learning and describes several local techniques for exploration in nite, discrete domains, embedded in a reinforcement learning framework (delayed reinforcement).},
  file = {/Users/qualia/Documents/Papers/Thrun - E cient Exploration In Reinforcement Learning 2.pdf},
  journal = {Technical Report},
  language = {en}
}

@article{Thruna,
  title = {Exploration in {{Active Learning}}},
  author = {Thrun, Sebastian},
  pages = {10},
  file = {/Users/qualia/Documents/Papers/Thrun - Exploration in Active Learning.pdf},
  language = {en}
}

@article{Thura2017,
  title = {The {{Basal Ganglia Do Not Select Reach Targets}} but {{Control}} the {{Urgency}} of {{Commitment}}},
  author = {Thura, David and Cisek, Paul},
  year = {2017},
  month = aug,
  volume = {95},
  pages = {1160-1170.e5},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.07.039},
  abstract = {Prominent theories of decision making suggest that the basal ganglia (BG) play a causal role in deliberation between action choices. An alternative hypothesis is that deliberation occurs in cortical regions, while the BG control the speed-accuracy trade-off (SAT) between committing to a choice versus continuing to deliberate. Here, we test these hypotheses by recording activity in the internal and external segments of the globus pallidus (GPi/GPe) while monkeys perform a task dissociating the process of deliberation, the moment of commitment, and adjustment of the SAT. Our data suggest that unlike premotor and motor cortical regions, pallidal output does not contribute to the process of deliberation but instead provides a time-varying signal that controls the SAT and reflects the growing urgency to commit to a choice. Once a target is selected by cortical regions, GP activity confirms commitment to the decision and invigorates the subsequent movement.},
  file = {/Users/qualia/Documents/Papers/Thura and Cisek - 2017 - The Basal Ganglia Do Not Select Reach Targets but .pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Thut2006,
  title = {-{{Band Electroencephalographic Activity}} over {{Occipital Cortex Indexes Visuospatial Attention Bias}} and {{Predicts Visual Target Detection}}},
  author = {Thut, G.},
  year = {2006},
  month = sep,
  volume = {26},
  pages = {9494--9502},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0875-06.2006},
  file = {/Users/qualia/Documents/Papers/2006 - Thut - -Band Electroencephalographic Activity over Occipital Cortex Indexes Visuospatial Attention Bias and Predicts Visual Targe.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {37}
}

@article{Tian2010,
  title = {Cortical Depth-Specific Microvascular Dilation Underlies Laminar Differences in Blood Oxygenation Level-Dependent Functional {{MRI}} Signal},
  author = {Tian, P. and Teng, I. C. and May, L. D. and Kurz, R. and Lu, K. and Scadeng, M. and Hillman, E. M. C. and De Crespigny, A. J. and D'Arceuil, H. E. and Mandeville, J. B. and Marota, J. J. A. and Rosen, B. R. and Liu, T. T. and Boas, D. A. and Buxton, R. B. and Dale, A. M. and Devor, A.},
  year = {2010},
  month = aug,
  volume = {107},
  pages = {15246--15251},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1006735107},
  file = {/Users/qualia/Documents/Papers/2010 - Tian et al. - Cortical depth-specific microvascular dilation underlies laminar differences in blood oxygenation level-dependent f.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {34}
}

@article{Tiesinga2001,
  title = {Optimal Information Transfer in Synchronized Neocortical Neurons},
  author = {Tiesinga, P.H.E. and Fellous, J.-M. and Jos{\'e}, J.V. and Sejnowski, T.J.},
  year = {2001},
  month = jun,
  volume = {38-40},
  pages = {397--402},
  issn = {09252312},
  doi = {10.1016/S0925-2312(01)00464-7},
  abstract = {The output precision and information transnl\textasciitilde{}ssionwas studied in a model neocortical neuron that was driven by a periodic presynaptic spike train w \textasciitilde{} t ha variable number of inhibitory inputs on each cycle. Spike-timing precision was maintained during feedforward propagation during entrainment. The range of presynaptic firing rates and precision for entrainment was determined. During entrainment the Shannon information of the output spike phase was reduced but the amount of information the neuron transmitted about the synaptic input was increased. We quantify how robust information transmission is against intrinsic neuronal noise. We propose how neurotnodulation, via entrainment, can regulate the information transfer in neocortical networks. 0 2001 Elsevier Science B.V. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/Tiesinga et al. - 2001 - Optimal information transfer in synchronized neoco.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{Tiesinga2008,
  title = {Regulation of Spike Timing in Visual Cortical Circuits},
  author = {Tiesinga, Paul and Fellous, Jean-Marc and Sejnowski, Terrence J.},
  year = {2008},
  month = feb,
  volume = {9},
  pages = {97--107},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn2315},
  abstract = {A train of action potentials (a spike train) can carry information in both the average firing rate and the pattern of spikes in the train. But can such a spike-pattern code be supported by cortical circuits? Neurons in vitro produce a spike pattern in response to the injection of a fluctuating current. However, cortical neurons in vivo are modulated by local oscillatory neuronal activity and by top-down inputs. In a cortical circuit, precise spike patterns thus reflect the interaction between internally generated activity and sensory information encoded by input spike trains. We review the evidence for precise and reliable spike timing in the cortex and discuss its computational role.},
  file = {/Users/qualia/Documents/Papers/2008 - Tiesinga, Fellous, Sejnowski - Regulation of spike timing in visual cortical circuits.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {2}
}

@article{Tiesinga2010,
  title = {Mechanisms for {{Phase Shifting}} in {{Cortical Networks}} and Their {{Role}} in {{Communication}} through {{Coherence}}},
  author = {Tiesinga, Paul H. and Sejnowski, Terrence J.},
  year = {2010},
  volume = {4},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2010.00196},
  abstract = {In the primate visual cortex, the phase of spikes relative to oscillations in the local field potential (LFP) in the gamma frequency range (30\textendash{}80 Hz) can be shifted by stimulus features such as orientation and thus the phase may carry information about stimulus identity. According to the principle of communication through coherence (CTC), the relative LFP phase between the LFPs in the sending and receiving circuits affects the effectiveness of the transmission. CTC predicts that phase shifting can be used for stimulus selection. We review and investigate phase shifting in models of periodically driven single neurons and compare it with phase shifting in models of cortical networks. In a single neuron, as the driving current is increased, the spike phase varies systematically while the firing rate remains constant. In a network model of reciprocally connected excitatory (E) and inhibitory (I) cells phase shifting occurs in response to both injection of constant depolarizing currents and to brief pulses to I cells. These simple models provide an account for phase-shifting observed experimentally and suggest a mechanism for implementing CTC. We discuss how this hypothesis can be tested experimentally using optogenetic techniques.},
  file = {/Users/qualia/Documents/Papers/2010 - Tiesinga, Sejnowski - Mechanisms for Phase Shifting in Cortical Networks and their Role in Communication through Coherence.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Timms2014,
  title = {Synchronization in Phase-Coupled {{Kuramoto}} Oscillator Networks with Axonal Delay and Synaptic Plasticity},
  author = {Timms, L. and English, L. Q.},
  year = {2014},
  month = mar,
  volume = {89},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.89.032906},
  file = {/Users/qualia/Documents/Papers/2014 - Timms, English - Synchronization in phase-coupled Kuramoto oscillator networks with axonal delay and synaptic plasticity.pdf;/Users/qualia/Documents/Papers/Timms and English - 2014 - Synchronization in phase-coupled Kuramoto oscillat.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {3}
}

@article{Tingley2016,
  title = {Transformation of {{Independent Oscillatory Inputs}} into {{Temporally Precise Rate Codes}}},
  author = {Tingley, David and Alexander, Andrew and Quinn, Laleh and Chiba, Andrea and Nitz, Douglas},
  year = {2016},
  month = may,
  doi = {10.1101/054163},
  abstract = {Complex behaviors demand temporal coordination among functionally distinct brain regions. The basal forebrain's afferent and efferent structure suggests a capacity for mediating such coordination. During performance of a selective attention task, synaptic activity in this region was dominated by four amplitude-oscillations temporally organized by the phase of the slowest, a theta rhythm. Further, oscillatory amplitudes were precisely organized by task epoch and a robust input/output transform, from synchronous synaptic activity to spiking rates of basal forebrain neurons, was identified. For many neurons, spiking was temporally organized as phase precessing sequences against theta band field potential oscillations. Remarkably, theta phase precession advanced in parallel to task progression, rather than absolute spatial location or time. Together, the findings reveal a process by which associative brain regions can integrate independent oscillatory inputs and transform them into sequence-specific, rate-coded outputs that are adaptive to the pace with which organisms interact with their environment.},
  file = {/Users/qualia/Documents/Papers/2013 - Tobergte, Curtis - Transformation!of!Independent!Oscillatory!Inputs!into!Temporally!Precise!Rate!Codes'.pdf;/Users/qualia/Documents/Papers/Tingley et al. - 2016 - Transformation of Independent Oscillatory Inputs i.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tinkhauser2017,
  title = {Beta Burst Dynamics in {{Parkinson}}'s Disease {{OFF}} and {{ON}} Dopaminergic Medication},
  author = {Tinkhauser, Gerd and Pogosyan, Alek and Tan, Huiling and Herz, Damian M and K{\"u}hn, Andrea A and Brown, Peter},
  year = {2017},
  month = nov,
  volume = {140},
  pages = {2968--2981},
  issn = {0006-8950, 1460-2156},
  doi = {10.1093/brain/awx252},
  file = {/Users/qualia/Documents/Papers/Tinkhauser et al. - 2017 - Beta burst dynamics in Parkinson’s disease OFF and.pdf},
  journal = {Brain},
  language = {en},
  number = {11}
}

@article{Tishby2000,
  title = {The {{Information Bottleneck Method}}},
  author = {Tishby, Naftali and Pereira, Fernando C and Bialek, William},
  year = {2000},
  volume = {0004057},
  pages = {11},
  abstract = {We define the relevant information in a signal x {$\in$} X as being the information that this signal provides about another signal y {$\in$} Y . Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal x requires more than just predicting y, it also requires specifying which features of X play a role in the prediction. We formalize the problem as that of finding a short code for X that preserves the maximum information about Y . That is, we squeeze the information that X provides about Y through a `bottleneck' formed by a limited set of codewords X\texttildelow{} . This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure d(x, x\texttildelow{}) emerges from the joint statistics of X and Y . The approach yields an exact set of self-consistent equations for the coding rules X \textrightarrow{} X\texttildelow{} and X\texttildelow{} \textrightarrow{} Y . Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut\textendash{}Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  file = {/Users/qualia/Documents/Papers/2000 - Tishby, Pereira, Bialek - The information bottleneck method.pdf;/Users/qualia/Documents/Papers/2011 - Thomas - The Knowledge Link How Firms Compete through Strategic Alliances.pdf;/Users/qualia/Documents/Papers/Tishby et al. - The Information Bottleneck Method.pdf},
  journal = {Arxiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Nonlinear Sciences - Adaptation and Self-Organizing Systems,Physics - Data Analysis; Statistics and Probability},
  language = {en}
}

@article{Tishby2015,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  year = {2015},
  month = mar,
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
  archivePrefix = {arXiv},
  eprint = {1503.02406},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Tishby and Zaslavsky - 2015 - Deep Learning and the Information Bottleneck Princ.pdf},
  journal = {arXiv:1503.02406 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Tognoli2014,
  title = {Enlarging the Scope: Grasping Brain Complexity},
  shorttitle = {Enlarging the Scope},
  author = {Tognoli, Emmanuelle and Kelso, J. A. Scott},
  year = {2014},
  month = jun,
  volume = {8},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2014.00122},
  file = {/Users/qualia/Documents/Papers/Tognoli and Kelso - 2014 - Enlarging the scope grasping brain complexity.pdf},
  journal = {Frontiers in Systems Neuroscience},
  language = {en}
}

@article{Tognoli2014a,
  title = {The {{Metastable Brain}}},
  author = {Tognoli, Emmanuelle and Kelso, J. A. Scott},
  year = {2014},
  month = jan,
  volume = {81},
  pages = {35--48},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.12.022},
  file = {/Users/qualia/Documents/Papers/Tognoli and Kelso - 2014 - The Metastable Brain.pdf},
  journal = {Neuron},
  language = {en},
  number = {1}
}

@article{Tokdar2010,
  title = {Detection of Bursts in Extracellular Spike Trains Using Hidden Semi-{{Markov}} Point Process Models},
  author = {Tokdar, Surya and Xi, Peiyi and Kelly, Ryan C. and Kass, Robert E.},
  year = {2010},
  month = aug,
  volume = {29},
  pages = {203--212},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-009-0182-2},
  file = {/Users/qualia/Documents/Papers/2010 - Tokdar et al. - Detection of bursts in extracellular spike trains using hidden semi-Markov point process models.pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {1-2}
}

@article{Tomasello2019,
  title = {Thirty Years of Great Ape Gestures},
  author = {Tomasello, Michael and Call, Josep},
  year = {2019},
  month = jul,
  volume = {22},
  pages = {461--469},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-018-1167-1},
  abstract = {We and our colleagues have been doing studies of great ape gestural communication for more than 30 years. Here we attempt to spell out what we have learned. Some aspects of the process have been reliably established by multiple researchers, for example, its intentional structure and its sensitivity to the attentional state of the recipient. Other aspects are more controversial. We argue here that it is a mistake to assimilate great ape gestures to the species-typical displays of other mammals by claiming that they are fixed action patterns, as there are many differences, including the use of attention-getters. It is also a mistake, we argue, to assimilate great ape gestures to human gestures by claiming that they are used referentially and declaratively in a human-like manner, as apes' ``pointing'' gesture has many limitations and they do not gesture iconically. Great ape gestures constitute a unique form of primate communication with their own unique qualities.},
  file = {/Users/qualia/Documents/Papers/Tomasello and Call - 2019 - Thirty years of great ape gestures.pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {4}
}

@article{Tomasello2019a,
  title = {Thirty Years of Great Ape Gestures},
  author = {Tomasello, Michael and Call, Josep},
  year = {2019},
  month = jul,
  volume = {22},
  pages = {461--469},
  issn = {1435-9448, 1435-9456},
  doi = {10.1007/s10071-018-1167-1},
  abstract = {We and our colleagues have been doing studies of great ape gestural communication for more than 30 years. Here we attempt to spell out what we have learned. Some aspects of the process have been reliably established by multiple researchers, for example, its intentional structure and its sensitivity to the attentional state of the recipient. Other aspects are more controversial. We argue here that it is a mistake to assimilate great ape gestures to the species-typical displays of other mammals by claiming that they are fixed action patterns, as there are many differences, including the use of attention-getters. It is also a mistake, we argue, to assimilate great ape gestures to human gestures by claiming that they are used referentially and declaratively in a human-like manner, as apes' ``pointing'' gesture has many limitations and they do not gesture iconically. Great ape gestures constitute a unique form of primate communication with their own unique qualities.},
  file = {/Users/qualia/Documents/Papers/Tomasello and Call - 2019 - Thirty years of great ape gestures 2.pdf},
  journal = {Anim Cogn},
  language = {en},
  number = {4}
}

@article{Tompson2014,
  title = {Efficient {{Object Localization Using Convolutional Networks}}},
  author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
  year = {2014},
  month = nov,
  abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].},
  archivePrefix = {arXiv},
  eprint = {1411.4280},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Tompson et al. - 2014 - Efficient Object Localization Using Convolutional .pdf},
  journal = {arXiv:1411.4280 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{Tort2010,
  title = {Measuring {{Phase}}-{{Amplitude Coupling Between Neuronal Oscillations}} of {{Different Frequencies}}},
  author = {Tort, Adriano B. L. and Komorowski, Robert and Eichenbaum, Howard and Kopell, Nancy},
  year = {2010},
  month = aug,
  volume = {104},
  pages = {1195--1210},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00106.2010},
  file = {/Users/qualia/Documents/Papers/2010 - Tort et al. - Measuring phase-amplitude coupling between neuronal oscillations of different frequencies(2).pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {2}
}

@article{Tort2018,
  title = {Parallel Detection of Theta and Respiration-Coupled Oscillations throughout the Mouse Brain},
  author = {Tort, Adriano B. L. and Ponsel, Simon and Jessberger, Jakob and Yanovsky, Yevgenij and Branka{\v c}k, Jurij and Draguhn, Andreas},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-24629-z},
  file = {/Users/qualia/Documents/Papers/Tort et al. - 2018 - Parallel detection of theta and respiration-couple.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Toth2011,
  title = {Dynamical Estimation of Neuron and Network Properties {{I}}: Variational Methods},
  shorttitle = {Dynamical Estimation of Neuron and Network Properties {{I}}},
  author = {Toth, Bryan A. and Kostuk, Mark and Meliza, C. Daniel and Margoliash, Daniel and Abarbanel, Henry D. I.},
  year = {2011},
  month = oct,
  volume = {105},
  pages = {217--237},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-011-0459-1},
  file = {/Users/qualia/Documents/Papers/2011 - Toth et al. - Dynamical estimation of neuron and network properties I Variational methods.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3-4}
}

@article{Touboul2008,
  title = {Bifurcation {{Analysis}} of a {{General Class}} of {{Nonlinear Integrate}}-and-{{Fire Neurons}}},
  author = {Touboul, Jonathan},
  year = {2008},
  month = jan,
  volume = {68},
  pages = {1045--1079},
  issn = {0036-1399, 1095-712X},
  doi = {10.1137/070687268},
  abstract = {In this paper we define a class of formal neuron models being computationally efficient and biologically plausible, i.e., able to reproduce a wide range of behaviors observed in in vivo or in vitro recordings of cortical neurons. This class includes, for instance, two models widely used in computational neuroscience, the Izhikevich and the Brette\textendash{}Gerstner models. These models consist of a 4-parameter dynamical system. We provide the full local bifurcation diagram of the members of this class and show that they all present the same bifurcations: an Andronov\textendash{}Hopf bifurcation manifold, a saddle-node bifurcation manifold, a Bogdanov\textendash{}Takens bifurcation, and possibly a Bautin bifurcation, i.e., all codimension two local bifurcations in a two-dimensional phase space except the cusp. Among other global bifurcations, this system shows a saddle homoclinic bifurcation curve. We show how this bifurcation diagram generates the most prominent cortical neuron behaviors. This study leads us to introduce a new neuron model, the quartic model, able to reproduce among all the behaviors of the Izhikevich and Brette\textendash{}Gerstner models self-sustained subthreshold oscillations, which are of great interest in neuroscience.},
  file = {/Users/qualia/Documents/Papers/2008 - Touboul - Bifurcation Analysis of a General Class of Nonlinear Integrate-and-Fire Neurons.pdf},
  journal = {SIAM Journal on Applied Mathematics},
  language = {en},
  number = {4}
}

@article{Touboul2008a,
  title = {Dynamics and Bifurcations of the Adaptive Exponential Integrate-and-Fire Model},
  author = {Touboul, Jonathan and Brette, Romain},
  year = {2008},
  month = nov,
  volume = {99},
  pages = {319--334},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-008-0267-4},
  abstract = {Recently, several two-dimensional spiking neuron models have been introduced, with the aim of reproducing the diversity of electrophysiological features displayed by real neurons while keeping a simple model, for simulation and analysis purposes. Among these models, the adaptive integrate-and-fire model is physiologically relevant in that its parameters can be easily related to physiological quantities. The interaction of the differential equations with the reset results in a rich and complex dynamical structure. We relate the subthreshold features of the model to the dynamical properties of the differential system and the spike patterns to the properties of a Poincare\textasciiacute{} map defined by the sequence of spikes. We find a complex bifurcation structure which has a direct interpretation in terms of spike trains. For some parameter values, spike patterns are chaotic.},
  file = {/Users/qualia/Documents/Papers/2008 - Touboul, Brette - Dynamics and bifurcations of the adaptive exponential integrate-and-fire model.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {4-5}
}

@article{Touboul2011,
  title = {Finite-Size and Correlation-Induced Effects in Mean-Field Dynamics},
  author = {Touboul, Jonathan D. and Ermentrout, G. Bard},
  year = {2011},
  month = nov,
  volume = {31},
  pages = {453--484},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-011-0320-5},
  file = {/Users/qualia/Documents/Papers/2011 - Touboul, Ermentrout - Finite-size and correlation-induced effects in mean-field dynamics(2).pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Touboul2012,
  title = {Mean-Field Equations for Stochastic Firing-Rate Neural Fields with Delays: {{Derivation}} and Noise-Induced Transitions},
  shorttitle = {Mean-Field Equations for Stochastic Firing-Rate Neural Fields with Delays},
  author = {Touboul, Jonathan},
  year = {2012},
  month = aug,
  volume = {241},
  pages = {1223--1244},
  issn = {01672789},
  doi = {10.1016/j.physd.2012.03.010},
  abstract = {In this manuscript we analyze the collective behavior of mean-field limits of large-scale, spatially extended stochastic neuronal networks with delays. Rigorously, the asymptotic regime of such systems is characterized by a very intricate stochastic delayed integro-differential McKean\textendash{}Vlasov equation that remain impenetrable, leaving the stochastic collective dynamics of such networks poorly understood. In order to study these macroscopic dynamics, we analyze networks of firing-rate neurons, i.e. with linear intrinsic dynamics and sigmoidal interactions. In that case, we prove that the solution of the mean-field equation is Gaussian, hence characterized by its two first moments, and that these two quantities satisfy a set of coupled delayed integro-differential equations. These equations are similar to usual neural field equations, and incorporate noise levels as a parameter, allowing analysis of noise-induced transitions. We identify through bifurcation analysis several qualitative transitions due to noise in the mean-field limit. In particular, stabilization of spatially homogeneous solutions, synchronized oscillations, bumps, chaotic dynamics, wave or bump splitting are exhibited and arise from static or dynamic Turing\textendash{}Hopf bifurcations. These surprising phenomena allow further exploring the role of noise in the nervous system. \textcopyright{} 2012 Elsevier B.V. All rights reserved.},
  file = {/Users/qualia/Documents/Papers/2012 - Touboul - Mean-field equations for stochastic firing-rate neural fields with delays Derivation and noise-induced transitions.pdf},
  journal = {Physica D: Nonlinear Phenomena},
  language = {en},
  number = {15}
}

@article{Touboul2017,
  title = {Power-Law Statistics and Universal Scaling in the Absence of Criticality},
  author = {Touboul, Jonathan and Destexhe, Alain},
  year = {2017},
  month = jan,
  volume = {95},
  pages = {012413},
  issn = {2470-0045, 2470-0053},
  doi = {10.1103/PhysRevE.95.012413},
  abstract = {Critical states are sometimes identified experimentally through power-law statistics or universal scaling functions. We show here that such features naturally emerge from networks in self-sustained irregular regimes away from criticality. In these regimes, statistical physics theory of large interacting systems predict a regime where the nodes have independent and identically distributed dynamics. We thus investigated the statistics of a system in which units are replaced by independent stochastic surrogates, and found the same power-law statistics, indicating that these are not sufficient to establish criticality. We rather suggest that these are universal features of large-scale networks when considered macroscopically. These results put caution on the interpretation of scaling laws found in nature.},
  archivePrefix = {arXiv},
  eprint = {1503.08033},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Touboul and Destexhe - 2017 - Power-law statistics and universal scaling in the .pdf},
  journal = {Phys. Rev. E},
  keywords = {Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {1}
}

@article{Toupo2014,
  title = {Limit {{Cycles Sparked}} by {{Mutation}} in the {{Repeated Prisoner}}'s {{Dilemma}}},
  author = {Toupo, Danielle F. P. and Rand, David G. and Strogatz, Steven H.},
  year = {2014},
  month = dec,
  volume = {24},
  pages = {1430035},
  issn = {0218-1274, 1793-6551},
  doi = {10.1142/S0218127414300353},
  file = {/Users/qualia/Documents/Papers/Toupo et al. - 2014 - Limit Cycles Sparked by Mutation in the Repeated P.pdf},
  journal = {International Journal of Bifurcation and Chaos},
  language = {en},
  number = {12}
}

@article{Toupo2015,
  title = {Evolutionary Game Dynamics of Controlled and Automatic Decision-Making},
  author = {Toupo, Danielle F. P. and Strogatz, Steven H. and Cohen, Jonathan D. and Rand, David G.},
  year = {2015},
  month = jul,
  volume = {25},
  pages = {073120},
  issn = {1054-1500, 1089-7682},
  doi = {10.1063/1.4927488},
  file = {/Users/qualia/Documents/Papers/Toupo et al. - 2015 - Evolutionary game dynamics of controlled and autom.pdf},
  journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  language = {en},
  number = {7}
}

@article{Toupo2015a,
  title = {Nonlinear Dynamics of the Rock-Paper-Scissors Game with Mutations},
  author = {Toupo, Danielle F. P. and Strogatz, Steven H.},
  year = {2015},
  month = may,
  volume = {91},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.91.052907},
  file = {/Users/qualia/Documents/Papers/Toupo and Strogatz - 2015 - Nonlinear dynamics of the rock-paper-scissors game.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Toyoizumi2006,
  title = {Fisher {{Information}} for {{Spike}}-{{Based Population Decoding}}},
  author = {Toyoizumi, Taro and Aihara, Kazuyuki and Amari, Shun-ichi},
  year = {2006},
  month = aug,
  volume = {97},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.97.098102},
  file = {/Users/qualia/Documents/Papers/2006 - Toyoizumi, Aihara, Amari - Fisher information for spike-based population decoding.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {9}
}

@article{Toyoizumi2011,
  title = {Beyond the Edge of Chaos: {{Amplification}} and Temporal Integration by Recurrent Networks in the Chaotic Regime},
  shorttitle = {Beyond the Edge of Chaos},
  author = {Toyoizumi, T. and Abbott, L. F.},
  year = {2011},
  month = nov,
  volume = {84},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.84.051908},
  file = {/Users/qualia/Documents/Papers/2011 - Toyoizumi, Abbott - Beyond the edge of chaos Amplification and temporal integration by recurrent networks in the chaotic regime.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {5}
}

@article{Tran2016,
  title = {Alpha Phase Dynamics Predict Age-Related Visual Working Memory Decline},
  author = {Tran, Tam T. and Hoffner, Nicole C. and LaHue, Sara C. and Tseng, Lisa and Voytek, Bradley},
  year = {2016},
  month = dec,
  volume = {143},
  pages = {196--203},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2016.08.052},
  abstract = {Alpha oscillations (7-14 Hz) are modulated in response to visual temporal and spatial cues. However, the neural response to alerting cues is less explored, as is how this response is affected by healthy aging. Using scalp EEG, we examined how visual cortical alpha activity relates to working memory performance. Younger (20-30 years) and older (60-70 years) participants were presented with a visual alerting cue uninformative of the position or size of a lateralized working memory array. Older adults showed longer response times overall and reduced accuracy when memory load was high. Older adults had less consistent cue-evoked alpha phase resetting than younger adults, which predicted worse performance. Alpha phase prior to memory array presentation predicted response time, but the relationship between phase and response time was weaker in older adults. These results suggest that changes in alpha phase dynamics, especially prior to presentation of task-relevant stimuli, potentially contribute to age-related cognitive decline.},
  file = {/Users/qualia/Documents/Papers/Tran et al. - 2016 - Alpha phase dynamics predict age-related visual wo.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Tran2017,
  title = {Ionic {{Current Correlations Are Ubiquitous Across Phyla}}},
  author = {Tran, Trinh and Unal, Cagri T. and Zaborszky, Laszlo and Rotstein, Horacio G. and Kirkwood, Alfredo and Golowasch, Jorge P.},
  year = {2017},
  month = may,
  doi = {10.1101/137133},
  abstract = {Ionic currents, whether measured as conductance amplitude or as ion channel transcript levels, can vary many-fold within a population of identified neurons. This variability has been observed in multiple invertebrate neuronal types, but they do so in a coordinated manner such that their magnitudes are correlated. These conductance correlations are thought to reflect a tight homeostasis of cellular excitability that enhances the robustness and stability of neuronal activity over long stretches of time. Notably, although such ionic current correlations are well documented in invertebrates, they have not been reported in vertebrates. Here we demonstrate with two examples, identified mouse hippocampal granule cells and cholinergic basal forebrain neurons, that ionic current correlations is a ubiquitous phenomenon expressed by a number of species across phyla.},
  file = {/Users/qualia/Documents/Papers/Tran et al. - 2017 - Ionic Current Correlations Are Ubiquitous Across P 2.pdf;/Users/qualia/Documents/Papers/Tran et al. - 2017 - Ionic Current Correlations Are Ubiquitous Across P.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tran2019,
  title = {Ionic Current Correlations Are Ubiquitous across Phyla},
  author = {Tran, Trinh and Unal, Cagri T. and Severin, Daniel and Zaborszky, Laszlo and Rotstein, Horacio G. and Kirkwood, Alfredo and Golowasch, Jorge},
  year = {2019},
  month = dec,
  volume = {9},
  pages = {1687},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-38405-6},
  file = {/Users/qualia/Documents/Papers/Tran et al. - 2019 - Ionic current correlations are ubiquitous across p.pdf},
  journal = {Sci Rep},
  language = {en},
  number = {1}
}

@article{Trautmann2017,
  title = {Accurate Estimation of Neural Population Dynamics without Spike Sorting},
  author = {Trautmann, Eric and Stavisky, Sergey and Lahiri, Subhaneil and Ames, Katherine and Kaufman, Matthew and Ryu, Stephen and Ganguli, Surya and Shenoy, Krishna},
  year = {2017},
  month = dec,
  doi = {10.1101/229252},
  abstract = {A central goal of systems neuroscience is to relate an organism's neural activity to behavior. Neural population analysis often begins by reducing the dimensionality of the data to focus on the patterns most relevant to a given task. A major practical hurdle to data analysis is spike sorting, and this problem is growing rapidly as the number of neurons measured increases. Here, we investigate whether spike sorting is necessary to estimate neural dynamics. The theory of random projections suggests that we can accurately estimate the geometry of low-dimensional manifolds from a small number of linear projections of the data. We re-analyzed data from three previous studies and found that neural dynamics and scientific conclusions are quite similar using multi-unit threshold crossings in place of sorted neurons. This finding unlocks existing data for new analyses and informs the design and use of new electrode arrays for laboratory and clinical use.},
  file = {/Users/qualia/Documents/Papers/Trautmann et al. - 2017 - Accurate estimation of neural population dynamics .pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tria2015,
  title = {The Dynamics of Correlated Novelties},
  author = {Tria, F. and Loreto, V. and Servedio, V. D. P. and Strogatz, S. H.},
  year = {2015},
  month = may,
  volume = {4},
  issn = {2045-2322},
  doi = {10.1038/srep05890},
  file = {/Users/qualia/Documents/Papers/2014 - Tria et al. - The dynamics of correlated novelties.pdf;/Users/qualia/Documents/Papers/Tria et al. - 2015 - The dynamics of correlated novelties.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Tripathy,
  title = {Transcriptomic Correlates of Neuron Electrophysiological Diversity},
  author = {Tripathy, Shreejoy J and Toker, Lilah and Li, Brenna and Crichlow, Cindy-Lee and Tebaykin, Dmitry and Mancarci, B Ogan and Pavlidis, Paul},
  pages = {28},
  abstract = {How neuronal diversity emerges from complex patterns of gene expression remains poorly understood. Here we present an approach to understand electrophysiological diversity through gene expression by integrating pooled- and single-cell transcriptomics with intracellular electrophysiology. Using neuroinformatics methods, we compiled a brain-wide dataset of 34 neuron types with paired gene expression and intrinsic electrophysiological features from publically accessible sources, the largest such collection to date. We identified 420 genes whose expression levels significantly correlated with variability in one or more of 11 physiological parameters. We next trained statistical models to infer cellular features from multivariate gene expression patterns. Such models were predictive of gene-electrophysiological relationships in an independent collection of 12 visual cortex cell types from the Allen Institute, suggesting that these correlations might reflect general principles relating expression patterns to phenotypic diversity across very different cell types. Many associations reported here have the potential to provide new insights into how neurons generate functional diversity, and correlations of ion channel genes like Gabrd and Scn1a (Nav1.1) with resting potential and spiking frequency are consistent with known causal mechanisms. Our work highlights the promise and inherent challenges in using cell type-specific transcriptomics to understand the mechanistic origins of neuronal diversity.},
  file = {/Users/qualia/Documents/Papers/Tripathy et al. - Transcriptomic correlates of neuron electrophysiol.pdf},
  language = {en}
}

@article{Tripp2007,
  title = {Neural {{Populations Can Induce Reliable Postsynaptic Currents}} without {{Observable Spike Rate Changes}} or {{Precise Spike Timing}}},
  author = {Tripp, B. and Eliasmith, C.},
  year = {2007},
  month = aug,
  volume = {17},
  pages = {1830--1840},
  issn = {1047-3211, 1460-2199},
  doi = {10.1093/cercor/bhl092},
  file = {/Users/qualia/Documents/Papers/2007 - Tripp, Eliasmith - Neural populations can induce reliable postsynaptic currents without observable spike rate changes or precise.pdf},
  journal = {Cerebral Cortex},
  language = {en},
  number = {8}
}

@article{Tripp2016,
  title = {Function Approximation in Inhibitory Networks},
  author = {Tripp, Bryan and Eliasmith, Chris},
  year = {2016},
  month = may,
  volume = {77},
  pages = {95--106},
  issn = {08936080},
  doi = {10.1016/j.neunet.2016.01.010},
  abstract = {In performance-optimized artificial neural networks, such as convolutional networks, each neuron makes excitatory connections with some of its targets and inhibitory connections with others. In contrast, physiological neurons are typically either excitatory or inhibitory, not both. This is a puzzle, because it seems to constrain computation, and because there are several counter-examples that suggest that it may not be a physiological necessity. Parisien et al. (2008) showed that any mixture of excitatory and inhibitory functional connections could be realized by a purely excitatory projection in parallel with a two-synapse projection through an inhibitory population. They showed that this works well with ratios of excitatory and inhibitory neurons that are realistic for the neocortex, suggesting that perhaps the cortex efficiently works around this apparent computational constraint. Extending this work, we show here that mixed excitatory and inhibitory functional connections can also be realized in networks that are dominated by inhibition, such as those of the basal ganglia. Further, we show that the function-approximation capacity of such connections is comparable to that of idealized mixed-weight connections. We also study whether such connections are viable in recurrent networks, and find that such recurrent networks can flexibly exhibit a wide range of dynamics. These results offer a new perspective on computation in the basal ganglia, and also perhaps on inhibitory networks within the cortex.},
  file = {/Users/qualia/Documents/Papers/Tripp and Eliasmith - 2016 - Function approximation in inhibitory networks.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Truccolo2014,
  title = {Neuronal {{Ensemble Synchrony}} during {{Human Focal Seizures}}},
  author = {Truccolo, W. and Ahmed, O. J. and Harrison, M. T. and Eskandar, E. N. and Cosgrove, G. R. and Madsen, J. R. and Blum, A. S. and Potter, N. S. and Hochberg, L. R. and Cash, S. S.},
  year = {2014},
  month = jul,
  volume = {34},
  pages = {9927--9944},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.4567-13.2014},
  file = {/Users/qualia/Documents/Papers/Truccolo et al. - 2014 - Neuronal Ensemble Synchrony during Human Focal Sei.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {30}
}

@article{Trujillo2013,
  title = {Altered Cortical Spectrotemporal Processing with Age-Related Hearing Loss},
  author = {Trujillo, Michael and Razak, Khaleel A.},
  year = {2013},
  month = dec,
  volume = {110},
  pages = {2873--2886},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.00423.2013},
  file = {/Users/qualia/Documents/Papers/Trujillo and Razak - 2013 - Altered cortical spectrotemporal processing with a.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {12}
}

@article{Tschopp2018,
  title = {A {{Connectome Based Hexagonal Lattice Convolutional Network Model}} of the {{Drosophila Visual System}}},
  author = {Tschopp, Fabian David and Reiser, Michael B. and Turaga, Srinivas C.},
  year = {2018},
  month = jun,
  abstract = {What can we learn from a connectome? We constructed a simplified model of the first two stages of the fly visual system, the lamina and medulla. The resulting hexagonal lattice convolutional network was trained using backpropagation through time to perform object tracking in natural scene videos. Networks initialized with weights from connectome reconstructions automatically discovered well-known orientation and direction selectivity properties in T4 neurons and their inputs, while networks initialized at random did not. Our work is the first demonstration, that knowledge of the connectome can enable in silico predictions of the functional properties of individual neurons in a circuit, leading to an understanding of circuit function from structure alone.},
  archivePrefix = {arXiv},
  eprint = {1806.04793},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Tschopp et al. - 2018 - A Connectome Based Hexagonal Lattice Convolutional.pdf},
  journal = {arXiv:1806.04793 [cs, q-bio]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Quantitative Biology - Neurons and Cognition},
  language = {en},
  primaryClass = {cs, q-bio}
}

@techreport{Tseng2019,
  title = {Distinct {{Oscillation Dynamics Selectively Coordinate Excitatory}} and {{Inhibitory Neurons}} in {{Prefrontal Cortex}} during {{Sensory Discrimination}}},
  author = {Tseng, Hua-an and Han, Xue},
  year = {2019},
  month = may,
  institution = {{Neuroscience}},
  doi = {10.1101/629659},
  abstract = {The prefrontal cortex (PFC) is crucial for many cognitive functions. PFC individual neuron activity and ensemble oscillation dynamics have been linked to unique aspects of behavior. However, it remains largely unclear how different neuron types relate to oscillation features. To understand how excitatory and inhibitory neurons in PFC are coordinated by distinct oscillation signatures, we designed a 3-choice auditory discrimination task and used tetrode devices to examine individual PFC neuron activity and ensemble LFP oscillations in task performing mice. We found that PFC neurons and ensemble LFP oscillations exhibited complex sensory evoked responses that are context and task-progression dependent. While both excitatory and inhibitory neurons were transiently modulated at different phases of the task, inhibitory neurons were increasingly recruited as trial progressed compared to excitatory neurons. Inhibitory neurons in general showed higher spike-field coherence with LFP oscillations than excitatory neurons throughout the task period, first at higher frequencies at the beginning of the task, and then transitioned to lower frequencies in the middle of the task that sustained beyond task completion. Together, our results demonstrate that excitatory and inhibitory neurons selectively engage distinct oscillation dynamics during sensory discrimination in mice.},
  file = {/Users/qualia/Documents/Papers/Tseng and Han - 2019 - Distinct Oscillation Dynamics Selectively Coordina.pdf},
  language = {en},
  type = {Preprint}
}

@article{Tsirogiannis2010,
  title = {A Population Level Computational Model of the Basal Ganglia That Generates Parkinsonian Local Field Potential Activity},
  author = {Tsirogiannis, George L. and Tagaris, George A. and Sakas, Damianos and Nikita, Konstantina S.},
  year = {2010},
  month = feb,
  volume = {102},
  pages = {155--176},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-009-0360-3},
  abstract = {Recordings from the basal ganglia's subthalamic nucleus are acquired via microelectrodes immediately prior to the application of Deep Brain Stimulation (DBS) treatment for Parkinson's Disease (PD) to assist in the selection of the final point for the implantation of the DBS electrode. The acquired recordings reveal a persistent characteristic beta band peak in the power spectral density function of the Local Field Potential (LFP) signals. This peak is considered to lie at the core of the causality\textendash{}effect relationships of the parkinsonian pathophysiology. Based on LFPs acquired from human subjects during DBS for PD, we constructed a computational model of the basal ganglia on the population level that generates LFPs to identify the critical pathophysiological alterations that lead to the expression of the beta band peak. To this end, we used experimental data reporting that the strengths of the synaptic connections are modified under dopamine depletion. The hypothesis that the altered dopaminergic modulation may affect both the amplitude and the time course of the postsynaptic potentials is validated by the model. The results suggest a pivotal role of both of these parameters to the pathophysiology of PD.},
  file = {/Users/qualia/Documents/Papers/2010 - Tsirogiannis et al. - A population level computational model of the basal ganglia that generates parkinsonian local field potenti.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {2}
}

@article{Tulving2002,
  title = {Episodic {{Memory}}: {{From Mind}} to {{Brain}}},
  shorttitle = {Episodic {{Memory}}},
  author = {Tulving, Endel},
  year = {2002},
  month = feb,
  volume = {53},
  pages = {1--25},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.53.100901.135114},
  file = {/Users/qualia/Documents/Papers/Tulving - 2002 - Episodic Memory From Mind to Brain.pdf},
  journal = {Annu. Rev. Psychol.},
  language = {en},
  number = {1}
}

@article{Turner2012,
  title = {Spatiotemporal Activity Estimation for Multivoxel Pattern Analysis with Rapid Event-Related Designs},
  author = {Turner, Benjamin O. and Mumford, Jeanette A. and Poldrack, Russell A. and Ashby, F. Gregory},
  year = {2012},
  month = sep,
  volume = {62},
  pages = {1429--1438},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.05.057},
  abstract = {Despite growing interest in multi-voxel pattern analysis (MVPA) methods for fMRI, a major problem remains \textemdash{}that of generating estimates in rapid event-related (ER) designs, where the BOLD responses of temporally adjacent events will overlap. While this problem has been investigated for methods that reduce each event to a single parameter per voxel (Mumford et al., 2012), most of these methods make strong parametric assumptions about the shape of the hemodynamic response, and require exact knowledge of the temporal profile of the underlying neural activity. A second class of methods uses multiple parameters per event (per voxel) to capture temporal information more faithfully. In addition to enabling a more accurate estimate of ER responses, this allows for the extension of the standard classification paradigm into the temporal domain (e.g., Mour{\~a}o-Miranda et al., 2007). However, existing methods in this class were developed for use with block and slow ER data, and there has not yet been an exploration of how to adapt such methods to data collected using rapid ER designs. Here, we demonstrate that the use of multiple parameters preserves or improves classification accuracy, while additionally providing information on the evolution of class discrimination. Additionally, we explore an alternative to the method of Mour{\~a}o-Miranda et al. tailored to use in rapid ER designs that yields equivalent classification accuracies, but is better at unmixing responses to temporally adjacent events. The current work paves the way for wider adoption of spatiotemporal classification analyses, and greater use of MVPA with rapid ER designs.},
  file = {/Users/qualia/Documents/Papers/2012 - Turner et al. - Spatiotemporal activity estimation for multivoxel pattern analysis with rapid event-related designs.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Turnquist2017,
  title = {Quadratization: {{From Conductance}}-{{Based Models To Caricature Models With Parabolic Nonlinearities}}},
  shorttitle = {Quadratization},
  author = {Turnquist, Axel G. R. and Rotstein, Horacio},
  year = {2017},
  month = may,
  doi = {10.1101/137422},
  abstract = {Quadratization of biophysical (conductance-based) models having a parabolic-like voltage nullcline in the subthreshold voltage regime refers to the process by which these models are substituted by ``caricature'' models having a strictly parabolic voltage nullcline and a linear nullcline  for the recovery variable. We refer to the latter as quadratic or parabolic models. The parabolic-like and strictly parabolic voltage nullclines coincide at their extrema (minima or maxima) and  are well approximated by each other in vicinities of these extrema whose size depend on the model parameters. Quadratic models are simplified by a change of variables that translates these extrema into the origin of the phase-plane diagram. A further simplification (parameter reduction) can be achieved by nondimensionalizing  the quadratic models. This procedure can be extended to three-dimensional models having a parabolic-cylinder-like shaped voltage nullsurface and to models having time-dependent inputs and synaptic currents.},
  file = {/Users/qualia/Documents/Papers/Turnquist and Rotstein - 2017 - Quadratization From Conductance-Based Models To C.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Tuyls2007,
  title = {What Evolutionary Game Theory Tells Us about Multiagent Learning},
  author = {Tuyls, Karl and Parsons, Simon},
  year = {2007},
  month = may,
  volume = {171},
  pages = {406--416},
  issn = {00043702},
  doi = {10.1016/j.artint.2007.01.004},
  abstract = {This paper discusses If multi-agent learning is the answer, what is the question? [Y. Shoham, R. Powers, T. Grenager, If multiagent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365\textendash{}377, this issue] from the perspective of evolutionary game theory. We briefly discuss the concepts of evolutionary game theory, and examine the main conclusions from [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Intelligence 171 (7) (2007) 365\textendash{}377, this issue] with respect to some of our previous work. Overall we find much to agree with, concluding, however, that the central concerns of multiagent learning are rather narrow compared with the broad variety of work identified in [Y. Shoham, R. Powers, T. Grenager, If multi-agent learning is the answer, what is the question? Artificial Inteligence 171 (7) (2007) 365\textendash{}377, this issue].},
  file = {/Users/qualia/Documents/Papers/2007 - Tuyls, Parsons - What evolutionary game theory tells us about multiagent learning.pdf},
  journal = {Artificial Intelligence},
  language = {en},
  number = {7}
}

@article{Tuyls2018,
  title = {Symmetric {{Decomposition}} of {{Asymmetric Games}}},
  author = {Tuyls, Karl and P{\'e}rolat, Julien and Lanctot, Marc and Ostrovski, Georg and Savani, Rahul and Leibo, Joel Z and Ord, Toby and Graepel, Thore and Legg, Shane},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-19194-4},
  file = {/Users/qualia/Documents/Papers/Tuyls et al. - 2018 - Symmetric Decomposition of Asymmetric Games.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Uhlhaas2008,
  title = {The {{Role}} of {{Oscillations}} and {{Synchrony}} in {{Cortical Networks}} and {{Their Putative Relevance}} for the {{Pathophysiology}} of {{Schizophrenia}}},
  author = {Uhlhaas, P. J. and Haenschel, C. and Nikolic, D. and Singer, W.},
  year = {2008},
  month = jul,
  volume = {34},
  pages = {927--943},
  issn = {0586-7614, 1745-1701},
  doi = {10.1093/schbul/sbn062},
  file = {/Users/qualia/Documents/Papers/2008 - Uhlhaas et al. - The role of oscillations and synchrony in cortical networks and their putative relevance for the pathophysiology.pdf},
  journal = {Schizophrenia Bulletin},
  language = {en},
  number = {5}
}

@article{Uhlhaas2009,
  title = {Neural Synchrony in Cortical Networks: History, Concept and Current Status},
  shorttitle = {Neural Synchrony in Cortical Networks},
  author = {Uhlhaas, Peter},
  year = {2009},
  volume = {3},
  issn = {16625145},
  doi = {10.3389/neuro.07.017.2009},
  abstract = {Following the discovery of context-dependent synchronization of oscillatory neuronal responses in the visual system, the role of neural synchrony in cortical networks has been expanded to provide a general mechanism for the coordination of distributed neural activity patterns. In the current paper, we present an update of the status of this hypothesis through summarizing recent results from our laboratory that suggest important new insights regarding the mechanisms, function and relevance of this phenomenon. In the first part, we present recent results derived from animal experiments and mathematical simulations that provide novel explanations and mechanisms for zero and nero-zero phase lag synchronization. In the second part, we shall discuss the role of neural synchrony for expectancy during perceptual organization and its role in conscious experience. This will be followed by evidence that indicates that in addition to supporting conscious cognition, neural synchrony is abnormal in major brain disorders, such as schizophrenia and autism spectrum disorders. We conclude this paper with suggestions for further research as well as with critical issues that need to be addressed in future studies.},
  file = {/Users/qualia/Documents/Papers/2009 - Neuroscience et al. - Neural synchrony in cortical networks history , concept and current status.pdf},
  journal = {Frontiers in Integrative Neuroscience},
  language = {en}
}

@article{Uhlhaas2012,
  title = {Neuronal {{Dynamics}} and {{Neuropsychiatric Disorders}}: {{Toward}} a {{Translational Paradigm}} for {{Dysfunctional Large}}-{{Scale Networks}}},
  shorttitle = {Neuronal {{Dynamics}} and {{Neuropsychiatric Disorders}}},
  author = {Uhlhaas, Peter J. and Singer, Wolf},
  year = {2012},
  month = sep,
  volume = {75},
  pages = {963--980},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.09.004},
  file = {/Users/qualia/Documents/Papers/2012 - Uhlhaas, Singer - Neuronal Dynamics and Neuropsychiatric Disorders Toward a Translational Paradigm for Dysfunctional Large-Scale.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Ujfalussy2015,
  title = {Dendritic Nonlinearities Are Tuned for Efficient Spike-Based Computations in Cortical Circuits},
  author = {Ujfalussy, Bal{\'a}zs B and Makara, Judit K and Branco, Tiago and Lengyel, M{\'a}t{\'e}},
  year = {2015},
  month = dec,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.10056},
  file = {/Users/qualia/Documents/Papers/Ujfalussy et al. - 2015 - Dendritic nonlinearities are tuned for efficient s.pdf},
  journal = {eLife},
  language = {en}
}

@techreport{Ujfalussy2017,
  title = {Global, Multiplexed Dendritic Computations under in Vivo-like Conditions},
  author = {Ujfalussy, Balazs B and Lengyel, Mate and Branco, Tiago},
  year = {2017},
  month = dec,
  institution = {{Neuroscience}},
  doi = {10.1101/235259},
  abstract = {Dendrites integrate inputs in highly non-linear ways, but it is unclear how these non-linearities contribute to the overall input-output transformation of single neurons. Here, we developed statistically principled methods using a hierarchical cascade of linear-nonlinear subunits (hLN) to model the dynamically evolving somatic response of neurons receiving complex spatio-temporal synaptic input patterns. We used the hLN to predict the membrane potential of a detailed biophysical model of a L2/3 pyramidal cell receiving in vivo-like synaptic input and reproducing in vivo dendritic recordings. We found that more than 90\% of the somatic response could be captured by linear integration followed a single global non-linearity. Multiplexing inputs into parallel processing channels could improve prediction accuracy by as much as additional layers of local non-linearities. These results provide a data-driven characterisation of a key building block of cortical circuit computations: dendritic integration and the input-output transformation of single neurons during in vivo-like conditions.},
  file = {/Users/qualia/Documents/Papers/Ujfalussy et al. - 2017 - Global, multiplexed dendritic computations under i.pdf},
  language = {en},
  type = {Preprint}
}

@article{UniversidadTecnologicadePereira2016,
  title = {Estimation of the Neuromodulation Parameters from the Planned Volume of Tissue Activated in Deep Brain Stimulation},
  author = {{Universidad Tecnol{\'o}gica de Pereira} and {G{\'o}mez-Orozco}, Viviana and {\'A}lvarez-L{\'o}pez, Mauricio Alexander and {Universidad Tecnol{\'o}gica de Pereira} and {Henao-Gallo}, {\'O}scar Alberto and {Universidad Tecnol{\'o}gica de Pereira} and {Daza-Santacoloma}, Genaro and {Instituto de Epilepsia y Parkinson del Eje Cafetero - Neurocentro} and {Orozco-Guti{\'e}rrez}, {\'A}lvaro {\'A}ngel and {Universidad Tecnol{\'o}gica de Pereira}},
  year = {2016},
  month = jun,
  issn = {01206230},
  doi = {10.17533/udea.redin.n79a02},
  abstract = {Deep brain stimulation (DBS) is a therapy with promissory results for the treatment of movement disorders. It delivers electric stimulation via an electrode to a specific target brain region. The spatial extent of neural response to this stimulation is known as volume of tissue activated (VTA). Changes in stimulation parameters that control VTA, such as amplitude, pulse width and electrode configuration can affect the effectiveness of the DBS therapy. In this study, we develop a novel methodology for estimating suitable DBS neuromodulation parameters, from planned VTA, that attempts to maximize the therapeutic effects, and to minimize the adverse effects of DBS. For estimating the continuous outputs (amplitude and pulse width), we use multi-output support vector regression, taking the geometry of the VTA as input space. For estimating the electrode polarity configuration, we perform several classification problems, also using support vector machines from the same input space. Our methodology attains promising results for both the regression setting, and for predicting electrode active contacts and their polarity. Combining biological neural modeling techniques together with machine learning, we introduce a novel area of research where parameters of neuromodulation in DBS can be tuned by manually specifying a desired geometric volume.},
  file = {/Users/qualia/Documents/Papers/Universidad Tecnológica de Pereira et al. - 2016 - Estimation of the neuromodulation parameters from .pdf},
  journal = {Revista Facultad de Ingenier{\'i}a Universidad de Antioquia},
  language = {en},
  number = {79}
}

@article{Unsworth2007,
  title = {On the Division of Short-Term and Working Memory: {{An}} Examination of Simple and Complex Span and Their Relation to Higher Order Abilities.},
  shorttitle = {On the Division of Short-Term and Working Memory},
  author = {Unsworth, Nash and Engle, Randall W.},
  year = {2007},
  volume = {133},
  pages = {1038--1066},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/0033-2909.133.6.1038},
  abstract = {Research has suggested that short-term memory and working memory (as measured by simple and complex span tasks, respectively) are separate constructs that are differentially related to higher order cognitive abilities. This claim is critically evaluated by reviewing research that has compared simple and complex span tasks in both experimental and correlational studies. In addition, a meta-analysis and re-analyses of key data sets were conducted. The review and analyses suggest that simple and complex span tasks largely measure the same basic subcomponent processes (e.g., rehearsal, maintenance, updating, controlled search) but differ in the extent to which these processes operate in a particular task. These differences largely depend on the extent to which phonological processes are maximized and variability from long list lengths is present. Potential methodological, psychometric, and assessment implications are discussed and a theoretical account of the data is proposed.},
  file = {/Users/qualia/Documents/Papers/2007 - Unsworth, Engle - On the division of short-term and working memory An examination of simple and complex span and their relatio(2).pdf;/Users/qualia/Documents/Papers/2007 - Unsworth, Engle - On the division of short-term and working memory An examination of simple and complex span and their relation t.pdf},
  journal = {Psychological Bulletin},
  language = {en},
  number = {6}
}

@article{Urban2015,
  title = {Real-Time Imaging of Brain Activity in Freely Moving Rats Using Functional Ultrasound},
  author = {Urban, Alan and Dussaux, Clara and Martel, Guillaume and Brunner, Cl{\'e}ment and Mace, Emilie and Montaldo, Gabriel},
  year = {2015},
  month = sep,
  volume = {12},
  pages = {873--878},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/nmeth.3482},
  file = {/Users/qualia/Documents/Papers/Urban et al. - 2015 - Real-time imaging of brain activity in freely movi.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {9}
}

@article{Ursino2010,
  title = {The Generation of Rhythms within a Cortical Region: {{Analysis}} of a Neural Mass Model},
  shorttitle = {The Generation of Rhythms within a Cortical Region},
  author = {Ursino, Mauro and Cona, Filippo and Zavaglia, Melissa},
  year = {2010},
  month = sep,
  volume = {52},
  pages = {1080--1094},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2009.12.084},
  abstract = {Rhythms in brain electrical activity are assumed to play a significant role in many cognitive and perceptual processes. It is thus of great value to analyze these rhythms and their mutual relationships in large scale models of cortical regions. In the present work, we modified the neural mass model by Wendling et al. (Eur. J. Neurosci. 15 (2002) 1499\textendash{}1508) by including a new inhibitory self-loop among GABAA,fast interneurons. A theoretical analysis was performed to demonstrate that, thanks to this loop, GABAA,fast interneurons can produce a {$\gamma$} rhythm in the power spectral density (PSD) even without the participation of the other neural populations. Then, the model of a whole cortical region, built upon four interconnected neural populations (pyramidal cells, excitatory, GABAA,slow and GABAA,fast interneurons) was investigated by changing the internal connectivity parameters. Results show that different rhythm combinations ({$\beta$} and {$\gamma$}, {$\alpha$} and {$\gamma$}, or a wide spectrum) can be obtained within the same region by simply altering connectivity values, without the need to change synaptic kinetics. Finally, two or three cortical regions were connected by using different topologies of long range connections. Results show that long-range connections directed from pyramidal neurons to GABAA,fast interneurons are the most efficient to transmit rhythms from one region to another. In this way, PSD with three or four peaks can be obtained using simple connectivity patterns. The model can be of value to gain a deeper insight into the mechanisms involved in the generation of {$\gamma$} rhythms and provide a better understanding of cortical EEG spectra.},
  file = {/Users/qualia/Documents/Papers/2010 - Ursino, Cona, Zavaglia - The generation of rhythms within a cortical region Analysis of a neural mass model.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {3}
}

@article{Uva2015,
  title = {Synchronous {{Inhibitory Potentials Precede Seizure}}-{{Like Events}} in {{Acute Models}} of {{Focal Limbic Seizures}}},
  author = {Uva, L. and Breschi, G. L. and Gnatkovsky, V. and Taverna, S. and {de Curtis}, M.},
  year = {2015},
  month = feb,
  volume = {35},
  pages = {3048--3055},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3692-14.2015},
  file = {/Users/qualia/Documents/Papers/Uva et al. - 2015 - Synchronous Inhibitory Potentials Precede Seizure-.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {7}
}

@article{Valiant1984,
  title = {A Theory of the Learnable},
  author = {Valiant, Leselie},
  year = {1984},
  volume = {27},
  pages = {1134--1142},
  file = {/Users/qualia/Documents/Papers/Valiant.pdf},
  journal = {Communications of the ACM},
  number = {11}
}

@article{vanderWesthuizen2018,
  title = {The Unreasonable Effectiveness of the Forget Gate},
  author = {{van der Westhuizen}, Jos and Lasenby, Joan},
  year = {2018},
  month = apr,
  abstract = {Given the success of the gated recurrent unit, a natural question is whether all the gates of the long short-term memory (LSTM) network are necessary. Previous research has shown that the forget gate is one of the most important gates in the LSTM. Here we show that a forget-gate-only version of the LSTM with chronoinitialized biases, not only provides computational savings but outperforms the standard LSTM on multiple benchmark datasets and competes with some of the best contemporary models. Our proposed network, the JANET, achieves accuracies of 99\% and 92.5\% on the MNIST and pMNIST datasets, outperforming the standard LSTM which yields accuracies of 98.5\% and 91\%.},
  archivePrefix = {arXiv},
  eprint = {1804.04849},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/van der Westhuizen and Lasenby - 2018 - The unreasonable effectiveness of the forget gate.pdf;/Users/qualia/Zotero/storage/ZVD94T7I/van der Westhuizen and Lasenby - 2018 - The unreasonable effectiveness of the forget gate.pdf},
  journal = {arXiv:1804.04849 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{vanDijk2008,
  title = {Prestimulus {{Oscillatory Activity}} in the {{Alpha Band Predicts Visual Discrimination Ability}}},
  author = {{van Dijk}, H. and Schoffelen, J.-M. and Oostenveld, R. and Jensen, O.},
  year = {2008},
  month = feb,
  volume = {28},
  pages = {1816--1823},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1853-07.2008},
  file = {/Users/qualia/Documents/Papers/2008 - Van Dijk et al. - Prestimulus Oscillatory Activity in the Alpha Band Predicts Visual Discrimination Ability.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {8}
}

@article{vanEde2018,
  title = {Neural {{Oscillations}}: {{Sustained Rhythms}} or {{Transient Burst}}-{{Events}}?},
  shorttitle = {Neural {{Oscillations}}},
  author = {{van Ede}, Freek and Quinn, Andrew J. and Woolrich, Mark W. and Nobre, Anna C.},
  year = {2018},
  month = jul,
  volume = {41},
  pages = {415--417},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.04.004},
  file = {/Users/qualia/Documents/Papers/van Ede et al. - 2018 - Neural Oscillations Sustained Rhythms or Transien.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {7}
}

@article{vanEde2018a,
  title = {Neural {{Oscillations}}: {{Sustained Rhythms}} or {{Transient Burst}}-{{Events}}?},
  shorttitle = {Neural {{Oscillations}}},
  author = {{van Ede}, Freek and Quinn, Andrew J. and Woolrich, Mark W. and Nobre, Anna C.},
  year = {2018},
  month = jul,
  volume = {41},
  pages = {415--417},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.04.004},
  file = {/Users/qualia/Documents/Papers/van Ede et al. - 2018 - Neural Oscillations Sustained Rhythms or Transien 2.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {7}
}

@article{vanEde2018b,
  title = {Neural {{Oscillations}}: {{Sustained Rhythms}} or {{Transient Burst}}-{{Events}}?},
  shorttitle = {Neural {{Oscillations}}},
  author = {{van Ede}, Freek and Quinn, Andrew J. and Woolrich, Mark W. and Nobre, Anna C.},
  year = {2018},
  month = jul,
  volume = {41},
  pages = {415--417},
  issn = {01662236},
  doi = {10.1016/j.tins.2018.04.004},
  file = {/Users/qualia/Documents/Papers/van Ede et al. - 2018 - Neural Oscillations Sustained Rhythms or Transien 3.pdf},
  journal = {Trends in Neurosciences},
  language = {en},
  number = {7}
}

@article{Vangel2015,
  title = {Randomly {{Spiking Dynamic Neural Fields}}},
  author = {Vangel, Beno{\^i}t Chappet De and {Torres-huitzil}, Cesar and Girau, Bernard},
  year = {2015},
  month = apr,
  volume = {11},
  pages = {1--26},
  issn = {15504832},
  doi = {10.1145/2629517},
  file = {/Users/qualia/Documents/Papers/2014 - de Vangel, Torres-Huitzil, Girau - Randomly spiking dynamic neural fields.pdf;/Users/qualia/Documents/Papers/Vangel et al. - 2015 - Randomly Spiking Dynamic Neural Fields.pdf},
  journal = {ACM Journal on Emerging Technologies in Computing Systems},
  language = {en},
  number = {4}
}

@article{vanHasselt2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q}}-Learning},
  author = {{van Hasselt}, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = dec,
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archivePrefix = {arXiv},
  eprint = {1509.06461},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/van Hasselt et al. - 2015 - Deep Reinforcement Learning with Double Q-learning.pdf},
  journal = {arXiv:1509.06461 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{Vanschoren2018,
  title = {Meta-{{Learning}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}}},
  author = {Vanschoren, Joaquin},
  year = {2018},
  month = oct,
  abstract = {Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.},
  archivePrefix = {arXiv},
  eprint = {1810.03548},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Vanschoren - 2018 - Meta-Learning A Survey.pdf},
  journal = {arXiv:1810.03548 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Varela2001,
  title = {The Brainweb: {{Phase}} Synchronization and Large-Scale Integration},
  shorttitle = {The Brainweb},
  author = {Varela, Francisco and Lachaux, Jean-Philippe and Rodriguez, Eugenio and Martinerie, Jacques},
  year = {2001},
  month = apr,
  volume = {2},
  pages = {229--239},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/35067550},
  file = {/Users/qualia/Documents/Papers/Varela et al. - 2001 - The brainweb Phase synchronization and large-scal.pdf},
  journal = {Nat Rev Neurosci},
  language = {en},
  number = {4}
}

@article{Varma2006,
  title = {Bias in Error Estimation When Using Cross-Validation for Model Selection},
  author = {Varma, Sudhir and Simon, Richard},
  year = {2006},
  pages = {8},
  abstract = {Background: Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data.
Results: We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" and "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error. The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30\% on 18.5\% of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30\% on 38\% of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance. The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions.
Conclusion: We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error.},
  file = {/Users/qualia/Documents/Papers/2006 - Varma, Simon - Bias in error estimation when using cross-validation for model selection.pdf},
  journal = {BMC Bioinformatics},
  language = {en}
}

@article{Vegue2017,
  title = {On {{The Structure Of Cortical Micro}}-{{Circuits Inferred From Small Sample Sizes}}},
  author = {Vegue, Marina and Perin, Rodrigo and Roxin, Alex},
  year = {2017},
  month = apr,
  doi = {10.1101/118471},
  abstract = {The structure in cortical micro-circuits deviates from what would be expected in a purely random network, which has been seen as evidence of clustering. To address this issue we sought to reproduce the non-random features of cortical circuits by considering several distinct classes of network topology, including clustered networks, networks with distance-dependent connectivity and those with broad degree distributions. To our surprise we found that all these qualitatively distinct topologies could account equally well for all reported non-random features, despite being easily distinguishable from one another at the network level. This apparent paradox was a consequence of estimating network properties given only small sample sizes. In other words, networks which differ markedly in their global structure can look quite similar locally. This makes inferring network structure from small sample sizes, a necessity given the technical difficulty inherent in simultaneous intracellular recordings, problematic. We found that a network statistic called the sample degree correlation (SDC) overcomes this difficulty. The SDC depends only on parameters which can be reliably estimated given small sample sizes, and is an accurate fingerprint of every topological family. We applied the SDC criterion to data from rat visual and somatosensory cortex and discovered that the connectivity was not consistent with any of these main topological classes. However, we were able to fit the experimental data with a more general network class, of which all previous topologies were special cases. The resulting network topology could be interpreted as a combination of physical spatial dependence and non-spatial, hierarchical clustering.},
  file = {/Users/qualia/Documents/Papers/Vegue et al. - 2017 - On The Structure Of Cortical Micro-Circuits Inferr.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Veness,
  title = {Online {{Learning}} with {{Gated Linear Networks}}},
  author = {Veness, Joel and Lattimore, Tor and Bhoopchand, Avishkar and {Grabska-Barwinska}, Agnieszka and Mattern, Christopher and Toth, Peter},
  pages = {40},
  abstract = {This paper describes a family of probabilistic architectures designed for online learning under the logarithmic loss. Rather than relying on non-linear transfer functions, our method gains representational power by the use of data conditioning. We state under general conditions a learnable capacity theorem that shows this approach can in principle learn any bounded Borel-measurable function on a compact subset of euclidean space; the result is stronger than many universality results for connectionist architectures because we provide both the model and the learning procedure for which convergence is guaranteed.},
  file = {/Users/qualia/Documents/Papers/Veness et al. - Online Learning with Gated Linear Networks.pdf},
  language = {en}
}

@article{Vezhnevets,
  title = {{{FeUdal Networks}} for {{Hierarchical Reinforcement Learning}}},
  author = {Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  pages = {12},
  abstract = {We introduce FeUdal Networks (FuNs): a novel architecture for hierarchical reinforcement learning. Our approach is inspired by the feudal reinforcement learning proposal of Dayan and Hinton, and gains power and efficacy by decoupling end-to-end learning across multiple levels \textendash{} allowing it to utilise different resolutions of time. Our framework employs a Manager module and a Worker module. The Manager operates at a lower temporal resolution and sets abstract goals which are conveyed to and enacted by the Worker. The Worker generates primitive actions at every tick of the environment. The decoupled structure of FuN conveys several benefits \textendash{} in addition to facilitating very long timescale credit assignment it also encourages the emergence of sub-policies associated with different goals set by the Manager. These properties allow FuN to dramatically outperform a strong baseline agent on tasks that involve long-term credit assignment or memorisation. We demonstrate the performance of our proposed system on a range of tasks from the ATARI suite and also from a 3D DeepMind Lab environment.},
  file = {/Users/qualia/Documents/Papers/Vezhnevets et al. - FeUdal Networks for Hierarchical Reinforcement Lea.pdf},
  language = {en}
}

@article{Vicente2008,
  title = {Dynamical Relaying Can Yield Zero Time Lag Neuronal Synchrony despite Long Conduction Delays},
  author = {Vicente, R. and Gollo, L. L. and Mirasso, C. R. and Fischer, I. and Pipa, G.},
  year = {2008},
  month = nov,
  volume = {105},
  pages = {17157--17162},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0809353105},
  file = {/Users/qualia/Documents/Papers/2008 - Vicente et al. - Dynamical relaying can yield zero time lag neuronal synchrony despite long conduction delays.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {44}
}

@article{Vicente2012,
  title = {Looking at the {{Trees}} in the {{Central Forest}}: {{A New Pallidal}}-{{Striatal Cell Type}}},
  shorttitle = {Looking at the {{Trees}} in the {{Central Forest}}},
  author = {Vicente, Ana M. and Costa, Rui M.},
  year = {2012},
  month = jun,
  volume = {74},
  pages = {967--969},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.06.003},
  file = {/Users/qualia/Documents/Papers/2012 - Vicente, Costa - Looking at the Trees in the Central Forest A New Pallidal-Striatal Cell Type.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Vijayan2012,
  title = {Thalamic Model of Awake Alpha Oscillations and Implications for Stimulus Processing},
  author = {Vijayan, S. and Kopell, N. J.},
  year = {2012},
  month = nov,
  volume = {109},
  pages = {18553--18558},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1215385109},
  file = {/Users/qualia/Documents/Papers/2012 - Vijayan, Kopell - Thalamic model of awake alpha oscillations and implications for stimulus processing.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {45}
}

@article{Vilares2017,
  title = {Dopaminergic Medication Increases Reliance on Current Information in {{Parkinson}}'s Disease},
  author = {Vilares, Iris and Kording, Konrad P.},
  year = {2017},
  month = aug,
  volume = {1},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0129},
  file = {/Users/qualia/Documents/Papers/Vilares and Kording - 2017 - Dopaminergic medication increases reliance on curr.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {8}
}

@article{Vinyals,
  title = {{{StarCraft II}}: {{A New Challenge}} for {{Reinforcement Learning}}},
  author = {Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and Agapiou, John and Schrittwieser, Julian and Quan, John and Gaffney, Stephen and Petersen, Stig and Simonyan, Karen and Schaul, Tom and {van Hasselt}, Hado and Silver, David and Lillicrap, Timothy and Calderone, Kevin and Keet, Paul and Brunasso, Anthony and Lawrence, David and Ekermo, Anders and Repp, Jacob and Tsing, Rodney},
  pages = {20},
  abstract = {This paper introduces SC2LE (StarCraft II Learning Environment), a reinforcement learning environment based on the game StarCraft II. This domain poses a new grand challenge for reinforcement learning, representing a more difficult class of problems than considered in most prior work. It is a multi-agent problem with multiple players interacting; there is imperfect information due to a partially observed map; it has a large action space involving the selection and control of hundreds of units; it has a large state space that must be observed solely from raw input feature planes; and it has delayed credit assignment requiring long-term strategies over thousands of steps. We describe the observation, action, and reward specification for the StarCraft II domain and provide an open source Python-based interface for communicating with the game engine. In addition to the main game maps, we provide a suite of mini-games focusing on different elements of StarCraft II gameplay. For the main game maps, we also provide an accompanying dataset of game replay data from human expert players. We give initial baseline results for neural networks trained from this data to predict game outcomes and player actions. Finally, we present initial baseline results for canonical deep reinforcement learning agents applied to the StarCraft II domain. On the mini-games, these agents learn to achieve a level of play that is comparable to a novice player. However, when trained on the main game, these agents are unable to make significant progress. Thus, SC2LE offers a new and challenging environment for exploring deep reinforcement learning algorithms and architectures.},
  file = {/Users/qualia/Documents/Papers/Vinyals et al. - StarCraft II A New Challenge for Reinforcement Le.pdf},
  language = {en}
}

@article{Viriyopase2012,
  title = {When {{Long}}-{{Range Zero}}-{{Lag Synchronization}} Is {{Feasible}} in {{Cortical Networks}}},
  author = {Viriyopase, Atthaphon and Bojak, Ingo and Zeitler, Magteld and Gielen, Stan},
  year = {2012},
  volume = {6},
  issn = {1662-5188},
  doi = {10.3389/fncom.2012.00049},
  file = {/Users/qualia/Documents/Papers/2012 - Viriyopase et al. - When Long-Range Zero-Lag Synchronization is Feasible in Cortical Networks.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Vogel2004,
  title = {Neural Activity Predicts Individual Differences in Visual Working Memory Capacity},
  author = {Vogel, Edward K. and Machizawa, Maro G.},
  year = {2004},
  month = apr,
  volume = {428},
  pages = {748--751},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature02447},
  file = {/Users/qualia/Documents/Papers/2004 - Vogel, Machizawa - Neural activity predicts individual differences in visual working memory capacity.pdf},
  journal = {Nature},
  language = {en},
  number = {6984}
}

@article{Vogels2009,
  title = {Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks},
  author = {Vogels, Tim P and Abbott, L F},
  year = {2009},
  month = apr,
  volume = {12},
  pages = {483--491},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.2276},
  file = {/Users/qualia/Documents/Papers/2009 - Vogels, Abbott - Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks T.P.pdf;/Users/qualia/Documents/Papers/2009 - Vogels, Abbott - Gating Multiple Signals through Detailed Balance of Excitation and Inhibition in Spiking Networks T.P(2).pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {4}
}

@article{Vogelstein2014,
  title = {Discovery of {{Brainwide Neural}}-{{Behavioral Maps}} via {{Multiscale Unsupervised Structure Learning}}},
  author = {Vogelstein, J. T. and Park, Y. and Ohyama, T. and Kerr, R. A. and Truman, J. W. and Priebe, C. E. and Zlatic, M.},
  year = {2014},
  month = apr,
  volume = {344},
  pages = {386--392},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1250298},
  file = {/Users/qualia/Documents/Papers/2014 - Vogelstein et al. - Discovery of Brainwide Neural-Behavioral Maps via Multiscale Unsupervised Structure Learning.pdf;/Users/qualia/Documents/Papers/Vogelstein et al. - 2014 - Discovery of Brainwide Neural-Behavioral Maps via .pdf},
  journal = {Science},
  language = {en},
  number = {6182}
}

@article{vonNicolai2014,
  title = {Corticostriatal {{Coordination}} through {{Coherent Phase}}-{{Amplitude Coupling}}},
  author = {{von Nicolai}, C. and Engler, G. and Sharott, A. and Engel, A. K. and Moll, C. K. and Siegel, M.},
  year = {2014},
  month = apr,
  volume = {34},
  pages = {5938--5948},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5007-13.2014},
  file = {/Users/qualia/Documents/Papers/von Nicolai et al. - 2014 - Corticostriatal Coordination through Coherent Phas.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {17}
}

@article{Voytek2013,
  title = {A Method for Event-Related Phase/Amplitude Coupling},
  author = {Voytek, Bradley and D'Esposito, Mark and Crone, Nathan and Knight, Robert T.},
  year = {2013},
  month = jan,
  volume = {64},
  pages = {416--424},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2012.09.023},
  abstract = {Phase/amplitude coupling (PAC) is emerging as an important electrophysiological measure of local and long-distance neuronal communication. Current techniques for calculating PAC provide a numerical index that represents an average value across an arbitrarily long time period. This requires researchers to rely on block design experiments and temporal concatenation at the cost of the sub-second temporal resolution afforded by electrophysiological recordings. Here we present a method for calculating event-related phase/ amplitude coupling (ERPAC) designed to capture the temporal evolution of task-related changes in PAC across events or between distant brain regions that is applicable to human or animal electromagnetic recording.},
  file = {/Users/qualia/Documents/Papers/Voytek et al. - 2013 - A method for event-related phaseamplitude couplin.pdf},
  journal = {NeuroImage},
  language = {en}
}

@article{Voytek2013a,
  title = {Stimulating the Aging Brain},
  author = {Voytek, Bradley and Gazzaley, Adam},
  year = {2013},
  month = jan,
  volume = {73},
  pages = {1--3},
  issn = {03645134},
  doi = {10.1002/ana.23790},
  file = {/Users/qualia/Documents/Papers/Voytek and Gazzaley - 2013 - Stimulating the aging brain.pdf},
  journal = {Annals of Neurology},
  language = {en},
  number = {1}
}

@article{Voytek2015,
  title = {Dynamic {{Network Communication}} as a {{Unifying Neural Basis}} for {{Cognition}}, {{Development}}, {{Aging}}, and {{Disease}}},
  author = {Voytek, Bradley and Knight, Robert T.},
  year = {2015},
  month = jun,
  volume = {77},
  pages = {1089--1097},
  issn = {00063223},
  doi = {10.1016/j.biopsych.2015.04.016},
  file = {/Users/qualia/Documents/Papers/Voytek and Knight - 2015 - Dynamic Network Communication as a Unifying Neural 2.pdf;/Users/qualia/Documents/Papers/Voytek and Knight - 2015 - Dynamic Network Communication as a Unifying Neural.pdf},
  journal = {Biological Psychiatry},
  language = {en},
  number = {12}
}

@article{Voytek2015a,
  title = {Oscillatory Dynamics Coordinating Human Frontal Networks in Support of Goal Maintenance},
  author = {Voytek, Bradley and Kayser, Andrew S and Badre, David and Fegen, David and Chang, Edward F and Crone, Nathan E and Parvizi, Josef and Knight, Robert T and D'Esposito, Mark},
  year = {2015},
  month = sep,
  volume = {18},
  pages = {1318--1324},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4071},
  file = {/Users/qualia/Documents/Papers/Voytek et al. - 2015 - Oscillatory dynamics coordinating human frontal ne.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {9}
}

@article{Voytek2015c,
  title = {Age-{{Related Changes}} in 1/f {{Neural Electrophysiological Noise}}},
  author = {Voytek, B. and Kramer, M. A. and Case, J. and Lepage, K. Q. and Tempesta, Z. R. and Knight, R. T. and Gazzaley, A.},
  year = {2015},
  month = sep,
  volume = {35},
  pages = {13257--13265},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2332-14.2015},
  file = {/Users/qualia/Documents/Papers/Voytek et al. - 2015 - Age-Related Changes in 1f Neural Electrophysiolog.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {38}
}

@article{Vreeswijk1996,
  title = {Chaos in {{Neuronal Networks}} with {{Balanced Excitatory}} and {{Inhibitory Activity}}},
  author = {v. Vreeswijk, C. and Sompolinsky, H.},
  year = {1996},
  month = dec,
  volume = {274},
  pages = {1724--1726},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.274.5293.1724},
  file = {/Users/qualia/Documents/Papers/1996 - van Vreeswijk, Sompolinsky - Chaos in neuronal networks with balanced excitatory and inhibitory activity.pdf},
  journal = {Science},
  language = {en},
  number = {5293}
}

@article{Vul2008,
  title = {Measuring the {{Crowd Within}}: {{Probabilistic Representations Within Individuals}}},
  shorttitle = {Measuring the {{Crowd Within}}},
  author = {Vul, Edward and Pashler, Harold},
  year = {2008},
  month = jul,
  volume = {19},
  pages = {645--647},
  issn = {0956-7976, 1467-9280},
  doi = {10.1111/j.1467-9280.2008.02136.x},
  file = {/Users/qualia/Documents/Papers/2008 - Vul, Pashler - Measuring the crowd within probabilistic representations within individuals.pdf},
  journal = {Psychological Science},
  language = {en},
  number = {7}
}

@article{Wach2013,
  title = {The Effect of 10 {{Hz}} Transcranial Alternating Current Stimulation ({{tACS}}) on Corticomuscular Coherence},
  author = {Wach, Claudia and Krause, Vanessa and Moliadze, Vera and Paulus, Walter and Schnitzler, Alfons and Pollok, Bettina},
  year = {2013},
  volume = {7},
  issn = {1662-5161},
  doi = {10.3389/fnhum.2013.00511},
  file = {/Users/qualia/Documents/Papers/2013 - Wach et al. - The effect of 10 Hz transcranial alternating current stimulation (tACS) on corticomuscular coherence.pdf;/Users/qualia/Documents/Papers/Wach et al. - 2013 - The effect of 10 Hz transcranial alternating curre.pdf},
  journal = {Frontiers in Human Neuroscience},
  language = {en}
}

@article{Wagenmakers2004,
  title = {{{AIC}} Model Selection Using {{Akaike}} Weights},
  author = {Wagenmakers, Eric-Jan and Farrell, Simon},
  year = {2004},
  month = feb,
  volume = {11},
  pages = {192--196},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03206482},
  file = {/Users/qualia/Documents/Papers/2004 - Wagenmakers, Farrell - AIC model selection using Akaike weights.pdf},
  journal = {Psychonomic Bulletin \& Review},
  language = {en},
  number = {1}
}

@article{Wainrib2013,
  title = {Topological and {{Dynamical Complexity}} of {{Random Neural Networks}}},
  author = {Wainrib, Gilles and Touboul, Jonathan},
  year = {2013},
  month = mar,
  volume = {110},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.110.118101},
  abstract = {Random neural networks are dynamical descriptions of randomly interconnected neural units. These show a phase transition to chaos as a disorder parameter is increased. The microscopic mechanisms underlying this phase transition are unknown, and similarly to spin-glasses, shall be fundamentally related to the behavior of the system. In this Letter we investigate the explosion of complexity arising near that phase transition. We show that the mean number of equilibria undergoes a sharp transition from one equilibrium to a very large number scaling exponentially with the dimension on the system. Near criticality, we compute the exponential rate of divergence, called topological complexity. Strikingly, we show that it behaves exactly as the maximal Lyapunov exponent, a classical measure of dynamical complexity. This relationship unravels a microscopic mechanism leading to chaos which we further demonstrate on a simpler class of disordered systems, suggesting a deep and underexplored link between topological and dynamical complexity.},
  archivePrefix = {arXiv},
  eprint = {1210.5082},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2013 - Wainrib, Touboul - Topological and dynamical complexity of random neural networks.pdf;/Users/qualia/Documents/Papers/Wainrib and Touboul - 2013 - Topological and Dynamical Complexity of Random Neu.pdf},
  journal = {Physical Review Letters},
  keywords = {Condensed Matter - Disordered Systems and Neural Networks,Mathematical Physics,Quantitative Biology - Neurons and Cognition},
  language = {en},
  number = {11}
}

@article{Wald1947,
  title = {Foundations of a {{General Theory}} of {{Sequential Decision Functions}}},
  author = {Wald, Abraham},
  year = {1947},
  month = oct,
  volume = {15},
  pages = {279},
  issn = {00129682},
  doi = {10.2307/1905331},
  file = {/Users/qualia/Documents/Papers/2012 - Unknown - No Title.pdf},
  journal = {Econometrica},
  language = {en},
  number = {4}
}

@article{Walker2001,
  title = {Minimax {{Play}} at {{Wimbledon}}},
  author = {Walker, Mark and Wooders, John},
  year = {2001},
  month = dec,
  volume = {91},
  pages = {1521--1538},
  issn = {0002-8282},
  doi = {10.1257/aer.91.5.1521},
  abstract = {We use data from classic professional tennis matches to provide an empirical test of the theory of mixed strategy equilibrium. We {\TH}nd that the serve-andreturn play of John McEnroe, Bjorn Borg, Boris Becker, Pete Sampras and others is consistent with equilibrium play. The same statistical tests soundly reject the assumption of equilibrium play in experimental data, including the data from Barry O'Neill's celebrated experiment.},
  file = {/Users/qualia/Documents/Papers/Walker and Wooders - 2001 - Minimax Play at Wimbledon.pdf},
  journal = {American Economic Review},
  language = {en},
  number = {5}
}

@article{Walther2009,
  title = {Natural {{Scene Categories Revealed}} in {{Distributed Patterns}} of {{Activity}} in the {{Human Brain}}},
  author = {Walther, D. B. and Caddigan, E. and {Fei-Fei}, L. and Beck, D. M.},
  year = {2009},
  month = aug,
  volume = {29},
  pages = {10573--10581},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0559-09.2009},
  file = {/Users/qualia/Documents/Papers/2009 - Walther et al. - Natural scene categories revealed in distributed patterns of activity in the human brain.pdf;/Users/qualia/Zotero/storage/B245PG9T/2015 - Biphenyls - HHS Public Access.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {34}
}

@article{Wang1996,
  title = {Gamma {{Oscillation}} by {{Synaptic Inhibition}} in a {{Hippocampal Interneuronal Network Model}}},
  author = {Wang, Xiao-Jing and Buzs{\'a}ki, Gy{\"o}rgy},
  year = {1996},
  month = oct,
  volume = {16},
  pages = {6402--6413},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.16-20-06402.1996},
  file = {/Users/qualia/Documents/Papers/Wang and Buzsáki - 1996 - Gamma Oscillation by Synaptic Inhibition in a Hipp.pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {20}
}

@article{Wang1998,
  title = {Calcium {{Coding}} and {{Adaptive Temporal Computation}} in {{Cortical Pyramidal Neurons}}},
  author = {Wang, Xiao-Jing},
  year = {1998},
  month = mar,
  volume = {79},
  pages = {1549--1566},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.1998.79.3.1549},
  file = {/Users/qualia/Documents/Papers/1998 - Wang - Calcium coding and adaptive temporal computation in cortical pyramidal neurons.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {3}
}

@article{Wang2002,
  title = {Probabilistic {{Decision Making}} by {{Slow Reverberation}} in {{Cortical Circuits}}},
  author = {Wang, Xiao-Jing},
  year = {2002},
  month = dec,
  volume = {36},
  pages = {955--968},
  issn = {08966273},
  doi = {10.1016/S0896-6273(02)01092-9},
  abstract = {Recent physiological studies of alert primates have revealed cortical neural correlates of key steps in a perceptual decision-making process. To elucidate synaptic mechanisms of decision making, I investigated a biophysically realistic cortical network model for a visual discrimination experiment. In the model, slow recurrent excitation and feedback inhibition produce attractor dynamics that amplify the difference between conflicting inputs and generates a binary choice. The model is shown to account for salient characteristics of the observed decision-correlated neural activity, as well as the animal's psychometric function and reaction times. These results suggest that recurrent excitation mediated by NMDA receptors provides a candidate cellular mechanism for the slow time integration of sensory stimuli and the formation of categorical choices in a decision-making neocortical network.},
  file = {/Users/qualia/Documents/Papers/2002 - Wang - by Slow Reverberation in Cortical Circuits.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Wang2010,
  title = {Neurophysiological and {{Computational Principles}} of {{Cortical Rhythms}} in {{Cognition}}},
  author = {Wang, Xiao-Jing},
  year = {2010},
  month = jul,
  volume = {90},
  pages = {1195--1268},
  issn = {0031-9333, 1522-1210},
  doi = {10.1152/physrev.00035.2008},
  file = {/Users/qualia/Documents/Papers/2010 - Wang - Neurophysiological and computational principles of cortical rhythms in cognition.pdf;/Users/qualia/Documents/Papers/2010 - Wang - Neurophysiological and computational principles of cortical rhythms in cognition(2).pdf},
  journal = {Physiological Reviews},
  language = {en},
  number = {3}
}

@article{Wang2010a,
  title = {Synchrony of {{Thalamocortical Inputs Maximizes Cortical Reliability}}},
  author = {Wang, H. P. and Spencer, D. and Fellous, J. M. and Sejnowski, T. J.},
  year = {2010},
  month = apr,
  volume = {328},
  pages = {106--109},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1183108},
  file = {/Users/qualia/Documents/Papers/2010 - Wang et al. - Synchrony of Thalamocortical Inputs Maximizes Cortical Reliability.pdf},
  journal = {Science},
  language = {en},
  number = {5974}
}

@article{Wang2013,
  title = {A {{Realistic Neural Mass Model}} of the {{Cortex}} with {{Laminar}}-{{Specific Connections}} and {{Synaptic Plasticity}} \textendash{} {{Evaluation}} with {{Auditory Habituation}}},
  author = {Wang, Peng and Kn{\"o}sche, Thomas R.},
  editor = {Lytton, William W.},
  year = {2013},
  month = oct,
  volume = {8},
  pages = {e77876},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0077876},
  abstract = {In this work we propose a biologically realistic local cortical circuit model (LCCM), based on neural masses, that incorporates important aspects of the functional organization of the brain that have not been covered by previous models: (1) activity dependent plasticity of excitatory synaptic couplings via depleting and recycling of neurotransmitters and (2) realistic interlaminar dynamics via laminar-specific distribution of and connections between neural populations. The potential of the LCCM was demonstrated by accounting for the process of auditory habituation. The model parameters were specified using Bayesian inference. It was found that: (1) besides the major serial excitatory information pathway (layer 4 to layer 2/3 to layer 5/6), there exists a parallel ``short-cut'' pathway (layer 4 to layer 5/6), (2) the excitatory signal flow from the pyramidal cells to the inhibitory interneurons seems to be more intra-laminar while, in contrast, the inhibitory signal flow from inhibitory interneurons to the pyramidal cells seems to be both intra- and inter-laminar, and (3) the habituation rates of the connections are unsymmetrical: forward connections (from layer 4 to layer 2/3) are more strongly habituated than backward connections (from Layer 5/6 to layer 4). Our evaluation demonstrates that the novel features of the LCCM are of crucial importance for mechanistic explanations of brain function. The incorporation of these features into a mass model makes them applicable to modeling based on macroscopic data (like EEG or MEG), which are usually available in human experiments. Our LCCM is therefore a valuable building block for future realistic models of human cognitive function.},
  file = {/Users/qualia/Documents/Papers/2013 - Wang, KnÃ¶sche - A Realistic Neural Mass Model of the Cortex with Laminar-Specific Connections and Synaptic Plasticity Evaluati.pdf;/Users/qualia/Documents/Papers/Wang and Knösche - 2013 - A Realistic Neural Mass Model of the Cortex with L.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {10}
}

@article{Wang2016,
  title = {Brain Structure and Dynamics across Scales: In Search of Rules},
  shorttitle = {Brain Structure and Dynamics across Scales},
  author = {Wang, Xiao-Jing and Kennedy, Henry},
  year = {2016},
  month = apr,
  volume = {37},
  pages = {92--98},
  issn = {09594388},
  doi = {10.1016/j.conb.2015.12.010},
  file = {/Users/qualia/Documents/Papers/Wang and Kennedy - 2016 - Brain structure and dynamics across scales in sea.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{Wang2017,
  title = {{{SAMPLE EFFICIENT ACTOR}}-{{CRITIC WITH EXPERIENCE REPLAY}}},
  author = {Wang, Ziyu and Bapst, Victor and Mnih, Volodymyr and Munos, Remi and {de Freitas}, Nando and Heess, Nicolas and Kavukcuoglu, Koray},
  year = {2017},
  pages = {20},
  abstract = {This paper presents an actor-critic deep reinforcement learning agent with experience replay that is stable, sample efficient, and performs remarkably well on challenging environments, including the discrete 57-game Atari domain and several continuous control problems. To achieve this, the paper introduces several innovations, including truncated importance sampling with bias correction, stochastic dueling network architectures, and a new trust region policy optimization method.},
  file = {/Users/qualia/Documents/Papers/Wang et al. - 2017 - SAMPLE EFFICIENT ACTOR-CRITIC WITH EXPERIENCE REPL.pdf},
  language = {en}
}

@article{Wang2019,
  title = {Monkeys Are Curious about Counterfactual Outcomes},
  author = {Wang, Maya Zhe and Hayden, Benjamin Y.},
  year = {2019},
  month = aug,
  volume = {189},
  pages = {1--10},
  issn = {00100277},
  doi = {10.1016/j.cognition.2019.03.009},
  abstract = {Many non-human animals show exploratory behaviors. It remains unclear whether any possess human-like curiosity. We previously proposed three criteria for applying the term curiosity to animal behavior: (1) the subject is willing to sacrifice reward to obtain information, (2) the information provides no immediate instrumental or strategic benefit, and (3) the amount the subject is willing to pay depends systematically on the amount of information available. In previous work on information-seeking in animals, information generally predicts upcoming rewards, and animals' decisions may therefore be a byproduct of reinforcement processes. Here we get around this potential confound by taking advantage of macaques' ability to reason counterfactually (that is, about outcomes that could have occurred had the subject chosen differently). Specifically, macaques sacrificed fluid reward to obtain information about counterfactual outcomes. Moreover, their willingness to pay scaled with the information (Shannon entropy) offered by the counterfactual option. These results demonstrate the existence of human-like curiosity in non-human primates according to our criteria, which circumvent several confounds associated with less stringent criteria.},
  file = {/Users/qualia/Documents/Papers/Wang and Hayden - 2019 - Monkeys are curious about counterfactual outcomes.pdf},
  journal = {Cognition},
  language = {en}
}

@article{Waschke2017,
  title = {States and Traits of Neural Irregularity in the Age-Varying Human Brain},
  author = {Waschke, Leonhard and Woestmann, Malte and Obleser, Jonas},
  year = {2017},
  month = may,
  doi = {10.1101/103432},
  abstract = {Humans sometimes do perceive differences where physically there are none. It is thus tenable that perception is susceptible to seemingly random fluctuations in brain activity or ``neural noise''. Here, we demonstrate the potency of both trial-aggregated as well as trial-by-trial measures in the human electroencephalogram (EEG) to characterize neural noise as (i) a trait of individuals of varying age (n = 19; 19\textendash{}74 years), and (ii) a brain state that predicts an individual's impending perceptual decision. Human participants were instructed to discriminate two identical, consecutively presented pure tones. Behaviorally, all participants reported perceiving pitch differences of first versus second tone. Neurally, decisions for the first versus the second tone were preceded by more consistently phase-locked responses to the first tone in the theta (4\textendash{}9 Hz) band at central scalp electrodes. Second, a trial-wise information-theoretic measure quantifying the irregularity of broadband EEG, Weighted Permutation Entropy (WPE), prior to stimulus onset allowed to classify a listener's impending decision on this trial. Average entropy not only increased with participants' age, but correlated with previously suggested measures of an altered excitation\textendash{}inhibition balance in the aging brain. Therefore, neural noise is best conceived not only as a state variable that can shape perceptual decisions but moreover can capture trait-like changes with age.},
  file = {/Users/qualia/Documents/Papers/Waschke et al. - 2017 - States and traits of neural irregularity in the ag 2.pdf;/Users/qualia/Documents/Papers/Waschke et al. - 2017 - States and traits of neural irregularity in the ag.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Watanabe2011,
  title = {Prediction of Subsequent Recognition Performance Using Brain Activity in the Medial Temporal Lobe},
  author = {Watanabe, Takamitsu and Hirose, Satoshi and Wada, Hiroyuki and Katsura, Masaki and Chikazoe, Junichi and Jimura, Koji and Imai, Yoshio and Machida, Toru and Shirouzu, Ichiro and Miyashita, Yasushi and Konishi, Seiki},
  year = {2011},
  month = feb,
  volume = {54},
  pages = {3085--3092},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.10.066},
  abstract = {Application of multivoxel pattern analysis (MVPA) to functional magnetic resonance imaging (fMRI) data enables reconstruction and classification of cognitive status from brain activity. However, previous studies using MVPA have extracted information about cognitive status that is experienced simultaneously with fMRI scanning, but not one that will be observed after the scanning. In this study, by focusing on activity in the medial temporal lobe (MTL), we demonstrate that MVPA on fMRI data is capable of predicting subsequent recognition performance. In this experiment, six runs of fMRI signals were acquired during encoding of phonogram stimuli. In the analysis, using data acquired in runs 1\textendash{}3, we first conducted MVPA-based voxelwise search for the clusters in the MTL whose signals contained the most information about subsequent recognition performance. Next, using the fMRI signals acquired in runs 1\textendash{}3 from the selected clusters, we trained a classifier function in MVPA. Finally, the trained classifier function was applied to fMRI signals acquired in runs 4\textendash{}6. Consequently, we succeeded in predicting the subsequent recognition performance for stimuli studied in runs 4\textendash{}6 with significant accuracy. This accurate prediction suggests that MVPA can extract information that is associated not only with concurrent cognitive status, but also with behavior in the near future.},
  file = {/Users/qualia/Documents/Papers/2011 - Watanabe et al. - Prediction of subsequent recognition performance using brain activity in the medial temporal lobe.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {4}
}

@article{Watrous2013,
  title = {Frequency-Specific Network Connectivity Increases Underlie Accurate Spatiotemporal Memory Retrieval},
  author = {Watrous, Andrew J and Tandon, Nitin and Conner, Chris R and Pieters, Thomas and Ekstrom, Arne D},
  year = {2013},
  month = mar,
  volume = {16},
  pages = {349--356},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3315},
  file = {/Users/qualia/Documents/Papers/Watrous et al. - 2013 - Frequency-specific network connectivity increases .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Watts1998,
  title = {Collective Dynamics of `Small-World' Networks},
  author = {Watts, Duncan J and Strogatz, Steven H},
  year = {1998},
  volume = {393},
  pages = {3},
  file = {/Users/qualia/Documents/Papers/1998 - Watts, Strogatz - Collective dynamics of ‘small-world’ networks.pdf},
  language = {en}
}

@article{Wayne,
  title = {Unsupervised {{Predictive Memory}} in a {{Goal}}-{{Directed Agent}}},
  author = {Wayne, Greg and Hung, Chia-Chun and Amos, David and Mirza, Mehdi and Ahuja, Arun and {Grabska-Barwinska}, Agnieszka and Rae, Jack and Mirowski, Piotr and Leibo, Joel Z and Santoro, Adam and Gemici, Mevlana and Reynolds, Malcolm and Harley, Tim and Abramson, Josh and Mohamed, Shakir and Rezende, Danilo and Saxton, David and Cain, Adam and Hillier, Chloe and Silver, David and Kavukcuoglu, Koray and Botvinick, Matt and Hassabis, Demis and Lillicrap, Timothy},
  pages = {57},
  file = {/Users/qualia/Documents/Papers/Wayne et al. - Unsupervised Predictive Memory in a Goal-Directed .pdf},
  language = {en}
}

@article{Weber,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Th{\'e}ophane and Racani{\`e}re, S{\'e}bastien and Reichert, David P and Buesing, Lars and Guez, Arthur and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  pages = {20},
  abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
  file = {/Users/qualia/Documents/Papers/Weber et al. - Imagination-Augmented Agents for Deep Reinforcemen.pdf},
  language = {en}
}

@article{Weibull,
  title = {{{WHAT HAVE WE LEARNED FROM EVOLUTIONARY GAME THEORY SO FAR}}?},
  author = {Weibull, J{\"o}rgen W},
  pages = {30},
  file = {/Users/qualia/Documents/Papers/1997 - Wiebull - What have we learned from Evolutionary Game Theory so far.pdf},
  language = {en}
}

@article{Weichwald2015,
  title = {Causal Interpretation Rules for Encoding and Decoding Models in Neuroimaging},
  author = {Weichwald, Sebastian and Meyer, Timm and {\"O}zdenizci, Ozan and Sch{\"o}lkopf, Bernhard and Ball, Tonio and {Grosse-Wentrup}, Moritz},
  year = {2015},
  month = apr,
  volume = {110},
  pages = {48--59},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2015.01.036},
  abstract = {Causal terminology is often introduced in the interpretation of encoding and decoding models trained on neuroimaging data. In this article, we investigate which causal statements are warranted and which ones are not supported by empirical evidence. We argue that the distinction between encoding and decoding models is not sufficient for this purpose: relevant features in encoding and decoding models carry a different meaning in stimulus- and in response-based experimental paradigms.We show that only encoding models in the stimulus-based setting support unambiguous causal interpretations. By combining encoding and decoding models trained on the same data, however, we obtain insights into causal relations beyond those that are implied by each individual model type. We illustrate the empirical relevance of our theoretical findings on EEG data recorded during a visuo-motor learning task.},
  file = {/Users/qualia/Documents/Papers/Weichwald et al. - 2015 - Causal interpretation rules for encoding and decod 2.pdf;/Users/qualia/Documents/Papers/Weichwald et al. - 2015 - Causal interpretation rules for encoding and decod.pdf},
  journal = {NeuroImage},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Applications,Statistics - Machine Learning},
  language = {en}
}

@article{Weinstein,
  title = {Structure {{Learning}} in {{Motor Control}}: {{A Deep Reinforcement Learning Model}}},
  author = {Weinstein, Ari and Botvinick, Matthew M},
  pages = {6},
  abstract = {Motor adaptation displays a structure-learning effect: adaptation to a new perturbation occurs more quickly when the subject has prior exposure to perturbations with related structure. Although this `learning-to-learn' effect is well documented, its underlying computational mechanisms are poorly understood. We present a new model of motor structure learning, approaching it from the point of view of deep reinforcement learning. Previous work outside of motor control has shown how recurrent neural networks can account for learning-to-learn effects. We leverage this insight to address motor learning, by importing it into the setting of model-based reinforcement learning. We apply the resulting processing architecture to empirical findings from a landmark study of structure learning in targetdirected reaching (Braun et al., 2009), and discuss its implications for a wider range of learning-to-learn phenomena.},
  file = {/Users/qualia/Documents/Papers/Weinstein and Botvinick - Structure Learning in Motor Control A Deep Reinfo.pdf},
  language = {en}
}

@article{Weissman2004,
  title = {Calcium {{Waves Propagate}} through {{Radial Glial Cells}} and {{Modulate Proliferation}} in the {{Developing Neocortex}}},
  author = {Weissman, Tamily A. and Riquelme, Patricio A. and Ivic, Lidija and Flint, Alexander C. and Kriegstein, Arnold R.},
  year = {2004},
  month = sep,
  volume = {43},
  pages = {647--661},
  issn = {08966273},
  doi = {10.1016/j.neuron.2004.08.015},
  abstract = {The majority of neurons in the adult neocortex are produced embryonically during a brief but intense period of neuronal proliferation. The radial glial cell, a transient embryonic cell type known for its crucial role in neuronal migration, has recently been shown to function as a neuronal progenitor cell and appears to produce most cortical pyramidal neurons. Radial glial cell modulation could thus affect neuron production, neuronal migration, and overall cortical architecture; however, signaling mechanisms among radial glia have not been studied directly. We demonstrate here that calcium waves propagate through radial glial cells in the proliferative cortical ventricular zone (VZ). Radial glial calcium waves occur spontaneously and require connexin hemichannels, P2Y1 ATP receptors, and intracellular IP3-mediated calcium release. Furthermore, we show that wave disruption decreases VZ proliferation during the peak of embryonic neurogenesis. Taken together, these results demonstrate a radial glial signaling mechanism that may regulate cortical neuronal production.},
  file = {/Users/qualia/Documents/Papers/Weissman et al. - 2004 - Calcium Waves Propagate through Radial Glial Cells.pdf},
  journal = {Neuron},
  language = {en},
  number = {5}
}

@article{Welford1962,
  title = {Note on a {{Method}} for {{Calculating Corrected Sums}} of {{Squares}} and {{Products}}},
  author = {Welford, B. P.},
  year = {1962},
  month = aug,
  volume = {4},
  pages = {419--420},
  issn = {0040-1706, 1537-2723},
  doi = {10.1080/00401706.1962.10490022},
  file = {/Users/qualia/Documents/Papers/1962 - Welford - Note on a Method for Calculating Corrected Sums of Squares and Products.pdf;/Users/qualia/Documents/Papers/Welford - 1962 - Note on a Method for Calculating Corrected Sums of.pdf},
  journal = {Technometrics},
  language = {en},
  number = {3}
}

@article{Wen2016,
  title = {Separating {{Fractal}} and {{Oscillatory Components}} in the {{Power Spectrum}} of {{Neurophysiological Signal}}},
  author = {Wen, Haiguang and Liu, Zhongming},
  year = {2016},
  month = jan,
  volume = {29},
  pages = {13--26},
  issn = {0896-0267, 1573-6792},
  doi = {10.1007/s10548-015-0448-0},
  file = {/Users/qualia/Documents/Papers/Wen and Liu - 2016 - Separating Fractal and Oscillatory Components in t 2.pdf;/Users/qualia/Documents/Papers/Wen and Liu - 2016 - Separating Fractal and Oscillatory Components in t.pdf},
  journal = {Brain Topography},
  language = {en},
  number = {1}
}

@article{Wendling2002,
  title = {Epileptic Fast Activity Can Be Explained by a Model of Impaired {{GABAergic}} Dendritic Inhibition: {{Epileptic}} Activity Explained by Dendritic Dis-Inhibition},
  shorttitle = {Epileptic Fast Activity Can Be Explained by a Model of Impaired {{GABAergic}} Dendritic Inhibition},
  author = {Wendling, F. and Bartolomei, F. and Bellanger, J. J. and Chauvel, P.},
  year = {2002},
  month = may,
  volume = {15},
  pages = {1499--1508},
  issn = {0953816X},
  doi = {10.1046/j.1460-9568.2002.01985.x},
  abstract = {This paper focuses on high-frequency (gamma band) EEG activity, the most characteristic electrophysiological pattern in focal seizures of human epilepsy. It starts with recent hypotheses about: (i) the behaviour of inhibitory interneurons in hippocampal or neocortical networks in the generation of gamma frequency oscillations; (ii) the nonuniform alteration of GABAergic inhibition in experimental epilepsy (reduced dendritic inhibition and increased somatic inhibition); and (iii) the possible depression of GABAA,fast circuit activity by GABAA,slow inhibitory postsynaptic currents. In particular, these hypotheses are introduced in a new computational macroscopic model of EEG activity that includes a physiologically relevant fast inhibitory feedback loop. Results show that strikingly realistic activity is produced by the model when compared to real EEG signals recorded with intracerebral electrodes. They show that, in the model, the transition from interictal to fast ictal activity is explained by the impairment of dendritic inhibition.},
  file = {/Users/qualia/Documents/Papers/2002 - Wendling et al. - Epileptic fast activity can be explained by a model of impaired GABAergic dendritic inhibition.pdf},
  journal = {European Journal of Neuroscience},
  language = {en},
  number = {9}
}

@article{Wessel2020,
  title = {{$\beta$}-{{Bursts Reveal}} the {{Trial}}-to-{{Trial Dynamics}} of {{Movement Initiation}} and {{Cancellation}}},
  author = {Wessel, Jan R.},
  year = {2020},
  month = jan,
  volume = {40},
  pages = {411--423},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1887-19.2019},
  file = {/Users/qualia/Documents/Papers/Wessel - 2020 - β-Bursts Reveal the Trial-to-Trial Dynamics of Mov.pdf},
  journal = {J. Neurosci.},
  language = {en},
  number = {2}
}

@article{West2002,
  title = {Sexual {{Selection}}, {{Temperature}}, and the {{Lion}}'s {{Mane}}},
  author = {West, P. M.},
  year = {2002},
  month = aug,
  volume = {297},
  pages = {1339--1343},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1073257},
  file = {/Users/qualia/Documents/Papers/West - 2002 - Sexual Selection, Temperature, and the Lion's Mane.pdf},
  journal = {Science},
  language = {en},
  number = {5585}
}

@article{West2005,
  title = {Neither a Token of Royalty nor a Shield for Fighting, the Mane Is a Signal of Quality to Mates and Rivals, but One That Comes with Consequences},
  author = {West, Peyton M},
  year = {2005},
  pages = {11},
  file = {/Users/qualia/Documents/Papers/West - 2005 - Neither a token of royalty nor a shield for fighti.pdf},
  language = {en}
}

@article{White2012,
  title = {Perceptual {{Criteria}} in the {{Human Brain}}},
  author = {White, C. N. and Mumford, J. A. and Poldrack, R. A.},
  year = {2012},
  month = nov,
  volume = {32},
  pages = {16716--16724},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1744-12.2012},
  file = {/Users/qualia/Documents/Papers/2012 - White, Mumford, Poldrack - Perceptual Criteria in the Human Brain.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {47}
}

@techreport{White2019,
  title = {A Neural Network for Information Seeking},
  author = {White, J Kael and {Bromberg-Martin}, Ethan S and Heilbronner, Sarah R and Zhang, Kaining and Pai, Julia and Haber, Suzanne N and Monosov, Ilya E},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/720433},
  abstract = {Humans and other animals often show a strong desire to know the uncertain rewards their future has in store, even when they cannot use this information to influence the outcome. However, it is unknown how the brain predicts opportunities to gain information and motivates this information seeking behavior. Here we show that neurons in a network of interconnected subregions of primate anterior cingulate cortex and basal ganglia predict the moment of gaining information about uncertain rewards. Spontaneous increases in their information prediction signals are followed by gaze shifts toward objects associated with resolving uncertainty, and pharmacologically disrupting this network reduces the motivation to seek information. These findings demonstrate a cortico-basal ganglia mechanism responsible for motivating actions to resolve uncertainty by seeking knowledge about the future.},
  file = {/Users/qualia/Documents/Papers/White et al. - 2019 - A neural network for information seeking.pdf},
  language = {en},
  type = {Preprint}
}

@article{Whitten2011,
  title = {A Better Oscillation Detection Method Robustly Extracts {{EEG}} Rhythms across Brain State Changes: {{The}} Human Alpha Rhythm as a Test Case},
  shorttitle = {A Better Oscillation Detection Method Robustly Extracts {{EEG}} Rhythms across Brain State Changes},
  author = {Whitten, Tara A. and Hughes, Adam M. and Dickson, Clayton T. and Caplan, Jeremy B.},
  year = {2011},
  month = jan,
  volume = {54},
  pages = {860--874},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2010.08.064},
  abstract = {Oscillatory activity is a principal mode of operation in the brain. Despite an intense resurgence of interest in the mechanisms and functions of brain rhythms, methods for the detection and analysis of oscillatory activity in neurophysiological recordings are still highly variable across studies. We recently proposed a method for detecting oscillatory activity from time series data, which we call the BOSC (Better OSCillation detection) method. This method produces systematic, objective, and consistent results across frequencies, brain regions and tasks. It does so by modeling the functional form of the background spectrum by fitting the empirically observed spectrum at the recording site. This minimizes bias in oscillation detection across frequency, region and task. Here we show that the method is also robust to dramatic changes in state that are known to influence the shape of the power spectrum, namely, the presence versus absence of the alpha rhythm, and can be applied to independent components, which are thought to reflect underlying sources, in addition to individual raw signals. This suggests that the BOSC method is an effective tool for measuring changes in rhythmic activity in the more common research scenario wherein state is unknown.},
  file = {/Users/qualia/Documents/Papers/2011 - Whitten et al. - A better oscillation detection method robustly extracts EEG rhythms across brain state changes The human alpha r.pdf},
  journal = {NeuroImage},
  language = {en},
  number = {2}
}

@article{Whittington2019,
  title = {Theories of {{Error Back}}-{{Propagation}} in the {{Brain}}},
  author = {Whittington, James C.R. and Bogacz, Rafal},
  year = {2019},
  month = mar,
  volume = {23},
  pages = {235--250},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.12.005},
  file = {/Users/qualia/Documents/Papers/Whittington and Bogacz - 2019 - Theories of Error Back-Propagation in the Brain.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {3}
}

@article{Whittington2019a,
  title = {Theories of {{Error Back}}-{{Propagation}} in the {{Brain}}},
  author = {Whittington, James C.R. and Bogacz, Rafal},
  year = {2019},
  month = mar,
  volume = {23},
  pages = {235--250},
  issn = {13646613},
  doi = {10.1016/j.tics.2018.12.005},
  file = {/Users/qualia/Documents/Papers/Whittington and Bogacz - 2019 - Theories of Error Back-Propagation in the Brain 2.pdf},
  journal = {Trends in Cognitive Sciences},
  language = {en},
  number = {3}
}

@article{Wickham,
  title = {A Grammar of Graphics: Past, Present, and Future},
  author = {Wickham, Hadley},
  pages = {45},
  file = {/Users/qualia/Documents/Papers/Wickham - A grammar of graphics past, present, and future.pdf},
  language = {en}
}

@article{Widmann2015,
  title = {Digital Filter Design for Electrophysiological Data \textendash{} a Practical Approach},
  author = {Widmann, Andreas and Schr{\"o}ger, Erich and Maess, Burkhard},
  year = {2015},
  month = jul,
  volume = {250},
  pages = {34--46},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2014.08.002},
  abstract = {Background: Filtering is a ubiquitous step in the preprocessing of electroencephalographic (EEG) and magnetoencephalographic (MEG) data. Besides the intended effect of the attenuation of signal components considered as noise, filtering can also result in various unintended adverse filter effects (distortions such as smoothing) and filter artifacts.
Method: We give some practical guidelines for the evaluation of filter responses (impulse and frequency response) and the selection of filter types (high-pass/low-pass/band-pass/band-stop; finite/infinite impulse response, FIR/IIR) and filter parameters (cutoff frequencies, filter order and roll-off, ripple, delay and causality) to optimize signal-to-noise ratio and avoid or reduce signal distortions for selected electrophysiological applications.
Results: Various filter implementations in common electrophysiology software packages are introduced and discussed. Resulting filter responses are compared and evaluated.
Conclusion: We present strategies for recognizing common adverse filter effects and filter artifacts and demonstrate them in practical examples. Best practices and recommendations for the selection and reporting of filter parameters, limitations, and alternatives to filtering are discussed.},
  file = {/Users/qualia/Documents/Papers/Widmann et al. - 2015 - Digital filter design for electrophysiological dat.pdf},
  journal = {Journal of Neuroscience Methods},
  language = {en}
}

@article{Williams2018,
  title = {Escape {{Dynamics}} in {{Learning Models}}},
  author = {Williams, Noah},
  year = {2018},
  month = jun,
  issn = {0034-6527, 1467-937X},
  doi = {10.1093/restud/rdy033},
  file = {/Users/qualia/Documents/Papers/2014 - Williams - Escape Dynamics in Learning Models.pdf;/Users/qualia/Documents/Papers/Williams - 2018 - Escape Dynamics in Learning Models.pdf},
  journal = {The Review of Economic Studies},
  language = {en}
}

@techreport{Williams2019,
  title = {Discovering Precise Temporal Patterns in Large-Scale Neural Recordings through Robust and Interpretable Time Warping},
  author = {Williams, Alex H. and Poole, Ben and Maheswaranathan, Niru and Dhawale, Ashesh K. and Fisher, Tucker and Wilson, Christopher D. and Brann, David H. and Trautmann, Eric and Ryu, Stephen and Shusterman, Roman and Rinberg, Dmitry and {\"O}lveczky, Bence P. and Shenoy, Krishna V. and Ganguli, Surya},
  year = {2019},
  month = jun,
  institution = {{Neuroscience}},
  doi = {10.1101/661165},
  abstract = {Abstract
          Though the temporal precision of neural computation has been studied intensively, a data-driven determination of this precision remains a fundamental challenge. Reproducible spike time patterns may be obscured on single trials by uncontrolled temporal variability in behavior and cognition, or may not even be time locked to measurable signatures in either behavior or local field potentials (LFP). To overcome these challenges, we describe a general-purpose time warping framework that reveals precise spike-time patterns in an unsupervised manner, even when spiking is decoupled from behavior or is temporally stretched across single trials. We demonstrate this method across diverse systems: cued reaching in nonhuman primates, motor sequence production in rats, and olfaction in mice. This approach flexibly uncovers diverse dynamical firing patterns, including pulsatile responses to behavioral events, LFP-aligned oscillatory spiking, and even unanticipated patterns, like 7 Hz oscillations in rat motor cortex that are not time-locked to measured behaviors or LFP.},
  file = {/Users/qualia/Documents/Papers/Williams et al. - 2019 - Discovering precise temporal patterns in large-sca.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{Williams2019a,
  title = {Discovering Precise Temporal Patterns in Large-Scale Neural Recordings through Robust and Interpretable Time Warping},
  author = {Williams, Alex H. and Poole, Ben and Maheswaranathan, Niru and Dhawale, Ashesh K. and Fisher, Tucker and Wilson, Christopher D. and Brann, David H. and Trautmann, Eric and Ryu, Stephen and Shusterman, Roman and Rinberg, Dmitry and {\"O}lveczky, Bence P. and Shenoy, Krishna V. and Ganguli, Surya},
  year = {2019},
  month = jun,
  institution = {{Neuroscience}},
  doi = {10.1101/661165},
  abstract = {Abstract
          Though the temporal precision of neural computation has been studied intensively, a data-driven determination of this precision remains a fundamental challenge. Reproducible spike time patterns may be obscured on single trials by uncontrolled temporal variability in behavior and cognition, or may not even be time locked to measurable signatures in either behavior or local field potentials (LFP). To overcome these challenges, we describe a general-purpose time warping framework that reveals precise spike-time patterns in an unsupervised manner, even when spiking is decoupled from behavior or is temporally stretched across single trials. We demonstrate this method across diverse systems: cued reaching in nonhuman primates, motor sequence production in rats, and olfaction in mice. This approach flexibly uncovers diverse dynamical firing patterns, including pulsatile responses to behavioral events, LFP-aligned oscillatory spiking, and even unanticipated patterns, like 7 Hz oscillations in rat motor cortex that are not time-locked to measured behaviors or LFP.},
  file = {/Users/qualia/Documents/Papers/Williams et al. - 2019 - Discovering precise temporal patterns in large-sca 2.pdf},
  language = {en},
  type = {Preprint}
}

@article{Wilson1972,
  title = {Excitatory and {{Inhibitory Interactions}} in {{Localized Populations}} of {{Model Neurons}}},
  author = {Wilson, Hugh R. and Cowan, Jack D.},
  year = {1972},
  month = jan,
  volume = {12},
  pages = {1--24},
  issn = {00063495},
  doi = {10.1016/S0006-3495(72)86068-5},
  file = {/Users/qualia/Documents/Papers/1972 - Wilson, Cowan - Excitatory and inhibitory interactions in localized populations of model neurons.pdf},
  journal = {Biophysical Journal},
  language = {en},
  number = {1}
}

@article{Wilson1973,
  title = {A Mathematical Theory of the Functional Dynamics of Cortical and Thalamic Nervous Tissue},
  author = {Wilson, H. R. and Cowan, J. D.},
  year = {1973},
  month = sep,
  volume = {13},
  pages = {55--80},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/BF00288786},
  abstract = {It is proposed that distinct anatomical regions of cerebral cortex and of thalamic nuclei are functionally two-dimensional. On this view, the third (radial) dimension of cortical and thalamic structures is associated with a redundancy of circuits and functions so that reliable signal processing obtains in the presence of noisy or ambiguous stimuli.},
  file = {/Users/qualia/Documents/Papers/1973 - Wilson, Cowan - A mathematical theory of the functional dynamics of cortical and thalamic nervous tissue.pdf},
  journal = {Kybernetik},
  language = {en},
  number = {2}
}

@article{Wilson2015,
  title = {Clustered {{Desynchronization}} from {{High}}-{{Frequency Deep Brain Stimulation}}},
  author = {Wilson, Dan and Moehlis, Jeff},
  editor = {Diedrichsen, J{\"o}rn},
  year = {2015},
  month = dec,
  volume = {11},
  pages = {e1004673},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004673},
  file = {/Users/qualia/Documents/Papers/Wilson and Moehlis - 2015 - Clustered Desynchronization from High-Frequency De.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{Wilson2015a,
  title = {Is {{Model Fitting Necessary}} for {{Model}}-{{Based fMRI}}?},
  author = {Wilson, Robert C. and Niv, Yael},
  editor = {Boorman, Erie Dell},
  year = {2015},
  month = jun,
  volume = {11},
  pages = {e1004237},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004237},
  abstract = {OPEN ACCESS Citation: Wilson RC, Niv Y (2015) Is Model Fitting Necessary for Model-Based fMRI?. PLoS Comput Biol 11(6): e1004237. doi:10.1371/journal.},
  file = {/Users/qualia/Documents/Papers/Wilson and Niv - 2015 - Is Model Fitting Necessary for Model-Based fMRI.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {6}
}

@inproceedings{Wilson2018,
  title = {Evolving Simple Programs for Playing Atari Games},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} on   - {{GECCO}} '18},
  author = {Wilson, Dennis G and {Cussat-Blanc}, Sylvain and Luga, Herv{\'e} and Miller, Julian F},
  year = {2018},
  pages = {229--236},
  publisher = {{ACM Press}},
  address = {{Kyoto, Japan}},
  doi = {10.1145/3205455.3205578},
  abstract = {Cartesian Genetic Programming (CGP) has previously shown capabilities in image processing tasks by evolving programs with a function set specialized for computer vision. A similar approach can be applied to Atari playing. Programs are evolved using mixed type CGP with a function set suited for matrix operations, including image processing, but allowing for controller behavior to emerge. While the programs are relatively small, many controllers are competitive with state of the art methods for the Atari benchmark set and require less training time. By evaluating the programs of the best evolved individuals, simple but e ective strategies can be found.},
  file = {/Users/qualia/Documents/Papers/Wilson et al. - 2018 - Evolving simple programs for playing atari games.pdf},
  isbn = {978-1-4503-5618-3},
  language = {en}
}

@article{Wiltschko2015,
  title = {Mapping {{Sub}}-{{Second Structure}} in {{Mouse Behavior}}},
  author = {Wiltschko, Alexander B. and Johnson, Matthew J. and Iurilli, Giuliano and Peterson, Ralph E. and Katon, Jesse M. and Pashkovski, Stan L. and Abraira, Victoria E. and Adams, Ryan P. and Datta, Sandeep Robert},
  year = {2015},
  month = dec,
  volume = {88},
  pages = {1121--1135},
  issn = {08966273},
  doi = {10.1016/j.neuron.2015.11.031},
  abstract = {Complex animal behaviors are likely built from simpler modules, but their systematic identification in mammals remains a significant challenge. Here we use depth imaging to show that 3D mouse pose dynamics are structured at the sub-second timescale. Computational modeling of these fast dynamics effectively describes mouse behavior as a series of reused and stereotyped modules with defined transition probabilities. We demonstrate this combined 3D imaging and machine learning method can be used to unmask potential strategies employed by the brain to adapt to the environment, to capture both predicted and previously hidden phenotypes caused by genetic or neural manipulations, and to systematically expose the global structure of behavior within an experiment. This work reveals that mouse body language is built from identifiable components and is organized in a predictable fashion; deciphering this language establishes an objective framework for characterizing the influence of environmental cues, genes and neural activity on behavior.},
  file = {/Users/qualia/Documents/Papers/Wiltschko et al. - 2015 - Mapping Sub-Second Structure in Mouse Behavior.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Wimmer2014,
  title = {Bump Attractor Dynamics in Prefrontal Cortex Explains Behavioral Precision in Spatial Working Memory},
  author = {Wimmer, Klaus and Nykamp, Duane Q and Constantinidis, Christos and Compte, Albert},
  year = {2014},
  month = mar,
  volume = {17},
  pages = {431--439},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3645},
  file = {/Users/qualia/Documents/Papers/2014 - Wimmer et al. - Bump attractor dynamics in prefrontal cortex explains behavioral precision in spatial working memory.pdf;/Users/qualia/Documents/Papers/Wimmer et al. - 2014 - Bump attractor dynamics in prefrontal cortex expla.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {3}
}

@article{Wimmer2016,
  title = {Transitions between {{Multiband Oscillatory Patterns Characterize Memory}}-{{Guided Perceptual Decisions}} in {{Prefrontal Circuits}}},
  author = {Wimmer, K. and Ramon, M. and Pasternak, T. and Compte, A.},
  year = {2016},
  month = jan,
  volume = {36},
  pages = {489--505},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3678-15.2016},
  file = {/Users/qualia/Documents/Papers/Wimmer et al. - 2016 - Transitions between Multiband Oscillatory Patterns.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {2}
}

@article{Winder2017,
  title = {Weak Correlations between Hemodynamic Signals and Ongoing Neural Activity during the Resting State},
  author = {Winder, Aaron T. and Echagarruga, Christina and Zhang, Qingguang and Drew, Patrick J.},
  year = {2017},
  month = dec,
  volume = {20},
  pages = {1761--1769},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-017-0007-y},
  file = {/Users/qualia/Documents/Papers/Winder et al. - 2017 - Weak correlations between hemodynamic signals and .pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {12}
}

@article{Winterer2003,
  title = {Cortical Signal-to-Noise Ratio: Insight into the Pathophysiology and Genetics of Schizophrenia},
  shorttitle = {Cortical Signal-to-Noise Ratio},
  author = {Winterer, Georg and Weinberger, Daniel R.},
  year = {2003},
  month = may,
  volume = {3},
  pages = {55--66},
  issn = {15662772},
  doi = {10.1016/S1566-2772(03)00019-7},
  abstract = {During the past two decades, it has been convincingly demonstrated that schizophrenic patients and subjects genetically at risk for schizophrenia show abnormalities of cortical and particularly prefrontal function. Depending on clinical state and task conditions, hypo- and hyperfrontality have been frequently described with functional neuroimaging and electrophysiological techniques; however, the underlying neurophysiological deficits remained largely obscure. There is now growing empirical evidence that cortical signal-to-noise ratio (SNR) during information processing is fundamentally disturbed and may be key to a further understanding of schizophrenic pathophysiology. The evidence comes from animal and human electrophysiological and neuroimaging investigations as well as neuropsychological and computational simulation studies. This research has also shown that dopamine signaling in prefrontal cortex is a critical factor in modulation of cortical SNR and in neurocognitive performance. Moreover, it was recently demonstrated that genetically determined variations in dopamine signaling, mediated by a functional polymorphism in the gene for the enzyme catechol-o-methyltransferase, has a significant impact on the cortical SNR, prefrontal information processing, and as a result, is a susceptibility gene for schizophrenia. This review summarizes the current state of research on the pathophysiology of schizophrenia with emphasis on cortical-SNR and the involvement of potentially relevant, molecular and genetic determinants of the cortical dopaminergic signaling.},
  file = {/Users/qualia/Documents/Papers/2003 - Winterer, Weinberger - Cortical signal-to-noise ratio Insight into the pathophysiology and genetics of schizophrenia.pdf},
  journal = {Clinical Neuroscience Research},
  language = {en},
  number = {1-2}
}

@article{Womelsdorf2014,
  title = {Dynamic Circuit Motifs Underlying Rhythmic Gain Control, Gating and Integration},
  author = {Womelsdorf, Thilo and Valiante, Taufik A and Sahin, Ned T and Miller, Kai J and Tiesinga, Paul},
  year = {2014},
  month = aug,
  volume = {17},
  pages = {1031--1039},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3764},
  file = {/Users/qualia/Documents/Papers/Womelsdorf et al. - 2014 - Dynamic circuit motifs underlying rhythmic gain co.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {8}
}

@article{Wong2006,
  title = {A {{Recurrent Network Mechanism}} of {{Time Integration}} in {{Perceptual Decisions}}},
  author = {Wong, K.-F.},
  year = {2006},
  month = jan,
  volume = {26},
  pages = {1314--1328},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.3733-05.2006},
  file = {/Users/qualia/Documents/Papers/2006 - Wong - A Recurrent Network Mechanism of Time Integration in Perceptual Decisions(2).pdf;/Users/qualia/Documents/Papers/2006 - Wong, Wang - A Recurrent Network Mechanism of Time Integration in Perceptual Decisions.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {4}
}

@article{Wongsarnpigoon2010,
  title = {Efficiency {{Analysis}} of {{Waveform Shape}} for {{Electrical Excitation}} of {{Nerve Fibers}}},
  author = {Wongsarnpigoon, Amorn and Woock, John P and Grill, Warren M},
  year = {2010},
  month = jun,
  volume = {18},
  pages = {319--328},
  issn = {1534-4320, 1558-0210},
  doi = {10.1109/TNSRE.2010.2047610},
  abstract = {Stimulation efficiency is an important consideration in the stimulation parameters of implantable neural stimulators. The objective of this study was to analyze the effects of waveform shape and duration on the charge, power, and energy efficiency of neural stimulation. Using a population model of mammalian axons and in vivo experiments on cat sciatic nerve, we analyzed the stimulation efficiency of four waveform shapes: square, rising exponential, decaying exponential, and rising ramp. No waveform was simultaneously energy-, charge-, and power-optimal, and differences in efficiency among waveform shapes varied with pulse width (PW) For short PWs ({$\leq$} 0.1 ms), square waveforms were no less energy-efficient than exponential waveforms, and the most charge-efficient shape was the ramp. For long PWs ({$\geq$}0.5 ms), the square was the least energy-efficient and charge-efficient shape, but across most PWs, the square was the most powerefficient shape. Rising exponentials provided no practical gains in efficiency over the other shapes, and our results refute previous claims that the rising exponential is the energy-optimal shape. An improved understanding of how stimulation parameters affect stimulation efficiency will help improve the design and programming of implantable stimulators to minimize tissue damage and extend battery life.},
  file = {/Users/qualia/Documents/Papers/2010 - Wongsarnpigoon, Woock, Grill - Efficiency analysis of waveform shape for electrical excitation of nerve fibers.pdf},
  journal = {IEEE Transactions on Neural Systems and Rehabilitation Engineering},
  language = {en},
  number = {3}
}

@article{Wongsarnpigoon2010a,
  title = {Energy-Efficient Waveform Shapes for Neural Stimulation Revealed with a Genetic Algorithm},
  author = {Wongsarnpigoon, Amorn and Grill, Warren M},
  year = {2010},
  month = aug,
  volume = {7},
  pages = {046009},
  issn = {1741-2560, 1741-2552},
  doi = {10.1088/1741-2560/7/4/046009},
  abstract = {The energy efficiency of stimulation is an important consideration for battery-powered implantable stimulators. We used a genetic algorithm (GA) to determine the energy-optimal waveform shape for neural stimulation. The GA was coupled to a computational model of extracellular stimulation of a mammalian myelinated axon. As the GA progressed, waveforms became increasingly energy-efficient and converged upon an energy-optimal shape. The results of the GA were consistent across several trials, and resulting waveforms resembled truncated Gaussian curves. When constrained to monophasic cathodic waveforms, the GA produced waveforms that were symmetric about the peak, which occurred approximately during the middle of the pulse. However, when the cathodic waveforms were coupled to rectangular chargebalancing anodic pulses, the location and sharpness of the peak varied with the duration and timing (i.e., before or after cathodic phase) of the anodic phase. In a model of a population of mammalian axons and in vivo experiments on cat sciatic nerve, the GA-optimized waveforms were more energy-efficient and charge-efficient than several conventional waveform shapes used in neural stimulation. If used in implantable neural stimulators, GA-optimized waveforms could prolong battery life, thereby reducing the frequency of recharge intervals, the volume of implanted pulse generators, and the costs and risks of battery-replacement surgeries.},
  file = {/Users/qualia/Documents/Papers/2011 - Wongsarnpigoon, Grill - Energy-efficient waveform shapes for neural stimulation revealed with genetic algorithm.pdf},
  journal = {Journal of Neural Engineering},
  language = {en},
  number = {4}
}

@article{Woodgate2017,
  title = {Continuous {{Radar Tracking Illustrates}} the {{Development}} of {{Multi}}-Destination {{Routes}} of {{Bumblebees}}},
  author = {Woodgate, Joseph L. and Makinson, James C. and Lim, Ka S. and Reynolds, Andrew M. and Chittka, Lars},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-17553-1},
  file = {/Users/qualia/Documents/Papers/Woodgate et al. - 2017 - Continuous Radar Tracking Illustrates the Developm.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Worden,
  title = {Anticipatory {{Biasing}} of {{Visuospatial Attention Indexed}} by {{Retinotopically Specific}} \textvisiblespace{}-{{Band Electroencephalography Increases}} over {{Occipital Cortex}}},
  author = {Worden, Michael S and Foxe, John J and Wang, Norman and Simpson, Gregory V},
  pages = {6},
  file = {/Users/qualia/Documents/Papers/2000 - Worden et al. - Anticipatory biasing of visuospatial attention indexed by retinotopically specific alpha-band electroencephalogra.pdf},
  language = {en}
}

@article{Wu2018,
  title = {Generalization Guides Human Exploration in Vast Decision Spaces},
  author = {Wu, Charley M. and Schulz, Eric and Speekenbrink, Maarten and Nelson, Jonathan D. and Meder, Bj{\"o}rn},
  year = {2018},
  month = dec,
  volume = {2},
  pages = {915--924},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0467-4},
  file = {/Users/qualia/Documents/Papers/Wu et al. - 2018 - Generalization guides human exploration in vast de.pdf},
  journal = {Nature Human Behaviour},
  language = {en},
  number = {12}
}

@article{Wullimann2011,
  title = {Basal {{Ganglia}}: {{Insights}} into {{Origins}} from {{Lamprey Brains}}},
  shorttitle = {Basal {{Ganglia}}},
  author = {Wullimann, Mario F.},
  year = {2011},
  month = jul,
  volume = {21},
  pages = {R497-R500},
  issn = {09609822},
  doi = {10.1016/j.cub.2011.05.052},
  file = {/Users/qualia/Documents/Papers/Wullimann - 2011 - Basal Ganglia Insights into Origins from Lamprey .pdf},
  journal = {Current Biology},
  language = {en},
  number = {13}
}

@article{Wybo2019,
  title = {Electrical {{Compartmentalization}} in {{Neurons}}},
  author = {Wybo, Willem A.M. and {Torben-Nielsen}, Benjamin and Nevian, Thomas and Gewaltig, Marc-Oliver},
  year = {2019},
  month = feb,
  volume = {26},
  pages = {1759-1773.e7},
  issn = {22111247},
  doi = {10.1016/j.celrep.2019.01.074},
  abstract = {The dendritic tree of neurons plays an important role in information processing in the brain. While it is thought that dendrites require independent subunits to perform most of their computations, it is still not understood how they compartmentalize into functional subunits. Here, we show how these subunits can be deduced from the properties of dendrites. We devised a formalism that links the dendritic arborization to an impedance-based tree graph and show how the topology of this graph reveals independent subunits. This analysis reveals that cooperativity between synapses decreases slowly with increasing electrical separation and thus that few independent subunits coexist. We nevertheless find that balanced inputs or shunting inhibition can modify this topology and increase the number and size of the subunits in a context-dependent manner. We also find that this dynamic recompartmentalization can enable branch-specific learning of stimulus features. Analysis of dendritic patch-clamp recording experiments confirmed our theoretical predictions.},
  file = {/Users/qualia/Documents/Papers/Wybo et al. - 2019 - Electrical Compartmentalization in Neurons.pdf},
  journal = {Cell Reports},
  language = {en},
  number = {7}
}

@article{Wynn2015,
  title = {{{EEG Findings}} of {{Reduced Neural Synchronization}} during {{Visual Integration}} in {{Schizophrenia}}},
  author = {Wynn, Jonathan K. and Roach, Brian J. and Lee, Junghee and Horan, William P. and Ford, Judith M. and Jimenez, Amy M. and Green, Michael F.},
  editor = {Kotz, Sonja},
  year = {2015},
  month = mar,
  volume = {10},
  pages = {e0119849},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0119849},
  file = {/Users/qualia/Documents/Papers/Wynn et al. - 2015 - EEG Findings of Reduced Neural Synchronization dur.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {3}
}

@article{Xiong2016,
  title = {Dynamic {{Memory Networks}} for {{Visual}} and {{Textual Question Answering}}},
  author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  year = {2016},
  month = mar,
  abstract = {Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the bAbI-10k text question-answering dataset without supporting fact supervision.},
  archivePrefix = {arXiv},
  eprint = {1603.01417},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Xiong et al. - 2016 - Dynamic Memory Networks for Visual and Textual Que.pdf},
  journal = {arXiv:1603.01417 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{Xu2012,
  title = {Regularized Hyperalignment of Multi-Set {{fMRI}} Data},
  booktitle = {2012 {{IEEE Statistical Signal Processing Workshop}} ({{SSP}})},
  author = {Xu, Hao and Lorbert, Alexander and Ramadge, Peter J. and Guntupalli, J. Swaroop and Haxby, James V.},
  year = {2012},
  month = aug,
  pages = {229--232},
  publisher = {{IEEE}},
  address = {{Ann Arbor, MI, USA}},
  doi = {10.1109/SSP.2012.6319668},
  abstract = {Inter-subject correspondence is an important aspect of multisubject fMRI studies. Recently, a new approach, called hyperalignment, has shown very promising results in fMRI functional alignment. Hyperalignment is based on Procrustean rotations and is connected, mathematically, to canonical correlation analysis. We review the core details of each approach, relate them through an SVD analysis, and indicate why they can yield different levels of performance. We then examine the effectiveness of regularization in mediating between the extremes of these methods. An inter-subject classification experiment based on functional aligned fMRI datasets illustrates the resulting improved performance.},
  file = {/Users/qualia/Documents/Papers/2012 - Xu et al. - Regularized Hyperalignment of Multi-set FMRI Data.pdf},
  isbn = {978-1-4673-0182-4 978-1-4673-0181-7},
  language = {en}
}

@article{Xu2013,
  title = {Reduction in {{LFP}} Cross-Frequency Coupling between Theta and Gamma Rhythms Associated with Impaired {{STP}} and {{LTP}} in a Rat Model of Brain Ischemia},
  author = {Xu, Xiaxia and Zheng, Chenguang and Zhang, Tao},
  year = {2013},
  volume = {7},
  issn = {1662-5188},
  doi = {10.3389/fncom.2013.00027},
  file = {/Users/qualia/Documents/Papers/Xu et al. - 2013 - Reduction in LFP cross-frequency coupling between .pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Xue2014,
  title = {Equalizing Excitation\textendash{}Inhibition Ratios across Visual Cortical Neurons},
  author = {Xue, Mingshan and Atallah, Bassam V. and Scanziani, Massimo},
  year = {2014},
  month = jun,
  volume = {511},
  pages = {596--600},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature13321},
  file = {/Users/qualia/Documents/Papers/Xue et al. - 2014 - Equalizing excitation–inhibition ratios across vis.pdf},
  journal = {Nature},
  language = {en},
  number = {7511}
}

@article{Yagodin1994,
  title = {Nonlinear Propagation of Agonist-Induced Cytoplasmic Calcium Waves in Single Astrocytes},
  author = {Yagodin, Sergey V. and Holtzclaw, Lynne and Sheppard, Carol A. and Russell, James T.},
  year = {1994},
  month = mar,
  volume = {25},
  pages = {265--280},
  issn = {0022-3034, 1097-4695},
  doi = {10.1002/neu.480250307},
  file = {/Users/qualia/Documents/Papers/Yagodin et al. - 1994 - Nonlinear propagation of agonist-induced cytoplasm.pdf},
  journal = {J. Neurobiol.},
  language = {en},
  number = {3}
}

@article{Yamazaki2007,
  title = {The Cerebellum as a Liquid State Machine},
  author = {Yamazaki, Tadashi and Tanaka, Shigeru},
  year = {2007},
  month = apr,
  volume = {20},
  pages = {290--297},
  issn = {08936080},
  doi = {10.1016/j.neunet.2007.04.004},
  abstract = {We examined closely the cerebellar circuit model that we have proposed previously. The model granular layer generates a finite but very long sequence of active neuron populations without recurrence, which is able to represent the passage of time. For all the possible binary patterns fed into mossy fibres, the circuit generates the same number of different sequences of active neuron populations. Model Purkinje cells that receive parallel fiber inputs from neurons in the granular layer learn to stop eliciting spikes at the timing instructed by the arrival of signals from the inferior olive. These functional roles of the granular layer and Purkinje cells are regarded as a liquid state generator and readout neurons, respectively. Thus, the cerebellum that has been considered to date as a biological counterpart of a perceptron is reinterpreted to be a liquid state machine that possesses powerful information processing capability more than a perceptron.},
  file = {/Users/qualia/Documents/Papers/2007 - Yamazaki, Tanaka - The cerebellum as a liquid state machine.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {3}
}

@article{Yamazaki2013,
  title = {Realtime Cerebellum: {{A}} Large-Scale Spiking Network Model of the Cerebellum That Runs in Realtime Using a Graphics Processing Unit},
  shorttitle = {Realtime Cerebellum},
  author = {Yamazaki, Tadashi and Igarashi, Jun},
  year = {2013},
  month = nov,
  volume = {47},
  pages = {103--111},
  issn = {08936080},
  doi = {10.1016/j.neunet.2013.01.019},
  abstract = {The cerebellum plays an essential role in adaptive motor control. Once we are able to build a cerebellar model that runs in realtime, which means that a computer simulation of 1 s in the simulated world completes within 1 s in the real world, the cerebellar model could be used as a realtime adaptive neural controller for physical hardware such as humanoid robots. In this paper, we introduce ``Realtime Cerebellum (RC)'', a new implementation of our large-scale spiking network model of the cerebellum, which was originally built to study cerebellar mechanisms for simultaneous gain and timing control and acted as a general-purpose supervised learning machine of spatiotemporal information known as reservoir computing, on a graphics processing unit (GPU). Owing to the massive parallel computing capability of a GPU, RC runs in realtime, while reproducing qualitatively the same simulation results of the Pavlovian delay eyeblink conditioning with the previous version. RC is adopted as a realtime adaptive controller of a humanoid robot, which is instructed to learn a proper timing to swing a bat to hit a flying ball online. These results suggest that RC provides a means to apply the computational power of the cerebellum as a versatile supervised learning machine towards engineering applications.},
  file = {/Users/qualia/Documents/Papers/2013 - Yamazaki, Igarashi - Realtime cerebellum A large-scale spiking network model of the cerebellum that runs in realtime using a grap.pdf;/Users/qualia/Documents/Papers/Yamazaki and Igarashi - 2013 - Realtime cerebellum A large-scale spiking network.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{Yan2014,
  title = {{{HD}}-{{CNN}}: {{Hierarchical Deep Convolutional Neural Network}} for {{Large Scale Visual Recognition}}},
  shorttitle = {{{HD}}-{{CNN}}},
  author = {Yan, Zhicheng and Zhang, Hao and Piramuthu, Robinson and Jagadeesh, Vignesh and DeCoste, Dennis and Di, Wei and Yu, Yizhou},
  year = {2014},
  month = oct,
  abstract = {In image classification, visual separability between different object categories is highly uneven, and some categories are more difficult to distinguish than others. Such difficult categories demand more dedicated classifiers. However, existing deep convolutional neural networks (CNN) are trained as flat N-way classifiers, and few efforts have been made to leverage the hierarchical structure of categories. In this paper, we introduce hierarchical deep CNNs (HD-CNNs) by embedding deep CNNs into a category hierarchy. An HD-CNN separates easy classes using a coarse category classifier while distinguishing difficult classes using fine category classifiers. During HD-CNN training, component-wise pretraining is followed by global finetuning with a multinomial logistic loss regularized by a coarse category consistency term. In addition, conditional executions of fine category classifiers and layer parameter compression make HD-CNNs scalable for large-scale visual recognition. We achieve state-of-the-art results on both CIFAR100 and large-scale ImageNet 1000-class benchmark datasets. In our experiments, we build up three different HD-CNNs and they lower the top-1 error of the standard CNNs by 2.65\%, 3.1\% and 1.1\%, respectively.},
  archivePrefix = {arXiv},
  eprint = {1410.0736},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2014 - Yan et al. - HD-CNN Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition.pdf;/Users/qualia/Documents/Papers/Yan et al. - 2014 - HD-CNN Hierarchical Deep Convolutional Neural Net.pdf},
  journal = {arXiv:1410.0736 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Yang,
  title = {Deep {{Neural Decision Trees}}},
  author = {Yang, Yongxin and Morillo, Irene Garcia and Hospedales, Timothy M},
  pages = {7},
  abstract = {Deep neural networks have been proven powerful at processing perceptual data, such as images and audio. However for tabular data, tree-based models are more popular. A nice property of tree-based models is their natural interpretability. In this work, we present Deep Neural Decision Trees (DNDT) \textendash{} tree models realised by neural networks. A DNDT is intrinsically interpretable, as it is a tree. Yet as it is also a neural network (NN), it can be easily implemented in NN toolkits, and trained with gradient descent rather than greedy splitting. We evaluate DNDT on several tabular datasets, verify its efficacy, and investigate similarities and differences between DNDT and vanilla decision trees. Interestingly, DNDT self-prunes at both split and feature-level.},
  file = {/Users/qualia/Documents/Papers/Yang et al. - Deep Neural Decision Trees.pdf},
  language = {en}
}

@article{Yang2018,
  title = {Mean {{Field Multi}}-{{Agent Reinforcement Learning}}},
  author = {Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  year = {2018},
  month = feb,
  abstract = {Existing multi-agent reinforcement learning methods are limited typically to a small number of agents. When the agent number increases largely, the learning becomes intractable due to the curse of the dimensionality and the exponential growth of agent interactions. In this paper, we present Mean Field Reinforcement Learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies. We develop practical mean field Q-learning and mean field Actor-Critic algorithms and analyze the convergence of the solution to Nash equilibrium. Experiments on Gaussian squeeze, Ising model, and battle games justify the learning effectiveness of our mean field approaches. In addition, we report the first result to solve the Ising model via model-free reinforcement learning methods.},
  archivePrefix = {arXiv},
  eprint = {1802.05438},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Yang et al. - 2018 - Mean Field Multi-Agent Reinforcement Learning.pdf},
  journal = {arXiv:1802.05438 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  language = {en},
  primaryClass = {cs}
}

@article{Yang2019,
  title = {Exploration via {{Flow}}-{{Based Intrinsic Rewards}}},
  author = {Yang, Hsuan-Kung and Chiang, Po-Han and Hong, Min-Fong and Lee, Chun-Yi},
  year = {2019},
  month = may,
  abstract = {Exploration bonuses derived from the novelty of observations in an environment have become a popular approach to motivate exploration for reinforcement learning (RL) agents in the past few years. Recent methods such as curiosity-driven exploration usually estimate the novelty of new observations by the prediction errors of their system dynamics models. In this paper, we introduce the concept of optical flow estimation from the field of computer vision to the RL domain and utilize the errors from optical flow estimation to evaluate the novelty of new observations. We introduce a flow-based intrinsic curiosity module (FICM) capable of learning the motion features and understanding the observations in a more comprehensive and efficient fashion. We evaluate our method and compare it with a number of baselines on several benchmark environments, including Atari games, Super Mario Bros., and ViZDoom. Our results show that the proposed method is superior to the baselines in certain environments, especially for those featuring sophisticated moving patterns or with high-dimensional observation spaces. We further analyze the hyper-parameters used in the training phase and discuss our insights into them.},
  archivePrefix = {arXiv},
  eprint = {1905.10071},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Yang et al. - 2019 - Exploration via Flow-Based Intrinsic Rewards.pdf},
  journal = {arXiv:1905.10071 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Yavuz2016,
  title = {{{GeNN}}: A Code Generation Framework for Accelerated Brain Simulations},
  shorttitle = {{{GeNN}}},
  author = {Yavuz, Esin and Turner, James and Nowotny, Thomas},
  year = {2016},
  month = may,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep18854},
  file = {/Users/qualia/Documents/Papers/Yavuz et al. - 2016 - GeNN a code generation framework for accelerated .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Ye2014,
  title = {Estimating the Biophysical Properties of Neurons with Intracellular Calcium Dynamics},
  author = {Ye, Jingxin and Rozdeba, Paul J. and Morone, Uriel I. and Daou, Arij and Abarbanel, Henry D. I.},
  year = {2014},
  month = jun,
  volume = {89},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.89.062714},
  file = {/Users/qualia/Documents/Papers/2014 - Ye, Rozdeba, Morone - Estimating the biophysical properties of neurons with intracellular calcium dynamics.pdf;/Users/qualia/Documents/Papers/Ye et al. - 2014 - Estimating the biophysical properties of neurons w.pdf},
  journal = {Physical Review E},
  language = {en},
  number = {6}
}

@article{Yeung1999,
  title = {Time {{Delay}} in the {{Kuramoto Model}} of {{Coupled Oscillators}}},
  author = {Yeung, M. K. Stephen and Strogatz, Steven H.},
  year = {1999},
  month = jan,
  volume = {82},
  pages = {648--651},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.82.648},
  file = {/Users/qualia/Documents/Papers/1999 - Yeung, Strogatz - Time Delay in the Kuramoto Model of Coupled Oscillators.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {3}
}

@article{Yi2015,
  title = {Spike-Frequency Adaptation of a Two-Compartment Neuron Modulated by Extracellular Electric Fields},
  author = {Yi, Guosheng and Wang, Jiang and Tsang, Kai-Ming and Wei, Xile and Deng, Bin and Han, Chunxiao},
  year = {2015},
  month = jun,
  volume = {109},
  pages = {287--306},
  issn = {0340-1200, 1432-0770},
  doi = {10.1007/s00422-014-0642-2},
  file = {/Users/qualia/Documents/Papers/Yi et al. - 2015 - Spike-frequency adaptation of a two-compartment ne.pdf},
  journal = {Biological Cybernetics},
  language = {en},
  number = {3}
}

@article{Yi2017,
  title = {Morphology Controls How Hippocampal {{CA1}} Pyramidal Neuron Responds to Uniform Electric Fields: A Biophysical Modeling Study},
  shorttitle = {Morphology Controls How Hippocampal {{CA1}} Pyramidal Neuron Responds to Uniform Electric Fields},
  author = {Yi, Guo-Sheng and Wang, Jiang and Deng, Bin and Wei, Xi-Le},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-03547-6},
  file = {/Users/qualia/Documents/Papers/Yi et al. - 2017 - Morphology controls how hippocampal CA1 pyramidal .pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{Yochum2018,
  title = {Reconstruction of Post-Synaptic Potentials by Reverse Modeling of Local Field Potentials},
  author = {Yochum, Maxime and Modolo, Julien and Benquet, Pascal and Wendling, Fabrice},
  year = {2018},
  month = sep,
  doi = {10.1101/346148},
  abstract = {Among electrophysiological signals, Local Field Potentials (LFPs) are extensively used to study brain activity, either in vivo or in vitro. LFPs are recorded with extracellular electrodes implanted in brain tissue. They reflect intermingled excitatory and inhibitory processes in neuronal assemblies. In cortical structures, LFPs mainly originate from the summation of post-synaptic potentials (PSPs), either excitatory (ePSPs) and inhibitory (iPSPs) generated at the level of pyramidal cells. The challenging issue, addressed in this paper, is to estimate, from a single extracellularly-recorded signal, both ePSP and iPSP components of the LFP. The proposed method is based on a model-based reverse engineering approach in which the measured LFP is fed into a physiologically-grounded neural mass model (mesoscopic level) in order to estimate the synaptic activity of a sub-population of pyramidal cells interacting with local GABAergic interneurons. The method was first validated using simulated LFPs for which excitatory and inhibitory components are known a priori and can thus serve as a ground truth. It was then evaluated on in vivo data (PTZ-induced seizures, rat; PTZ-induced excitability increase, mouse; epileptiform discharges, mouse) and on in clinico data (human seizures recorded with depth-EEG electrodes). Under these various conditions, results showed that the proposed reverse engineering method provides a reliable estimation of the average excitatory and inhibitory post-synaptic potentials at the origin of the measured LFPs. They also indicated that the method allows for monitoring of the excitation/inhibition ratio. The method has potential for multiple applications in neuroscience, typically when a time tracking of local excitability changes is required.},
  file = {/Users/qualia/Documents/Papers/Yochum et al. - 2018 - Reconstruction of post-synaptic potentials by reve.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Yoon2008,
  title = {Multivariate {{Pattern Analysis}} of {{Functional Magnetic Resonance Imaging Data Reveals Deficits}} in {{Distributed Representations}} in {{Schizophrenia}}},
  author = {Yoon, Jong H. and Tamir, Diana and Minzenberg, Michael J. and Ragland, J. Daniel and Ursu, Stefan and Carter, Cameron S.},
  year = {2008},
  month = dec,
  volume = {64},
  pages = {1035--1041},
  issn = {00063223},
  doi = {10.1016/j.biopsych.2008.07.025},
  abstract = {Background: Multivariate pattern analysis is an alternative method of analyzing functional magnetic resonance imaging (fMRI) data, which is capable of decoding distributed neural representations. We applied this method to test the hypothesis of the impairment in distributed representations in schizophrenia. We also compared the results of this method with traditional general linear model (GLM)-based univariate analysis.
Methods: Nineteen schizophrenia and 15 control subjects viewed two runs of stimuli\textemdash{} exemplars of faces, scenes, objects, and scrambled images. To verify engagement with stimuli, subjects completed a 1-back matching task. A multivoxel pattern classifier was trained to identify category-specific activity patterns on one run of fMRI data. Classification testing was conducted on the remaining run. Correlation of voxelwise activity across runs evaluated variance over time in activity patterns.
Results: Patients performed the task less accurately. This group difference was reflected in the pattern analysis results with diminished classification accuracy in patients compared with control subjects, 59\% and 72\%, respectively. In contrast, there was no group difference in GLM-based univariate measures. In both groups, classification accuracy was significantly correlated with behavioral measures. Both groups showed highly significant correlation between interrun correlations and classification accuracy.
Conclusions: Distributed representations of visual objects are impaired in schizophrenia. This impairment is correlated with diminished task performance, suggesting that decreased integrity of cortical activity patterns is reflected in impaired behavior. Comparisons with univariate results suggest greater sensitivity of pattern analysis in detecting group differences in neural activity and reduced likelihood of nonspecific factors driving these results.},
  file = {/Users/qualia/Documents/Papers/2008 - Yoon et al. - Multivariate pattern analysis of functional magnetic resonance imaging data reveals deficits in distributed represe.pdf},
  journal = {Biological Psychiatry},
  language = {en},
  number = {12}
}

@article{Yoon2018,
  title = {Control of Movement Vigor and Decision Making during Foraging},
  author = {Yoon, Tehrim and Geary, Robert B. and Ahmed, Alaa A. and Shadmehr, Reza},
  year = {2018},
  month = oct,
  volume = {115},
  pages = {E10476-E10485},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1812979115},
  abstract = {During foraging, animals decide how long to stay at a patch and harvest reward, and then, they move with certain vigor to another location. How does the brain decide when to leave, and how does it determine the speed of the ensuing movement? Here, we considered the possibility that both the decision-making and the motor control problems aimed to maximize a single normative utility: the sum of all rewards acquired minus all efforts expended divided by total time. This optimization could be achieved if the brain compared a local measure of utility with its history. To test the theory, we examined behavior of people as they gazed at images: they chose how long to look at the image (harvesting information) and then moved their eyes to another image, controlling saccade speed. We varied reward via image content and effort via image eccentricity, and then, we measured how these changes affected decision making (gaze duration) and motor control (saccade speed). After a history of low rewards, people increased gaze duration and decreased saccade speed. In anticipation of future effort, they lowered saccade speed and increased gaze duration. After a history of high effort, they elevated their saccade speed and increased gaze duration. Therefore, the theory presented a principled way with which the brain may control two aspects of behavior: movement speed and harvest duration. Our experiments confirmed many (but not all) of the predictions, suggesting that harvest duration and movement speed, fundamental aspects of behavior during foraging, may be governed by a shared principle of control.},
  file = {/Users/qualia/Documents/Papers/Yoon et al. - 2018 - Control of movement vigor and decision making duri.pdf},
  journal = {Proc Natl Acad Sci USA},
  language = {en},
  number = {44}
}

@article{Yoshimura2005,
  title = {Fine-Scale Specificity of Cortical Networks Depends on Inhibitory Cell Type and Connectivity},
  author = {Yoshimura, Yumiko and Callaway, Edward M},
  year = {2005},
  month = nov,
  volume = {8},
  pages = {1552--1559},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn1565},
  file = {/Users/qualia/Documents/Papers/2005 - Yoshimura, Callaway - Fine-scale specificity of cortical networks depends on inhibitory cell type and connectivity.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{Yosinski2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  month = nov,
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  archivePrefix = {arXiv},
  eprint = {1411.1792},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/2014 - Yosinski et al. - How transferable are features in deep neural networks.pdf;/Users/qualia/Documents/Papers/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf},
  journal = {arXiv:1411.1792 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{Young2014,
  title = {Differential Effects of Aging on Dendritic Spines in Visual Cortex and Prefrontal Cortex of the Rhesus Monkey},
  author = {Young, M.E. and Ohm, D.T. and Dumitriu, D. and Rapp, P.R. and Morrison, J.H.},
  year = {2014},
  month = aug,
  volume = {274},
  pages = {33--43},
  issn = {03064522},
  doi = {10.1016/j.neuroscience.2014.05.008},
  file = {/Users/qualia/Documents/Papers/Young et al. - 2014 - Differential effects of aging on dendritic spines .pdf},
  journal = {Neuroscience},
  language = {en}
}

@article{Yu,
  title = {Sequential Effects: {{Superstition}} or Rational Behavior?},
  author = {Yu, Angela J and Cohen, Jonathan D},
  pages = {8},
  abstract = {In a variety of behavioral tasks, subjects exhibit an automatic and apparently suboptimal sequential effect: they respond more rapidly and accurately to a stimulus if it reinforces a local pattern in stimulus history, such as a string of repetitions or alternations, compared to when it violates such a pattern. This is often the case even if the local trends arise by chance in the context of a randomized design, such that stimulus history has no real predictive power. In this work, we use a normative Bayesian framework to examine the hypothesis that such idiosyncrasies may reflect the inadvertent engagement of mechanisms critical for adapting to a changing environment. We show that prior belief in non-stationarity can induce experimentally observed sequential effects in an otherwise Bayes-optimal algorithm. The Bayesian algorithm is shown to be well approximated by linear-exponential filtering of past observations, a feature also apparent in the behavioral data. We derive an explicit relationship between the parameters and computations of the exact Bayesian algorithm and those of the approximate linear-exponential filter. Since the latter is equivalent to a leaky-integration process, a commonly used model of neuronal dynamics underlying perceptual decision-making and trial-to-trial dependencies, our model provides a principled account of why such dynamics are useful. We also show that parameter-tuning of the leaky-integration process is possible, using stochastic gradient descent based only on the noisy binary inputs. This is a proof of concept that not only can neurons implement near-optimal prediction based on standard neuronal dynamics, but that they can also learn to tune the processing parameters without explicitly representing probabilities.},
  file = {/Users/qualia/Documents/Papers/2009 - Yu, Cohen - Sequential effects Superstition or rational behavior.pdf},
  language = {en}
}

@article{Yu2009,
  title = {Gaussian-{{Process Factor Analysis}} for {{Low}}-{{Dimensional Single}}-{{Trial Analysis}} of {{Neural Population Activity}}},
  author = {Yu, Byron M. and Cunningham, John P. and Santhanam, Gopal and Ryu, Stephen I. and Shenoy, Krishna V. and Sahani, Maneesh},
  year = {2009},
  month = jul,
  volume = {102},
  pages = {614--635},
  issn = {0022-3077, 1522-1598},
  doi = {10.1152/jn.90941.2008},
  file = {/Users/qualia/Documents/Papers/2009 - Yu et al. - Gaussian-Process Factor Analysis for Low-Dimensional Single-Trial Analysis of Neural Population Activity (vol 102, pg.pdf},
  journal = {Journal of Neurophysiology},
  language = {en},
  number = {1}
}

@article{Yu2014,
  title = {Sparse {{Coding}} and {{Lateral Inhibition Arising}} from {{Balanced}} and {{Unbalanced Dendrodendritic Excitation}} and {{Inhibition}}},
  author = {Yu, Yuguo and Migliore, Michele and Hines, Michael L. and Shepherd, Gordon M.},
  year = {2014},
  month = oct,
  volume = {34},
  pages = {13701--13713},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1834-14.2014},
  file = {/Users/qualia/Documents/Papers/2014 - Yu et al. - Sparse Coding and Lateral Inhibition Arising from Balanced and Unbalanced Dendrodendritic Excitation and Inhibition.pdf;/Users/qualia/Documents/Papers/Yu et al. - 2014 - Sparse Coding and Lateral Inhibition Arising from .pdf},
  journal = {The Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{Yu2020,
  title = {Playing the Lottery with Rewards and Multiple Languages: Lottery Tickets in {{RL}} and {{NLP}}},
  shorttitle = {Playing the Lottery with Rewards and Multiple Languages},
  author = {Yu, Haonan and Edunov, Sergey and Tian, Yuandong and Morcos, Ari S.},
  year = {2020},
  month = feb,
  abstract = {The lottery ticket hypothesis proposes that over-parameterization of deep neural networks (DNNs) aids training by increasing the probability of a ``lucky'' sub-network initialization being present rather than by helping the optimization process (Frankle \& Carbin, 2019). Intriguingly, this phenomenon suggests that initialization strategies for DNNs can be improved substantially, but the lottery ticket hypothesis has only previously been tested in the context of supervised learning for natural image tasks. Here, we evaluate whether ``winning ticket'' initializations exist in two different domains: natural language processing (NLP) and reinforcement learning (RL). For NLP, we examined both recurrent LSTM models and large-scale Transformer models (Vaswani et al., 2017). For RL, we analyzed a number of discrete-action space tasks, including both classic control and pixel control. Consistent with work in supervised image classification, we confirm that winning ticket initializations generally outperform parameter-matched random initializations, even at extreme pruning rates for both NLP and RL. Notably, we are able to find winning ticket initializations for Transformers which enable models one-third the size to achieve nearly equivalent performance. Together, these results suggest that the lottery ticket hypothesis is not restricted to supervised learning of natural images, but rather represents a broader phenomenon in DNNs.},
  archivePrefix = {arXiv},
  eprint = {1906.02768},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Yu et al. - 2020 - Playing the lottery with rewards and multiple lang.pdf},
  journal = {arXiv:1906.02768 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Yu2020a,
  title = {Gradient {{Surgery}} for {{Multi}}-{{Task Learning}}},
  author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  year = {2020},
  month = jan,
  abstract = {While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multitask architectures for enhanced performance.},
  archivePrefix = {arXiv},
  eprint = {2001.06782},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Yu et al. - 2020 - Gradient Surgery for Multi-Task Learning.pdf},
  journal = {arXiv:2001.06782 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Yuan2016,
  title = {Theoretical {{Analysis}} of {{Transcranial Magneto}}-{{Acoustical Stimulation}} with {{Hodgkin}}-{{Huxley Neuron Model}}},
  author = {Yuan, Yi and Chen, Yudong and Li, Xiaoli},
  year = {2016},
  month = apr,
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00035},
  file = {/Users/qualia/Documents/Papers/Yuan et al. - 2016 - Theoretical Analysis of Transcranial Magneto-Acous 2.pdf;/Users/qualia/Documents/Papers/Yuan et al. - 2016 - Theoretical Analysis of Transcranial Magneto-Acous.pdf},
  journal = {Frontiers in Computational Neuroscience},
  language = {en}
}

@article{Yuval-Greenberg2008,
  title = {Transient {{Induced Gamma}}-{{Band Response}} in {{EEG}} as a {{Manifestation}} of {{Miniature Saccades}}},
  author = {{Yuval-Greenberg}, Shlomit and Tomer, Orr and Keren, Alon S. and Nelken, Israel and Deouell, Leon Y.},
  year = {2008},
  month = may,
  volume = {58},
  pages = {429--441},
  issn = {08966273},
  doi = {10.1016/j.neuron.2008.03.027},
  abstract = {The induced gamma-band EEG response (iGBR) recorded on the scalp is widely assumed to reflect synchronous neural oscillation associated with object representation, attention, memory, and consciousness. The most commonly reported EEG iGBR is a broadband transient increase in power at the gamma range \$200\textendash{}300 ms following stimulus onset. A conspicuous feature of this iGBR is the trial-to-trial poststimulus latency variability, which has been insufficiently addressed. Here, we show, using singletrial analysis of concomitant EEG and eye tracking, that this iGBR is tightly time locked to the onset of involuntary miniature eye movements and reflects a saccadic ``spike potential.'' The time course of the iGBR is related to an increase in the rate of saccades following a period of poststimulus saccadic inhibition. Thus, whereas neuronal gamma-band oscillations were shown conclusively with other methods, the broadband transient iGBR recorded by scalp EEG reflects properties of miniature saccade dynamics rather than neuronal oscillations.},
  file = {/Users/qualia/Documents/Papers/2008 - Yuval-Greenberg et al. - Transient Induced Gamma-Band Response in EEG as a Manifestation of Miniature Saccades.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Zador2019,
  title = {A Critique of Pure Learning and What Artificial Neural Networks Can Learn from Animal Brains},
  author = {Zador, Anthony M.},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {3770},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-11786-6},
  file = {/Users/qualia/Documents/Papers/Zador - 2019 - A critique of pure learning and what artificial ne.pdf},
  journal = {Nat Commun},
  language = {en},
  number = {1}
}

@article{Zandt2014,
  title = {A Neural Mass Model Based on Single Cell Dynamics to Model Pathophysiology},
  author = {Zandt, Bas-Jan and Visser, Sid and {van Putten}, Michel J. A. M. and {ten Haken}, Bennie},
  year = {2014},
  month = dec,
  volume = {37},
  pages = {549--568},
  issn = {0929-5313, 1573-6873},
  doi = {10.1007/s10827-014-0517-5},
  abstract = {Neural mass models are successful in modeling brain rhythms as observed in macroscopic measurements such as the electroencephalogram (EEG). While the synaptic current is explicitly modeled in current models, the single cell electrophysiology is not taken into account. To allow for investigations of the effects of channel pathologies, channel blockers and ion concentrations on macroscopic activity, we formulate neural mass equations explicitly incorporating the single cell dynamics by using a bottom-up approach. The mean and variance of the firing rate and synaptic input distributions are modeled. The firing rate curve (F(I)-curve) is used as link between the single cell and macroscopic dynamics. We show that this model accurately reproduces the behavior of two populations of synaptically connected Hodgkin-Huxley neurons, also in non-steady state.},
  file = {/Users/qualia/Documents/Papers/2014 - Zandt et al. - A neural mass model based on single cell dynamics to model pathophysiology.pdf;/Users/qualia/Documents/Papers/Zandt et al. - 2014 - A neural mass model based on single cell dynamics .pdf},
  journal = {Journal of Computational Neuroscience},
  language = {en},
  number = {3}
}

@article{Zanin2012,
  title = {Permutation {{Entropy}} and {{Its Main Biomedical}} and {{Econophysics Applications}}: {{A Review}}},
  shorttitle = {Permutation {{Entropy}} and {{Its Main Biomedical}} and {{Econophysics Applications}}},
  author = {Zanin, Massimiliano and Zunino, Luciano and Rosso, Osvaldo A. and Papo, David},
  year = {2012},
  month = aug,
  volume = {14},
  pages = {1553--1577},
  issn = {1099-4300},
  doi = {10.3390/e14081553},
  file = {/Users/qualia/Documents/Papers/2012 - Zanin et al. - Permutation entropy and its main biomedical and econophysics applications A review.pdf},
  journal = {Entropy},
  language = {en},
  number = {8}
}

@article{Zanos2015,
  title = {A {{Sensorimotor Role}} for {{Traveling Waves}} in {{Primate Visual Cortex}}},
  author = {Zanos, Theodoros P. and Mineault, Patrick J. and Nasiotis, Konstantinos T. and Guitton, Daniel and Pack, Christopher C.},
  year = {2015},
  month = feb,
  volume = {85},
  pages = {615--627},
  issn = {08966273},
  doi = {10.1016/j.neuron.2014.12.043},
  abstract = {Traveling waves of neural activity are frequently observed to occur in concert with the presentation of a sensory stimulus or the execution of a movement. Although such waves have been studied for decades, little is known about their function. Here we show that traveling waves in the primate extrastriate visual cortex provide a means of integrating sensory and motor signals. Specifically, we describe a traveling wave of local field potential (LFP) activity in cortical area V4 of macaque monkeys that is triggered by the execution of saccadic eye movements. These waves sweep across the V4 retinotopic map, following a consistent path from the foveal to the peripheral representations of space; their amplitudes correlate with the direction and size of each saccade. Moreover, these waves are associated with a reorganization of the postsaccadic neuronal firing patterns, which follow a similar retinotopic progression, potentially prioritizing the processing of behaviorally relevant stimuli.},
  file = {/Users/qualia/Documents/Papers/Zanos et al. - 2015 - A Sensorimotor Role for Traveling Waves in Primate.pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{Zanto2013,
  title = {Age-{{Related Changes}} in {{Expectation}}-{{Based Modulation}} of {{Motion Detectability}}},
  author = {Zanto, Theodore P. and Sekuler, Robert and Dube, Chad and Gazzaley, Adam},
  editor = {Rypma, Bart},
  year = {2013},
  month = aug,
  volume = {8},
  pages = {e69766},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0069766},
  abstract = {Expecting motion in some particular direction biases sensitivity to that direction, which speeds detection of motion. However, the neural processes underlying this effect remain underexplored, especially in the context of normal aging. To address this, we examined younger and older adults' performance in a motion detection task. In separate conditions, the probability was either 50\% or 100\% that a field of dots would move coherently in the direction a participant expected (either vertically or horizontally). Expectation and aging effects were assessed via response times (RT) to detect motion and electroencephalography (EEG). In both age groups, RTs were fastest when motion was similar to the expected direction of motion. RT tuning curves exhibited a characteristic U-shape such that detection time increased with an increasing deviation from the participant's expected direction. Strikingly, EEG results showed an analogous, hyperbolic curve for N1 amplitude, reflecting neural biasing. Though the form of behavioral and EEG curves did not vary with age, older adults displayed a clear decline in the speed of detection and a corresponding reduction in EEG N1 amplitude when horizontal (but not vertical) motion was expected. Our results suggest that expectation-based detection ability varies with age and, for older adults, also with axis of motion.},
  file = {/Users/qualia/Documents/Papers/2013 - Zanto et al. - Age-related changes in expectation-based modulation of motion detectability.pdf;/Users/qualia/Documents/Papers/Zanto et al. - 2013 - Age-Related Changes in Expectation-Based Modulatio.pdf},
  journal = {PLoS ONE},
  language = {en},
  number = {8}
}

@article{Zavala2017,
  title = {Human Subthalamic Nucleus Activity during Non-Motor Decision Making},
  author = {Zavala, Baltazar A and Jang, Anthony I and Zaghloul, Kareem A},
  year = {2017},
  month = dec,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.31007},
  abstract = {Recent studies have implicated the subthalamic nucleus (STN) in decisions that involve 7 inhibiting movements. Many of the decisions that we make in our daily lives, however, do not 8 involve any motor actions. We studied non-motor decision making by recording intraoperative STN 9 and prefrontal cortex (PFC) electrophysiology as participants perform a novel task that required 10 them to decide whether to encode items into working memory. During all encoding trials, beta 11 band (15-30 Hz) activity decreased in the STN and PFC, and this decrease was progressively 12 enhanced as more items were stored into working memory. Crucially, the STN and lateral PFC beta 13 decrease was significantly attenuated during the trials in which participants were instructed not to 14 encode the presented stimulus. These changes were associated with increase lateral PFC-STN 15 coherence and altered STN neuronal spiking. Our results shed light on why states of altered basal 16 ganglia activity disrupt both motor function and cognition.},
  file = {/Users/qualia/Documents/Papers/Zavala et al. - 2017 - Human subthalamic nucleus activity during non-moto.pdf},
  journal = {eLife},
  language = {en}
}

@article{Zenke,
  title = {Improved Multitask Learning through Synaptic Intelligence},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  pages = {8},
  abstract = {Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.},
  file = {/Users/qualia/Documents/Papers/Zenke et al. - Improved multitask learning through synaptic intel.pdf},
  language = {en}
}

@article{Zenke2018,
  title = {{{SuperSpike}}: {{Supervised Learning}} in {{Multilayer Spiking Neural Networks}}},
  shorttitle = {{{SuperSpike}}},
  author = {Zenke, Friedemann and Ganguli, Surya},
  year = {2018},
  month = jun,
  volume = {30},
  pages = {1514--1541},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01086},
  abstract = {A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in-vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in-silico. Here we revisit the problem of supervised learning in temporally coding multi-layer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three factor learning rule capable of training multi-layer networks of deterministic integrateand-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike-time patterns.},
  file = {/Users/qualia/Documents/Papers/Zenke and Ganguli - 2018 - SuperSpike Supervised Learning in Multilayer Spik.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {6}
}

@article{Zhang,
  title = {A {{Study}} on {{Overfitting}} in {{Deep Reinforcement Learning}}},
  author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  pages = {25},
  abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen ``robustly'': commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  file = {/Users/qualia/Documents/Papers/Zhang et al. - A Study on Overfitting in Deep Reinforcement Learn.pdf},
  language = {en}
}

@article{Zhang2005,
  title = {Fixed Points and Stability in Differential Equations with Variable Delays},
  author = {Zhang, Bo},
  year = {2005},
  month = nov,
  volume = {63},
  pages = {e233-e242},
  issn = {0362546X},
  doi = {10.1016/j.na.2005.02.081},
  abstract = {In this paper we consider a linear scalar differential equation with variable delays and give conditions to ensure that the zero solution is asymptotically stable by means of fixed point theory. These conditions do not require the boundedness of delays, nor do they ask for a fixed sign on the coefficient functions. An asymptotic stability theorem with a necessary and sufficient condition is proved.},
  file = {/Users/qualia/Documents/Papers/2008 - Jin, Luo - Fixed points and stability in neutral differential equations with variable delays.pdf},
  journal = {Nonlinear Analysis: Theory, Methods \& Applications},
  language = {en},
  number = {5-7}
}

@article{Zhang2017,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and {Lopez-Paz}, David},
  year = {2017},
  month = oct,
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  archivePrefix = {arXiv},
  eprint = {1710.09412},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Zhang et al. - 2017 - mixup Beyond Empirical Risk Minimization.pdf},
  journal = {arXiv:1710.09412 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Zhang2017a,
  title = {Theta and Alpha Oscillations Are Traveling Waves in the Human Neocortex},
  author = {Zhang, Honghui and Watrous, Andrew J. and Patel, Ansh and Jacobs, Joshua},
  year = {2017},
  month = dec,
  doi = {10.1101/218198},
  abstract = {Human cognition requires the coordination of neural activity across widespread brain networks. Here we describe a new mechanism for large-scale coordination in the human brain: traveling waves of theta and alpha oscillations. Examining direct brain recordings from neurosurgical patients performing a memory task, we found that contiguous clusters of cortex in individual patients showed oscillations at specific frequencies in the range of 2 to 15 Hz. These clusters displayed spatial phase gradients, indicating that individual oscillation cycles moved across the cortex at {$\sim$}0.25\textendash{}0.75 m/s. We found that traveling waves were relevant behaviorally because their propagation correlated with task events and was more consistent during good performance. Traveling waves showed a correlation between propagation speed and temporal frequency, which suggests that they propagate across the cortex following principles of phasecoupled oscillatory networks. By demonstrating that theta and alpha traveling waves are widespread and behaviorally relevant, our results suggest a broad role for brain oscillations in supporting cortical connectivity by organizing neural activity across space and time.},
  file = {/Users/qualia/Documents/Papers/Zhang et al. - 2017 - Theta and alpha oscillations are traveling waves i.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{Zhang2018,
  title = {Theta and {{Alpha Oscillations Are Traveling Waves}} in the {{Human Neocortex}}},
  author = {Zhang, Honghui and Watrous, Andrew J. and Patel, Ansh and Jacobs, Joshua},
  year = {2018},
  month = jun,
  volume = {98},
  pages = {1269-1281.e4},
  issn = {08966273},
  doi = {10.1016/j.neuron.2018.05.019},
  abstract = {Human cognition requires the coordination of neural activity across widespread brain networks. Here, we describe a new mechanism for large-scale coordination in the human brain: traveling waves of theta and alpha oscillations. Examining direct brain recordings from neurosurgical patients performing a memory task, we found contiguous clusters of cortex in individual patients with oscillations at specific frequencies within 2 to 15 Hz. These oscillatory clusters displayed spatial phase gradients, indicating that they formed traveling waves that propagated at \$0.25\textendash{}0.75 m/s. Traveling waves were relevant behaviorally because their propagation correlated with task events and was more consistent when subjects performed the task well. Human traveling theta and alpha waves can be modeled by a network of coupled oscillators because the direction of wave propagation correlated with the spatial orientation of local frequency gradients. Our findings suggest that oscillations support brain connectivity by organizing neural processes across space and time.},
  file = {/Users/qualia/Documents/Papers/Zhang et al. - 2018 - Theta and Alpha Oscillations Are Traveling Waves i.pdf},
  journal = {Neuron},
  language = {en},
  number = {6}
}

@article{Zhang2018a,
  title = {A {{Dissection}} of {{Overfitting}} and {{Generalization}} in {{Continuous Reinforcement Learning}}},
  author = {Zhang, Amy and Ballas, Nicolas and Pineau, Joelle},
  year = {2018},
  month = jun,
  abstract = {The risks and perils of overfitting in machine learning are well known. However most of the treatment of this, including diagnostic tools and remedies, was developed for the supervised learning case. In this work, we aim to offer new perspectives on the characterization and prevention of overfitting in deep Reinforcement Learning (RL) methods, with a particular focus on continuous domains. We examine several aspects, such as how to define and diagnose overfitting in MDPs, and how to reduce risks by injecting sufficient training diversity. This work complements recent findings on the brittleness of deep RL methods and offers practical observations for RL researchers and practitioners.},
  archivePrefix = {arXiv},
  eprint = {1806.07937},
  eprinttype = {arxiv},
  file = {/Users/qualia/Documents/Papers/Zhang et al. - 2018 - A Dissection of Overfitting and Generalization in .pdf},
  journal = {arXiv:1806.07937 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Zhanga,
  title = {Theory of {{Deep Learning III}}: {{Generalization Properties}} of {{SGD}}},
  author = {Zhang, Chiyuan and Liao, Qianli and Rakhlin, Alexander and Miranda, Brando and Golowich, Noah and Poggio, Tomaso},
  pages = {38},
  file = {/Users/qualia/Documents/Papers/Zhang et al. - Theory of Deep Learning III Generalization Proper.pdf},
  language = {en}
}

@article{Zhangb,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  pages = {15},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
  file = {/Users/qualia/Documents/Papers/Zhang et al. - Understanding deep learning requires rethinking ge.pdf},
  language = {en}
}

@article{Zhao2017,
  title = {Variational {{Latent Gaussian Process}} for {{Recovering Single}}-{{Trial Dynamics}} from {{Population Spike Trains}}},
  author = {Zhao, Yuan and Park, Il Memming},
  year = {2017},
  month = may,
  volume = {29},
  pages = {1293--1316},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00953},
  abstract = {A small number of common factors often explain most of the interdependence among simultaneously recorded neurons, a signature of underlying low-dimensional dynamics. We posit that simple neural coding and computation manifest as low-dimensional nonlinear dynamics implemented redundantly within a large population of neurons. Recovering the latent dynamics from observations can offer a deeper understanding of neural computation. We improve upon previously-proposed methods for recovering latent dynamics, which assume either an inappropriate observation model or linear dynamics. We propose a practical and efficient inference method for a generative model with explicit point process observations and an assumption of smooth nonlinear dynamics. We validate our method on both simulated data and population recording from primary visual cortex.},
  file = {/Users/qualia/Documents/Papers/Zhao and Park - 2017 - Variational Latent Gaussian Process for Recovering.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {5}
}

@article{Zhao2019,
  title = {Inception of Memories That Guide Vocal Learning in the Songbird},
  author = {Zhao, Wenchan and {Garcia-Oscos}, Francisco and Dinh, Daniel and Roberts, Todd F.},
  year = {2019},
  month = oct,
  volume = {366},
  pages = {83--89},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaw4226},
  abstract = {Animals learn many complex behaviors by emulating the behavior of more experienced individuals. This essential, yet still poorly understood, form of learning relies on the ability to encode lasting memories of observed behaviors. We identified a vocal-motor pathway in the zebra finch where memories that guide learning of song-element durations can be implanted. Activation of synapses in this pathway seeds memories that guide learning of song-element duration and can override learning from social interactions with other individuals. Genetic lesions of this circuit after memory formation, however, do not disrupt subsequent song imitation, which suggests that these memories are stored at downstream synapses. Thus, activity at these sensorimotor synapses can bypass learning from auditory and social experience and embed memories that guide learning of song timing.},
  file = {/Users/qualia/Documents/Papers/Zhao et al. - 2019 - Inception of memories that guide vocal learning in.pdf},
  journal = {Science},
  language = {en},
  number = {6461}
}

@article{Zhou2014,
  title = {Scaling down of Balanced Excitation and Inhibition by Active Behavioral States in Auditory Cortex},
  author = {Zhou, Mu and Liang, Feixue and Xiong, Xiaorui R and Li, Lu and Li, Haifu and Xiao, Zhongju and Tao, Huizhong W and Zhang, Li I},
  year = {2014},
  month = jun,
  volume = {17},
  pages = {841--850},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3701},
  file = {/Users/qualia/Documents/Papers/2014 - Zhou et al. - Scaling down of balanced excitation and inhibition by active behavioral states in auditory cortex.pdf;/Users/qualia/Documents/Papers/Zhou et al. - 2014 - Scaling down of balanced excitation and inhibition.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {6}
}

@article{Zhou2015,
  title = {Establishing a {{Statistical Link}} between {{Network Oscillations}} and {{Neural Synchrony}}},
  author = {Zhou, Pengcheng and Burton, Shawn D. and Snyder, Adam C. and Smith, Matthew A. and Urban, Nathaniel N. and Kass, Robert E.},
  editor = {Sporns, Olaf},
  year = {2015},
  month = oct,
  volume = {11},
  pages = {e1004549},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004549},
  abstract = {Pairs of active neurons frequently fire action potentials or ``spikes'' nearly synchronously (i.e., within 5 ms of each other). This spike synchrony may occur by chance, based solely on the neurons' fluctuating firing patterns, or it may occur too frequently to be explicable by chance alone. When spike synchrony above chances levels is present, it may subserve computation for a specific cognitive process, or it could be an irrelevant byproduct of such computation. Either way, spike synchrony is a feature of neural data that should be explained. A point process regression framework has been developed previously for this purpose, using generalized linear models (GLMs). In this framework, the observed number of synchronous spikes is compared to the number predicted by chance under varying assumptions about the factors that affect each of the individual neuron's firing-rate functions. An important possible source of spike synchrony is network-wide oscillations, which may provide an essential mechanism of network information flow. To establish the statistical link between spike synchrony and network-wide oscillations, we have integrated oscillatory field potentials into our point process regression framework. We first extended a previouslypublished model of spike-field association and showed that we could recover phase relationships between oscillatory field potentials and firing rates. We then used this new framework to demonstrate the statistical relationship between oscillatory field potentials and spike synchrony in: 1) simulated neurons, 2) in vitro recordings of hippocampal CA1 pyramidal cells, and 3) in vivo recordings of neocortical V4 neurons. Our results provide a rigorous method for establishing a statistical link between network oscillations and neural synchrony.},
  file = {/Users/qualia/Documents/Papers/Zhou et al. - 2015 - Establishing a Statistical Link between Network Os.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {10}
}

@article{zotero-2129,
  title = {Continuity {{Debate}}},
  pages = {22},
  file = {/Users/qualia/Documents/Papers/2000 - Unknown - No Title.pdf},
  language = {en}
}

@misc{zotero-2170,
  title = {1929 - {{Hans}} - {{Uber}} Das Elektrenkephalogramm Des Menshen.Pdf},
  file = {/Users/qualia/Documents/Papers/1929 - Hans - Uber das elektrenkephalogramm des menshen.pdf}
}

@misc{zotero-2171,
  title = {1961 - {{Surwillo}} - {{Frequency}} of the `{{Alpha}}' {{Rhythm}}, {{Reaction Time}} and {{Age}}.Pdf},
  file = {/Users/qualia/Documents/Papers/1961 - Surwillo - Frequency of the ‘Alpha’ Rhythm, Reaction Time and Age.pdf}
}

@misc{zotero-2172,
  title = {1955 - {{Neyman}} - {{The}} Problem of Inductive Inference.Pdf},
  file = {/Users/qualia/Documents/Papers/1955 - Neyman - The problem of inductive inference.pdf}
}

@misc{zotero-2173,
  title = {1983 - {{Chan}}, {{Golub}}, {{LeVeque}} - {{Algorithms}} for {{Computing}} the {{Sample Variance Analysis}} and {{Recommendations}}.Pdf},
  file = {/Users/qualia/Documents/Papers/1983 - Chan, Golub, LeVeque - Algorithms for Computing the Sample Variance Analysis and Recommendations.pdf}
}

@misc{zotero-2174,
  title = {1984 - {{Hindmarsh}}, {{Rose}} - {{A Model}} of {{Neuronal Bursting Using Three Coupled First Order Differential Equations}}.Pdf},
  file = {/Users/qualia/Documents/Papers/1984 - Hindmarsh, Rose - A Model of Neuronal Bursting Using Three Coupled First Order Differential Equations.pdf}
}

@misc{zotero-2175,
  title = {1987 - {{Reiter}} - {{Nonmonotonic}} Reasoning.Pdf},
  file = {/Users/qualia/Documents/Papers/1987 - Reiter - Nonmonotonic reasoning.pdf}
}

@misc{zotero-2176,
  title = {1988 - {{Intended}}, {{Minsky}} - {{No Harm Intended}}.Pdf},
  file = {/Users/qualia/Documents/Papers/1988 - Intended, Minsky - No Harm Intended.pdf}
}

@misc{zotero-2180,
  title = {1989 - {{LeCun}} et al. - {{Backpropagation}} Applied to Handwritten Zip Code Recognition.Pdf},
  file = {/Users/qualia/Documents/Papers/1989 - LeCun et al. - Backpropagation applied to handwritten zip code recognition.pdf}
}

@misc{zotero-2182,
  title = {1992 - {{Heeger}} - {{Normalization}} of Cell Responses in Cat Striate Cortex.Pdf},
  file = {/Users/qualia/Documents/Papers/1992 - Heeger - Normalization of cell responses in cat striate cortex.pdf}
}

@misc{zotero-2183,
  title = {1992 - {{Williams}} - {{Simple}} Statistical Gradient-Following Algorithmns for Connectionist Reinforcement Learning.Pdf},
  file = {/Users/qualia/Documents/Papers/1992 - Williams - Simple statistical gradient-following algorithmns for connectionist reinforcement learning.pdf}
}

@misc{zotero-2184,
  title = {1996 - {{Mainen}}, {{Sejnowski}} - {{Influence}} of Dendritic Structure on Firing Pattern in Model Neocortical Neurons.Pdf},
  file = {/Users/qualia/Documents/Papers/1996 - Mainen, Sejnowski - Influence of dendritic structure on firing pattern in model neocortical neurons.pdf}
}

@article{zotero-2832,
  title = {Detailed Dendritic Excitatory/Inhibitory Balance through Heterosynaptic Spike-Timing-Dependent Plasticity},
  pages = {30},
  file = {/Users/qualia/Documents/Papers/2015 - Hiratani, Fukai - Detailed dendritic excitatory inhibitory balance through heterosynaptic spike-timing-dependent plasticity.pdf;/Users/qualia/Documents/Papers/Detailed dendritic excitatoryinhibitory balance t.pdf},
  language = {en}
}

@misc{zotero-3828,
  title = {[{{No}} Title Found]},
  file = {/Users/qualia/Documents/Papers/[No title found].pdf},
  language = {en}
}

@misc{zotero-4225,
  title = {[{{No}} Title Found]},
  file = {/Users/qualia/Documents/Papers/[No title found].pdf},
  language = {en}
}

@article{Zylberberg2017,
  title = {Counterfactual Reasoning Underlies the Learning of Priors in Decision Making},
  author = {Zylberberg, Ariel and Wolpert, Daniel M and Shadlen, Michael N},
  year = {2017},
  month = nov,
  doi = {10.1101/227421},
  abstract = {Accurate decisions require knowledge of prior probabilities (e.g., prevalence or base rate) but it is unclear how prior probability is learned in the absence of a teacher. We hypothesized that humans could learn base rates from experience making decisions, even without feedback. Participants made difficult decisions about the direction of dynamic random dot motion. For each block of 15-42 trials, the base rate favored left or right by a different amount. Participants were not informed of the base rate, yet they gradually biased their choices and thereby increased accuracy and confidence in their decisions. They achieved this by updating knowledge of base rate after each decision, using a counterfactual representation of confidence that simulates a neutral prior. The strategy is consistent with Bayesian updating of belief and suggests that humans represent both true confidence, that incorporates the evolving belief of the prior, and counterfactual confidence that discounts the prior.},
  file = {/Users/qualia/Documents/Papers/Zylberberg et al. - 2017 - Counterfactual reasoning underlies the learning of.pdf},
  journal = {bioRxiv},
  language = {en}
}


